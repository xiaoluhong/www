<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first</title>
      <link href="/rancher/waiting-for-node-to-register-either-cluster-is-not-ready-for-registering-or-etcd-and-controlplane-node-have-to-be-registered-first/"/>
      <url>/rancher/waiting-for-node-to-register-either-cluster-is-not-ready-for-registering-or-etcd-and-controlplane-node-have-to-be-registered-first/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/waiting-for-node-to-register-either-cluster-is-not-ready-for-registering-or-etcd-and-controlplane-node-have-to-be-registered-first/" target="_blank" title="https://www.xtplayer.cn/rancher/waiting-for-node-to-register-either-cluster-is-not-ready-for-registering-or-etcd-and-controlplane-node-have-to-be-registered-first/">https://www.xtplayer.cn/rancher/waiting-for-node-to-register-either-cluster-is-not-ready-for-registering-or-etcd-and-controlplane-node-have-to-be-registered-first/</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">INFO: Environment: CATTLE_ADDRESS=10.1xx.xx.xx CATTLE_AGENT_CONNECT=<span class="literal">true</span> CATTLE_CA_CHECKSUM=99e6ccda7c91855xxxxxxxxx4f760c0278713b95b30ab0616b66df1a CATTLE_CLUSTER=<span class="literal">false</span> CATTLE_INTERNAL_ADDRESS= CATTLE_K8S_MANAGED=<span class="literal">true</span> CATTLE_NODE_NAME=cncxxxx060vl CATTLE_SERVER=https://rancher.xxxx.com</span><br><span class="line">INFO: Using resolv.conf: nameserver 10.1xx.xx.xx nameserver 10.1xx.xx.xx  search lnx.fxxxxx fmcxxxxx.cn</span><br><span class="line">INFO: https://rancher.xxxx.com/ping is accessible</span><br><span class="line">INFO: rancher.xxxx.com resolves to 10.1xx.xx.xx</span><br><span class="line">INFO: Value from https://rancher.xxxx.com/v3/settings/cacerts is an x509 certificate</span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Rancher agent version v2.4.8 is starting&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Listening on /tmp/log.sock&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Option customConfig=map[address:10.1xx.xx.xx internalAddress: label:map[] roles:[] taints:[]]&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Option etcd=false&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Option controlPlane=false&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Option worker=false&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Option requestedHostname=cncxxxx060vl&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Connecting to wss://rancher.xxxx.com/v3/connect with token ks5rgcxxxxxxxpkb7nd2zj4qsk6snclcxqnn&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Connecting to proxy&quot;</span> url=<span class="string">&quot;wss://rancher.xxxx.com/v3/connect&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:47Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:49Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:51Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:53Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:55Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:57Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:10:59Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:11:01Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br><span class="line">time=<span class="string">&quot;2022-09-17T06:11:03Z&quot;</span> level=info msg=<span class="string">&quot;Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first&quot;</span></span><br></pre></td></tr></table></figure><p>如上日志所示，对于 rancher custom 集群，有时候在 node agent pod 中可以看到有 <code>Waiting for node to register</code> 的日志信息。出现这个日志后，说明当前节点没有正常注册到 rancher 中。没有注册到 rancher 中，对于后期 custom 集群版本升级，这个节点上的基础组件将无法正常升级。虽然它没有正常注册到 rancher 中，但是它是正常注册到底层的 k8s 中，因为它不影响 k8s 的业务 pod 创建等操作。</p><p>对于这个问题，可以通过删除节点，然后重新添加节点的方式来处理。但是对于有业务运行的生产环境，可能不能删除节点，那么就只能通过以下方法手动处理。</p><h2 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h2><ol><li><p>执行以下命令，查看 cluster id、node id、node ip 之间的对应关系。根据报 Waiting for node to register 日志对应 pod 所在节点的 ip，找到相应的 cluster id、node id。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get nodes.management.cattle.io -A \</span><br><span class="line">-o=custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,IP:spec.customConfig.address,HostnameOverride:.spec.requestedHostname</span><br></pre></td></tr></table></figure></li><li><p>然后找一个相同集群下正常节点和不正常节点，分别执行以下命令打印节点的配置 YAML。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get nodes.management.cattle.io  -n c-xxx m-xxxx -oyaml</span><br></pre></td></tr></table></figure></li><li><p>可以发现在 YAML 配置的结尾，正常节点有 rkeNode 配置，而异常节点没有，接下来的处理方法就是手动把 rkeNode 配置添加到异常节点配置上去。</p><p>请参照正常节点的 rkeNode 配置，修改其中的 address、hostnameOverride、nodeName、role。如果添加节点时候设置的参数不同，那么此处的 rkeNode 配置可能也不一样，所以请参照当前集群下正常节点的 rkeNode 配置。</p><p>以下是一个配置示例：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">rkeNode:</span></span><br><span class="line">  <span class="attr">address:</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.224</span></span><br><span class="line">  <span class="attr">hostnameOverride:</span> <span class="string">alihost01</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">c-9p9ck:m-4cb5bfd0709c</span></span><br><span class="line">  <span class="attr">port:</span> <span class="string">&quot;22&quot;</span></span><br><span class="line">  <span class="attr">role:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">etcd</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">controlplane</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">worker</span></span><br><span class="line">  <span class="attr">user:</span> <span class="string">root</span></span><br></pre></td></tr></table></figure></li><li><p>执行以下命令编辑 node 资源</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl edit nodes.management.cattle.io -n c-xxx m-xxxx</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>如何查看 MySQL 数据库容量大小，表容量大小，索引容量大小？</title>
      <link href="/mysql/how-to-get-the-sizes-of-the-tables-of-a-mysql-database/"/>
      <url>/mysql/how-to-get-the-sizes-of-the-tables-of-a-mysql-database/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/mysql/how-to-get-the-sizes-of-the-tables-of-a-mysql-database/" target="_blank" title="https://www.xtplayer.cn/mysql/how-to-get-the-sizes-of-the-tables-of-a-mysql-database/">https://www.xtplayer.cn/mysql/how-to-get-the-sizes-of-the-tables-of-a-mysql-database/</a></p><h2 id="查看-MySQL「所有库」的容量大小"><a href="#查看-MySQL「所有库」的容量大小" class="headerlink" title="查看 MySQL「所有库」的容量大小"></a>查看 MySQL「所有库」的容量大小</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">table_schema as &#x27;数据库&#x27;,</span><br><span class="line">sum(table_rows) as &#x27;记录数&#x27;,</span><br><span class="line">sum(truncate(data_length/1024/1024, 2)) as &#x27;数据容量(MB)&#x27;,</span><br><span class="line">sum(truncate(index_length/1024/1024, 2)) as &#x27;索引容量(MB)&#x27;,</span><br><span class="line">sum(truncate(DATA_FREE/1024/1024, 2)) as &#x27;碎片占用(MB)&#x27;</span><br><span class="line">from information_schema.tables</span><br><span class="line">group by table_schema</span><br><span class="line">order by sum(data_length) desc, sum(index_length) desc;</span><br></pre></td></tr></table></figure><blockquote><p>特别提示：<code>data_length</code> 、<code>index_length</code> 等字段，所存储的容量信息单位是字节，所以我们要除以 2 个 1024 把字节转化为可读性更强的 MB。</p></blockquote><h2 id="查看-MySQL「指定库」的容量大小"><a href="#查看-MySQL「指定库」的容量大小" class="headerlink" title="查看 MySQL「指定库」的容量大小"></a>查看 MySQL「指定库」的容量大小</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">table_schema as &#x27;数据库&#x27;,</span><br><span class="line">sum(table_rows) as &#x27;记录数&#x27;,</span><br><span class="line">sum(truncate(data_length/1024/1024, 2)) as &#x27;数据容量(MB)&#x27;,</span><br><span class="line">sum(truncate(index_length/1024/1024, 2)) as &#x27;索引容量(MB)&#x27;,</span><br><span class="line">sum(truncate(DATA_FREE/1024/1024, 2)) as &#x27;碎片占用(MB)&#x27;</span><br><span class="line">from information_schema.tables</span><br><span class="line">where table_schema=&#x27;&lt;数据库名&gt;&#x27;</span><br><span class="line">order by data_length desc, index_length desc;</span><br></pre></td></tr></table></figure><blockquote><p>特别提示：<code>data_length</code> 、<code>index_length</code> 等字段，所存储的容量信息单位是字节，所以我们要除以 2 个 1024 把字节转化为可读性更强的 MB。</p><p><strong>注意：</strong>请将代码中 ‘<code>kalacloud_test_data</code>‘ 数据库名改为你要查询的数据库名。</p></blockquote><h2 id="查看-MySQL「指定库」中「所有表」的容量大小"><a href="#查看-MySQL「指定库」中「所有表」的容量大小" class="headerlink" title="查看 MySQL「指定库」中「所有表」的容量大小"></a>查看 MySQL「指定库」中「所有表」的容量大小</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  table_schema <span class="keyword">as</span> <span class="string">&#x27;数据库&#x27;</span>,</span><br><span class="line">  table_name <span class="keyword">as</span> <span class="string">&#x27;表名&#x27;</span>,</span><br><span class="line">  table_rows <span class="keyword">as</span> <span class="string">&#x27;记录数&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(data_length<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;数据容量(MB)&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(index_length<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;索引容量(MB)&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(DATA_FREE<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;碎片占用(MB)&#x27;</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">  information_schema.tables</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">  table_schema<span class="operator">=</span><span class="string">&#x27;&lt;数据库名&gt;&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> </span><br><span class="line">  data_length <span class="keyword">desc</span>, index_length <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong>请将代码中 ‘<code>kalacloud_test_data</code>‘ 数据库名改为你要查询的数据库名。</p><blockquote><p>特别提示：<code>data_length</code> 、<code>index_length</code> 等字段，所存储的容量信息单位是字节，所以我们要除以 2 个 1024 把字节转化为可读性更强的 MB。</p></blockquote><h2 id="查看-MySQL「指定库」中「指定表」的容量大小"><a href="#查看-MySQL「指定库」中「指定表」的容量大小" class="headerlink" title="查看 MySQL「指定库」中「指定表」的容量大小"></a>查看 MySQL「指定库」中「指定表」的容量大小</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  table_schema <span class="keyword">as</span> <span class="string">&#x27;数据库&#x27;</span>,</span><br><span class="line">  table_name <span class="keyword">as</span> <span class="string">&#x27;表名&#x27;</span>,</span><br><span class="line">  table_rows <span class="keyword">as</span> <span class="string">&#x27;记录数&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(data_length<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;数据容量(MB)&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(index_length<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;索引容量(MB)&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(DATA_FREE<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;碎片占用(MB)&#x27;</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">  information_schema.tables</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">  table_schema<span class="operator">=</span><span class="string">&#x27;&lt;数据库名&gt;&#x27;</span> <span class="keyword">and</span> table_name<span class="operator">=</span><span class="string">&#x27;&lt;表名&gt;&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> </span><br><span class="line">  data_length <span class="keyword">desc</span>, index_length <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong>请将代码中 <code>kalacloud_test_data</code> 数据库名改为你要查询的数据库名，<code>product_demo</code> 改为你要查询的表名。</p><blockquote><p>特别提示：<code>data_length</code> 、<code>index_length</code> 等字段，所存储的容量信息单位是字节，所以我们要除以 2 个 1024 把字节转化为可读性更强的 MB。</p></blockquote><h2 id="查看-MySQL-数据库中，容量排名前-10-的表"><a href="#查看-MySQL-数据库中，容量排名前-10-的表" class="headerlink" title="查看 MySQL 数据库中，容量排名前 10 的表"></a>查看 MySQL 数据库中，容量排名前 10 的表</h2><p>首先，先进入 <code>information_schema</code> 库里，然后执行以下命令：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">USE information_schema;</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  TABLE_SCHEMA <span class="keyword">as</span> <span class="string">&#x27;数据库&#x27;</span>,</span><br><span class="line">  table_name <span class="keyword">as</span> <span class="string">&#x27;表名&#x27;</span>,</span><br><span class="line">  table_rows <span class="keyword">as</span> <span class="string">&#x27;记录数&#x27;</span>,</span><br><span class="line">  ENGINE <span class="keyword">as</span> <span class="string">&#x27;存储引擎&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(data_length<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;数据容量(MB)&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(index_length<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;索引容量(MB)&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(DATA_FREE<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;碎片占用(MB)&#x27;</span></span><br><span class="line"><span class="keyword">from</span>  tables </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> table_rows <span class="keyword">desc</span> limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure><blockquote><p>特别提示：<code>data_length</code> 、<code>index_length</code> 等字段，所存储的容量信息单位是字节，所以我们要除以 2 个 1024 把字节转化为可读性更强的 MB。</p></blockquote><h2 id="查看-MySQL「指定库」中，容量排名前-10-的表"><a href="#查看-MySQL「指定库」中，容量排名前-10-的表" class="headerlink" title="查看 MySQL「指定库」中，容量排名前 10 的表"></a>查看 MySQL「指定库」中，容量排名前 10 的表</h2><p>我们先进入 <code>information_schema</code> 库里，再执行以下命令：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">USE information_schema;</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  TABLE_SCHEMA <span class="keyword">as</span> <span class="string">&#x27;数据库&#x27;</span>,</span><br><span class="line">  table_name <span class="keyword">as</span> <span class="string">&#x27;表名&#x27;</span>,</span><br><span class="line">  table_rows <span class="keyword">as</span> <span class="string">&#x27;记录数&#x27;</span>,</span><br><span class="line">  ENGINE <span class="keyword">as</span> <span class="string">&#x27;存储引擎&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(data_length<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;数据容量(MB)&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(index_length<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;索引容量(MB)&#x27;</span>,</span><br><span class="line">  <span class="keyword">truncate</span>(DATA_FREE<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span>, <span class="number">2</span>) <span class="keyword">as</span> <span class="string">&#x27;碎片占用(MB)&#x27;</span></span><br><span class="line"><span class="keyword">from</span>  tables </span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">  table_schema<span class="operator">=</span><span class="string">&#x27;&lt;数据库名&gt;&#x27;</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> table_rows <span class="keyword">desc</span> limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure><blockquote><p>特别提示：<code>data_length</code> 、<code>index_length</code> 等字段，所存储的容量信息单位是字节，所以我们要除以 2 个 1024 把字节转化为可读性更强的 MB。</p></blockquote><h2 id="统计单个表记录条数"><a href="#统计单个表记录条数" class="headerlink" title="统计单个表记录条数"></a>统计单个表记录条数</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">use <span class="string">&#x27;&lt;数据库名&gt;&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> <span class="operator">&lt;</span>表名<span class="operator">&gt;</span>;</span><br><span class="line"># 计算具有某个字段值的记录条数</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> <span class="operator">&lt;</span>表名<span class="operator">&gt;</span> <span class="keyword">where</span> operation<span class="operator">=</span>&quot;GET&quot;;</span><br></pre></td></tr></table></figure><h2 id="删除具有某个字段值的记录"><a href="#删除具有某个字段值的记录" class="headerlink" title="删除具有某个字段值的记录"></a>删除具有某个字段值的记录</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">DELETE FROM audit_log_parsed where operation=&quot;GET&quot; LIMIT 10000;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>node-agent 报错 Either cluster is not ready for registering, cluster is currently provisioning, or etcd, controlplane and worker node have to be registered</title>
      <link href="/rancher/either-cluster-is-not-ready-for-registering/"/>
      <url>/rancher/either-cluster-is-not-ready-for-registering/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/either-cluster-is-not-ready-for-registering/" target="_blank" title="https://www.xtplayer.cn/rancher/either-cluster-is-not-ready-for-registering/">https://www.xtplayer.cn/rancher/either-cluster-is-not-ready-for-registering/</a></p><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>某天一客户突然反馈 rancher local 集群节点 32G 内存资源耗尽，local 集群中只运行了 rancher 相关业务。</p><p>经过一系列排查，发现耗尽内存的是 k8s apiserver 进程。在 apiserver 容器日志中发现了大量以下日志：</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;log&quot;</span><span class="punctuation">:</span><span class="string">&quot;I0507 08:24:37.492998       1 pathrecorder.go:253] kube-apiserver: \&quot;/apis/management.cattle.io/v3/namespaces/c-dql6k/nodes/machine-h46dc\&quot; satisfied by NotFoundHandler\n&quot;</span><span class="punctuation">,</span><span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span><span class="string">&quot;stderr&quot;</span><span class="punctuation">,</span><span class="attr">&quot;time&quot;</span><span class="punctuation">:</span><span class="string">&quot;2022-05-07T08:24:37.495887119Z&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;log&quot;</span><span class="punctuation">:</span><span class="string">&quot;I0507 08:24:37.493004       1 handler.go:153] apiextensions-apiserver: PUT \&quot;/apis/management.cattle.io/v3/namespaces/c-dql6k/nodes/machine-h46dc\&quot; satisfied by nonGoRestful\n&quot;</span><span class="punctuation">,</span><span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span><span class="string">&quot;stderr&quot;</span><span class="punctuation">,</span><span class="attr">&quot;time&quot;</span><span class="punctuation">:</span><span class="string">&quot;2022-05-07T08:24:37.495891177Z&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;log&quot;</span><span class="punctuation">:</span><span class="string">&quot;I0507 08:24:37.493010       1 pathrecorder.go:247] apiextensions-apiserver: \&quot;/apis/management.cattle.io/v3/namespaces/c-dql6k/nodes/machine-h46dc\&quot; satisfied by prefix /apis/\n&quot;</span><span class="punctuation">,</span><span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span><span class="string">&quot;stderr&quot;</span><span class="punctuation">,</span><span class="attr">&quot;time&quot;</span><span class="punctuation">:</span><span class="string">&quot;2022-05-07T08:24:37.495894009Z&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;log&quot;</span><span class="punctuation">:</span><span class="string">&quot;I0507 08:24:37.493103       1 handler.go:153] kube-aggregator: PUT \&quot;/apis/management.cattle.io/v3/namespaces/c-dql6k/nodes/machine-h46dc\&quot; satisfied by nonGoRestful\n&quot;</span><span class="punctuation">,</span><span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span><span class="string">&quot;stderr&quot;</span><span class="punctuation">,</span><span class="attr">&quot;time&quot;</span><span class="punctuation">:</span><span class="string">&quot;2022-05-07T08:24:37.495896746Z&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;log&quot;</span><span class="punctuation">:</span><span class="string">&quot;I0507 08:24:37.493113       1 pathrecorder.go:247] kube-aggregator: \&quot;/apis/management.cattle.io/v3/namespaces/c-dql6k/nodes/machine-h46dc\&quot; satisfied by prefix /apis/management.cattle.io/v3/\n&quot;</span><span class="punctuation">,</span><span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span><span class="string">&quot;stderr&quot;</span><span class="punctuation">,</span><span class="attr">&quot;time&quot;</span><span class="punctuation">:</span><span class="string">&quot;2022-05-07T08:24:37.495899592Z&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;log&quot;</span><span class="punctuation">:</span><span class="string">&quot;I0507 08:24:37.493121       1 handler.go:153] kube-apiserver: PUT \&quot;/apis/management.cattle.io/v3/namespaces/c-dql6k/nodes/machine-h46dc\&quot; satisfied by nonGoRestful\n&quot;</span><span class="punctuation">,</span><span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span><span class="string">&quot;stderr&quot;</span><span class="punctuation">,</span><span class="attr">&quot;time&quot;</span><span class="punctuation">:</span><span class="string">&quot;2022-05-07T08:24:37.495902397Z&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h2><p>rancher 中保存的数据，均是以 CRD（CustomResourceDefinition）（<a href="https://kubernetes.io/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/%EF%BC%89%E8%B5%84%E6%BA%90%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%AD%98%E5%82%A8%E5%9C%A8">https://kubernetes.io/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/）资源的形式存储在</a> K8S 中，rancher 会通过 <code>kubernetes svc</code> 调用 local k8s apiserver 来读写 CRD 资源。</p><p><code>management.cattle.io</code> 是 rancher 存储数据的 <code>API Group</code>，所有与 rancher 相关的 CRD 资源均在这个 <code>API Group</code>中。</p><p>在 rancher 中有一个叫做 <code>nodes.management.cattle.io</code> 的 CRD 资源，这个 CRD 资源用于保存所有集群的 node 配置信息。比如在 local 集群中执行 <code>kubectl get nodes.management.cattle.io -A</code> 可以看到以 CLUSTER-ID 为 NAMESPACE 的 node 列表。</p><p>查看 local apiserver 的日志，发现频繁的刷新着 <code>/apis/management.cattle.io/v3/namespaces/c-dql6k/nodes/machine-h46dc</code>  相关字样的日志信息，通过这些信息可以初步判断集群 c-dql6k 的 machine-h46dc 节点可能存在异常。在 rancher 架构中，每个业务集群的节点上均运行着一个 <code>node-agent pod</code> ，这个 pod 主要负责 node 注册以及节点配置更新，如果 rancher 频繁的读写 <code>nodes.management.cattle.io</code> CRD，那么也说明了对应节点的  <code>node-agent pod</code> 很有可能运行异常。</p><p>根据 local apiserver 的日志知道了 cluster-id 和 node-id， 通过在 local 集群中执行以下命令快速定位集群名称以及节点 IP。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get clusters.management.cattle.io &lt;cluster-id&gt; -ojson|jq .status.appliedSpec.displayName</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get nodes.management.cattle.io -n &lt;cluster-id&gt; &lt;node-id&gt; -ojson|jq .status.internalNodeStatus.addresses</span><br></pre></td></tr></table></figure><p>通过以上命令快速定位到具体集群和具体的 node 名称，切换到 system 项目下，点击 cattle-system 命名空间下的 cattle-node-agent 服务，根据查询到的 node ip 可以快速的定位到具体的 node-agent pod 名称。</p><h3 id="查询-node-agent-pod-日志"><a href="#查询-node-agent-pod-日志" class="headerlink" title="查询 node-agent pod 日志"></a>查询 node-agent pod 日志</h3><p>根据以上操作定位到具体集群以及具体 node agent pod 后，通过 rancher ui 查看  node agent pod 日志。</p><img src="/rancher/either-cluster-is-not-ready-for-registering/image-20220514141236271.png" class="" title="image-20220514141236271"><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>Rancher 中有一个 <code>nodessyncer controller</code>，它会根据下游集群的 node 情况，把 K8S node 同步到 local 的<code>nodes.management.cattle.io CRD</code>中。</p><ul><li>如果是 <strong>import</strong> 集群，因为 node 已存在，那么在 <code>nodes.management.cattle.io CRD</code>中将会以  <code>machine-</code> 命名方式创建 <code>nodes.management.cattle.io</code> 资源。</li><li>如果自定义集群，正常自定义创建集群时，在 local 集群的 c-xxx ns中创建了 m-xxx node，此时nodessyncer会判断已经存在了这个资源，就不会创建 <code>machine-</code> node。</li></ul><p>基于以上逻辑，出现 node-agent 报以上错误有以下两种可能：</p><ol><li>在 rancher ui 删除节点时，可能某些原因导致 rancher <code>nodes.management.cattle.io CRD</code>中的 node 资源被删除了，但是下游 k8s 集群却未能正常的把节点从 k8s 中剔除。当再次通过 rancher ui 复制添加节点的命令执行后，因为这个时候 node 已经存在 k8s 集群中，因此 node-agent 不会再次注册，所以抛出了错误日志。</li><li>在清理下游集群节点时，如果先删除的 local m-xxx node，nodessyncer 可能瞬间同步了下游的 native node 并创建machine-xxx node。因为当时下游集群的native node在这个时间窗口，还没有被清理，从程序逻辑执行角度，会出现这种情况。</li></ol><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ol><li><p>如果集群运行踢出节点，那么最简单的方法就是把异常的节点从集群中中踢出，执行 kubectl get node 命令确认节点已经删除后，再把节点初始化之后重新添加到集群中。</p></li><li><p>如果节点不能踢出，那么只能通过手动更新配置了。通过执行 <code>kubectl get nodes.management.cattle.io -n &lt;cluster-id&gt; &lt;node-id&gt; -oyaml</code> 导出一个正常节点和异常节点的 YAML ，然后进行对比。</p><img src="/rancher/either-cluster-is-not-ready-for-registering/image-20220514142815300.png" class="" title="image-20220514142815300"></li></ol><p>可以发现主要的差异在 <strong>status.rkeNode</strong>  和 <strong>spec.customConfig</strong> 。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">rkeNode:</span></span><br><span class="line">  <span class="attr">address:</span> <span class="string">&lt;当前节点</span> <span class="string">ip&gt;</span></span><br><span class="line">  <span class="attr">hostnameOverride:</span> <span class="string">&lt;</span> <span class="string">spec.requestedHostname</span> <span class="string">的值</span> <span class="string">&gt;</span></span><br><span class="line"><span class="string">  nodeName: &lt;cluster_id&gt;:&lt;node_id&gt;</span></span><br><span class="line"><span class="string">  port: &quot;22&quot;</span></span><br><span class="line"><span class="string">  role:</span></span><br><span class="line"><span class="string">  - worker</span></span><br><span class="line"><span class="string">  user: root</span></span><br></pre></td></tr></table></figure><p><strong>spec.customConfig</strong> 配置是做的自定义配置，根据实际清楚修改。</p><p>最后执行 <code>kubectl edit nodes.management.cattle.io -n &lt;cluster-id&gt; &lt;node-id&gt; -oyaml</code> ，将以上配置更新即可。</p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Prometheus Adapter 安装</title>
      <link href="/prometheus/prometheus-adapter/"/>
      <url>/prometheus/prometheus-adapter/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/prometheus/prometheus-adapter/" target="_blank" title="https://www.xtplayer.cn/prometheus/prometheus-adapter/">https://www.xtplayer.cn/prometheus/prometheus-adapter/</a></p><h2 id="Prometheus-准备"><a href="#Prometheus-准备" class="headerlink" title="Prometheus 准备"></a>Prometheus 准备</h2><p>从 Rancher v2.4.8-ent（监控 chart 版本 0.1.2000）开始，只能通过应用商店跳转的链接访问 Prometheus，直接通过 Pod IP 、svc 或者 Nodeport 无法访问。如果想通过 Nodeport 或者 Ingress 代理访问 Prometheus ，配置方法请访问 <a href="https://ee2.docs.rancher.cn/docs/monitor/cluster-monitoring/ingress-nodeport-access-prometheus/">通过 Nodeport 或者 Ingress 访问 Prometheus</a>。</p><p>对于集群监控 chart 版本高于 0.1.4001 的环境，在集群监控配置页添加应答 <code>prometheus.serviceNodePort = true</code>，将会自动创建一个 Nodeport svc，通过 Nodeport 端口即可访问。</p><h2 id="Prometheus-Adapter-安装"><a href="#Prometheus-Adapter-安装" class="headerlink" title="Prometheus Adapter 安装"></a>Prometheus Adapter 安装</h2><p>Prometheus Adapter chart 地址：<a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter">https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter</a></p><ol><li><p>将 chart 添加到 rancher 应用商店，如图：</p><img src="/prometheus/prometheus-adapter/image-20220404162948341.png" class="" title="image-20220404162948341"></li><li><p>进入 <strong>system 项目|应用商店</strong>，点击<strong>启动</strong>可用看到 Prometheus-Adapter。</p><img src="/prometheus/prometheus-adapter/image-20220404163042640.png" class="" title="image-20220404163042640"></li></ol><h2 id="Prometheus-Adapter-配置"><a href="#Prometheus-Adapter-配置" class="headerlink" title="Prometheus Adapter 配置"></a>Prometheus Adapter 配置</h2><p>根据 Prometheus 准备 中说明的，如果你的 集群监控 chart 版本高于 0.1.2000，并且低于 0.1.4001，需要访问 <a href="https://ee2.docs.rancher.cn/docs/monitor/cluster-monitoring/ingress-nodeport-access-prometheus/">通过 nodeport 或者 ingress 访问 Prometheus</a> 去配置 Prometheus 的访问地址。如果 chart 版本高于 0.1.4001，则在集群监控配置页添加应答 <code>prometheus.serviceNodePort = true</code>，将会自动创建一个 Nodeport svc，通过 Nodeport 端口即可访问。</p><p>本示例中以为 ingress 代理访问 Prometheus 为例，</p><img src="/prometheus/prometheus-adapter/image-20220404163104047.png" class="" title="image-20220404163104047"><h2 id="Prometheus-Adapter-测试"><a href="#Prometheus-Adapter-测试" class="headerlink" title="Prometheus Adapter 测试"></a>Prometheus Adapter 测试</h2><p>最简单的只需要在命令行中执行命令 <code>kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1</code> ，正常情况会有很多数据输出。此处截取一小部分：</p><img src="/prometheus/prometheus-adapter/image-20220404163133215.png" class="" title="image-20220404163133215"><h2 id="Prometheus-Adapter-自定义配置"><a href="#Prometheus-Adapter-自定义配置" class="headerlink" title="Prometheus Adapter 自定义配置"></a>Prometheus Adapter 自定义配置</h2><p>自定义配置可以参考 <a href="https://github.com/kubernetes-sigs/prometheus-adapter/tree/master/docs">https://github.com/kubernetes-sigs/prometheus-adapter/tree/master/docs</a> 。</p>]]></content>
      
      
      <categories>
          
          <category> prometheus </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>自定义集群监控参数</title>
      <link href="/prometheus/custom-parameter/"/>
      <url>/prometheus/custom-parameter/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/prometheus/custom-parameter/" target="_blank" title="https://www.xtplayer.cn/prometheus/custom-parameter/">https://www.xtplayer.cn/prometheus/custom-parameter/</a></p><p>默认的集群监控配置可能不适用于所有的环境，比如内存 limit 大小，需要根据实际需求进行参数的调整。</p><h2 id="调整组件内存"><a href="#调整组件内存" class="headerlink" title="调整组件内存"></a>调整组件内存</h2><p>有时候可能会发现 <strong>prometheus-cluster-monitoring-0</strong> Pod 中的 prometheus 容器在反复重启，在 <strong>promethues</strong> 容器日志中并未发现异常错误。类似的错误还在 prometheus-agent、prometheus-proxy 都可能会出现。</p><p>如果出现以上现象，说明很有可能是容器内存超过了限制值，容器进程被强制 kill ，导致容器频繁重启。</p><ol><li><p>调整 prometheus 内存</p><p>在 <strong>集群|工具|监控</strong> 配置页面中，可以看到如图的限制配置，可以适当的调整，比如 Prometheus CPU 限制 设置为 4000，Prometheus 内存限制设置为 8192，Node Exporter CPU 限制设置为 500，Node Exporter 内存限制设置为 500。</p><img src="/prometheus/custom-parameter/2022-03-28-19-34-14.png" class="" title="imag"></li><li><p>调整其他组件</p><p>在高级选项中添加以下应答</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">prometheus.resources.proxy.limits.cpu=500m</span><br><span class="line">prometheus.resources.proxy.limits.memory=500Mi</span><br><span class="line"></span><br><span class="line">prometheus.resources.auth.limits.cpu=500m</span><br><span class="line">prometheus.resources.auth.limits.memory=500Mi</span><br><span class="line"></span><br><span class="line">exporter-kube-state.resources.limits.cpu=500m</span><br><span class="line">exporter-kube-state.resources.limits.memory=1024Mi</span><br><span class="line"></span><br><span class="line">grafana.resources.core.limits.cpu=200m</span><br><span class="line">grafana.resources.core.limits.memory=500Mi</span><br><span class="line"></span><br><span class="line">grafana.resources.proxy.limits.cpu=100m</span><br><span class="line">grafana.resources.proxy.limits.memory=100Mi</span><br><span class="line"></span><br><span class="line">alertmanager.resources.core.limits.cpu=1000m</span><br><span class="line">alertmanager.resources.core.limits.memory=500Mi</span><br><span class="line"></span><br><span class="line">alertmanager.resources.config.limits.cpu=100m</span><br><span class="line">alertmanager.resources.config.limits.memory=100Mi</span><br><span class="line"></span><br><span class="line">exporter-kube-node.resources.limits.cpu=200m</span><br><span class="line">exporter-kube-node.resources.limits.memory=100Mi</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其他组件资源配置调整，可以参考此文档 <a href="https://github.com/cnrancher/system-charts/blob/release-v2.5-ent/charts/rancher-monitoring/v0.2.2001/values.yaml">https://github.com/cnrancher/system-charts/blob/release-v2.5-ent/charts/rancher-monitoring/v0.2.2001/values.yaml</a></p></li></ol><h2 id="调整-prometheus-参数"><a href="#调整-prometheus-参数" class="headerlink" title="调整 prometheus 参数"></a>调整 prometheus 参数</h2><p>比如，prometheus 默认是 1 分钟获取一次数据，如果想缩短收集的时间间隔可以通过调整 scrape_interval 参数来设置。</p><p>在集群监控配置页的高级选项中添加以下应答：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">prometheus.scrapeInterval = 30s</span><br></pre></td></tr></table></figure><p>prometheus 其他参数可以查询此文档: <a href="https://github.com/cnrancher/system-charts/blob/release-v2.5-ent/charts/rancher-monitoring/v0.2.2001/charts/prometheus/values.yaml">https://github.com/cnrancher/system-charts/blob/release-v2.5-ent/charts/rancher-monitoring/v0.2.2001/charts/prometheus/values.yaml</a></p>]]></content>
      
      
      <categories>
          
          <category> prometheus </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>全新论坛启用，Rancher 中文社区迈入新阶段</title>
      <link href="/rancher/rancher-new-forums/"/>
      <url>/rancher/rancher-new-forums/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/rancher-new-forums/" target="_blank" title="https://www.xtplayer.cn/rancher/rancher-new-forums/">https://www.xtplayer.cn/rancher/rancher-new-forums/</a></p><p>伴随着 Rancher 旗下各种开源产品的快速发展，Rancher 中文社区群体日益壮大。统计微信群和 QQ 群等主要聚集地，社区群体数量已达数万人之众。为了能够更好地发挥社区互助的力量，Rancher 中文社区 (<a href="https://forums.rancher.cn/">https://forums.rancher.cn</a>) 正式启用新的论坛，并于今日正式对外开放。 </p><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>在社区发展的过程中，微信社交软件拥有不可磨灭的贡献，Rancher 社区通过各种市场活动，吸纳技术群体加入到微信群进行交流，相互切磋。然而，随着社区群体的急速壮大，以微信群为主的社区治理模式逐渐遇到发展瓶颈。其最大的痛点在于知识传递障碍。早期成立的微信群中，经过多轮打磨，其成员普遍技艺精湛。而相对成立较晚的微信群，很多还在挣扎于很基础的安装部署。同时，社区的管理人员需要付出大量精力在割裂的群体中传递这些知识。其次，微信在本质上并不是适合社区交流的通信工具，其内容无法被有效的索引到搜索引擎，每个群500人的限制可能无法找到合适的交流对象，并且会干扰工作与生活时间的平衡。最后，微信的聊天属性，也非常不利于技术问题的讨论，对一些技术问题的背景上下文很难以结构化的体现，沟通效率低下。 </p><p>经过深思熟虑，在启用新的中文论坛的同时，我们也宣布开始逐步建设以论坛为主要阵地的社区发展方向。微信群将会逐步打造成一个精选内容分发平台，它将是只读的，用户不必担心过多被与自己无关的技术探讨交流而干扰，我们鼓励用户多在论坛进行技术交流，这会更利于社区的技术互助和经验沉淀。</p><h2 id="关于新的-Rancher-论坛"><a href="#关于新的-Rancher-论坛" class="headerlink" title="关于新的 Rancher 论坛"></a>关于新的 Rancher 论坛</h2><p>为了让技术交流和分享变得更高效，也为了让大家在参与社群互动时有更好的体验，我们在建立新的 Rancher 论坛时有了一些新的想法。</p><h3 id="打破人数限制"><a href="#打破人数限制" class="headerlink" title="打破人数限制"></a>打破人数限制</h3><p>在 Rancher 论坛内，知识和信息的传播将进一步扩大，500人不再是人数上限，所有的小伙伴都有和全部社区成员交流的机会。同时，为保证用户群体的精准性，我们会对注册人员进行审批。每天会至少保持一位审批人员在线，保证大家的顺利入场。 </p><h3 id="更合理的问题框架"><a href="#更合理的问题框架" class="headerlink" title="更合理的问题框架"></a>更合理的问题框架</h3><p>无需在不同技术交流群内切换，我们在 Rancher 论坛内一站式就能完成交流、提问和解答。你只需要根据你的需求找到对应的讨论区，或在站内进行关键词搜索，来获取你想要的信息，简单且高效。同时，避免反复沟通上下文而浪费精力，我们对发帖内容进行结构性的提示，用户按照引导填写问题内容，将会更加有利于知识的沉淀和索引，也确保了沟通的高效率。 </p><h3 id="全新的激励机制"><a href="#全新的激励机制" class="headerlink" title="全新的激励机制"></a>全新的激励机制</h3><p>知识付费的时代，在论坛内，你的每一次发言，每一点贡献我们都看得到。我们会根据你的活跃度和论坛贡献给予相应的奖励，更多奖励机制等你来探索。先前更多依靠 Rancher 官方技术人员驻场，而论坛模式则会激励更多参与社区交流和贡献的人群。 </p><h3 id="微信内推送论坛精选"><a href="#微信内推送论坛精选" class="headerlink" title="微信内推送论坛精选"></a>微信内推送论坛精选</h3><p>担心错过论坛内的精彩信息？没关系，我们会根据话题讨论的热度，精选优质内容，定期推送到微信群内，符合当下快速阅读的节奏和习惯。推送过程也将会努力减少干扰，保证干净且轻松的阅读体验。</p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>coreDNS 加速外部域名解析</title>
      <link href="/coredns/accelerate-external-domain-resolution/"/>
      <url>/coredns/accelerate-external-domain-resolution/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/coredns/accelerate-external-domain-resolution/" target="_blank" title="https://www.xtplayer.cn/coredns/accelerate-external-domain-resolution/">https://www.xtplayer.cn/coredns/accelerate-external-domain-resolution/</a></p><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>有时候业务可能对 DNS 解析有很高要求，通过以下脚本循环去访问一个域名，时而会出现解析到过 1s 的错误提示。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `<span class="built_in">seq</span> 1 500`;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    curl -LSs -I api.mch.weixin.qq.com --connect-timeout 1 | grep HTTP/1.1</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>在 coredns 配置中添加 log 参数可以打印详细的请求日志。</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">.<span class="punctuation">:</span><span class="number">53</span> <span class="punctuation">&#123;</span></span><br><span class="line">        log</span><br><span class="line">    errors</span><br><span class="line">    health <span class="punctuation">&#123;</span></span><br><span class="line">      lameduck <span class="number">5</span>s</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    ready</span><br><span class="line">    kubernetes cluster.local in-addr.arpa ip6.arpa <span class="punctuation">&#123;</span></span><br><span class="line">      pods insecure</span><br><span class="line">      fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    template ANY AAAA . <span class="punctuation">&#123;</span></span><br><span class="line">       rcode NXDOMAIN</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    prometheus <span class="punctuation">:</span><span class="number">9153</span></span><br><span class="line">    forward . <span class="string">&quot;/etc/resolv.conf&quot;</span></span><br><span class="line">    bufsize <span class="number">4096</span></span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    loop</span><br><span class="line">    reload</span><br><span class="line">    loadbalance</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>以 <code>api.mch.weixin.qq.com</code> 为例，通过查看日志发现，在最后成功解析域名之前经过了多次搜索域的查询，从而增加了查询时间。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[INFO] 10.42.46.33:49935 - 64126 <span class="string">&quot;AAAA IN api.mch.weixin.qq.com.prod-mail.svc.cluster.local. udp 67 false 512&quot;</span> NXDOMAIN qr,aa,rd 67 0.000146797s</span><br><span class="line">[INFO] 10.42.46.33:49935 - 10869 <span class="string">&quot;A IN api.mch.weixin.qq.com.prod-mail.svc.cluster.local. udp 67 false 512&quot;</span> NXDOMAIN qr,aa,rd 160 0.000168806s</span><br><span class="line">[INFO] 10.42.46.33:41691 - 27825 <span class="string">&quot;A IN api.mch.weixin.qq.com.svc.cluster.local. udp 57 false 512&quot;</span> NXDOMAIN qr,aa,rd 150 0.000093044s</span><br><span class="line">[INFO] 10.42.46.33:41691 - 699 <span class="string">&quot;AAAA IN api.mch.weixin.qq.com.svc.cluster.local. udp 57 false 512&quot;</span> NXDOMAIN qr,aa,rd 57 0.000090373s</span><br><span class="line">[INFO] 10.42.46.33:58255 - 30725 <span class="string">&quot;A IN api.mch.weixin.qq.com.cluster.local. udp 53 false 512&quot;</span> NXDOMAIN qr,aa,rd 146 0.000050077s</span><br><span class="line">[INFO] 10.42.46.33:58255 - 41997 <span class="string">&quot;AAAA IN api.mch.weixin.qq.com.cluster.local. udp 53 false 512&quot;</span> NXDOMAIN qr,aa,rd 53 0.000106777s</span><br><span class="line">[INFO] 10.42.46.33:47452 - 34532 <span class="string">&quot;AAAA IN api.mch.weixin.qq.com. udp 39 false 512&quot;</span> NXDOMAIN qr,aa,rd 39 0.000119399s</span><br><span class="line">[INFO] 10.42.46.33:47452 - 26334 <span class="string">&quot;A IN api.mch.weixin.qq.com. udp 39 false 512&quot;</span> NOERROR qr,aa,rd,ra 234 0.000161447s</span><br></pre></td></tr></table></figure><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>可以通过添加子域名的方式，跳过对其他搜索域的查询以提高解析速度。</p><p>在 system 项目下修改 coredns 的配置映射文件，参考以下配置，添加了一组 <code>weixin.qq.com:53</code>，这个是固定格式，根据需要修改搜索域即可。保存后默认一分钟配置会自动同步到 coredns pod，然后 coredns 会自动加载配置。</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">.<span class="punctuation">:</span><span class="number">53</span> <span class="punctuation">&#123;</span></span><br><span class="line">        log</span><br><span class="line">    errors</span><br><span class="line">    health <span class="punctuation">&#123;</span></span><br><span class="line">      lameduck <span class="number">5</span>s</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    ready</span><br><span class="line">    kubernetes cluster.local in-addr.arpa ip6.arpa <span class="punctuation">&#123;</span></span><br><span class="line">      pods insecure</span><br><span class="line">      fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    template ANY AAAA . <span class="punctuation">&#123;</span></span><br><span class="line">       rcode NXDOMAIN</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    prometheus <span class="punctuation">:</span><span class="number">9153</span></span><br><span class="line">    forward . <span class="string">&quot;/etc/resolv.conf&quot;</span></span><br><span class="line">    bufsize <span class="number">4096</span></span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    loop</span><br><span class="line">    reload</span><br><span class="line">    loadbalance</span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">weixin.qq.com<span class="punctuation">:</span><span class="number">53</span> <span class="punctuation">&#123;</span></span><br><span class="line">    log</span><br><span class="line">    errors</span><br><span class="line">    ready</span><br><span class="line">    reload</span><br><span class="line">    cache <span class="number">300</span></span><br><span class="line">    bufsize <span class="number">4096</span></span><br><span class="line">    forward . <span class="number">114.114</span><span class="number">.114</span><span class="number">.114</span></span><br><span class="line">    loadbalance</span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">wx.gtimg.com<span class="punctuation">:</span><span class="number">53</span> <span class="punctuation">&#123;</span></span><br><span class="line">    log</span><br><span class="line">    errors</span><br><span class="line">    ready</span><br><span class="line">    reload</span><br><span class="line">    cache <span class="number">300</span></span><br><span class="line">    bufsize <span class="number">4096</span></span><br><span class="line">    forward . <span class="number">114.114</span><span class="number">.114</span><span class="number">.114</span></span><br><span class="line">    loadbalance</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><p>以上通过配置 coredns 搜索域的方法虽然可以提高部分域名解析速度，但是目前存在一个已知的 dns 解析问题，因为内核 conntrack 模块的 bug 导致。具体原因请参考: <a href="https://tencentcloudcontainerteam.github.io/2018/10/26/DNS-5-seconds-delay/">https://tencentcloudcontainerteam.github.io/2018/10/26/DNS-5-seconds-delay/</a></p><h2 id="问题的根本解决"><a href="#问题的根本解决" class="headerlink" title="问题的根本解决"></a>问题的根本解决</h2><p>经过测试，在工作负载的 YAML 文件中添加以下配置，可以加快 DNS 解析速度。</p><img src="/coredns/accelerate-external-domain-resolution/image-20210517202827735-0cd742389728dca0f4bf747e792816ce.png" class="" title="image-20210517202827735"><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">dnsConfig:</span></span><br><span class="line">  <span class="attr">options:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">single-request-reopen</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ndots</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line"><span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line"><span class="attr">imagePullSecrets:</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> coredns </category>
          
      </categories>
      
      
        <tags>
            
            <tag> coredns </tag>
            
            <tag> coredns 最佳实践 </tag>
            
            <tag> coreDNS 加速外部域名解析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI at the Edge with K3s and NVIDIA Jetson Nano: Object Detection and Real-Time Video Analytics</title>
      <link href="/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/"/>
      <url>/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/" target="_blank" title="https://www.xtplayer.cn/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/">https://www.xtplayer.cn/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/</a></p><p>With the advent of new and powerful GPU-capable devices, the possible use cases that we can execute at the edge are expanding. The edge is growing in size and getting more efficient as technology advances. NVIDIA, with its industry-leading GPUs, and Arm, the leading technology provider of processor IP, are making significant innovations and investments in the edge ecosystem landscape. For instance, the NVIDIA Jetson Nano is the most cost-effective device that can run GPU-enabled workloads and can handle AI&#x2F;ML data processing jobs. Additionally, cloud native technologies like Kubernetes have enabled developers to build lightweight applications using containers for the edge. To enable a seamless cloud native software experience across a compute-diverse edge ecosystem, Arm has launched <a href="https://www.arm.com/solutions/infrastructure/edge-computing/project-cassini">Project Cassini</a> – an open, collaborative standards-based initiative. It leverages the power of these heterogenous Arm-based platforms to create a secure foundation for edge applications. </p><p>K3s, developed by Rancher Labs and now a CNCF Sandbox project, has been a key orchestration platform for these compact footprint edge devices. As a Kubernetes distro built for the edge, it is lightweight enough to not put a strain on device RAM and CPU. Taking advantage of the Kubernetes device plugin framework, the workloads running on top of these devices can access the GPU capabilities with efficiency.  </p><p>In typical scenarios, the devices at the edge are used to collect the data, and analytics and decoding are performed with cloud counterparts. With the edge devices becoming more powerful, we can now perform the AI&#x2F;ML processing at the edge location itself. </p><p>As highlighted by Mark Abrams from SUSE in his <a href="https://rancher.com/blog/2020/get-up-and-running-with-nvidia-gpus">previous blog</a> on how to use GPUs on Rancher Kubernetes clusters deployed on the cloud, it’s pretty seamless and efficient. </p><p> In this blog, we’ll see how a combination of NVIDIA’s Jetson Nano and K3s enables GPUs at the edge and makes a compelling platform. The figure below depicts the high-level architecture of the use case: </p><img src="/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/Picture1-300x139-20210421002853588.png" class="" title="img"><p><em>Object Detection and Video Analytics at the Edge</em></p><p>We see an edge location with a video camera connected to a Jetson Nano device. The NVIDIA Jetson Nano ships with JetPack OS – a set of libraries enabling the GPU devices.</p><p>In this setup, we have two video streams that are being passed as input to the NVIDIA DeepStream containers:</p><ul><li>A live video feed from the camera angled towards a parking lot</li><li>A second video feed is a pre-built video with different types of objects – cars, bicycles, humans, etc.</li><li>We also have a K3s cluster deployed on Jetson Nano that hosts the NVIDIA DeepStream pods. When the video streams are passed to the DeepStream pods, the analytics are done at the device itself.</li><li>The output is then passed onto a display attached to the Jetson Nano. On the display, we can see the object classifications – cars, humans, etc.</li></ul><p>For more details on the architecture and the demo of the use case, check out this video: <a href="https://www.youtube.com/watch?v=LVLllVIQUDA">Object Detection and Video Analytics at the Edge</a></p><h2 id="Configurations"><a href="#Configurations" class="headerlink" title="Configurations"></a>Configurations</h2><p><strong>Prerequisites:</strong> The following components should be installed and configured</p><ul><li>Jetson Nano board</li><li>Jetson OS (Tegra)</li><li>Display attached to the Jetson Nano via HDMI</li><li>A webcam attached to the Jetson Nano via USB</li><li>Change Docker runtime to Nvidia runtime and install K3s</li></ul><p>Jetson OS comes with Docker installed out of the box. We need to use the latest Docker version as it is GPU compatible. To check the default runtime, use the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker info | grep Runtime</span><br></pre></td></tr></table></figure><p>You can also see the current runtime by checking the docker daemon:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /etc/docker/daemon.json</span><br></pre></td></tr></table></figure><p>Now, change the contents of the docker daemon to the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> &quot;default-runtime&quot;: &quot;nvidia&quot;,</span><br><span class="line"></span><br><span class="line">&quot;runtimes&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;nvidia&quot;: &#123;</span><br><span class="line"> &quot;path&quot;: &quot;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: []</span><br><span class="line"></span><br><span class="line">&#125; &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><img src="/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/Picture2-300x122-20210421002853630.png" class="" title="img"><p>After editing the daemon.json, restart the docker service. Then you should be able to see the Nvidia default runtime.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl restart docker</span><br><span class="line">sudo docker info | grep Runtime</span><br></pre></td></tr></table></figure><p>Before installing K3s, please run the following commands:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt update sudo apt upgrade -y sudo apt install curl</span><br></pre></td></tr></table></figure><p>This will make sure we are using the latest versions of the software stack.</p><p>To install K3s, use the following command</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -sfL https://get.k3s.io/ | INSTALL_K3S_EXEC=&quot;--docker&quot; sh -s –</span><br></pre></td></tr></table></figure><img src="/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/Picture3-300x44-20210421002853641.png" class="" title="img"><p>To check the installed version, execute the following command</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo kubectl version</span><br></pre></td></tr></table></figure><p>Now, let’s create a pod with a Deepstream SDK sample container and run the sample app.</p><p>Create a pod manifest file with the text editor of your choice. Add the following content to the file:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line"></span><br><span class="line">kind: Pod</span><br><span class="line"></span><br><span class="line">metadata:</span><br><span class="line"></span><br><span class="line">  name: demo-pod</span><br><span class="line"></span><br><span class="line">  labels:</span><br><span class="line"></span><br><span class="line">    name: demo-pod</span><br><span class="line"></span><br><span class="line">spec:</span><br><span class="line"></span><br><span class="line">  hostNetwork: true</span><br><span class="line"></span><br><span class="line">  containers:</span><br><span class="line"></span><br><span class="line">  - name: demo-stream</span><br><span class="line"></span><br><span class="line">    image: nvcr.io/nvidia/deepstream-l4t:5.0-20.07-samples</span><br><span class="line"></span><br><span class="line">    securityContext:</span><br><span class="line"></span><br><span class="line">      privileged: true</span><br><span class="line"></span><br><span class="line">      allowPrivilegeEscalation: true</span><br><span class="line"></span><br><span class="line">    command:</span><br><span class="line"></span><br><span class="line">    - sleep</span><br><span class="line"></span><br><span class="line">    -  &quot;150000&quot;</span><br><span class="line"></span><br><span class="line">    workingDir: /opt/nvidia/deepstream/deepstream-5.0</span><br><span class="line"></span><br><span class="line">    volumeMounts:</span><br><span class="line"></span><br><span class="line">    - mountPath: /tmp/.X11-unix/</span><br><span class="line"></span><br><span class="line">      name: x11</span><br><span class="line"></span><br><span class="line">    - mountPath: /dev/video0</span><br><span class="line"></span><br><span class="line">      name: cam</span><br><span class="line"></span><br><span class="line">  volumes:</span><br><span class="line"></span><br><span class="line">    - name: x11</span><br><span class="line"></span><br><span class="line">      hostPath:</span><br><span class="line"></span><br><span class="line">        path: /tmp/.X11-unix/</span><br><span class="line"></span><br><span class="line">    - name: cam</span><br><span class="line"></span><br><span class="line">      hostPath:</span><br><span class="line"></span><br><span class="line">        path: /dev/video0</span><br></pre></td></tr></table></figure><p>Create the pod using the YAML manifest from the previous step.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo kubectl apply -f pod.yaml</span><br></pre></td></tr></table></figure><p>The pod is using the deepstream-l4t:5.0-20.07-samples container, which needs to be pulled before the container can start.<br>Use the <code>sudo kubectl get pods</code> command to check the pod status. Please wait until it’s running.</p><p>When the pod is deployed and running, sign in and unset the Display variable inside the pod by using the following commands:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo kubectl exec -ti demo-pod /bin/bash unset DISPLAY</span><br></pre></td></tr></table></figure><p>The “unset DISPLAY” command should be run inside the pod.</p><p>To run the sample app inside the pod, use the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deepstream-app -c /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/source1_usb_dec_infer_resnet_int8.txt</span><br></pre></td></tr></table></figure><p>This may take several minutes for the video stream to start.</p><p>Your video analysis app is now using the webcam input end, providing live results on the display, attached to the Jetson Nano board. To quit the app, simply press “q” within the pod.<br>To exit the pod, use the “exit” command.</p><p>To provide the full hardware details of the Jetson Nano, run another pod with the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run -i -t nvidia --image=jitteam/devicequery --restart=Never</span><br></pre></td></tr></table></figure><img src="/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/Picture4-300x207-20210421002853918.png" class="" title="img">]]></content>
      
      
      <categories>
          
          <category> longhorn </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>MountVolume.WaitForAttach failed for volume with “structure needs cleaning”</title>
      <link href="/kubernetes/mountvolume-waitforattach-failed-for-volume-with-structure-needs-cleaning/"/>
      <url>/kubernetes/mountvolume-waitforattach-failed-for-volume-with-structure-needs-cleaning/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/mountvolume-waitforattach-failed-for-volume-with-structure-needs-cleaning/" target="_blank" title="https://www.xtplayer.cn/kubernetes/mountvolume-waitforattach-failed-for-volume-with-structure-needs-cleaning/">https://www.xtplayer.cn/kubernetes/mountvolume-waitforattach-failed-for-volume-with-structure-needs-cleaning/</a></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>MountVolume.WaitForAttach failed for volume with “structure needs cleaning”</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2:02:45 PM Warning Failed Mount</span><br><span class="line">MountVolume.WaitForAttach failed <span class="keyword">for</span> volume <span class="string">&quot;pvc-62c6563e-8cac-11e9-bba9-005056b0bf31&quot;</span> : Heuristic determination of mount point failed:<span class="built_in">stat</span> /var/lib/origin/openshift.local.volumes/plugins/kubernetes.io/iscsi/iface-default/XX.XX.XX.XX:3260-iqn.2016-12.org.gluster-block:3eeb31cf-3289-4052-a690-5831f6aac34a-lun-0: Structure needs cleaning</span><br></pre></td></tr></table></figure><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>Find the block volume:</p><p>From targetcli ls:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">|   o- blockvol_3bf564d2e026d332818b95a8e6c534b2  [vol_233afdbb99425b720167aa3e39bacad2@XX.XX.XX.XX/block-store/3eeb31cf-3289-4052-a690-5831f6aac34a (98.0GiB) activated]</span><br></pre></td></tr></table></figure><hr><p>Mount the block host volume:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># mount -t glusterfs localhost:vol_233afdbb99425b720167aa3e39bacad2 /mnt</span><br></pre></td></tr></table></figure><p>Then</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># cd /mnt # cd block-store # ls 3eeb31cf-3289-4052-a690-5831f6aac34a</span><br></pre></td></tr></table></figure><p>Run xfs_repair on 3eeb31cf-3289-4052-a690-5831f6aac34a, using the “-L’ option if necessary</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># xfs_repair -L 3eeb31cf-3289-4052-a690-5831f6aac34a</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>记一次 controller manager and scheduler unavailable 问题分析</title>
      <link href="/kubernetes/controller-manager-and-scheduler-unavailable/"/>
      <url>/kubernetes/controller-manager-and-scheduler-unavailable/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/controller-manager-and-scheduler-unavailable/" target="_blank" title="https://www.xtplayer.cn/kubernetes/controller-manager-and-scheduler-unavailable/">https://www.xtplayer.cn/kubernetes/controller-manager-and-scheduler-unavailable/</a></p><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>用户反馈经常收到告警提示: controller manager and scheduler unavailable。</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><ol><li><p>controller manager 和 scheduler 都是通过连接 API SERVER 去读写数据，假如 API SERVER 出现异常无法访问，将会影响 controller manager 和 scheduler 运行。</p></li><li><p>API SERVER 运行依赖 ETCD 服务，如果 ETCD 不可访问或者不可读写，那么 API SERVER 也无法向 controller manager 和 scheduler 或者其他连接 API SERVER 的应用提供服务。</p></li><li><p>有很多因素导致 ETCD 服务不可访问或者不可读写，比如：</p><ul><li><p>网络断开或者网络闪断；</p></li><li><p>三个 ETCD 节点丢失三个，最后一个节点将变成只读模式；</p></li><li><p>多个 ETCD 实例之间会通过 2380 端口通信来选举 leader，并且彼此保持心跳检测。如果节点负载增加导致 ETCD 心跳检测响应延迟，超过预定的心跳超时时间后会进行 leader 的重新选举，选举时候将会出现 ETCD 服务不可用。</p></li></ul></li></ol><h2 id="K8S-组件检查"><a href="#K8S-组件检查" class="headerlink" title="K8S 组件检查"></a>K8S 组件检查</h2><p>根据上面的问题分析，接下来查看 K8S 服务组件的日志来进一步分析。</p><p><strong>kube-scheduler</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">E0321 22:47:48.503411       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-scheduler: Get https://127.0.0.1:6443/api/v1/namespaces/kube-system/endpoints/kube-scheduler?<span class="built_in">timeout</span>=10s: context deadline exceeded (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</span><br><span class="line">E0321 22:51:57.230575       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-scheduler: Get https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?<span class="built_in">timeout</span>=10s: context deadline exceeded</span><br><span class="line">E0321 22:54:39.196072       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-scheduler: Get https://127.0.0.1:6443/api/v1/namespaces/kube-system/endpoints/kube-scheduler?<span class="built_in">timeout</span>=10s: net/http: request canceled (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</span><br></pre></td></tr></table></figure><p><strong>kube-controller-manager</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">E0321 22:47:50.884336       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-controller-manager: Get https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?<span class="built_in">timeout</span>=10s: net/http: request canceled (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</span><br><span class="line">I0321 22:47:53.739694       1 leaderelection.go:252] successfully acquired lease kube-system/kube-controller-manager</span><br><span class="line">I0321 22:47:53.739746       1 event.go:278] Event(v1.ObjectReference&#123;Kind:<span class="string">&quot;Endpoints&quot;</span>, Namespace:<span class="string">&quot;kube-system&quot;</span>, Name:<span class="string">&quot;k</span></span><br></pre></td></tr></table></figure><p>根据以上信息可以看出，在 UTC 0322 20:55 左右，出现了连接 API SERVER 超时的情况，接下来查看 API SERVER 服务日志。</p><p><strong>kube-apiserver</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">E0321 22:47:41.289207       1 authentication.go:53] Unable to authenticate the request due to an error: [invalid bearer token, context canceled]</span><br><span class="line">E0321 22:47:48.503439       1 status.go:71] apiserver received an error that is not an metav1.Status: &amp;errors.errorString&#123;s:<span class="string">&quot;context canceled&quot;</span>&#125;</span><br><span class="line">I0321 22:47:48.503648       1 trace.go:116] Trace[1655495025]: <span class="string">&quot;Get&quot;</span> url:/api/v1/namespaces/kube-system/endpoints/kube-scheduler,user-agent:kube-scheduler/v1.18.10 (linux/amd64) kubernetes/62876fc/leader-election,client:127.0.0.1 (started: 2021-03-21 22:47:38.517639233 +0000 UTC m=+1182359.771988386) (total time: 9.985984157s):</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">Trace[723022686]: [21.463074932s] [21.463070777s] About to write a response</span><br><span class="line">W0321 22:52:07.981746       1 lease.go:224] Resetting endpoints <span class="keyword">for</span> master service <span class="string">&quot;kubernetes&quot;</span> to [10.151.130.137 10.151.130.138 10.151.130.139]</span><br><span class="line">E0321 22:54:34.697599       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError&#123;code:0xe, desc:<span class="string">&quot;etcdserver: leader changed&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>根据以上信息，可以确定因为 ETCD leader changed 导致 controller manager and scheduler unavailable。</p><h2 id="ETCD-检查"><a href="#ETCD-检查" class="headerlink" title="ETCD 检查"></a>ETCD 检查</h2><p>在 API SERVER 日志中查看到了 <code>etcdserver: leader changed</code> 的提示，接下来检查 ETCD 服务日志来具体分析。</p><p><strong>ETCD</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2021-03-21 22:47:38.503291 W | rafthttp: lost the TCP streaming connection with peer 496ad58099e4ad83 (stream Message reader)</span><br><span class="line">2021-03-21 22:47:38.503313 E | rafthttp: failed to <span class="built_in">read</span> 496ad58099e4ad83 on stream Message (<span class="built_in">read</span> tcp 10.151.130.138:51268-&gt;10.151.130.139:2380: i/o <span class="built_in">timeout</span>)</span><br><span class="line">2021-03-21 22:47:38.503317 I | rafthttp: peer 496ad58099e4ad83 became inactive (message send to peer failed)</span><br><span class="line">2021-03-21 22:47:38.503628 W | rafthttp: lost the TCP streaming connection with peer ee17b2c8bf87d608 (stream Message reader)</span><br><span class="line">2021-03-21 22:47:38.503641 E | rafthttp: failed to <span class="built_in">read</span> ee17b2c8bf87d608 on stream Message (<span class="built_in">read</span> tcp 10.151.130.138:34276-&gt;10.151.130.137:2380: i/o <span class="built_in">timeout</span>)</span><br><span class="line">2021-03-21 22:47:38.503645 I | rafthttp: peer ee17b2c8bf87d608 became inactive (message send to peer failed)</span><br><span class="line">2021-03-21 22:47:38.517019 W | rafthttp: lost the TCP streaming connection with peer ee17b2c8bf87d608 (stream MsgApp v2 reader)</span><br><span class="line">2021-03-21 22:47:38.521398 W | rafthttp: lost the TCP streaming connection with peer 496ad58099e4ad83 (stream MsgApp v2 reader)</span><br><span class="line">2021-03-21 22:47:38.531432 I | rafthttp: peer ee17b2c8bf87d608 became active</span><br><span class="line">2021-03-21 22:47:38.531455 I | rafthttp: established a TCP streaming connection with peer ee17b2c8bf87d608 (stream MsgApp v2 reader)</span><br><span class="line">2021-03-21 22:47:38.531461 I | rafthttp: peer 496ad58099e4ad83 became active</span><br><span class="line">2021-03-21 22:47:38.531474 I | rafthttp: established a TCP streaming connection with peer 496ad58099e4ad83 (stream MsgApp v2 reader)</span><br><span class="line">2021-03-21 22:47:38.531599 I | rafthttp: established a TCP streaming connection with peer 496ad58099e4ad83 (stream Message reader)</span><br><span class="line">2021-03-21 22:47:38.533011 I | rafthttp: established a TCP streaming connection with peer ee17b2c8bf87d608 (stream Message reader)</span><br><span class="line">2021-03-21 22:47:38.631853 E | rafthttp: failed to write ee17b2c8bf87d608 on stream Message (write tcp 10.151.130.138:2380-&gt;10.151.130.137:36700: write: connection reset by peer)</span><br><span class="line">2021-03-21 22:47:38.631869 I | rafthttp: peer ee17b2c8bf87d608 became inactive (message send to peer failed)</span><br><span class="line">2021-03-21 22:47:38.632307 W | rafthttp: lost the TCP streaming connection with peer ee17b2c8bf87d608 (stream Message writer)</span><br><span class="line">2021-03-21 22:47:38.634600 I | rafthttp: peer ee17b2c8bf87d608 became active</span><br><span class="line">2021-03-21 22:47:40.880842 W | rafthttp: closed an existing TCP streaming connection with peer 496ad58099e4ad83 (stream Message writer)</span><br><span class="line">2021-03-21 22:47:40.880863 I | rafthttp: established a TCP streaming connection with peer 496ad58099e4ad83 (stream Message writer)</span><br><span class="line">2021-03-21 22:47:40.881069 W | rafthttp: closed an existing TCP streaming connection with peer ee17b2c8bf87d608 (stream MsgApp v2 writer)</span><br><span class="line">2021-03-21 22:47:40.881086 I | rafthttp: established a TCP streaming connection with peer ee17b2c8bf87d608 (stream MsgApp v2 writer)</span><br><span class="line">2021-03-21 22:47:40.882339 W | rafthttp: closed an existing TCP streaming connection with peer 496ad58099e4ad83 (stream MsgApp v2 writer)</span><br><span class="line">2021-03-21 22:47:40.882354 I | rafthttp: established a TCP streaming connection with peer 496ad58099e4ad83 (stream MsgApp v2 writer)</span><br><span class="line">2021-03-21 22:47:41.425089 I | rafthttp: established a TCP streaming connection with peer ee17b2c8bf87d608 (stream Message writer)</span><br></pre></td></tr></table></figure><p>通过 ETCD 日志可以确定，在 UTC 2021-03-21 22:47:38 出现了  lost the TCP streaming connection with peer 。</p><h2 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h2><ul><li><p>根据 ETCD 日志提示，一开始怀疑是网络闪断导致 ETCD peer 心跳超时，然后查看外部交换机网络监控，并通过工具去定时检查 ETCD 2379 和 2380 端口的健康检查，经过几天的观察未发现网络连通性异常。</p></li><li><p>通过查看系统磁盘监控，发现在告警发生的时间段，磁盘 IO 有明显的波动。</p></li><li><p>通过沟通，了解到虚拟机每天有定时快照，并且时间正好在 UTC 时间晚上 22 点 50 左右。</p></li></ul><h2 id="后续观察"><a href="#后续观察" class="headerlink" title="后续观察"></a>后续观察</h2><p>经过沟通，将定时快照的时间提前了 2 小时。根据观察告警时间以及 ETCD 出现 lost the TCP streaming connection with peer 时间相应的提前了两小时。</p><p><strong>kube-scheduler</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">E0322 20:48:20.871852       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-scheduler: Get https://127.0.0.1:6443/api/v1/namespaces/kube-system/endpoints/kube-scheduler?<span class="built_in">timeout</span>=10s: context deadline exceeded (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</span><br><span class="line">I0322 20:48:31.622309       1 leaderelection.go:252] successfully acquired lease kube-system/kube-scheduler</span><br><span class="line">E0322 20:52:34.275658       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-scheduler: etcdserver: leader changed</span><br><span class="line">E0322 20:55:12.950234       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-scheduler: Get https://127.0.0.1:6443/api/v1/namespaces/kube-system/endpoints/kube-scheduler?<span class="built_in">timeout</span>=10s: context deadline exceeded (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</span><br><span class="line">I0322 20:55:12.950297       1 leaderelection.go:277] failed to renew lease kube-system/kube-scheduler: timed out waiting <span class="keyword">for</span> the condition</span><br><span class="line">F0322 20:55:12.950314       1 server.go:244] leaderelection lost</span><br></pre></td></tr></table></figure><p><strong>kube-controller-manager</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">E0322 20:48:14.476644       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-controller-manager: etcdserver: leader changed</span><br><span class="line">E0322 20:48:28.187307       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-controller-manager: Get https://127.0.0.1:6443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager?<span class="built_in">timeout</span>=10s: context deadline exceeded (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</span><br><span class="line">E0322 20:52:41.226524       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-controller-manager: Get https://127.0.0.1:6443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager?<span class="built_in">timeout</span>=10s: net/http: request canceled (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</span><br><span class="line">E0322 20:55:12.902921       1 leaderelection.go:320] error retrieving resource lock kube-system/kube-controller-manager: Get https://127.0.0.1:6443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager?<span class="built_in">timeout</span>=10s: context deadline exceeded (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</span><br></pre></td></tr></table></figure><p><strong>kube-apiserver</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">E0322 20:48:29.475790       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError&#123;code:0xe, desc:<span class="string">&quot;etcdserver: request timed out&quot;</span>&#125;</span><br><span class="line">E0322 20:48:29.475844       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError&#123;code:0xe, desc:<span class="string">&quot;etcdserver: request timed out&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">E0322 20:52:34.274857       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError&#123;code:0xe, desc:<span class="string">&quot;etcdserver: leader changed&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p><strong>ETCD</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2021-03-22 20:30:46.088174 I | mvcc: finished scheduled compaction at 23035178 (took 26.164333ms)</span><br><span class="line">2021-03-22 20:34:52.330573 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/persistentvolumeclaims/default/nginx-logs-pvc\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:1405&quot;</span> took too long (1.669619791s) to execute</span><br><span class="line">2021-03-22 20:34:52.332649 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/services/endpoints/kube-system/kube-controller-manager\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:615&quot;</span> took too long (1.471652409s) to execute</span><br><span class="line">2021-03-22 20:34:52.333036 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/services/endpoints/kube-system/kube-scheduler\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:588&quot;</span> took too long (217.627474ms) to execute</span><br><span class="line">2021-03-22 20:34:52.333058 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/resourcequotas/\&quot; range_end:\&quot;/registry/resourcequotas0\&quot; count_only:true &quot;</span> with result <span class="string">&quot;range_response_count:0 size:9&quot;</span> took too long (630.275951ms) to execute</span><br><span class="line">2021-03-22 20:34:52.333081 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/services/endpoints/local-path-storage/rancher.io-local-path\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:653&quot;</span> took too long (394.73969ms) to execute</span><br><span class="line">2021-03-22 20:34:52.333308 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/services/specs/default/kubernetes\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:616&quot;</span> took too long (430.624112ms) to execute</span><br><span class="line">2021-03-22 20:35:46.067308 I | mvcc: store.index: compact 23036390</span><br><span class="line">2021-03-22 20:35:46.093276 I | mvcc: finished scheduled compaction at 23036390 (took 25.599134ms)</span><br><span class="line">2021-03-22 20:40:46.077355 I | mvcc: store.index: compact 23037604</span><br><span class="line">2021-03-22 20:40:46.104071 I | mvcc: finished scheduled compaction at 23037604 (took 26.305942ms)</span><br><span class="line">2021-03-22 20:43:58.958177 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/services/endpoints/kube-system/kube-controller-manager\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:615&quot;</span> took too long (702.184455ms) to execute</span><br><span class="line">2021-03-22 20:43:58.959252 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/leases/kube-system/kube-scheduler\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:491&quot;</span> took too long (687.254ms) to execute</span><br><span class="line">2021-03-22 20:43:58.959275 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/leases/kube-system/kube-controller-manager\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:518&quot;</span> took too long (651.871463ms) to execute</span><br><span class="line">2021-03-22 20:43:58.959363 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/events/\&quot; range_end:\&quot;/registry/events0\&quot; count_only:true &quot;</span> with result <span class="string">&quot;range_response_count:0 size:9&quot;</span> took too long (647.299563ms) to execute</span><br><span class="line">2021-03-22 20:45:46.088764 I | mvcc: store.index: compact 23038829</span><br><span class="line">2021-03-22 20:45:46.117371 I | mvcc: finished scheduled compaction at 23038829 (took 28.090255ms)</span><br><span class="line">2021-03-22 20:48:13.545676 W | rafthttp: lost the TCP streaming connection with peer 4bd420be9a532df8 (stream Message reader)</span><br><span class="line">2021-03-22 20:48:13.545703 E | rafthttp: failed to <span class="built_in">read</span> 4bd420be9a532df8 on stream Message (<span class="built_in">read</span> tcp 10.151.130.137:54864-&gt;10.151.130.138:2380: i/o <span class="built_in">timeout</span>)</span><br><span class="line">2021-03-22 20:48:13.545707 I | rafthttp: peer 4bd420be9a532df8 became inactive (message send to peer failed)</span><br><span class="line">2021-03-22 20:48:13.549798 W | rafthttp: lost the TCP streaming connection with peer 4bd420be9a532df8 (stream MsgApp v2 reader)</span><br><span class="line">raft2021/03/22 20:48:14 INFO: ee17b2c8bf87d608 [term: 164] received a MsgVote message with higher term from 496ad58099e4ad83 [term: 165]</span><br><span class="line">raft2021/03/22 20:48:14 INFO: ee17b2c8bf87d608 became follower at term 165</span><br><span class="line">raft2021/03/22 20:48:14 INFO: ee17b2c8bf87d608 [logterm: 164, index: 29128056, vote: 0] cast MsgVote <span class="keyword">for</span> 496ad58099e4ad83 [logterm: 164, index: 29128056] at term 165</span><br><span class="line">raft2021/03/22 20:48:14 INFO: raft.node: ee17b2c8bf87d608 lost leader 4bd420be9a532df8 at term 165</span><br><span class="line">raft2021/03/22 20:48:14 INFO: raft.node: ee17b2c8bf87d608 elected leader 496ad58099e4ad83 at term 165</span><br><span class="line">2021-03-22 20:48:14.475506 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/services/endpoints/kube-system/kube-controller-manager\&quot; &quot;</span> with result <span class="string">&quot;error:etcdserver: leader changed&quot;</span> took too long (5.299921652s) to execute</span><br><span class="line">2021-03-22 20:48:14.477440 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/services/endpoints/local-path-storage/rancher.io-local-path\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:653&quot;</span> took too long (4.456789691s) to execute</span><br><span class="line">2021-03-22 20:48:14.477462 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/storageclasses/\&quot; range_end:\&quot;/registry/storageclasses0\&quot; count_only:true &quot;</span> with result <span class="string">&quot;range_response_count:0 size:9&quot;</span> took too long (3.623175886s) to execute</span><br><span class="line">2021-03-22 20:48:14.477539 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/networkpolicies/\&quot; range_end:\&quot;/registry/networkpolicies0\&quot; count_only:true &quot;</span> with result <span class="string">&quot;range_response_count:0 size:7&quot;</span> took too long (478.668022ms) to execute</span><br><span class="line">2021-03-22 20:48:14.477622 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/rolebindings/\&quot; range_end:\&quot;/registry/rolebindings0\&quot; count_only:true &quot;</span> with result <span class="string">&quot;range_response_count:0 size:9&quot;</span> took too long (1.244980828s) to execute</span><br><span class="line">2021-03-22 20:48:14.477732 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/apiregistration.k8s.io/apiservices/\&quot; range_end:\&quot;/registry/apiregistration.k8s.io/apiservices0\&quot; count_only:true &quot;</span> with result <span class="string">&quot;range_response_count:0 size:9&quot;</span> took too long (2.397168285s) to execute</span><br><span class="line">2021-03-22 20:48:16.618038 I | rafthttp: peer 4bd420be9a532df8 became active</span><br><span class="line">2021-03-22 20:48:16.618059 I | rafthttp: established a TCP streaming connection with peer 4bd420be9a532df8 (stream MsgApp v2 reader)</span><br><span class="line">2021-03-22 20:48:16.624716 W | rafthttp: closed an existing TCP streaming connection with peer 4bd420be9a532df8 (stream MsgApp v2 writer)</span><br><span class="line">2021-03-22 20:48:16.624736 I | rafthttp: established a TCP streaming connection with peer 4bd420be9a532df8 (stream MsgApp v2 writer)</span><br><span class="line">2021-03-22 20:48:16.624878 I | rafthttp: established a TCP streaming connection with peer 4bd420be9a532df8 (stream Message reader)</span><br><span class="line">2021-03-22 20:48:16.627175 W | rafthttp: closed an existing TCP streaming connection with peer 4bd420be9a532df8 (stream Message writer)</span><br><span class="line">2021-03-22 20:48:16.627188 I | rafthttp: established a TCP streaming connection with peer 4bd420be9a532df8 (stream Message writer)</span><br><span class="line">WARNING: 2021/03/22 20:48:23 grpc: Server.processUnaryRPC failed to write status: connection error: desc = <span class="string">&quot;transport is closing&quot;</span></span><br><span class="line">2021-03-22 20:50:46.088074 I | mvcc: store.index: compact 23040043</span><br></pre></td></tr></table></figure><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>通过商议，决定取消 ETCD 虚拟机的定时快照。经过一天运行，ETCD 运行日志中没有再出现  lost the TCP streaming connection with peer 。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2021-03-23 19:58:04.190676 I | etcdserver: saved snapshot at index 29600607</span><br><span class="line">2021-03-23 19:58:04.190832 I | etcdserver: compacted raft <span class="built_in">log</span> at 29595607</span><br><span class="line">2021-03-23 19:58:33.388856 I | pkg/fileutil: purged file /var/lib/rancher/etcd/member/snap/00000000000000a7-0000000001bfdaca.snap successfully</span><br><span class="line">2021-03-23 20:00:48.065975 I | mvcc: store.index: compact 23379302</span><br><span class="line">2021-03-23 20:00:48.091487 I | mvcc: finished scheduled compaction at 23379302 (took 25.110115ms)</span><br><span class="line">2021-03-23 20:02:39.378543 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/configmaps/ingress-nginx/ingress-controller-leader-nginx\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:607&quot;</span> took too long (117.791832ms) to execute</span><br><span class="line">2021-03-23 20:05:48.073024 I | mvcc: store.index: compact 23380519</span><br><span class="line">2021-03-23 20:05:48.100228 I | mvcc: finished scheduled compaction at 23380519 (took 26.693346ms)</span><br><span class="line">2021-03-23 20:10:48.079058 I | mvcc: store.index: compact 23381735</span><br><span class="line">2021-03-23 20:10:48.106893 I | mvcc: finished scheduled compaction at 23381735 (took 27.437585ms)</span><br><span class="line">2021-03-23 20:15:30.155491 I | wal: segmented wal file /var/lib/rancher/etcd/member/wal/000000000000015a-0000000001c3c1fc.wal is created</span><br><span class="line">2021-03-23 20:15:33.414322 I | pkg/fileutil: purged file /var/lib/rancher/etcd/member/wal/0000000000000155-0000000001bcc554.wal successfully</span><br><span class="line">2021-03-23 20:15:48.095615 I | mvcc: store.index: compact 23382954</span><br><span class="line">2021-03-23 20:15:48.122662 I | mvcc: finished scheduled compaction at 23382954 (took 26.68459ms)</span><br><span class="line">2021-03-23 20:20:48.092179 I | mvcc: store.index: compact 23384166</span><br><span class="line">2021-03-23 20:20:48.118647 I | mvcc: finished scheduled compaction at 23384166 (took 26.050565ms)</span><br><span class="line">2021-03-23 20:25:48.097468 I | mvcc: store.index: compact 23385389</span><br><span class="line">2021-03-23 20:25:48.125829 I | mvcc: finished scheduled compaction at 23385389 (took 28.006118ms)</span><br><span class="line">2021-03-23 20:30:48.103474 I | mvcc: store.index: compact 23386608</span><br><span class="line">2021-03-23 20:30:48.131849 I | mvcc: finished scheduled compaction at 23386608 (took 27.984451ms)</span><br><span class="line">2021-03-23 20:35:48.118190 I | mvcc: store.index: compact 23387819</span><br><span class="line">2021-03-23 20:35:48.144807 I | mvcc: finished scheduled compaction at 23387819 (took 26.220415ms)</span><br><span class="line">2021-03-23 20:40:48.124117 I | mvcc: store.index: compact 23389036</span><br><span class="line">2021-03-23 20:40:48.150120 I | mvcc: finished scheduled compaction at 23389036 (took 25.643958ms)</span><br><span class="line">2021-03-23 20:45:48.130181 I | mvcc: store.index: compact 23390248</span><br><span class="line">2021-03-23 20:45:48.156873 I | mvcc: finished scheduled compaction at 23390248 (took 26.303108ms)</span><br><span class="line">2021-03-23 20:50:48.136542 I | mvcc: store.index: compact 23391473</span><br><span class="line">2021-03-23 20:50:48.168088 I | mvcc: finished scheduled compaction at 23391473 (took 31.146475ms)</span><br><span class="line">2021-03-23 20:55:48.141884 I | mvcc: store.index: compact 23392692</span><br><span class="line">2021-03-23 20:55:48.168249 I | mvcc: finished scheduled compaction at 23392692 (took 25.983679ms)</span><br><span class="line">2021-03-23 21:00:48.148486 I | mvcc: store.index: compact 23393921</span><br><span class="line">2021-03-23 21:00:48.175167 I | mvcc: finished scheduled compaction at 23393921 (took 26.323769ms)</span><br><span class="line">2021-03-23 21:05:48.153553 I | mvcc: store.index: compact 23395139</span><br><span class="line">2021-03-23 21:05:48.179843 I | mvcc: finished scheduled compaction at 23395139 (took 25.890462ms)</span><br><span class="line">2021-03-23 21:10:48.159706 I | mvcc: store.index: compact 23396363</span><br><span class="line">2021-03-23 21:10:48.186493 I | mvcc: finished scheduled compaction at 23396363 (took 26.422307ms)</span><br><span class="line">2021-03-23 21:15:48.165114 I | mvcc: store.index: compact 23397590</span><br><span class="line">2021-03-23 21:15:48.191193 I | mvcc: finished scheduled compaction at 23397590 (took 25.715209ms)</span><br><span class="line">2021-03-23 21:20:48.171112 I | mvcc: store.index: compact 23398807</span><br><span class="line">2021-03-23 21:20:48.198116 I | mvcc: finished scheduled compaction at 23398807 (took 26.516055ms)</span><br><span class="line">2021-03-23 21:25:48.176894 I | mvcc: store.index: compact 23400025</span><br><span class="line">2021-03-23 21:25:48.205417 I | mvcc: finished scheduled compaction at 23400025 (took 28.06852ms)</span><br><span class="line">2021-03-23 21:30:48.182368 I | mvcc: store.index: compact 23401248</span><br><span class="line">2021-03-23 21:30:48.208162 I | mvcc: finished scheduled compaction at 23401248 (took 25.359328ms)</span><br><span class="line">2021-03-23 21:35:48.188427 I | mvcc: store.index: compact 23402496</span><br><span class="line">2021-03-23 21:35:48.214977 I | mvcc: finished scheduled compaction at 23402496 (took 26.130896ms)</span><br><span class="line">2021-03-23 21:40:48.203757 I | mvcc: store.index: compact 23403714</span><br><span class="line">2021-03-23 21:40:48.230346 I | mvcc: finished scheduled compaction at 23403714 (took 26.234451ms)</span><br><span class="line">2021-03-23 21:45:48.203085 I | mvcc: store.index: compact 23404944</span><br><span class="line">2021-03-23 21:45:48.232110 I | mvcc: finished scheduled compaction at 23404944 (took 28.66739ms)</span><br><span class="line">2021-03-23 21:50:48.211722 I | mvcc: store.index: compact 23406161</span><br><span class="line">2021-03-23 21:50:48.238941 I | mvcc: finished scheduled compaction at 23406161 (took 26.80059ms)</span><br><span class="line">2021-03-23 21:55:48.221035 I | mvcc: store.index: compact 23407381</span><br><span class="line">2021-03-23 21:55:48.248858 I | mvcc: finished scheduled compaction at 23407381 (took 27.422759ms)</span><br><span class="line">2021-03-23 22:00:48.229789 I | mvcc: store.index: compact 23408597</span><br><span class="line">2021-03-23 22:00:48.257402 I | mvcc: finished scheduled compaction at 23408597 (took 27.233652ms)</span><br><span class="line">2021-03-23 22:05:48.243738 I | mvcc: store.index: compact 23409819</span><br><span class="line">2021-03-23 22:05:48.269861 I | mvcc: finished scheduled compaction at 23409819 (took 25.728754ms)</span><br><span class="line">2021-03-23 22:10:48.249241 I | mvcc: store.index: compact 23411044</span><br><span class="line">2021-03-23 22:10:48.275749 I | mvcc: finished scheduled compaction at 23411044 (took 26.150918ms)</span><br><span class="line">2021-03-23 22:15:48.254890 I | mvcc: store.index: compact 23412261</span><br><span class="line">2021-03-23 22:15:48.281103 I | mvcc: finished scheduled compaction at 23412261 (took 25.838699ms)</span><br><span class="line">2021-03-23 22:20:13.800090 W | etcdserver: read-only range request <span class="string">&quot;key:\&quot;/registry/ranges/serviceips\&quot; &quot;</span> with result <span class="string">&quot;range_response_count:1 size:8223&quot;</span> took too long (208.151939ms) to execute</span><br><span class="line">2021-03-23 22:20:48.264931 I | mvcc: store.index: compact 23413481</span><br><span class="line">2021-03-23 22:20:48.290955 I | mvcc: finished scheduled compaction at 23413481 (took 25.550081ms)</span><br><span class="line">2021-03-23 22:25:38.543370 I | etcdserver: start to snapshot (applied: 29650608, lastsnap: 29600607)</span><br><span class="line">2021-03-23 22:25:38.547640 I | etcdserver: saved snapshot at index 29650608</span><br><span class="line">2021-03-23 22:25:38.547810 I | etcdserver: compacted raft <span class="built_in">log</span> at 29645608</span><br><span class="line">2021-03-23 22:25:48.270855 I | mvcc: store.index: compact 23414700</span><br><span class="line">2021-03-23 22:25:48.298973 I | mvcc: finished scheduled compaction at 23414700 (took 27.738717ms)</span><br><span class="line">2021-03-23 22:26:03.434776 I | pkg/fileutil: purged file /var/lib/rancher/etcd/member/snap/00000000000000a7-0000000001c09e1b.snap successfully</span><br><span class="line">2021-03-23 22:30:48.275888 I | mvcc: store.index: compact 23415924</span><br><span class="line">2021-03-23 22:30:48.305413 I | mvcc: finished scheduled compaction at 23415924 (took 29.128337ms)</span><br><span class="line">2021-03-23 22:35:48.280997 I | mvcc: store.index: compact 23417141</span><br><span class="line">2021-03-23 22:35:48.307847 I | mvcc: finished scheduled compaction at 23417141 (took 26.42452ms)</span><br><span class="line">2021-03-23 22:40:48.287720 I | mvcc: store.index: compact 23418356</span><br><span class="line">2021-03-23 22:40:48.317291 I | mvcc: finished scheduled compaction at 23418356 (took 29.183932ms)</span><br><span class="line">2021-03-23 22:45:48.295599 I | mvcc: store.index: compact 23419585</span><br><span class="line">2021-03-23 22:45:48.322346 I | mvcc: finished scheduled compaction at 23419585 (took 26.370195ms)</span><br><span class="line">2021-03-23 22:50:48.301478 I | mvcc: store.index: compact 23420817</span><br><span class="line">2021-03-23 22:50:48.327555 I | mvcc: finished scheduled compaction at 23420817 (took 25.656891ms)</span><br><span class="line">2021-03-23 22:55:48.307901 I | mvcc: store.index: compact 23422034</span><br><span class="line">2021-03-23 22:55:48.337474 I | mvcc: finished scheduled compaction at 23422034 (took 29.204046ms)</span><br><span class="line">2021-03-23 23:00:48.314532 I | mvcc: store.index: compact 23423249</span><br><span class="line">2021-03-23 23:00:48.340979 I | mvcc: finished scheduled compaction at 23423249 (took 26.109985ms)</span><br><span class="line">2021-03-23 23:05:48.319811 I | mvcc: store.index: compact 23424469</span><br><span class="line">2021-03-23 23:05:48.346462 I | mvcc: finished scheduled compaction at 23424469 (took 26.255207ms)</span><br><span class="line">2021-03-23 23:10:48.326553 I | mvcc: store.index: compact 23425683</span><br><span class="line">2021-03-23 23:10:48.353562 I | mvcc: finished scheduled compaction at 23425683 (took 26.632973ms)</span><br><span class="line">2021-03-23 23:15:48.333217 I | mvcc: store.index: compact 23426896</span><br><span class="line">2021-03-23 23:15:48.359319 I | mvcc: finished scheduled compaction at 23426896 (took 25.72745ms)</span><br><span class="line">2021-03-23 23:20:48.339558 I | mvcc: store.index: compact 23428115</span><br><span class="line">2021-03-23 23:20:48.366617 I | mvcc: finished scheduled compaction at 23428115 (took 26.658721ms)</span><br><span class="line">2021-03-23 23:25:48.345722 I | mvcc: store.index: compact 23429337</span><br><span class="line">2021-03-23 23:25:48.373084 I | mvcc: finished scheduled compaction at 23429337 (took 27.017076ms)</span><br><span class="line">2021-03-23 23:30:48.351550 I | mvcc: store.index: compact 23430561</span><br><span class="line">2021-03-23 23:30:48.378308 I | mvcc: finished scheduled compaction at 23430561 (took 26.3218ms)</span><br><span class="line">2021-03-23 23:35:48.356635 I | mvcc: store.index: compact 23431805</span><br><span class="line">2021-03-23 23:35:48.382235 I | mvcc: finished scheduled compaction at 23431805 (took 25.195416ms)</span><br><span class="line">2021-03-23 23:40:48.362534 I | mvcc: store.index: compact 23433019</span><br><span class="line">2021-03-23 23:40:48.388744 I | mvcc: finished scheduled compaction at 23433019 (took 25.639283ms)</span><br><span class="line">2021-03-23 23:45:48.368555 I | mvcc: store.index: compact 23434235</span><br><span class="line">2021-03-23 23:45:48.395256 I | mvcc: finished scheduled compaction at 23434235 (took 26.290222ms)</span><br><span class="line">2021-03-23 23:50:48.373567 I | mvcc: store.index: compact 23435452</span><br><span class="line">2021-03-23 23:50:48.405420 I | mvcc: finished scheduled compaction at 23435452 (took 31.411081ms)</span><br><span class="line">2021-03-23 23:55:48.379145 I | mvcc: store.index: compact 23436674</span><br><span class="line">2021-03-23 23:55:48.405956 I | mvcc: finished scheduled compaction at 23436674 (took 26.306194ms)</span><br><span class="line">2021-03-24 00:00:48.384095 I | mvcc: store.index: compact 23437902</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>变更 Rancher Server IP 或域名</title>
      <link href="/rancher/replace-ip-domain/"/>
      <url>/rancher/replace-ip-domain/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/replace-ip-domain/" target="_blank" title="https://www.xtplayer.cn/rancher/replace-ip-domain/">https://www.xtplayer.cn/rancher/replace-ip-domain/</a></p><h2 id="准备全部集群的直连-kubeconfig-配置文件"><a href="#准备全部集群的直连-kubeconfig-配置文件" class="headerlink" title="准备全部集群的直连 kubeconfig 配置文件"></a>准备全部集群的直连 kubeconfig 配置文件</h2><p>默认情况，在 Rancher UI 上复制的 kubeconfig 是通过 <code>cluster agent</code> 代理连接到 K8S 集群的。在变更 SSL 证书后，因为一些参数发送变化，需要通过 <code>kubectl</code> 命令行去修改配置。在变更 SSL 证书后会导致 <code>cluster agent</code> 无法连接 Rancher server，从而导致 <code>kubectl</code> 无法使用 Rancher UI 上复制的 kubeconfig 去操作 K8S 集群。因此，建议在做域名或 IP 变更之前，准备好所有集群的直连 kubeconfig 配置文件。具体可参考：<a href="/rancher/restore-kubecfg">恢复 kubectl 配置文件</a></p><p>提示</p><ol><li><p>2.1.x 以前的版本，可在 Master 节点的 <code>/etc/kubernetes/.tmp</code> 路径下找到 <code>kubecfg-kube-admin.yml</code>，这个是具有集群管理员权限的直连 kubeconfig 配置文件；</p><img src="/rancher/replace-ip-domain/image-20190309173154246.52ca6150.png" class="" title="image-20190309173154246"></li><li><p>操作之前备份 Rancher Server</p></li></ol><h2 id="准备证书"><a href="#准备证书" class="headerlink" title="准备证书"></a>准备证书</h2><p>SSL 证书与<code>域名或 IP</code> 有绑定关系，客户端在访问 Server 端时，ssl 证书用于客户端访问地址的检验作用。客户端通过<code>域名或 IP</code> 访问 Server 端时，会先进行 SSL 证书验证，如果客户端访问的 <code>IP 或域名</code>与 SSL 证书中预先绑定的 <code>IP 或域名</code>不一致，那么 SSL 可以认为 Server 端是伪造了，SSL 验证无法通过从而导致客户端无法连接 Server 端。</p><p>因此，如果更换了 Server 端的 <code>IP 或域名</code>，一般就会涉及到 SSL 证书更换，除非在最开始生成 SSL 证书的时候加入了后期可能需要更换的 <code>IP 或域名</code>。</p><ul><li><p>自签名 ssl 证书</p><p>复制以下代码另存为 <code>create_self-signed-cert.sh</code> 或者其他您喜欢的文件名。修改代码开头的 <code>CN</code>(域名)，如果需要使用 ip 去访问 Rancher Server，那么需要给 ssl 证书添加扩展 IP，多个 IP 用逗号隔开。如果想实现多个域名访问 Rancher Server，则添加扩展域名(SSL_DNS),多个 <code>SSL_DNS</code> 用逗号隔开。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash -e</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">help</span></span> ()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; ================================================================ &#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-domain: 生成 ssl 证书需要的主域名，如不指定则默认为 www.rancher.local，如果是 ip 访问服务，则可忽略；&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-trusted-ip: 一般 ssl 证书只信任域名的访问请求，有时候需要使用 ip 去访问 server，那么需要给 ssl 证书添加扩展 IP，多个 IP 用逗号隔开；&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-trusted-domain: 如果想多个域名访问，则添加扩展域名（SSL_TRUSTED_DOMAIN）,多个扩展域名用逗号隔开；&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-size: ssl 加密位数，默认 2048；&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-cn: 国家代码(2 个字母的代号),默认 CN;&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; 使用示例:&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; ./create_self-signed-cert.sh --ssl-domain=www.test.com --ssl-trusted-domain=www.test2.com \ &#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-trusted-ip=1.1.1.1,2.2.2.2,3.3.3.3 --ssl-size=2048 --ssl-date=3650&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; ================================================================&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&quot;<span class="variable">$1</span>&quot;</span> <span class="keyword">in</span></span><br><span class="line">    -h|--<span class="built_in">help</span>) <span class="built_in">help</span>; <span class="built_in">exit</span>;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$1</span> == <span class="string">&#x27;&#x27;</span> ]];<span class="keyword">then</span></span><br><span class="line">    <span class="built_in">help</span>;</span><br><span class="line">    <span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">CMDOPTS=<span class="string">&quot;$*&quot;</span></span><br><span class="line"><span class="keyword">for</span> OPTS <span class="keyword">in</span> <span class="variable">$CMDOPTS</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    key=$(<span class="built_in">echo</span> <span class="variable">$&#123;OPTS&#125;</span> | awk -F<span class="string">&quot;=&quot;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line">    value=$(<span class="built_in">echo</span> <span class="variable">$&#123;OPTS&#125;</span> | awk -F<span class="string">&quot;=&quot;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span> )</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;<span class="variable">$key</span>&quot;</span> <span class="keyword">in</span></span><br><span class="line">        --ssl-domain) SSL_DOMAIN=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-trusted-ip) SSL_TRUSTED_IP=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-trusted-domain) SSL_TRUSTED_DOMAIN=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-size) SSL_SIZE=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-date) SSL_DATE=<span class="variable">$value</span> ;;</span><br><span class="line">        --ca-date) CA_DATE=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-cn) CN=<span class="variable">$value</span> ;;</span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CA 相关配置</span></span><br><span class="line">CA_DATE=<span class="variable">$&#123;CA_DATE:-3650&#125;</span></span><br><span class="line">CA_KEY=<span class="variable">$&#123;CA_KEY:-cakey.pem&#125;</span></span><br><span class="line">CA_CERT=<span class="variable">$&#123;CA_CERT:-cacerts.pem&#125;</span></span><br><span class="line">CA_DOMAIN=cattle-ca</span><br><span class="line"></span><br><span class="line"><span class="comment"># ssl 相关配置</span></span><br><span class="line">SSL_CONFIG=<span class="variable">$&#123;SSL_CONFIG:-<span class="variable">$PWD</span>/openssl.cnf&#125;</span></span><br><span class="line">SSL_DOMAIN=<span class="variable">$&#123;SSL_DOMAIN:-&#x27;www.rancher.local&#x27;&#125;</span></span><br><span class="line">SSL_DATE=<span class="variable">$&#123;SSL_DATE:-3650&#125;</span></span><br><span class="line">SSL_SIZE=<span class="variable">$&#123;SSL_SIZE:-2048&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 国家代码(2 个字母的代号),默认 CN;</span></span><br><span class="line">CN=<span class="variable">$&#123;CN:-CN&#125;</span></span><br><span class="line"></span><br><span class="line">SSL_KEY=<span class="variable">$SSL_DOMAIN</span>.key</span><br><span class="line">SSL_CSR=<span class="variable">$SSL_DOMAIN</span>.csr</span><br><span class="line">SSL_CERT=<span class="variable">$SSL_DOMAIN</span>.crt</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ---------------------------- \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m       | 生成 SSL Cert |       \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ---------------------------- \033[0m&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -e ./<span class="variable">$&#123;CA_KEY&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 1. 发现已存在 CA 私钥，备份&quot;</span><span class="variable">$&#123;CA_KEY&#125;</span><span class="string">&quot;为&quot;</span><span class="variable">$&#123;CA_KEY&#125;</span><span class="string">&quot;-bak，然后重新创建 \033[0m&quot;</span></span><br><span class="line">    <span class="built_in">mv</span> <span class="variable">$&#123;CA_KEY&#125;</span> <span class="string">&quot;<span class="variable">$&#123;CA_KEY&#125;</span>&quot;</span>-bak</span><br><span class="line">    openssl genrsa -out <span class="variable">$&#123;CA_KEY&#125;</span> <span class="variable">$&#123;SSL_SIZE&#125;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 1. 生成新的 CA 私钥 <span class="variable">$&#123;CA_KEY&#125;</span> \033[0m&quot;</span></span><br><span class="line">    openssl genrsa -out <span class="variable">$&#123;CA_KEY&#125;</span> <span class="variable">$&#123;SSL_SIZE&#125;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -e ./<span class="variable">$&#123;CA_CERT&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 2. 发现已存在 CA 证书，先备份&quot;</span><span class="variable">$&#123;CA_CERT&#125;</span><span class="string">&quot;为&quot;</span><span class="variable">$&#123;CA_CERT&#125;</span><span class="string">&quot;-bak，然后重新创建 \033[0m&quot;</span></span><br><span class="line">    <span class="built_in">mv</span> <span class="variable">$&#123;CA_CERT&#125;</span> <span class="string">&quot;<span class="variable">$&#123;CA_CERT&#125;</span>&quot;</span>-bak</span><br><span class="line">    openssl req -x509 -sha256 -new -nodes -key <span class="variable">$&#123;CA_KEY&#125;</span> -days <span class="variable">$&#123;CA_DATE&#125;</span> -out <span class="variable">$&#123;CA_CERT&#125;</span> -subj <span class="string">&quot;/C=<span class="variable">$&#123;CN&#125;</span>/CN=<span class="variable">$&#123;CA_DOMAIN&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 2. 生成新的 CA 证书 <span class="variable">$&#123;CA_CERT&#125;</span> \033[0m&quot;</span></span><br><span class="line">    openssl req -x509 -sha256 -new -nodes -key <span class="variable">$&#123;CA_KEY&#125;</span> -days <span class="variable">$&#123;CA_DATE&#125;</span> -out <span class="variable">$&#123;CA_CERT&#125;</span> -subj <span class="string">&quot;/C=<span class="variable">$&#123;CN&#125;</span>/CN=<span class="variable">$&#123;CA_DOMAIN&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 3. 生成 Openssl 配置文件 <span class="variable">$&#123;SSL_CONFIG&#125;</span> \033[0m&quot;</span></span><br><span class="line"><span class="built_in">cat</span> &gt; <span class="variable">$&#123;SSL_CONFIG&#125;</span> &lt;&lt;<span class="string">EOM</span></span><br><span class="line"><span class="string">[req]</span></span><br><span class="line"><span class="string">req_extensions = v3_req</span></span><br><span class="line"><span class="string">distinguished_name = req_distinguished_name</span></span><br><span class="line"><span class="string">[req_distinguished_name]</span></span><br><span class="line"><span class="string">[ v3_req ]</span></span><br><span class="line"><span class="string">basicConstraints = CA:FALSE</span></span><br><span class="line"><span class="string">keyUsage = nonRepudiation, digitalSignature, keyEncipherment</span></span><br><span class="line"><span class="string">extendedKeyUsage = clientAuth, serverAuth</span></span><br><span class="line"><span class="string">EOM</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -n <span class="variable">$&#123;SSL_TRUSTED_IP&#125;</span> || -n <span class="variable">$&#123;SSL_TRUSTED_DOMAIN&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">cat</span> &gt;&gt; <span class="variable">$&#123;SSL_CONFIG&#125;</span> &lt;&lt;<span class="string">EOM</span></span><br><span class="line"><span class="string">subjectAltName = @alt_names</span></span><br><span class="line"><span class="string">[alt_names]</span></span><br><span class="line"><span class="string">EOM</span></span><br><span class="line">    IFS=<span class="string">&quot;,&quot;</span></span><br><span class="line">    dns=(<span class="variable">$&#123;SSL_TRUSTED_DOMAIN&#125;</span>)</span><br><span class="line">    dns+=(<span class="variable">$&#123;SSL_DOMAIN&#125;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;!dns[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">      <span class="built_in">echo</span> DNS.$((i+<span class="number">1</span>)) = <span class="variable">$&#123;dns[$i]&#125;</span> &gt;&gt; <span class="variable">$&#123;SSL_CONFIG&#125;</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> [[ -n <span class="variable">$&#123;SSL_TRUSTED_IP&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">        ip=(<span class="variable">$&#123;SSL_TRUSTED_IP&#125;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;!ip[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">          <span class="built_in">echo</span> IP.$((i+<span class="number">1</span>)) = <span class="variable">$&#123;ip[$i]&#125;</span> &gt;&gt; <span class="variable">$&#123;SSL_CONFIG&#125;</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 4. 生成服务 SSL KEY <span class="variable">$&#123;SSL_KEY&#125;</span> \033[0m&quot;</span></span><br><span class="line">openssl genrsa -out <span class="variable">$&#123;SSL_KEY&#125;</span> <span class="variable">$&#123;SSL_SIZE&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 5. 生成服务 SSL CSR <span class="variable">$&#123;SSL_CSR&#125;</span> \033[0m&quot;</span></span><br><span class="line">openssl req -sha256 -new -key <span class="variable">$&#123;SSL_KEY&#125;</span> -out <span class="variable">$&#123;SSL_CSR&#125;</span> -subj <span class="string">&quot;/C=<span class="variable">$&#123;CN&#125;</span>/CN=<span class="variable">$&#123;SSL_DOMAIN&#125;</span>&quot;</span> -config <span class="variable">$&#123;SSL_CONFIG&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 6. 生成服务 SSL CERT <span class="variable">$&#123;SSL_CERT&#125;</span> \033[0m&quot;</span></span><br><span class="line">openssl x509 -sha256 -req -<span class="keyword">in</span> <span class="variable">$&#123;SSL_CSR&#125;</span> -CA <span class="variable">$&#123;CA_CERT&#125;</span> \</span><br><span class="line">    -CAkey <span class="variable">$&#123;CA_KEY&#125;</span> -CAcreateserial -out <span class="variable">$&#123;SSL_CERT&#125;</span> \</span><br><span class="line">    -days <span class="variable">$&#123;SSL_DATE&#125;</span> -extensions v3_req \</span><br><span class="line">    -extfile <span class="variable">$&#123;SSL_CONFIG&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 7. 证书制作完成 \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 8. 以 YAML 格式输出结果 \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;----------------------------------------------------------&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ca_key: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$CA_KEY</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ca_cert: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$CA_CERT</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ssl_key: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$SSL_KEY</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ssl_csr: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$SSL_CSR</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ssl_cert: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$SSL_CERT</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 9. 附加 CA 证书到 Cert 文件 \033[0m&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$&#123;CA_CERT&#125;</span> &gt;&gt; <span class="variable">$&#123;SSL_CERT&#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ssl_cert: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$SSL_CERT</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 10. 重命名服务证书 \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;cp <span class="variable">$&#123;SSL_DOMAIN&#125;</span>.key tls.key&quot;</span></span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$&#123;SSL_DOMAIN&#125;</span>.key tls.key</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;cp <span class="variable">$&#123;SSL_DOMAIN&#125;</span>.crt tls.crt&quot;</span></span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$&#123;SSL_DOMAIN&#125;</span>.crt tls.crt</span><br></pre></td></tr></table></figure></li><li><p>权威认证证书</p><p>把权威证书文件重命名为需要的文件名：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> xxx.key tls.key</span><br><span class="line"><span class="built_in">cp</span> xxx.crt tls.crt</span><br></pre></td></tr></table></figure></li></ul><h2 id="更新证书（可选）"><a href="#更新证书（可选）" class="headerlink" title="更新证书（可选）"></a>更新证书（可选）</h2><blockquote><p>提示</p></blockquote><p>证书与域名或 IP 有绑定关系，一般情况更换域名或 IP 需更换证书。如果之前配置的证书是一个通配证书或者之前配置的证书已经包含了需要变更的域名或 IP，那么证书则可以不用更换。</p><h3 id="Rancher-单节点运行（默认容器自动生成自签名-SSL-证书）"><a href="#Rancher-单节点运行（默认容器自动生成自签名-SSL-证书）" class="headerlink" title="Rancher 单节点运行（默认容器自动生成自签名 SSL 证书）"></a>Rancher 单节点运行（默认容器自动生成自签名 SSL 证书）</h3><p>默认情况，通过 <code>docker run</code> 运行的 Rancher server 容器，会自动为 Rancher 生成 SSL 证书，这个证书会自动绑定 Rancher 系统设置中 <code>server-url</code> 配置的<code>域名或 IP</code>。如果更换了<code>域名或 IP</code>，证书会自动更新，无需单独操作。</p><h3 id="Rancher-单节点运行（外置自签名-SSL-证书）"><a href="#Rancher-单节点运行（外置自签名-SSL-证书）" class="headerlink" title="Rancher 单节点运行（外置自签名 SSL 证书）"></a>Rancher 单节点运行（外置自签名 SSL 证书）</h3><blockquote><p>注意：操作前先备份，<a href="/rancher/backup-restore/rancher-single-container-backups/">备份和恢复</a>。</p></blockquote><p>如果是以<code>映射证书文件</code>的方式运行的单容器 Rancher Server，只需要停止原有 Rancher Server 容器，用新证书替换旧证书，保持文件名不变，然后重新运行容器即可。</p><h3 id="Rancher-HA-运行"><a href="#Rancher-HA-运行" class="headerlink" title="Rancher HA 运行"></a>Rancher HA 运行</h3><blockquote><p>注意：操作前先备份，<a href="/rancher/backup-restore/rancher-rke-cluster-backups/">备份和恢复</a>。</p></blockquote><ol><li><p>备份原有证书 YAML 文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_configxxx.yml -n cattle-system \</span><br><span class="line">get secret tls-rancher-ingress -o yaml &gt; tls-ingress.yml</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=kube_configxxx.yml -n cattle-system \</span><br><span class="line">get secret tls-ca -o yaml &gt; tls-ca.yml</span><br></pre></td></tr></table></figure></li><li><p>删除旧的 secret，然后创建新的 secret</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定 kube 配置文件路径</span></span><br><span class="line"></span><br><span class="line">kubeconfig=xxx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除旧的 secret</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">delete secret tls-rancher-ingress</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">delete secret tls-ca</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建新的 secret</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">create secret tls tls-rancher-ingress --cert=./tls.crt --key=./tls.key</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">create secret generic tls-ca --from-file=cacerts.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启 Pod</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">delete pod `kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">get pod |grep -E <span class="string">&quot;cattle-cluster-agent|cattle-node-agent|rancher&quot;</span> | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>`</span><br></pre></td></tr></table></figure></li></ol><blockquote><p><strong>重要提示:</strong> 如果环境不是按照标准的 rancher 安装文档安装，<code>secret</code> 名称可能不相同，请根据实际 secret 名称操作。</p></blockquote><h2 id="修改-Rancher-Server-IP-或域名"><a href="#修改-Rancher-Server-IP-或域名" class="headerlink" title="修改 Rancher Server IP 或域名"></a>修改 Rancher Server IP 或域名</h2><ol><li><p>依次访问<code>全局 》系统设置</code>，页面往下翻找到 <code>server-url</code>；</p><img src="/rancher/replace-ip-domain/image-20200220190647637.c81432b3.png" class="" title="image-20200220190647637"></li><li><p>点击右侧的省略号菜单，选择升级；</p><img src="/rancher/replace-ip-domain/image-20200220190801613.ed966ae4.png" class="" title="image-20200220190801613"></li><li><p>修改 <code>server-url</code> 地址；</p><img src="/rancher/replace-ip-domain/image-20200220190821898.bc3a435a.png" class="" title="image-20200220190821898"></li><li><p>最后点击 <code>保存</code></p></li></ol><h2 id="更新-agent-配置文件"><a href="#更新-agent-配置文件" class="headerlink" title="更新 agent 配置文件"></a>更新 agent 配置文件</h2><ol><li><p>通过<code>新域名或 IP</code> 登录 Rancher Server；</p><blockquote><p>警告: 这一步非常重要！</p></blockquote></li><li><p>通过浏览器地址栏查询<code>集群 ID</code>， <code>c/</code> 后面以 <code>c</code> 开头的字段即为集群 ID；</p><img src="/rancher/replace-ip-domain/image-20200220205103937.436592ca.png" class="" title="image-20200220205103937"></li><li><p>访问 <code>https://&lt;新的 server_url&gt;/v3/clusters/&lt;集群 ID&gt;/clusterregistrationtokens</code> 页面；</p></li><li><p>打开<strong>clusterRegistrationTokens</strong>页面后，定位到 <code>data</code> 字段；</p><img src="/rancher/replace-ip-domain/image-20191214193741315.dba1666e.png" class="" title="image-20191214193741315"></li><li><p>找到 <code>insecureCommand</code> 字段，复制 YAML 连接备用；</p><img src="/rancher/replace-ip-domain/image-20191214194320570.2b3207b3.png" class="" title="image-20191214194320570"><blockquote><p>警告: 可能会有多组<code>&quot;baseType&quot;: &quot;clusterRegistrationToken&quot;</code>，如下图。这种情况以 <code>createdTS</code> 最大、时间最新的一组为准，一般是最后一组。</p></blockquote><img src="/rancher/replace-ip-domain/image-20191214193550681.c3dbb87c.png" class="" title="image-20191214193550681"></li><li><p>使用 <code>kubectl</code> 工具，通过第一步准备的<code>直连 kubeconfig 配置文件</code>和上面步骤中获取的 YAML 文件，执行以下命令更新 <code>agent</code> 相关配置。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -L -k &lt;替换为上面步骤获取的 YAML 文件链接&gt; | kubectl --kubeconfig=&lt;直连 kubeconfig 配置文件&gt; apply -f -</span><br></pre></td></tr></table></figure></li></ol><h2 id="其他所有集群均需按照以上方法进行-agent-配置更新。"><a href="#其他所有集群均需按照以上方法进行-agent-配置更新。" class="headerlink" title="其他所有集群均需按照以上方法进行 agent 配置更新。"></a>其他所有集群均需按照以上方法进行 agent 配置更新。</h2>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> 更换域名 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记一次镜像仓库凭证相关问题分析</title>
      <link href="/kubernetes/docker-credentials-secret/"/>
      <url>/kubernetes/docker-credentials-secret/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/docker-credentials-secret/" target="_blank" title="https://www.xtplayer.cn/kubernetes/docker-credentials-secret/">https://www.xtplayer.cn/kubernetes/docker-credentials-secret/</a></p><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>用户反馈说在 Rancher UI 部署应用时， 在以下几种凭证均未使用的情况下，也依然可以下载私有仓库中的镜像。</p><p><strong>即使未配置集群镜像仓库</strong></p><img src="/kubernetes/docker-credentials-secret/41a66ac58d44489-20210323201348908.png" class="" title="img"><p><strong>也没有配置项目中的镜像仓库凭证</strong></p><img src="/kubernetes/docker-credentials-secret/41a66ac58d44489-1-20210323201421890.png" class="" title="img"><p><strong>主机上也执行了 docker logout xxxx 退出了所有已登录的镜像仓库。</strong></p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>Dcoker 本身是 server&#x2F;client 架构，根据 Docker 私有镜像仓库拉取镜像时的认证机制，client 需要先登录仓库，然后在拉取镜像时传递认证 token 去鉴权。</p><p>通过 Rancher UI 部署应用时，容器创建大概有以下几个步骤：</p><ol><li>Rancher server 发送指令给 Rancher agent；</li><li>Rancher agent 再通过 K8S API 接口发送给 API SERVER；</li><li>API SERVER 经过处理后再把请求发送给 kubelet；</li><li>kubelet 再通过 Docker API 发送指令给 Docker;</li><li>最后 Docker 去执行创建容器的动作。</li></ol><p>所以这个时候 kubelet 可以看做是 Docker server 的一个客户端，对于下载私有仓库的镜像，kubelet 则需要携带对应的认证 token 去做鉴权。</p><p>因为每个 docker client 都需要登录，并在下载镜像时候传递认证 token 镜像鉴权。所以主机上 docker cli 有没有登录镜像仓库，并不会影响 kubelet 下载镜像。</p><p>在 K8S 创建 workload 时，会通过 imagePullSecrets 字段传递认证 token，kubelet 拿着这个 token 访问私有镜像仓库即可正常下载镜像。</p><p>根据以上原理，查看 workload YAML 并没有发现 imagePullSecrets 相关字段，说明创建 workload 时并没有传递 token 给 kubelet。</p><h3 id="查看-kubelet-日志"><a href="#查看-kubelet-日志" class="headerlink" title="查看 kubelet 日志"></a>查看 kubelet 日志</h3><p>根据以上分析，可以排除 Rancher UI 传递 token 的因素，那么问题基本可以确定出自 kubelet 本身。</p><p>在 Rancher 创建的 K8S 环境中，执行 docker logs kubelet –tail 100 -f 查看 kubelet 的运行日志。在日志中发现了这样的日志</p><p>复制</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Refreshing cache <span class="keyword">for</span> provider: *credentialprovider.defaultDockerConfigProvider</span><br></pre></td></tr></table></figure><p>出现以上日志说明了 kubelet 中缓存了镜像仓库凭证，所以即使 Rancher UI 未配置镜像仓库凭据，也依然可以正常拉取镜像。</p><h3 id="kubelet-查找凭据顺序"><a href="#kubelet-查找凭据顺序" class="headerlink" title="kubelet 查找凭据顺序"></a>kubelet 查找凭据顺序</h3><ul><li>kubelet 拉取 Pod 镜像时，首先使用 Pod 中指定的 ImagePullSecrets。</li><li>如果 Pod 没有指定 ImagePullSecrets，则依次在 <code>–root-dir(默认是/var/lib/kubelet)</code>、<code>$HOME/.docker/</code>、<code>/.docker/</code> 中查找<code>config.json</code>文件。<strong>注意</strong>：因为 Rancher K8S 中 kubelet 是以容器方式运行，那么这里的路径指的是容器中的路径。不过 <code>/var/lib/kubelet</code>默认是映射到主机的<code>/var/lib/kubelet</code></li><li>如果没有找到 config.json，再按照同样的顺序查找<code>.dockercfg</code>文件。</li></ul><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>根据 kubelet 查找凭据的顺序，在 <code>/var/lib/kubelet</code> 中查询到 <code>config.json</code> 文件。这个是一个 json 格式的文件，里面可能包含了多个镜像仓库的认证信息，根据需求删除不需要的认证信息即可。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>重磅出击: Rancher 2.4.x 迁移自定义 k8s 集群</title>
      <link href="/rancher/Rancher2.x-migrate-custom-k8s-cluster/"/>
      <url>/rancher/Rancher2.x-migrate-custom-k8s-cluster/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/Rancher2.x-migrate-custom-k8s-cluster/" target="_blank" title="https://www.xtplayer.cn/rancher/Rancher2.x-migrate-custom-k8s-cluster/">https://www.xtplayer.cn/rancher/Rancher2.x-migrate-custom-k8s-cluster/</a></p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol><li>在做操作前，先检查所有 Rancher 环境（源 Rancher 环境和目标 Rancher 环境）中节点的时间是不是已同步（<strong>重要）</strong>。</li><li>对于 Rancher 低于 <code>v2.3.x</code> 并且自定义 k8s 集群版本低于 v1.16 的环境，因为 K8S 集群 API 版本变化，迁移过去后会提示 API 版本不匹配，以及其他权限问题。在迁移之前，需要升级源 Rancher 环境和目标 Rancher 环境到 v2.4.x 及以上版本，并升级自定义 k8s 集群到 v1.16 及以上版本，然后再进行迁移。</li><li>不支持降级迁移。</li><li>集群导入到目标 Rancher 环境后，在没有确认集群迁移完全正常之前，保留源 Rancher 环境，以便集群回退。</li><li>在将 Rancher 低版本环境自定义 k8s 集群迁移到 Rancher 高版本环境时，会触发 k8s 集群自动 <strong>更新&#x2F;升级</strong>，请确定好窗口期以免影响业务。</li></ol><h2 id="基础配置（可选）"><a href="#基础配置（可选）" class="headerlink" title="基础配置（可选）"></a>基础配置（可选）</h2><ol><li><p>对于 <code>docker run</code> 单容器运行的源 Rancher 环境，为了操作方便，建议在源 Rancher 容器中执行以下命令安装文本编辑工具 vim，并配置自动补全，离线环境可忽略此步骤。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line">sed -i <span class="string">&#x27;s/security.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">apt update &amp;&amp; apt install -y vim bash-com* jq</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;source &lt;(kubectl completion bash)&#x27;</span> &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;source /etc/bash_completion&#x27;</span> &gt;&gt; ~/.bashrc</span><br><span class="line">bash</span><br></pre></td></tr></table></figure></li><li><p>对于源 Rancher 环境是 HA 架构的，可直接在 local 集群中操作。</p></li><li><p>准备待迁移 k8s 集群的直连 <strong>kubeconfig</strong> 文件</p><p> 所谓直连，就是不经过 Rancher Agent 代理直接与业务 k8s 通信。通过 Rancher UI 获取到的 <strong>kubeconfig</strong> 配置文件默认是通过 Rancher Agent 进行代理。当 k8s 集群迁移到目标 Rancher 环境后，如果 Rancher Agent 无法连接 Rancher server，那么后面将无法对自定义 k8s 集群做任何编辑操作。所以需要准备可以直连 k8s 的配置文件，以备不时之需。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -ti &lt;Rancher 容器 ID&gt; bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 待迁移集群 ID，可通过浏览器地址栏查询</span></span><br><span class="line">CLUSTER_ID=c-xxx</span><br><span class="line"></span><br><span class="line">kubectl get secret c-<span class="variable">$&#123;CLUSTER_ID&#125;</span> -n cattle-system -o=jsonpath=<span class="string">&#x27;&#123;.data.cluster&#125;&#x27;</span> \</span><br><span class="line">| <span class="built_in">base64</span> --decode | jq .metadata.state | awk <span class="string">&#x27;&#123;print substr($0,2,length($0)-2)&#125;&#x27;</span> \</span><br><span class="line">&gt; kubeconfig-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br><span class="line"></span><br><span class="line">sed -i <span class="string">&#x27;s/\\n/\n/g&#x27;</span> kubeconfig-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml &amp;&amp; \</span><br><span class="line">sed -i <span class="string">&#x27;s/\\&quot;/&quot;/g&#x27;</span> kubeconfig-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br></pre></td></tr></table></figure></li></ol><h2 id="导出源-Rancher-环境中的-CRD-资源（-在-Rancher-容器中或者-local-集群中操作-）"><a href="#导出源-Rancher-环境中的-CRD-资源（-在-Rancher-容器中或者-local-集群中操作-）" class="headerlink" title="导出源 Rancher 环境中的 CRD 资源（ 在 Rancher 容器中或者 local 集群中操作 ）"></a>导出源 Rancher 环境中的 CRD 资源（ 在 Rancher 容器中或者 local 集群中操作 ）</h2><h3 id="clusters-management-cattle-io-必须"><a href="#clusters-management-cattle-io-必须" class="headerlink" title="clusters.management.cattle.io (必须)"></a>clusters.management.cattle.io (必须)</h3><ul><li><p>在源 Rancher UI 上查询并确定待迁移的 k8s 集群 ID，通过浏览器地址栏可查看<strong>集群 ID</strong>。</p></li><li><p>执行以下命令再次查看集群 ID。</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get --all-namespaces clusters.management.cattle.io</span><br><span class="line"></span><br><span class="line">NAME      AGE</span><br><span class="line">c-wskbw   43m</span><br></pre></td></tr></table></figure></li><li><p>执行以下命令导出 Clusters YAML 资源。</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CLUSTER_ID=<span class="string">&#x27;&#x27;</span> <span class="comment"># clusters id</span></span><br><span class="line">kubectl get clusters.management.cattle.io <span class="variable">$CLUSTER_ID</span> -oyaml &gt; clusters-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br></pre></td></tr></table></figure></li></ul><h3 id="projects-management-cattle-io-必须"><a href="#projects-management-cattle-io-必须" class="headerlink" title="projects.management.cattle.io (必须)"></a>projects.management.cattle.io (必须)</h3><ul><li><p>执行以下命令查看环境中所有的项目，以及对应的集群 ID。</p><ol><li>第一列的 <code>NAMESPACE</code> 对应 <code>集群 ID</code>；</li><li>第二列 <code>NAME</code> 对应 <code>项目 ID</code>；</li></ol>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get projects.management.cattle.io --all-namespaces</span><br><span class="line"></span><br><span class="line">NAMESPACE   NAME      AGE</span><br><span class="line">c-bwl9m     p-l86x6   17m</span><br><span class="line">c-bwl9m     p-lvdjq   17m</span><br></pre></td></tr></table></figure></li><li><p>以集群 ID 为命名空间，执行以下命令导出单个集群全部项目资源。</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CLUSTER_ID=<span class="string">&#x27;&#x27;</span></span><br><span class="line">kubectl -n <span class="variable">$CLUSTER_ID</span> get projects.management.cattle.io -oyaml &gt; projects-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br></pre></td></tr></table></figure></li></ul><h3 id="nodes-management-cattle-io-必须"><a href="#nodes-management-cattle-io-必须" class="headerlink" title="nodes.management.cattle.io (必须)"></a>nodes.management.cattle.io (必须)</h3><ul><li><p>执行以下命令查看集群对应的主机管理配置。</p><blockquote><p><code>NAMESPACE</code> 为集群 ID</p></blockquote>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get nodes.management.cattle.io --all-namespaces</span><br><span class="line"></span><br><span class="line">NAMESPACE   NAME             AGE</span><br><span class="line">c-bwl9m     m-59601d9748cf   14m</span><br></pre></td></tr></table></figure></li><li><p>以集群 ID 为命名空间，执行以下命令导出单个集群所有节点的 YAML 文件。</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CLUSTER_ID=<span class="string">&#x27;&#x27;</span></span><br><span class="line">kubectl -n <span class="variable">$CLUSTER_ID</span> get nodes.management.cattle.io -oyaml &gt; nodes-<span class="variable">$CLUSTER_ID</span>.yaml</span><br></pre></td></tr></table></figure></li></ul><h3 id="secrets-必须"><a href="#secrets-必须" class="headerlink" title="secrets (必须)"></a>secrets (必须)</h3><ul><li><p>创建集群会生成以下密文</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get secrets --all-namespaces</span><br><span class="line"></span><br><span class="line">NAMESPACE            NAME                  TYPE                                  DATA   AGE</span><br><span class="line">c-bwl9m              default-token-nl7mh   kubernetes.io/service-account-token   3      4m28s</span><br><span class="line">cattle-global-data   default-token-qgwn9   kubernetes.io/service-account-token   3      39m</span><br><span class="line">cattle-system        c-c-bwl9m             Opaque                                1      4m14s</span><br><span class="line">cattle-system        tls-rancher           kubernetes.io/tls                     2      39m</span><br><span class="line">default              default-token-rwm29   kubernetes.io/service-account-token   3      39m</span><br><span class="line">kube-public          default-token-6k2vc   kubernetes.io/service-account-token   3      39m</span><br><span class="line">kube-system          default-token-wg7rg   kubernetes.io/service-account-token   3      39m</span><br><span class="line">p-l86x6              default-token-bz2pg   kubernetes.io/service-account-token   3      4m28s</span><br><span class="line">p-lvdjq              default-token-7tbkn   kubernetes.io/service-account-token   3      4m28s</span><br><span class="line">user-vf5xr           default-token-ktvrm   kubernetes.io/service-account-token   3      21m</span><br></pre></td></tr></table></figure></li><li><p><code>kube*</code> 命名空间相关的可以忽略，<code>service-account-token*</code> 类型的可以忽略，<code>tls-rancher</code> 可以忽略。</p></li><li><p>提取 <code>cattle-system</code> 命名空间下 <code>集群 ID</code> 相关的密文。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl -n cattle-system get secrets</span><br><span class="line"></span><br><span class="line">NAME                  TYPE                                  DATA   AGE</span><br><span class="line">c-c-bwl9m             Opaque                                1      8m15s</span><br><span class="line">default-token-xhl6d   kubernetes.io/service-account-token   3      43m</span><br><span class="line">tls-rancher           kubernetes.io/tls                     2      43m</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CLUSTER_ID=<span class="string">&#x27;&#x27;</span></span><br><span class="line">kubectl -n cattle-system get secrets c-<span class="variable">$&#123;CLUSTER_ID&#125;</span> -oyaml &gt; secrets-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br></pre></td></tr></table></figure></li></ul><h3 id="apps-project-cattle-io-可选"><a href="#apps-project-cattle-io-可选" class="headerlink" title="apps.project.cattle.io (可选)"></a>apps.project.cattle.io (可选)</h3><p>如果迁移的集群启用了 <strong>cluster-monitoring</strong> 等应用商店 APP，需要导出 YAML 再导入目标 Rancher 环境，不然 Rancher 无法管理对应资源。</p><ul><li><p>查看所有集群项目下部署的应用商店 APP。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get apps.project.cattle.io --all-namespaces</span><br><span class="line"></span><br><span class="line">NAMESPACE   NAME                  AGE</span><br><span class="line">p-tjfmp     wp-p-tjfmp            265d</span><br><span class="line">p-w49bx     demoma-p-w49bx        6m2s</span><br><span class="line">p-w49bx     mongodb-replicaset    19d</span><br><span class="line">p-wfdpj     cluster-monitoring    27d</span><br><span class="line">p-wfdpj     monitoring-operator   27d</span><br><span class="line">p-wfdpj     rancher-logging       47d</span><br><span class="line">p-zkhr5     cluster-alerting      122d</span><br><span class="line">p-zkhr5     cluster-monitoring    38d</span><br><span class="line">p-zkhr5     longhorn-system       81d</span><br><span class="line">p-zkhr5     monitoring-operator   38d</span><br><span class="line">p-zkhr5     nfs-provisioner       229d</span><br><span class="line">p-zkhr5     rabbitmq              18d</span><br><span class="line">p-zkhr5     rancher-logging       9d</span><br><span class="line">p-zkhr5     systemapp-demo        203d</span><br></pre></td></tr></table></figure></li><li><p>通过 <code>集群 ID</code> 获取 <code>项目 ID</code>，并提取所有项目对应的 APP 的 YAML 文件。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CLUSTER_ID=xxx</span><br><span class="line">PROJECT_ID=$( kubectl get project -n <span class="variable">$CLUSTER_ID</span> | grep -v <span class="string">&#x27;NAME&#x27;</span> | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line"></span><br><span class="line">APPS_NS=$( kubectl get apps.project.cattle.io --all-namespaces | awk <span class="string">&#x27;&#123; print $1 &#125;&#x27;</span> | grep -v <span class="string">&#x27;NAMESPACE&#x27;</span> )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> project_id <span class="keyword">in</span> <span class="variable">$&#123;PROJECT_ID&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">echo</span> <span class="variable">$&#123;APPS_NS&#125;</span> | grep <span class="variable">$&#123;project_id&#125;</span>;</span><br><span class="line">        kubectl get app --all-namespaces | grep -v NAMESPACE | awk <span class="string">&#x27;&#123; print &quot;kubectl get apps -n &quot; $1&quot; &quot;$2 &quot; -oyaml &gt; &quot; $1&quot;-&quot;$2&quot;.yaml&quot; &#125;&#x27;</span> | sh -</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span>;</span><br></pre></td></tr></table></figure></li></ul><h3 id="multiclusterapps-management-cattle-io-可选"><a href="#multiclusterapps-management-cattle-io-可选" class="headerlink" title="multiclusterapps.management.cattle.io (可选)"></a>multiclusterapps.management.cattle.io (可选)</h3><p>与 <code>apps.project.cattle.io</code> 类型， 如果有多集群应用商店应用，也需要导出 YAML。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get multiclusterapps.management.cattle.io --all-namespaces</span><br></pre></td></tr></table></figure><h3 id="podsecuritypolicytemplates-management-cattle-io-可选"><a href="#podsecuritypolicytemplates-management-cattle-io-可选" class="headerlink" title="podsecuritypolicytemplates.management.cattle.io (可选)"></a>podsecuritypolicytemplates.management.cattle.io (可选)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get podsecuritypolicytemplates.management.cattle.io</span><br></pre></td></tr></table></figure><h3 id="authconfigs-management-cattle-io-可选"><a href="#authconfigs-management-cattle-io-可选" class="headerlink" title="authconfigs.management.cattle.io (可选)"></a>authconfigs.management.cattle.io (可选)</h3><ul><li><p>执行以下命令查看对应的认证提供商</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get authconfigs.management.cattle.io --all-namespaces</span><br><span class="line"></span><br><span class="line">NAME              AGE</span><br><span class="line">activedirectory   259d</span><br><span class="line">adfs              259d</span><br><span class="line">azuread           259d</span><br><span class="line">freeipa           259d</span><br><span class="line">github            259d</span><br><span class="line">googleoauth       172d</span><br><span class="line">keycloak          259d</span><br><span class="line"><span class="built_in">local</span>             259d</span><br><span class="line">okta              259d</span><br><span class="line">openldap          259d</span><br><span class="line">ping              259d</span><br></pre></td></tr></table></figure></li><li><p>根据实际配置导出对应提供商 yaml 文件</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">AUTH_PROVIDER=<span class="string">&#x27;&#x27;</span></span><br><span class="line">kubectl get authconfigs.management.cattle.io --all-namespaces \</span><br><span class="line"><span class="variable">$AUTH_PROVIDER</span> -oyaml &gt; authconfigs-<span class="variable">$&#123;AUTH_PROVIDER&#125;</span>.yaml</span><br></pre></td></tr></table></figure></li></ul><h3 id="users-management-cattle-io-可选"><a href="#users-management-cattle-io-可选" class="headerlink" title="users.management.cattle.io (可选)"></a>users.management.cattle.io (可选)</h3><p>对于使用 Rancher local 认证的环境，如果本地用户比较多，可以通过以下命令导出所有用户。但是有一点需要注意，在导入目标 Rancher 环境时，需要检查一下 <strong>user id</strong> 是否有相同。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排除本地 admin</span></span><br><span class="line">USERS=$( kubectl get users.management.cattle.io --all-namespaces | \</span><br><span class="line">grep -v user- | grep -v NAME | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 轮训所有用户，分别导出每个用户的 YAML 文件</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">users</span> <span class="keyword">in</span> <span class="variable">$USERS</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    kubectl get users.management.cattle.io <span class="variable">$users</span> --all-namespaces -oyaml &gt; user-<span class="variable">$&#123;users&#125;</span>.yaml;</span><br><span class="line"><span class="keyword">done</span>;</span><br></pre></td></tr></table></figure><blockquote><p>注意:  用户、角色、角色绑定，如果要导出，则需一起导出。</p></blockquote><h3 id="clusterrole-rbac-authorization-k8s-io-可选"><a href="#clusterrole-rbac-authorization-k8s-io-可选" class="headerlink" title="clusterrole.rbac.authorization.k8s.io (可选)"></a>clusterrole.rbac.authorization.k8s.io (可选)</h3><ul><li>创建用户时，会自动创建一个 <strong>clusterrole</strong>，比如 <code>u-$&#123;user_id&#125;-view</code>，这个可以不用导出。</li><li>如果有自定义集群角色，可通过 <strong>clusterrole</strong> 名称查询并导出。</li></ul><h3 id="clusterrolebindings-rbac-authorization-k8s-io-可选"><a href="#clusterrolebindings-rbac-authorization-k8s-io-可选" class="headerlink" title="clusterrolebindings.rbac.authorization.k8s.io (可选)"></a>clusterrolebindings.rbac.authorization.k8s.io (可选)</h3><ul><li><p>创建用户时，会自动创建一个 <strong>clusterrolebindings</strong>，比如 <code>grb-$&#123;user_id&#125;-view</code></p></li><li><p>在 Rancher UI 上为集群添加成员的时候，会自动创建 <strong>clusterrolebindings</strong>。在集群迁移完成后可选择手动为集群添加成员。如果为大量用户配置了集群成员权限，为了减少手动操作的工作量，可以选择把这些 <strong>clusterrolebindings</strong> 一并导出，然后导入目的 Rancher local 集群。</p><blockquote><p>注意：<strong>clusterrolebindings</strong> 会绑定 user id，所以如果选项导出 <strong>clusterrolebindings</strong> 那也需要一并导出 user。</p></blockquote></li></ul><h3 id="roles-rbac-authorization-k8s-io-可选"><a href="#roles-rbac-authorization-k8s-io-可选" class="headerlink" title="roles.rbac.authorization.k8s.io (可选)"></a>roles.rbac.authorization.k8s.io (可选)</h3><ul><li><p>在 Rancher UI 上为项目添加成员的时候，会以 <strong>项目 ID</strong> 为命名空间自动创建对应权限的角色。在集群迁移完成后可选择手动为项目添加成员，这些角色也会自动重新创建。如果为大量用户配置了项目成员权限，为了减少手动操作的工作量，可以选择把这些角色一并导出，然后导入目的 Rancher local 集群。</p></li><li><p>如果有自定义角色，可通过 <strong>role</strong> 名称查询并导出。</p></li><li><p>以项目 ID 和集群 ID 来过滤 <strong>role</strong></p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get role.rbac.authorization.k8s.io -A|grep 6zjs5</span><br><span class="line">c-q4mfl              p-6zjs5-projectmember                  2021-03-04T08:26:54Z</span><br><span class="line">p-6zjs5              edit                                   2021-03-04T08:26:54Z</span><br><span class="line">p-6zjs5              project-member                         2021-03-04T08:26:54Z</span><br></pre></td></tr></table></figure></li><li><p>以集群 ID 和项目 ID 为 <code>NAMESPACE</code> 导出全部 yaml 文件</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定需要迁移的集群 ID</span></span><br><span class="line">CLUSTER_ID=<span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">PROJECT_ID=$( kubectl get project -n <span class="variable">$&#123;CLUSTER_ID&#125;</span> |grep -v NAME | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line">NAMESPACE=$( <span class="built_in">echo</span> <span class="variable">$&#123;CLUSTER_ID&#125;</span> <span class="variable">$&#123;PROJECT_ID&#125;</span> cattle-global-data )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ns <span class="keyword">in</span> <span class="variable">$NAMESPACE</span> ;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    kubectl -n <span class="variable">$ns</span> get roles.rbac.authorization.k8s.io -oyaml &gt; roles-<span class="variable">$ns</span>.yaml</span><br><span class="line"><span class="keyword">done</span>;</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>注意:  用户、角色、角色绑定，如果要导出，则需一起导出。</p></blockquote><h3 id="rolebindings-rbac-authorization-k8s-io-可选"><a href="#rolebindings-rbac-authorization-k8s-io-可选" class="headerlink" title="rolebindings.rbac.authorization.k8s.io (可选)"></a>rolebindings.rbac.authorization.k8s.io (可选)</h3><p>与角色具有相同逻辑。</p><ul><li><p>以项目 ID 和集群 ID 来过滤 <strong>rolebindings</strong></p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get rolebindings.rbac.authorization.k8s.io -A | grep 6zjs5</span><br><span class="line">c-q4mfl     p-6zjs5-prtb-zqb6j-project-member             Role/project-member           2m8s</span><br><span class="line">c-q4mfl     p-6zjs5-u-6pz5k3vokn-member-project-member    Role/project-member           31m</span><br><span class="line">c-q4mfl     rolebinding-692hm                             Role/p-6zjs5-projectmember    2m8s</span><br><span class="line">c-q4mfl     rolebinding-fhrwt                             Role/p-6zjs5-projectmember    31m</span><br><span class="line">p-6zjs5     prtb-zqb6j-edit                               Role/edit                     2m8s</span><br><span class="line">p-6zjs5     prtb-zqb6j-project-member                     Role/project-member           2m8s</span><br><span class="line">p-6zjs5     u-6pz5k3vokn-member-edit                      Role/edit                     31m</span><br><span class="line">p-6zjs5     u-6pz5k3vokn-member-project-member            Role/project-member           31m</span><br></pre></td></tr></table></figure></li><li><p>以集群 ID 和项目 ID 为 <code>NAMESPACE</code> 导出全部 yaml 文件</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CLUSTER_ID=<span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">PROJECT_ID=$( kubectl get project -n <span class="variable">$CLUSTER_ID</span> |grep -v NAME | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line">NAMESPACE=$( <span class="built_in">echo</span> <span class="variable">$&#123;CLUSTER_ID&#125;</span> <span class="variable">$&#123;PROJECT_ID&#125;</span> cattle-global-data )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ns <span class="keyword">in</span> <span class="variable">$NAMESPACE</span> ;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    kubectl -n <span class="variable">$ns</span> get rolebindings.rbac.authorization.k8s.io -oyaml &gt; roles-<span class="variable">$ns</span>.yaml</span><br><span class="line"><span class="keyword">done</span>;</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>注意:  用户、角色、角色绑定，如果要导出，则需一起导出。</p></blockquote><h3 id="综合脚本"><a href="#综合脚本" class="headerlink" title="综合脚本"></a>综合脚本</h3><p>以下脚本在 Rancher 容器中执行，或者在 local 集群中执行。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定集群 ID</span></span><br><span class="line">CLUSTER_ID=<span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出集群直连 kubeconfig</span></span><br><span class="line">kubectl get secret c-<span class="variable">$&#123;CLUSTER_ID&#125;</span> -n cattle-system -o=jsonpath=<span class="string">&#x27;&#123;.data.cluster&#125;&#x27;</span> \</span><br><span class="line">| <span class="built_in">base64</span> --decode | jq .metadata.state | awk <span class="string">&#x27;&#123;print substr($0,2,length($0)-2)&#125;&#x27;</span> \</span><br><span class="line">&gt; kubeconfig-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br><span class="line">sed -i <span class="string">&#x27;s/\\n/\n/g&#x27;</span> kubeconfig-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml &amp;&amp; \</span><br><span class="line">sed -i <span class="string">&#x27;s/\\&quot;/&quot;/g&#x27;</span> kubeconfig-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出 clusters crd yaml</span></span><br><span class="line">kubectl get clusters.management.cattle.io <span class="variable">$CLUSTER_ID</span> -oyaml &gt; clusters-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br><span class="line"><span class="comment"># 导出 projects crd yaml</span></span><br><span class="line">kubectl -n <span class="variable">$CLUSTER_ID</span> get projects.management.cattle.io -oyaml &gt; projects-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br><span class="line"><span class="comment"># 导出 nodes crd yaml</span></span><br><span class="line">kubectl -n <span class="variable">$CLUSTER_ID</span> get nodes.management.cattle.io -oyaml &gt; nodes-<span class="variable">$CLUSTER_ID</span>.yaml</span><br><span class="line"><span class="comment"># 导出集群相关的 secrets</span></span><br><span class="line">kubectl -n cattle-system get secrets c-<span class="variable">$&#123;CLUSTER_ID&#125;</span> -oyaml &gt; secrets-<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br></pre></td></tr></table></figure><h2 id="整理导出的-CRD-YAML-文件"><a href="#整理导出的-CRD-YAML-文件" class="headerlink" title="整理导出的 CRD YAML 文件"></a>整理导出的 CRD YAML 文件</h2><p>对于导出的 YAML 文件，会残留一些源 Rancher 环境的配置信息，在导入目标 Rancher 环境之前需要进行配置更新。</p><h3 id="clusters-management-cattle-io-必须-1"><a href="#clusters-management-cattle-io-必须-1" class="headerlink" title="clusters.management.cattle.io (必须)"></a>clusters.management.cattle.io (必须)</h3><ol><li><p>在 Cluster CRD YAML 中，annotations 字段中的 <code>field.cattle.io/creatorId</code> 需要对应真实的用户 ID，这个 USER ID 需要为 超级管理员的 user id。可通过 <strong>全局|安全|用户</strong> 查看到 user id，这个 user id 默认为 <strong>user-</strong> 前缀。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    authz.management.cattle.io/creator-role-bindings: <span class="string">&#x27;&#123;&quot;created&quot;:[&quot;cluster-owner&quot;],&quot;required&quot;:[&quot;cluster-owner&quot;]&#125;&#x27;</span></span><br><span class="line">    field.cattle.io/creatorId: user-q6cst</span><br><span class="line">    lifecycle.cattle.io/create.cluster-agent-controller-cleanup: <span class="string">&quot;true&quot;</span></span><br><span class="line">    lifecycle.cattle.io/create.cluster-provisioner-controller: <span class="string">&quot;true&quot;</span></span><br><span class="line">    lifecycle.cattle.io/create.cluster-scoped-gc: <span class="string">&quot;true&quot;</span></span><br><span class="line">    lifecycle.cattle.io/create.mgmt-cluster-rbac-remove: <span class="string">&quot;true&quot;</span></span><br><span class="line">    provisioner.cattle.io/ke-driver-update: updated</span><br><span class="line">  creationTimestamp: <span class="string">&quot;2021-03-09T07:17:48Z&quot;</span></span><br><span class="line">  finalizers:</span><br></pre></td></tr></table></figure></li><li><p>删除 metadata 下的 <strong>selfLink 和 uid</strong> 字段。</p></li><li><p><strong>删除 <code>.status.conditions</code> 下的所有配置。（重要）</strong></p></li></ol><h2 id="目标-Rancher-环境中导入-CRD-资源"><a href="#目标-Rancher-环境中导入-CRD-资源" class="headerlink" title="目标 Rancher 环境中导入 CRD 资源"></a>目标 Rancher 环境中导入 CRD 资源</h2><ul><li>将上一步导出的 YAML 文件依次导入到目标 Rancher 环境，除密文（secrets）外其他 YAML 可使用 <code>kubeclt apply -f xxx.yaml</code> 去导入。</li><li><strong>注意：</strong>需要先导入 Cluster CRD YAML</li><li>因为密文（secrets）文件太大，需要通过 <code>kubectl replace --force -f xxx</code> 导入。</li></ul><h2 id="目标-Rancher-UI-操作"><a href="#目标-Rancher-UI-操作" class="headerlink" title="目标 Rancher UI 操作"></a>目标 Rancher UI 操作</h2><h3 id="创建-clusterregistrationtokens"><a href="#创建-clusterregistrationtokens" class="headerlink" title="创建 clusterregistrationtokens"></a>创建 clusterregistrationtokens</h3><p>在 YAML 文件导入目标 Rancher  环境后，访问<code>https://demo.xxxxx.com/v3/clusters/&lt;cluster_id&gt;/clusterregistrationtokens</code> 可以看到并没有注册命令，这个时候需要手动去创建。</p><ul><li><p>访问 <code>https://demo.xxxx.com/v3/clusters/&lt;cluster_id&gt;/clusterregistrationtokens</code> 页面，点击右上角的 <code>Create</code>；</p></li><li><p>选择迁移的 <strong>集群 ID</strong></p></li></ul><img src="/rancher/Rancher2.x-migrate-custom-k8s-cluster/image-20191208212745126.png" class="" title="image126"><ul><li>点击<strong>Show Request</strong>，然后再点击 <strong>Send Request</strong>。如果显示状态码为<code>201</code>即执行成功；</li></ul><img src="/rancher/Rancher2.x-migrate-custom-k8s-cluster/image-20191208212948123.png" class="" title="image3"><ul><li>执行完成后，在 <strong>clusterregistrationtokens</strong> 页面应该是可以看到如下的链接，复制 <strong>自签名 ssl 证书</strong> 对应那条命令备用。</li></ul><img src="/rancher/Rancher2.x-migrate-custom-k8s-cluster/image-20210304172506727.png" class="" title="image"><h3 id="添加集群成员"><a href="#添加集群成员" class="headerlink" title="添加集群成员"></a>添加集群成员</h3><p>集群迁移到目标 Ranher 环境后，因为用户 ID 有所变动，所以集群成员中将不会显示对于的用，这里需要手动添加用户授权。如图：</p><img src="/rancher/Rancher2.x-migrate-custom-k8s-cluster/2c6f29299fdad5d.png" class="" title="img"><img src="/rancher/Rancher2.x-migrate-custom-k8s-cluster/d634356e631c352.png" class="" title="img"><h3 id="执行-Agent-导入命令"><a href="#执行-Agent-导入命令" class="headerlink" title="执行 Agent 导入命令"></a>执行 Agent 导入命令</h3><p>回到源 Rancher 环境，这个时候 k8s 集群还是可以正常访问，前面的操作只是将必要的 CRD 资源迁移到目标 Rancher 环境。</p><p>进入 <strong>待迁移集群</strong> 详情页面，然后点击 <strong>执行 kubectl 命令行</strong> 按钮，接着执行上一步骤中拷贝的 <strong>clusterregistrationtokens</strong> 命令。</p><img src="/rancher/Rancher2.x-migrate-custom-k8s-cluster/image-20210309174347677.png" class="" title="image-20210309174347677"><h3 id="手动更新-k8s-集群（可选）"><a href="#手动更新-k8s-集群（可选）" class="headerlink" title="手动更新 k8s 集群（可选）"></a>手动更新 k8s 集群（可选）</h3><p>如果是低版本 Rancher 环境的自定义 k8s 集群迁移到高版本 Rancher 环境，因为元数据版本不一致，Rancher 会自动的更新 k8s 集群配置。如果集群页面长时间未恢复正常，也没有 k8s 集群更新状态显示，那可以手动编辑集群参数去触发集群更新。</p><img src="/rancher/Rancher2.x-migrate-custom-k8s-cluster/image-20210309183728852.png" class="" title="image-20210309183728852"><h2 id="CRD-资源清单"><a href="#CRD-资源清单" class="headerlink" title="CRD 资源清单"></a>CRD 资源清单</h2><p>以下是 <strong>Rancher v2.4.13</strong> 的 CRD 资源清单，文章前面部分仅指出了常用的一些资源。如果有使用其他资源，或者自定义过其他 CRD 资源，可按照上述方法进行导出再导入目标集群。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl  get crd| grep management.cattle.io</span><br><span class="line"></span><br><span class="line">authconfigs.management.cattle.io                                2021-03-04T06:47:55Z</span><br><span class="line">catalogs.management.cattle.io                                   2021-03-04T06:47:55Z</span><br><span class="line">catalogtemplates.management.cattle.io                           2021-03-04T06:47:55Z</span><br><span class="line">catalogtemplateversions.management.cattle.io                    2021-03-04T06:47:55Z</span><br><span class="line">cisbenchmarkversions.management.cattle.io                       2021-03-04T06:47:56Z</span><br><span class="line">cisconfigs.management.cattle.io                                 2021-03-04T06:47:56Z</span><br><span class="line">clusteralertgroups.management.cattle.io                         2021-03-04T06:47:55Z</span><br><span class="line">clusteralertrules.management.cattle.io                          2021-03-04T06:47:55Z</span><br><span class="line">clusteralerts.management.cattle.io                              2021-03-04T06:47:55Z</span><br><span class="line">clustercatalogs.management.cattle.io                            2021-03-04T06:47:55Z</span><br><span class="line">clusterloggings.management.cattle.io                            2021-03-04T06:47:55Z</span><br><span class="line">clustermonitorgraphs.management.cattle.io                       2021-03-04T06:47:55Z</span><br><span class="line">clusterregistrationtokens.management.cattle.io                  2021-03-04T06:47:55Z</span><br><span class="line">clusterroletemplatebindings.management.cattle.io                2021-03-04T06:47:55Z</span><br><span class="line">clusters.management.cattle.io                                   2021-03-04T06:47:55Z</span><br><span class="line">clusterscans.management.cattle.io                               2021-03-04T06:47:55Z</span><br><span class="line">clustertemplaterevisions.management.cattle.io                   2021-03-04T06:47:56Z</span><br><span class="line">clustertemplates.management.cattle.io                           2021-03-04T06:47:56Z</span><br><span class="line">composeconfigs.management.cattle.io                             2021-03-04T06:47:55Z</span><br><span class="line">dynamicschemas.management.cattle.io                             2021-03-04T06:47:55Z</span><br><span class="line">etcdbackups.management.cattle.io                                2021-03-04T06:47:55Z</span><br><span class="line">features.management.cattle.io                                   2021-03-04T06:47:55Z</span><br><span class="line">globaldnses.management.cattle.io                                2021-03-04T06:47:56Z</span><br><span class="line">globaldnsproviders.management.cattle.io                         2021-03-04T06:47:56Z</span><br><span class="line">globalrolebindings.management.cattle.io                         2021-03-04T06:47:55Z</span><br><span class="line">globalroles.management.cattle.io                                2021-03-04T06:47:55Z</span><br><span class="line">groupmembers.management.cattle.io                               2021-03-04T06:47:55Z</span><br><span class="line">groups.management.cattle.io                                     2021-03-04T06:47:55Z</span><br><span class="line">kontainerdrivers.management.cattle.io                           2021-03-04T06:47:55Z</span><br><span class="line">monitormetrics.management.cattle.io                             2021-03-04T06:47:55Z</span><br><span class="line">multiclusterapprevisions.management.cattle.io                   2021-03-04T06:47:55Z</span><br><span class="line">multiclusterapps.management.cattle.io                           2021-03-04T06:47:55Z</span><br><span class="line">nodedrivers.management.cattle.io                                2021-03-04T06:47:55Z</span><br><span class="line">nodepools.management.cattle.io                                  2021-03-04T06:47:55Z</span><br><span class="line">nodes.management.cattle.io                                      2021-03-04T06:47:55Z</span><br><span class="line">nodetemplates.management.cattle.io                              2021-03-04T06:47:55Z</span><br><span class="line">notificationtemplates.management.cattle.io                      2021-03-04T06:47:55Z</span><br><span class="line">notifiers.management.cattle.io                                  2021-03-04T06:47:55Z</span><br><span class="line">podsecuritypolicytemplateprojectbindings.management.cattle.io   2021-03-04T06:47:55Z</span><br><span class="line">podsecuritypolicytemplates.management.cattle.io                 2021-03-04T06:47:55Z</span><br><span class="line">preferences.management.cattle.io                                2021-03-04T06:47:55Z</span><br><span class="line">projectalertgroups.management.cattle.io                         2021-03-04T06:47:55Z</span><br><span class="line">projectalertrules.management.cattle.io                          2021-03-04T06:47:55Z</span><br><span class="line">projectalerts.management.cattle.io                              2021-03-04T06:47:55Z</span><br><span class="line">projectcatalogs.management.cattle.io                            2021-03-04T06:47:55Z</span><br><span class="line">projectloggings.management.cattle.io                            2021-03-04T06:47:55Z</span><br><span class="line">projectmonitorgraphs.management.cattle.io                       2021-03-04T06:47:55Z</span><br><span class="line">projectnetworkpolicies.management.cattle.io                     2021-03-04T06:47:56Z</span><br><span class="line">projectroletemplatebindings.management.cattle.io                2021-03-04T06:47:56Z</span><br><span class="line">projects.management.cattle.io                                   2021-03-04T06:47:56Z</span><br><span class="line">rkeaddons.management.cattle.io                                  2021-03-04T06:47:56Z</span><br><span class="line">rkek8sserviceoptions.management.cattle.io                       2021-03-04T06:47:56Z</span><br><span class="line">rkek8ssystemimages.management.cattle.io                         2021-03-04T06:47:56Z</span><br><span class="line">roletemplates.management.cattle.io                              2021-03-04T06:47:56Z</span><br><span class="line">samltokens.management.cattle.io                                 2021-03-04T06:47:56Z</span><br><span class="line">settings.management.cattle.io                                   2021-03-04T06:47:56Z</span><br><span class="line">templatecontents.management.cattle.io                           2021-03-04T06:47:56Z</span><br><span class="line">templates.management.cattle.io                                  2021-03-04T06:47:56Z</span><br><span class="line">templateversions.management.cattle.io                           2021-03-04T06:47:56Z</span><br><span class="line">tokens.management.cattle.io                                     2021-03-04T06:47:56Z</span><br><span class="line">userattributes.management.cattle.io                             2021-03-04T06:47:56Z</span><br><span class="line">users.management.cattle.io                                      2021-03-04T06:47:56Z</span><br></pre></td></tr></table></figure><h2 id="异常回退"><a href="#异常回退" class="headerlink" title="异常回退"></a>异常回退</h2><p>因为源 Rancher 环境保留了完整的配置信息，要想回退到源 Rancher 环境，只需将 k8s 集群重新注册到源 Rancher 环境即可。</p><ol><li><p>访问源 Rancher <code>https://demo.cnrancher.com/v3/clusters/&lt;cluster_id&gt;/clusterregistrationtokens</code> 页面 。</p></li><li><p>定位到 <code>insecureCommand</code>，复制 <code>insecureCommand</code> 后面的命令，在需要迁移的 k8s 集群中执行。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> 集群迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to k3d: Run K3s in Docker</title>
      <link href="/k3d/introduction-k3d-run-k3s-docker-src/"/>
      <url>/k3d/introduction-k3d-run-k3s-docker-src/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/k3d/introduction-k3d-run-k3s-docker-src/" target="_blank" title="https://www.xtplayer.cn/k3d/introduction-k3d-run-k3s-docker-src/">https://www.xtplayer.cn/k3d/introduction-k3d-run-k3s-docker-src/</a></p><p>In this blog post, we’re going to talk about k3d, a tool that allows you to run throwaway Kubernetes clusters anywhere you have Docker installed. I’ve anticipated your questions — so let’s go!</p><h2 id="What-is-k3d"><a href="#What-is-k3d" class="headerlink" title="What is k3d?"></a>What is k3d?</h2><p>k3d is a small program made for running a <a href="https://k3s.io/">K3s</a> cluster in Docker. K3s is a lightweight, CNCF-certified Kubernetes distribution and Sandbox project. Designed for low-resource environments, K3s is distributed as a single binary that uses under 512MB of RAM. To learn more about K3s, head over to <a href="https://rancher.com/docs/k3s/latest/en/">the documentation</a> or check out this <a href="https://rancher.com/blog/2019/2019-02-26-introducing-k3s-the-lightweight-kubernetes-distribution-built-for-the-edge/">blog post</a> or <a href="https://www.youtube.com/watch?v=hMr3prm9gDM">video. </a></p><p>k3d uses a <a href="https://hub.docker.com/r/rancher/k3s/tags">Docker image</a> built from the <a href="https://github.com/rancher/k3s">K3s repository</a> to spin up multiple K3s nodes in Docker containers on any machine with Docker installed. That way, a single physical (or virtual) machine (let’s call it Docker Host) can run multiple K3s clusters, with multiple server and agent nodes each, simultaneously.</p><h2 id="What-Can-k3d-Do"><a href="#What-Can-k3d-Do" class="headerlink" title="What Can k3d Do?"></a>What Can k3d Do?</h2><p>As of k3d version v4.0.0, released in January 2021, k3d’s abilities boil down to the following features:</p><ul><li>create&#x2F;stop&#x2F;start&#x2F;delete&#x2F;grow&#x2F;shrink K3s clusters (and individual nodes)<ul><li>via command line flags</li><li>via configuration file</li></ul></li><li>manage and interact with container registries that can be used with the cluster</li><li>manage Kubeconfigs for the clusters</li><li>import images from your local Docker daemon into the container runtime running in the cluster</li></ul><p>Obviously, there’s way more to it and you can tweak everything in great detail.</p><h2 id="What-is-k3d-Used-for"><a href="#What-is-k3d-Used-for" class="headerlink" title="What is k3d Used for?"></a>What is k3d Used for?</h2><p>The main use case for k3d is local development on Kubernetes with little hassle and resource usage. The intention behind the initial development of k3d was to provide developers with an easy tool that allowed them to run a lightweight Kubernetes cluster on their development machine, giving them fast iteration times in a production-like environment (as opposed to running docker-compose locally vs. Kubernetes in production).</p><p>Over time, k3d also evolved into a tool used by operations to test some Kubernetes (or, specifically K3s) features in an isolated environment. For example, with k3d you can easily create multi-node clusters, deploy something on top of it, simply stop a node and see how Kubernetes reacts and possibly reschedules your app to other nodes.</p><p>Additionally, you can use k3d in your continuous integration system to quickly spin up a cluster, deploy your test stack on top of it and run integration tests. Once you’re finished, you can simply decommission the cluster as a whole. No need to worry about proper cleanups and possible leftovers.</p><p>We also provide a <code>k3d-dind</code> image (similar to dreams within dreams in the movie Inception, we’ve got containers within containers within containers.) With that, you can create a docker-in-docker environment where you run k3d, which spawns a K3s cluster in Docker. That means that you only have a single container (k3d-dind) running on your Docker host, which in turn runs a whole K3s&#x2F;Kubernetes cluster inside.</p><h2 id="How-Do-I-Use-k3d"><a href="#How-Do-I-Use-k3d" class="headerlink" title="How Do I Use k3d?"></a>How Do I Use k3d?</h2><ol><li><a href="https://k3d.io/#installation">Install k3d</a> (and <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl </a>, if you want to use it)<br><strong>Note</strong>: to follow along with this post, use at least k3d v4.1.1</li><li>Try one of the following examples or use the <a href="https://k3d.io/usage/commands/">documentation</a> or the CLI help text to find your own way (<code>k3d [command] --help</code>)</li></ol><h3 id="The-“Simple”-Way"><a href="#The-“Simple”-Way" class="headerlink" title="The “Simple” Way"></a>The “Simple” Way</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3d cluster create</span><br></pre></td></tr></table></figure><p>This single command spawns a K3s cluster with two containers: A Kubernetes control-plane node (server) and a load balancer (serverlb) in front of it. It puts both of them in a dedicated Docker network and exposes the Kubernetes API on a randomly chosen free port on the Docker host. It also creates a named Docker volume in the background as a preparation for image imports.</p><p>By default, if you don’t provide a name argument, the cluster will be named <code>k3s-default</code><br>and the containers will show up as <code>k3d-&lt;-role&gt;-&lt;#&gt;</code>, so in this case <code>k3d-k3s-default-serverlb</code> and <code>k3d-k3s-default-server-0</code>.</p><p>k3d waits until everything is ready, pulls the Kubeconfig from the cluster and merges it with your default Kubeconfig (usually it’s in <code>$HOME/.kube/config</code> or whatever path your <code>KUBECONFIG</code> environment variable points to).</p><p>No worries, you can tweak that behavior as well.</p><p>Check out what you’ve just created using <code>kubectl</code> to show you the nodes: <code>kubectl get nodes</code>.</p><p>k3d also gives you some commands to list your creations: <code>k3d cluster|node|registry list</code>.</p><h3 id="The-“Simple-but-Sophisticated”-Way"><a href="#The-“Simple-but-Sophisticated”-Way" class="headerlink" title="The “Simple but Sophisticated” Way"></a>The “Simple but Sophisticated” Way</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3d cluster create mycluster --api-port 127.0.0.1:6445 --servers 3 --agents 2 --volume &#x27;/home/me/mycode:/code@agent[*]&#x27; --port &#x27;8080:80@loadbalancer&#x27;</span><br></pre></td></tr></table></figure><p>This single command spawns a K3s cluster with six containers:</p><ul><li>1 load balancer</li><li>3 servers (control-plane nodes)</li><li>2 agents (formerly worker nodes)</li></ul><p>With the <code>--api-port 127.0.0.1:6445</code>, you tell k3d to map the Kubernetes API Port (<code>6443</code> internally) to <code>127.0.0.1</code>&#x2F;localhost’s port <code>6445</code>. That means that you will have this connection string in your Kubeconfig: <code>server: https://127.0.0.1:6445</code> to connect to this cluster.</p><p>This port will be mapped from the load balancer to your host system. From there, requests will be proxied to your server nodes, effectively simulating a production setup, where server nodes also can go down and you would want to failover to another server.</p><p>The <code>--volume /home/me/mycode:/code@agent[*]</code> bind mounts your local directory <code>/home/me/mycode</code> to the path <code>/code</code> inside all (<code>[*]</code> of your agent nodes). Replace <code>*</code> with an index (here: 0 or 1) to only mount it into one of them.</p><p>The specification telling k3d which nodes it should mount the volume to is called “node filter” and it’s also used for other flags, like the <code>--port</code> flag for port mappings.</p><p>That said, <code>--port &#39;8080:80@loadbalancer&#39;</code> maps your local host’s port <code>8080</code> to port <code>80</code> on the load balancer (serverlb), which can be used to forward HTTP ingress traffic to your cluster. For example, you can now deploy a web app into the cluster (Deployment), which is exposed (Service) externally via an Ingress such as <code>myapp.k3d.localhost</code>.</p><p>Then (provided that everything is set up to resolve that domain to your local host IP), you can point your browser to <code>http://myapp.k3d.localhost:8080</code> to access your app. Traffic then flows from your host through the Docker bridge interface to the load balancer. From there, it’s proxied to the cluster, where it passes via Ingress and Service to your application Pod.</p><p><strong>Note</strong>: You have to have some mechanism set up to route to resolve <code>myapp.k3d.localhost</code> to your local host IP (<code>127.0.0.1</code>). The most common way is using entries of the form <code>127.0.0.1 myapp.k3d.localhost</code> in your <code>/etc/hosts</code> file (<code>C:\Windows\System32\drivers\etc\hosts</code> on Windows). However, this does not allow for wildcard entries (<code>*.localhost</code>), so it may become a bit cumbersome after a while, so you may want to have a look at tools like <a href="https://en.wikipedia.org/wiki/Dnsmasq"><code>dnsmasq</code> (MacOS&#x2F;UNIX</a>) or <a href="https://stackoverflow.com/a/9695861/6450189"><code>Acrylic</code> (Windows) </a> to ease the burden.</p><p><strong>Tip</strong>: You can install the package <code>libnss-myhostname</code> on some systems (at least Linux operating systems including SUSE Linux and openSUSE), to auto-resolve <code>*.localhost</code> domains to <code>127.0.0.1</code>, which means you don’t have to fiddle around with e.g. <code>/etc/hosts</code>, if you prefer to test via Ingress, where you need to set a domain.</p><p>One interesting thing to note here: if you create more than one server node, K3s will be given the <code>--cluster-init</code> flag, which means that it swaps its internal datastore (by default that’s SQLite) for etcd.</p><h3 id="The-“Configuration-as-Code”-Way"><a href="#The-“Configuration-as-Code”-Way" class="headerlink" title="The “Configuration as Code” Way"></a>The “Configuration as Code” Way</h3><p>As of k3d v4.0.0 (January 2021), we support config files to configure everything as code that you’d previously do via command line flags (and soon possibly even more than that).<br>As of this writing, the JSON-Schema used to validate the configuration file can be <a href="https://github.com/rancher/k3d/blob/092f26a4e27eaf9d3a5bc32b249f897f448bc1ce/pkg/config/v1alpha2/schema.json">found in the repository</a>.</p><p>Here’s an example config file:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># k3d configuration file, saved as e.g. /home/me/myk3dcluster.yaml</span></span><br><span class="line">apiVersion: k3d.io/v1alpha2 <span class="comment"># this will change in the future as we make everything more stable</span></span><br><span class="line">kind: Simple <span class="comment"># internally, we also have a Cluster config, which is not yet available externally</span></span><br><span class="line">name: mycluster <span class="comment"># name that you want to give to your cluster (will still be prefixed with `k3d-`)</span></span><br><span class="line">servers: 1 <span class="comment"># same as `--servers 1`</span></span><br><span class="line">agents: 2 <span class="comment"># same as `--agents 2`</span></span><br><span class="line">kubeAPI: <span class="comment"># same as `--api-port 127.0.0.1:6445`</span></span><br><span class="line"> hostIP: <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line"> hostPort: <span class="string">&quot;6445&quot;</span></span><br><span class="line">ports:</span><br><span class="line"> - port: 8080:80 <span class="comment"># same as `--port 8080:80@loadbalancer</span></span><br><span class="line"> nodeFilters:</span><br><span class="line"> - loadbalancer</span><br><span class="line">options:</span><br><span class="line"> k3d: <span class="comment"># k3d runtime settings</span></span><br><span class="line">   <span class="built_in">wait</span>: <span class="literal">true</span> <span class="comment"># wait for cluster to be usable before returining; same as `--wait` (default: true)</span></span><br><span class="line">   <span class="built_in">timeout</span>: <span class="string">&quot;60s&quot;</span> <span class="comment"># wait timeout before aborting; same as `--timeout 60s`</span></span><br><span class="line"> k3s: <span class="comment"># options passed on to K3s itself</span></span><br><span class="line">   extraServerArgs: <span class="comment"># additional arguments passed to the `k3s server` command</span></span><br><span class="line">     - --tls-san=my.host.domain</span><br><span class="line">   extraAgentArgs: [] <span class="comment"># addditional arguments passed to the `k3s agent` command</span></span><br><span class="line"> kubeconfig:</span><br><span class="line">   updateDefaultKubeconfig: <span class="literal">true</span> <span class="comment"># add new cluster to your default Kubeconfig; same as `--kubeconfig-update-default` (default: true)</span></span><br><span class="line">   switchCurrentContext: <span class="literal">true</span> <span class="comment"># also set current-context to the new cluster&#x27;s context; same as `--kubeconfig-switch-context` (default: true)</span></span><br></pre></td></tr></table></figure><p>Assuming that we saved this as <code>/home/me/myk3dcluster.yaml</code>, we can use it to configure a new cluster:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">k3d cluster create --config /home/me/myk3dcluster.yaml</span><br></pre></td></tr></table></figure><p>Note that you can still set additional arguments or flags, which will then take precedence (or will be merged) with whatever you have defined in the config file.</p><h2 id="What-More-Can-I-Do-with-k3d"><a href="#What-More-Can-I-Do-with-k3d" class="headerlink" title="What More Can I Do with k3d?"></a>What More Can I Do with k3d?</h2><p>You can use k3d in even more ways, including:</p><ul><li>Create a cluster together with a k3d-managed container <strong>registry</strong></li><li>Use the cluster for fast development with <strong>hot code reloading</strong></li><li>Use k3d in combination with other development tools like <code>Tilt</code> or <code>Skaffold</code><ul><li>both can leverage the power of importing images via <code>k3d image import</code></li><li>both can alternatively make use of a k3d-managed registry to speed up your development loop</li></ul></li><li>Use k3d in your CI system (we have a <a href="https://github.com/iwilltry42/k3d-demo/blob/main/.drone.yml">PoC for that</a>)</li><li>Integrate it in your vscode workflow using the awesome new <a href="https://github.com/inercia/vscode-k3d">community-maintained vscode extension</a></li><li>Use it to <a href="https://rancher.com/blog/2020/set-up-k3s-high-availability-using-k3d">set up K3s high availability</a></li></ul><p>You can try all of these yourself by using prepared scripts in <a href="https://github.com/iwilltry42/k3d-demo">this demo repository</a> or watch us showing them off in <a href="https://www.youtube.com/watch?v=d9JRb4fk5ag&feature=youtu.be">one of our meetups</a>.</p><p>Other than that, remember that k3d is a <a href="https://github.com/rancher/k3d/blob/main/CONTRIBUTING.md">community-driven</a> project, so we’re always happy to hear from you on <a href="https://github.com/rancher/k3d/issues">Issues</a>, <a href="https://github.com/rancher/k3d/issues">Pull Requests</a>, <a href="https://github.com/rancher/k3d/discussions">Discussions</a> and <a href="https://slack.rancher.io/">Slack Chats</a>!</p><p>Ready to give k3d a try? <a href="https://github.com/k3s-io/k3s/releases">Start by downloading K3s</a>.</p>]]></content>
      
      
      <categories>
          
          <category> k3d </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k3d </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 自动弹性伸缩</title>
      <link href="/kubernetes/k8s-automatic-elastic-expansion/"/>
      <url>/kubernetes/k8s-automatic-elastic-expansion/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/k8s-automatic-elastic-expansion/" target="_blank" title="https://www.xtplayer.cn/kubernetes/k8s-automatic-elastic-expansion/">https://www.xtplayer.cn/kubernetes/k8s-automatic-elastic-expansion/</a></p><p>Kubernetes 自动弹性伸缩可以根据业务流量，自动增加或减少服务。这一功能在实际的业务场景中十分重要。在本文中，我们将了解 Kubernetes 如何针对应用产生的自定义指标实现自动伸缩。</p><h2 id="为什么需要自定义指标？"><a href="#为什么需要自定义指标？" class="headerlink" title="为什么需要自定义指标？"></a>为什么需要自定义指标？</h2><p>应用程序的 CPU 或 RAM 的消耗并不一定能够正确表明是否需要进行扩展。例如，如果你有一个消息队列 consumer，它每秒可以处理 500 条消息而不会导致崩溃。一旦该 consumer 的单个实例每秒处理接近 500 条消息，你可能希望将应用程序扩展到两个实例，以便将负载分布在两个实例上。测量 CPU 或 RAM 对于扩展这样的应用程序来说有点矫枉过正了，你需要寻找一个与应用程序性质更为密切相关的指标。一个实例在特定时间点处理的消息数量能更贴切地反映该应用的实际负载。同样，可能有一些应用的其他指标更有意义。这些可以使用 Kubernetes 中的自定义指标进行定义。</p><h2 id="Metrics-流水线"><a href="#Metrics-流水线" class="headerlink" title="Metrics 流水线"></a>Metrics 流水线</h2><h3 id="Metrics-Server-和-API"><a href="#Metrics-Server-和-API" class="headerlink" title="Metrics Server 和 API"></a>Metrics Server 和 API</h3><p>最初，这些指标会通过 Heapster 暴露给用户，Heapster 可以从每个 kubelet 中查询指标。Kubelet 则与 localhost 上的 cAdvisor 对话，并检索出节点级和 pod 级的指标。Metric-server 的引入是为了取代 heapster，并使用 Kubernetes API 来暴露指标从而以 Kubernetes API 的方式提供指标。Metric server 仅提供核心的指标，比如 pod 和节点的内存和 CPU，对于其他指标，你需要构建完整的指标流水线。构建流水线和 Kubernetes 自动伸缩的机制将会保持不变。</p><h3 id="Aggregation-Layer"><a href="#Aggregation-Layer" class="headerlink" title="Aggregation Layer"></a>Aggregation Layer</h3><p>能够通过 Kubernetes API 层暴露指标的关键部分之一是 Aggregation Layer。该 aggregation layer 允许在集群中安装额外的 Kubernetes 格式的 API。这使得 API 像任何 Kubernetes 资源一样可用，但 API 的实际服务可以由外部服务完成，可能是一个部署到集群本身的 Pod（如果没有在集群级别完成，你需要启用 aggregation layer）。那么，这到底是如何发挥作用的呢？作为用户，用户需要提供 API Provider（比如运行 API 服务的 pod），然后使用 APIService 对象注册相同的 API。</p><p>让我们以核心指标流水线为例来说明 metrics server 如何使用 API Aggregation layer 注册自己。APIService 对象如下：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apiregistration.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">APIService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">v1beta1.metrics.k8s.io</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">service:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">group:</span> <span class="string">metrics.k8s.io</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">v1beta1</span></span><br><span class="line">  <span class="attr">insecureSkipTLSVerify:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">groupPriorityMinimum:</span> <span class="number">100</span></span><br><span class="line">  <span class="attr">versionPriority:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><p>部署使用 APIService 注册 API 的 metrics server 之后，我们可以看到 Kubernetes API 中提供了指标 API：</p><img src="/kubernetes/k8s-automatic-elastic-expansion/640-20210302223153958" class="" title="图片"><h3 id="Metrics-流水线：核心部分和完整流水线"><a href="#Metrics-流水线：核心部分和完整流水线" class="headerlink" title="Metrics 流水线：核心部分和完整流水线"></a>Metrics 流水线：核心部分和完整流水线</h3><p>我们已经了解了基本组件，让我们把它们放在一起组成核心 metrics 流水线。在核心流水线中，如果你已经恰当地安装了 metrics server，它也将创建 APIService 将自己注册到 Kubernetes API server 上。正如我们在上一节中所了解到的那样，这些指标将在&#x2F;apis&#x2F;metrics.k8s.io 中暴露，并被 HPA 使用。</p><p>大部分复杂的应用程序需要更多的指标，而不仅仅是内存和 CPU，这也是大多数企业使用监控工具的原因，最常见的监控工具有 Prometheus、Datadog 以及 Sysdig 等。而不同的工具所使用的格式也有所区别。在我们可以使用 Kubernetes API 聚合来暴露 endpoint 之前，我们需要将指标转换为合适的格式。此时需要使用小型的 adapter（适配器）——它可能是监控工具的一部分，也可能作为一个单独的组件，它在监控工具和 Kubernetes API 之间架起了一座桥梁。例如，Prometheus 有专门的 Prometheus adapter 或者 Datadog 有 Datadog Cluster Agent — 它们位于监控工具和 API 之间，并从一种格式转换到另一个种格式，如下图所示。这些指标在稍微不同的 endpoint 都可以使用。</p><img src="/kubernetes/k8s-automatic-elastic-expansion/640-20210302223153947" class="" title="图片"><h2 id="Demo：Kubernetes-自动伸缩"><a href="#Demo：Kubernetes-自动伸缩" class="headerlink" title="Demo：Kubernetes 自动伸缩"></a>Demo：Kubernetes 自动伸缩</h2><p>我们将演示如何使用自定义指标自动伸缩应用程序，并且借助 Prometheus 和 Prometheus adapter。你可以继续阅读文章，或者直接访问 Github repo 开始构建 demo：</p><p><a href="https://github.com/infracloudio/kubernetes-autoscaling">https://github.com/infracloudio/kubernetes-autoscaling</a></p><h3 id="设置-Prometheus"><a href="#设置-Prometheus" class="headerlink" title="设置 Prometheus"></a>设置 Prometheus</h3><p>为了让适配器可以使用指标，我们将使用 Prometheus Operator 来安装 Prometheus。它创建 CRD 来在集群中部署 Prometheus 的组件。CRD 是扩展 Kubernetes 资源的一种方式。使用 Operator 可以“以 Kubernetes 的方式”（通过在 YAML 文件中定义对象）轻松配置和维护 Prometheus 实例。由 Prometheus Operator 创建的 CRD 有：</p><ul><li>AlertManager</li><li>ServiceMonitor</li><li>Prometheus</li></ul><p>你可以根据下方链接的指导设置 Prometheus：</p><p><a href="https://github.com/infracloudio/kubernetes-autoscaling#installing-prometheus-operator-and-prometheus">https://github.com/infracloudio/kubernetes-autoscaling#installing-prometheus-operator-and-prometheus</a></p><h3 id="部署-Demo-应用程序"><a href="#部署-Demo-应用程序" class="headerlink" title="部署 Demo 应用程序"></a>部署 Demo 应用程序</h3><p>为了生成指标，我们将部署一个简单的应用程序 mockmetrics，它将在&#x2F;metrics 处生成 total_hit_count 值。这是一个用 Go 写的网络服务器。当 URL 被访问时，指标 total_hit_count 的值会不断增加。它使用 Prometheus 所要求的展示格式来显示指标。</p><p>根据以下链接来为这一应用程序创建 deployment 和服务，它同时也为应用程序创建 ServiceMonitor 和 HPA：</p><p><a href="https://github.com/infracloudio/kubernetes-autoscaling#deploying-the-mockmetrics-application">https://github.com/infracloudio/kubernetes-autoscaling#deploying-the-mockmetrics-application</a></p><h3 id="ServiceMonitor"><a href="#ServiceMonitor" class="headerlink" title="ServiceMonitor"></a>ServiceMonitor</h3><p>ServiceMonitor 为 Prometheus 创建了一个配置。它提到了服务的标签、路径、端口以及应该在什么时候抓取指标的时间间隔。在服务 label 的帮助下，选择了 pods。Prometheus 会从所有匹配的 Pod 中抓取指标。根据你的 Prometheus 配置，ServiceMonitor 应该放在相应的命名空间中。在本例中，它和 mockmetrics 在同一个命名空间。</p><h3 id="部署和配置-Prometheus-Adapter"><a href="#部署和配置-Prometheus-Adapter" class="headerlink" title="部署和配置 Prometheus Adapter"></a>部署和配置 Prometheus Adapter</h3><p>现在要为 HPA 提供 custom.metrics.k8s.io API endpoint，我们将部署 Prometheus Adapter。Adapter 希望它的配置文件在 Pod 中可用。我们将创建一个 configMap 并将其挂载在 pod 内部。我们还将创建 Service 和 APIService 来创建 API。APIService 将&#x2F;api&#x2F;custom.metrics.k8s.io&#x2F;v1beta1endpoint 添加到标准的 Kubernetes APIs。你可以根据以下教程来实现这一目标：</p><p><a href="https://github.com/infracloudio/kubernetes-autoscaling#deploying-the-custom-metrics-api-server-prometheus-adapter">https://github.com/infracloudio/kubernetes-autoscaling#deploying-the-custom-metrics-api-server-prometheus-adapter</a></p><p>接下来，我们看一下配置：</p><ul><li>seriesQuery 用于查询 Prometheus 的资源，标签为“default“和”mockmetrics-service“。</li><li>resources 部分提到标签如何被映射到 Kubernetes 资源。针对我们的情况，它将“namespace“标签与 Kubernetes 的”namespace“进行映射，服务也是如此。</li><li>metricsQuery 又是一个 Prometheus 查询，它可以将指标导入 adapter。我们使用的查询是获取 2 分钟内所有匹配 regexmockmetrics-deploy-(.*)的 pods 的平均 total_hit_count 总和。</li></ul><h3 id="Kubernetes-自动伸缩实践"><a href="#Kubernetes-自动伸缩实践" class="headerlink" title="Kubernetes 自动伸缩实践"></a>Kubernetes 自动伸缩实践</h3><p>一旦你根据下文中的步骤进行，指标值会不断增加。我们现在就来看 HPA：</p><p><a href="https://github.com/infracloudio/kubernetes-autoscaling#scaling-the-application">https://github.com/infracloudio/kubernetes-autoscaling#scaling-the-application</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get hpa -w</span><br><span class="line">NAME                  REFERENCE                       TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">mockmetrics-app-hpa   Deployment/mockmetrics-deploy   0/100     1         10        1          11h</span><br><span class="line">mockmetrics-app-hpa   Deployment/mockmetrics-deploy   56/100    1         10        1          11h</span><br><span class="line">mockmetrics-app-hpa   Deployment/mockmetrics-deploy   110/100   1         10        1          11h</span><br><span class="line">mockmetrics-app-hpa   Deployment/mockmetrics-deploy   90/100    1         10        2          11h</span><br><span class="line">mockmetrics-app-hpa   Deployment/mockmetrics-deploy   126/100   1         10        2          11h</span><br><span class="line">mockmetrics-app-hpa   Deployment/mockmetrics-deploy   306/100   1         10        2          11h</span><br><span class="line">mockmetrics-app-hpa   Deployment/mockmetrics-deploy   171/100   1         10        4          11h</span><br></pre></td></tr></table></figure><p>你可以看到当该值达到目标值时，副本数如何增加。</p><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><p>自动伸缩的整体流程如下图所示：</p><img src="/kubernetes/k8s-automatic-elastic-expansion/640-20210302223153959" class="" title="图片"><h2 id="结-论"><a href="#结-论" class="headerlink" title="结 论"></a>结 论</h2><p>你可以从下方链接中了解更多相关项目和参考资料。在过去的几个版本中，Kubernetes 中的监控流水线已经大有发展，而 Kubernetes 的自动伸缩主要基于该流水线工作。如果你不熟悉这个环境，很容易感到困惑和迷茫。</p><p><a href="https://github.com/infracloudio/kubernetes-autoscaling#other-references-and-credits">https://github.com/infracloudio/kubernetes-autoscaling#other-references-and-credits</a></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 弹性伸缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>创建一个成熟的 GitOps 流水线需要做哪些决定？</title>
      <link href="/cicd/create-a-full-fledged-gitops-pipeline/"/>
      <url>/cicd/create-a-full-fledged-gitops-pipeline/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/cicd/create-a-full-fledged-gitops-pipeline/" target="_blank" title="https://www.xtplayer.cn/cicd/create-a-full-fledged-gitops-pipeline/">https://www.xtplayer.cn/cicd/create-a-full-fledged-gitops-pipeline/</a></p><p>在软件交付领域，GitOps 是近期的热门趋势，它沿袭并扩展了 DevOps、基础架构即代码和 CI&#x2F;CD 等趋势。我们此前发布了许多关于 GitOps 的入门文章，您可以在公众号后台回复【<strong>Git 文章</strong>】获取 GitOps 文章合集。</p><p>GitOps 的优势可以简单地归纳如下：</p><ul><li>自由地审计更改</li><li>持续集成和交付</li><li>更好地控制更改管理</li></ul><p>然而，现实情况却是构建 GitOps 流水线并非易事，它涉及到许多大大小小的决定，而这些决定会给实施工作带来许多麻烦。我们将这些决定称为“GitOps 架构”，它可能会导致实施过程中面临许多挑战。</p><p>好的方面是只要有一定的规划和经验，就可以大大减少过渡到 GitOps 交付模式的痛苦。</p><p>在本文中，我将通过一家公司的故事来解释其中的一些挑战。这家公司从一个零散的小创业公司采用 GitOps，成长为一家规范的跨国企业。虽然这种加速成长的情况很少见，但它确实反映了大型组织中的许多团队从概念验证，到最小可行性产品（minimum viable product），再到成熟系统的经验。</p><h2 id="简单的开始"><a href="#简单的开始" class="headerlink" title="简单的开始"></a>简单的开始</h2><p>如果你刚刚开始，最简单的做法是创建一个单一的 Git repo，将所有需要的代码都放在里面。其中可能包括：</p><ul><li>应用程序代码</li><li>Dockerfile，用于构建应用程序镜像</li><li>一些 CI&#x2F;CD 流水线代码（例如 GitLab CI&#x2F;CD 或 GitHub Actions）</li><li>Terraform，以配置运行应用程序所需资源</li></ul><p>此外，所有的更改都是直接对 master 进行改动，因此更改可以直接生效。</p><p>这一方法的主要优势在于你有单一的参考点，以及所有代码都会紧密集成在一起。如果您的所有开发人员都受到完全信任，并且速度就是一切，那么这一方法会持续生效。</p><p>不幸的是，随着你的业务量不断增长，这种方法的弊端很快就会显现出来。</p><p>首先，随着越来越多的代码被添加到代码库中，代码库规模的膨胀会使得工程师们陷入困惑，因为他们会遇到更多必须解决的更改之间的冲突。如果团队成员大幅增长，那么随之而来的重新分配工作或合并会导致进一步的混乱。</p><p>其次，如果您需要分开控制流水线运行，则会遇到困难。有时，您只想快速测试对代码的更改，而不是进行部署以实现完整的端到端交付。这种方法在整体性方面产生了越来越多需要解决的问题，随着这些更改的进行可能会影响其他人的工作。</p><p>第三，随着您的成长，您可能会希望工程师和团队之间的责任边界更加细化。这可以通过一个单一的 repo 来实现，并且一个 repo 通常是一个更清晰和更干净的边界。</p><img src="/cicd/create-a-full-fledged-gitops-pipeline/640-20210302222646405" class="" title="图片"><h2 id="分离-Repository"><a href="#分离-Repository" class="headerlink" title="分离 Repository"></a>分离 Repository</h2><p>随着业务的增长，流水线会越来越拥挤，merge 开始变得痛苦。此外，您的团队也需要专业化，将不同的责任领域划分给不同的成员。</p><p>所以你需要将 Repo 分离出来。这时，你首先要面对大量的决定，譬如 repo 应该分离到什么程度？是否需要为应用程序代码建立一个单独的 repo？看起来是不是很合理？然后把 Docker 构建的东西也一起放进去？那这样的分离其实没有什么意义。</p><p>那所有团队的 Terraform 代码呢？应该放在一个新的 repo 里吗？这听起来很合理，但是：新创建的中央“平台”团队想要控制对 AWS 中核心 IAM（身份和访问管理）规则定义的访问，而团队 RDS 配置代码也在其中，开发团队需要定期对其进行调整。</p><p>所以你决定将 Terraform 分离成两个 repo：一个是“平台”repo，一个是“特定应用程序”repo。这就带来了另一个挑战，因为你现在还需要分离 Terraform 的状态文件。这并不是一个无法解决的问题，但这也并不是您所习惯的快速功能交付，所以产品经理将不得不解释为什么功能请求所需的时间比之前更长。</p><p>不幸的是，这些 GitOps 决策还没有既定的最佳实践或模式。</p><p>分离的问题还不止于此。以前，流水线内构建的组件之间的协调是微不足道的（因为所有需要的组件都是共处的），而现在你必须协调 repo 之间的信息流。例如：当构建一个新的 Docker 镜像时，这可能需要触发集中式平台 repo 中的部署，同时将新的镜像名称作为触发的一部分传递过来。</p><p>同样，这些也不是无法解决的问题，但在构建 GitOps 流水线的早期，这些挑战更容易实现。后来，当步骤、政策和流程更加成熟时，再做出改变就需要付出更多的时间代价。</p><h2 id="分布式-vs-集中式"><a href="#分布式-vs-集中式" class="headerlink" title="分布式 vs 集中式"></a>分布式 vs 集中式</h2><p>你的业务正在增长，你正在构建越来越多的应用程序和服务。越来越明显的是，在如何构建和部署应用程序方面，你需要某种结构上的一致性。中央平台团队需要尝试执行这些标准。但是你可能会遭到开发团队的反对，他们认为与在 DevOps 和 GitOps 出现之前，在集中式 IT 中他们被赋予了更多的自治和控制权。</p><p>如果上述情况您觉得似曾相识，那可能是因为 GitOps 和应用架构领域的单体与微服务争论之间有些类似。就像你在这些争论中看到的那样，随着系统的成熟，规模和范围的扩大，分布式和集中式 IT 之间的紧张关系会越来越频繁地出现。</p><p>从某种层面上来说，你的 GitOps 流程就像其他分布式系统一样，如果你设计得不好，其中一个部分出现问题可能会产生难以预料的问题。</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>在你决定分离 repo 的同时，你意识到你需要一种一致的方式来管理不同的部署环境。而直接上线已经行不通了，因为此时需要 QA 团队，在上线之前测试更改。</p><p>现在你需要为你的应用镜像在测试和 QA 环境中指定不同的 Docker 标签，你可能还希望在不同的环境中启用不同大小的实例大小或副本功能。你如何在源码中管理这些不同环境的配置？一个比较直接的方法是为每个环境建立一个单独的 Git 仓库（如：super-app-dev，super-app-qa，super-app-live）。</p><p>分离 repo 有 “泾渭分明” 的好处，就像我们在上面划分 Terraform 代码时看到的那样。然而，很少有人最终会喜欢这种解决方案，因为大多数团队不具备 Git 知识和相关专业水平，进而在不同的 repo 之间移植更改。更为复杂的是，repo 之间必然会存在很多重复的代码，而且随着时间的推移，也可能会出现很多漂移（drift）。</p><p>如果你想把事情保持在一个单一的 repo 中，你至少有三种选择：</p><ul><li>每个环境都有一个目录</li><li>每个环境都有一个分支</li><li>每个环境有一个标签</li></ul><img src="/cicd/create-a-full-fledged-gitops-pipeline/640-20210302222705860" class="" title="图片"><h2 id="同步步骤选择"><a href="#同步步骤选择" class="headerlink" title="同步步骤选择"></a>同步步骤选择</h2><p>如果你严重依赖 YAML 生成工具或模板，您可以考虑另一种方式。例如，Kustomize 强烈鼓励基于目录的环境分离。如果您使用的是原始 YAML，那么分支或标记的方法会更适合您。</p><img src="/cicd/create-a-full-fledged-gitops-pipeline/640-20210302222718451" class="" title="图片"><h2 id="运行时环境颗粒度"><a href="#运行时环境颗粒度" class="headerlink" title="运行时环境颗粒度"></a>运行时环境颗粒度</h2><p>然而，在您的运行时环境中，可以选择您想要什么级别的分离。在集群层面，如果您使用的是 Kubernetes，你可以在以下几种情况下选择：</p><ul><li>一个集群管理所有</li><li>每个环境一个集群</li><li>每个团队一个集群</li></ul><p>在极端情况下，你可以把所有的环境放到一个集群中。不过通常情况下，在大多数组织中至少有一个单独的集群用于生产。</p><p>一旦你想好了你的集群策略，在命名空间层面，你仍然可以选择：</p><ul><li>每个环境都有一个命名空间</li><li>每个应用程序&#x2F;服务拥有一个命名空间</li><li>每个工程师拥有一个命名空间</li><li>每个构建都有一个命名空间</li></ul><p>平台团队通常从 “dev”、“test”、“prod” 的命名空间设置开始，然后才意识到他们想要更精细地分化团队的工作。</p><p>你也可以混合和匹配这些选项——例如，为每个工程师提供”desk testing “命名空间，以及每个团队的命名空间。</p><h2 id="总-结"><a href="#总-结" class="headerlink" title="总 结"></a>总 结</h2><p>我们在这里只是对成熟的 GitOps 流程所需的决策领域做了一些简单的介绍。如果您的企业真的成长为那家跨国企业，你也可以考虑 RBAC&#x2F;IAM 等要求。</p><p>通常情况下，推出 GitOps 会让人觉得只是一种投资，可能最终没有太多令人满意的产出。但是在 GitOps 之前，团队常常会经历混乱和延迟，因为没有人能够确定任何东西应该是什么状态。这些都会导致二次成本，因为审计人员会进行抽查，而因意外和未记录的更改而导致的中断则占据了员工大量的注意力，这是一个很高的成本。</p><p>然而，随着你的 GitOps 流程的愈发成熟，好处倍增，该流程会解决许多之前存在的问题。但更多的时候，你面临的压力是要更快地展现出 GitOps 流程的优势。</p><p>目前 GitOps 最大的挑战是，没有既定的模式或是最佳实践来指导你的选择。GitOps 顾问，通常也只是引导团队找到最适合他们的解决方案，并根据经验将团队往某些方向引导。</p><p>但我观察到的情况是，早期因为看起来 “太复杂”而被放弃的选择，往往后来都会为此后悔。这并不意味着你应该直接跳到为每个构建创建一个命名空间，每个团队拥有一个 Kubernetes 集群的程度，原因有二：</p><ul><li>每当你给 GitOps 架构增加复杂性时，你最终都会增加交付一个可行的 GitOps 解决方案的成本和时间</li><li>你可能真的永远都不需要这种设置</li></ul><p>在我们接受这个领域的可行标准之前，正确的 GitOps 架构永远是一门艺术，而不是科学。</p>]]></content>
      
      
      <categories>
          
          <category> cicd </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Understanding the Kubernetes Node</title>
      <link href="/kubernetes/understanding-the-kubernetes-node/"/>
      <url>/kubernetes/understanding-the-kubernetes-node/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/understanding-the-kubernetes-node/" target="_blank" title="https://www.xtplayer.cn/kubernetes/understanding-the-kubernetes-node/">https://www.xtplayer.cn/kubernetes/understanding-the-kubernetes-node/</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>With over 48,000 stars on GitHub, more than 75,000 commits, and with major contributors like Google and Red Hat, Kubernetes has rapidly taken over the container ecosystem to become the true leader of container orchestration platforms. Kubernetes offers great features like rolling and rollback of deployments, container health checks, automatic container recovery, container auto-scaling based on metrics, service load balancing, service discovery (great for microservice architectures), and more. In this article, we will speak about some basic Kubernetes concepts and its master node architecture, concentrating on the node components.</p><h2 id="Understanding-Kubernetes-and-Its-Abstractions"><a href="#Understanding-Kubernetes-and-Its-Abstractions" class="headerlink" title="Understanding Kubernetes and Its Abstractions"></a>Understanding Kubernetes and Its Abstractions</h2><p>Kubernetes is an open-source orchestration engine for automating deployments, scaling, managing, and providing the infrastructure to host containerized applications. At the infrastructure level, a Kubernetes cluster is comprised of a set of physical or virtual machines, each acting in a specific role.</p><p>The <strong>master</strong> machines act as the brain of all operations and are charged with orchestrating containers that run on all of the <strong>node</strong> machines. Each node is equipped with a container runtime. The node receives instruction from the master and then takes actions to either create pods, delete them, or adjust networking rules.</p><img src="/kubernetes/understanding-the-kubernetes-node/01-rancher-k8s-node-components-architecture.png" class="" title="Fig. 1: Kubernetes node components architecture">Fig. 1: Kubernetes node components architecture<ul><li><a href="https://kubernetes.io/docs/concepts/overview/components/#master-components">Master components</a> are responsible for managing the Kubernetes cluster. They manage the life cycle of pods, the base unit of a deployment within a Kubernetes cluster. Master servers run the following components:<ul><li><code>kube-apiserver</code> - the main component, exposing APIs for the other master components.</li><li><code>etcd</code> - distributed key&#x2F;value store which Kubernetes uses for persistent storage of all cluster information.</li><li><code>kube-scheduler</code> - uses information in the pod spec to decide on which node to run a pod.</li><li><code>kube-controller-manager</code> - responsible for node management (detecting if a node fails), pod replication, and endpoint creation.</li><li><code>cloud-controller-manager</code> - daemon acting like an abstraction layer between the APIs and the different cloud providers’ tools (storage volumes, load balancers etc.)</li></ul></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#what-is-a-node">Node components</a> are worker machines in Kubernetes and are managed by the Master. A node may be a virtual machine (VM) or physical machine, and Kubernetes runs equally well on both types of systems. Each node contains the necessary components to run pods:<ul><li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"><code>kubelet</code></a> - watches the API server for pods on that node and makes sure they are running</li><li><a href="https://github.com/google/cadvisor"><code>cAdvisor</code></a> - collects metrics about pods running on that particular node</li><li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/"><code>kube-proxy</code></a> - watches the API server for pods&#x2F;services changes in order to maintain the network up to date</li><li>container runtime - responsible for managing container images and running containers on that node</li></ul></li></ul><h2 id="Kubernetes-Node-Components-in-Detail"><a href="#Kubernetes-Node-Components-in-Detail" class="headerlink" title="Kubernetes Node Components in Detail"></a>Kubernetes Node Components in Detail</h2><p>To summarize, the node runs the two most important components, the <code>kubelet</code> and the <code>kube-proxy</code>, as well as a container engine in charge of running the containerized applications.</p><h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><p>The <code>kubelet</code> agent handles all communication between the master and the node on which it is running. It receives commands from the master in the form of a <strong>manifest</strong> which defines the workload and the operating parameters. It interfaces with the container runtime that is responsible for creating, starting, and monitoring pods.</p><p>The <code>kubelet</code> also periodically executes any configured liveness probes and readiness checks. It constantly monitors the state of the pods and, in the event of a problem, launches a new instance instead. The <code>kubelet</code> also has an internal HTTP server exposing a read-only view at port 10255. There’s a health check endpoint at <code>/healthz</code> and also a few additional status endpoints. For example, we can get a list of running pods at <code>/pods</code>. We can also get specs of the machine the <code>kubelet</code> is running on at <code>/spec</code>.</p><h3 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h3><p>The <code>kube-proxy</code> component runs on each node and proxies UDP, TCP, and SCTP packets (it doesn’t understand HTTP). It maintains the network rules on the host and handles transmission of packets between pods, the host, and the outside world. It acts like a network proxy and load balancer for pods running on the node by implementing east&#x2F;west load-balancing using NAT in <code>iptables</code>.</p><p>The <code>kube-proxy</code> process stands in between the network Kubernetes is attached to and the pods that are running on that particular node. It is essentially the core networking component of Kubernetes and is responsible for ensuring that communication is maintained efficiently across all elements of the cluster. When a user creates a Kubernetes service object, the <code>kube-proxy</code> instance is responsible to translate that object into meaningful rules in the local <code>iptables</code> rule set on the worker node. <code>iptables</code> is used to translate the virtual IP assigned to the service object to all of the pod IPs mapped by the service.</p><h3 id="Container-Runtime"><a href="#Container-Runtime" class="headerlink" title="Container Runtime"></a>Container Runtime</h3><p>The container runtime is responsible for pulling the images from public or private registries and running containers based on those images. The most popular engine is <a href="https://www.docker.com/">Docker</a>, although Kubernetes supports container runtimes from <a href="https://coreos.com/rkt/">rkt</a>, <a href="https://github.com/opencontainers/runc">runc</a> and <a href="https://github.com/opencontainers/runtime-spec">others</a>. As previously mentioned, <code>kubelet</code> interacts directly with container runtime to start, stop, or delete containers.</p><h3 id="cAdvisor"><a href="#cAdvisor" class="headerlink" title="cAdvisor"></a>cAdvisor</h3><p><code>cAdvisor</code> is an open-source agent that monitors resource usage and analyzes the performance of containers. Originally created by Google, cAdvisor is now integrated with <code>kubelet</code>.</p><p>The <code>cAdvisor</code> instance on each node collects, aggregates, processes, and exports metrics such as CPU, memory, file, and network usage for all running containers. All data is sent to the scheduler to ensure that it knows about the performance and resource usage inside of the node. This information is used to perform various orchestration tasks like scheduling, horizontal pod scaling, and managing container resource limits.</p><h2 id="Observing-Node-Component-Endpoints"><a href="#Observing-Node-Component-Endpoints" class="headerlink" title="Observing Node Component Endpoints"></a>Observing Node Component Endpoints</h2><p>Next, we will be installing a Kubernetes cluster (with the help of Rancher) so we can explore few of the APIs exposed by the node components. To perform this demo, we will need the following: - a Google Cloud Platform account, the free tier provided is more than enough (any other cloud should work the same) - a host where Rancher will be running (can be a personal PC&#x2F;Mac or a VM in a public cloud) - on same host <a href="https://cloud.google.com/sdk/install">Google Cloud SDK</a> should be installed along <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a>. Make sure that <code>gcloud</code> has access to your Google Cloud account by authenticating with your credentials (<code>gcloud init</code> and <code>gcloud auth login</code>). - Kubernetes cluster running on Google Kubernetes Engine (running EKS or AKS should be the same)</p><h3 id="Starting-a-Rancher-Instance"><a href="#Starting-a-Rancher-Instance" class="headerlink" title="Starting a Rancher Instance"></a>Starting a Rancher Instance</h3><p>To begin, start a Rancher instance. There is a very intuitive <a href="https://rancher.com/quick-start/">getting started guide for Rancher</a> that you can follow for this purpose.</p><h3 id="Using-Rancher-to-deploy-a-GKE-cluster"><a href="#Using-Rancher-to-deploy-a-GKE-cluster" class="headerlink" title="Using Rancher to deploy a GKE cluster"></a>Using Rancher to deploy a GKE cluster</h3><p>Use Rancher to set up and configure your Kubernetes cluster, follow the how-to <a href="https://rancher.com/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/gke/">guide</a>.</p><p>As soon as cluster is deployed, we can make a quick Nginx deployment to use for testing:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> nginx_deployment.yaml</span><br><span class="line">---</span><br><span class="line"> apiVersion: extensions/v1beta1</span><br><span class="line"> kind: Deployment</span><br><span class="line"> metadata:</span><br><span class="line">   name: nginx-deployment</span><br><span class="line"> spec:</span><br><span class="line">   replicas: 3</span><br><span class="line">   template:</span><br><span class="line">     metadata:</span><br><span class="line">       labels:</span><br><span class="line">         app: nginx</span><br><span class="line">     spec:</span><br><span class="line">       containers:</span><br><span class="line">         - name: nginx</span><br><span class="line">           image: nginx</span><br><span class="line">           ports:</span><br><span class="line">             - containerPort: 80</span><br><span class="line">kubectl apply -f nginx_deployment.yaml</span><br><span class="line">deployment.extensions <span class="string">&quot;nginx-deployment&quot;</span> created</span><br></pre></td></tr></table></figure><p>In order to interact with Kubernetes API, we need to start a proxy server on your local machine:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl proxy &amp;</span><br><span class="line">[1] 3349</span><br><span class="line">$ Starting to serve on 127.0.0.1:8001</span><br></pre></td></tr></table></figure><p>Let’s check the progress to see it is running and listening to the default port:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">netstat -anp | grep 8001 | grep LISTEN</span><br><span class="line">tcp        0      0 127.0.0.1:8001          0.0.0.0:*               LISTEN      3349/kubectl</span><br></pre></td></tr></table></figure><p>Now, in your browser, check the various endpoints that <code>kubelet</code> exposes:</p><img src="/kubernetes/understanding-the-kubernetes-node/02-rancher-k8s-node-components-endpoints-all.png" class="" title="Fig. 2: kubelet API endpoints"><p>Fig. 2: kubelet API endpoints</p><p>Next, display the list of your cluster’s available nodes:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line">NAME                                     STATUS   ROLES    AGE     VERSION</span><br><span class="line">gke-c-cvknh-default-pool-fc8937e2-h2n9   Ready    &lt;none&gt;   3h13m   v1.12.5-gke.5</span><br><span class="line">gke-c-cvknh-default-pool-fc8937e2-hnfl   Ready    &lt;none&gt;   3h13m   v1.12.5-gke.5</span><br><span class="line">gke-c-cvknh-default-pool-fc8937e2-r4z1   Ready    &lt;none&gt;   3h13m   v1.12.5-gke.5</span><br></pre></td></tr></table></figure><p>We can check the <code>spec</code> for any of the listed nodes using the API. In this example, we created a 3 node cluster, using the <code>n1-standard-1</code> machine type (1 vCPU, 3.75GB RAM, root size disk of 10GB). We can confirm these specs by accessing the dedicated endpoint:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl http://localhost:8001/api/v1/nodes/gke-c-cvknh-default-pool-fc8937e2-h2n9/proxy/spec/</span><br><span class="line">curl http://localhost:8001/api/v1/nodes/gke-c-cvknh-default-pool-fc8937e2-hnfl/proxy/spec/</span><br><span class="line">curl http://localhost:8001/api/v1/nodes/gke-c-cvknh-default-pool-fc8937e2-r4z1/proxy/spec/</span><br></pre></td></tr></table></figure><img src="/kubernetes/understanding-the-kubernetes-node/04-rancher-k8s-node-components-endpoints-spec.png" class="" title="Fig. 3: Kubernetes node spec listing"><p>Fig. 3: Kubernetes node spec listing</p><p>Using same <code>kubelet</code> API with a different endpoint, we can check the Nginx pods we created to see what nodes they are running on.</p><p>First, list the running pods:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get pods</span><br><span class="line">NAME                               READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-c5b5c6f7c-d429q   1/1       Running   0          1m</span><br><span class="line">nginx-deployment-c5b5c6f7c-w7qtc   1/1       Running   0          1m</span><br><span class="line">nginx-deployment-c5b5c6f7c-x9t8g   1/1       Running   0          1m</span><br></pre></td></tr></table></figure><p>Now, we can <code>curl</code> the <code>/proxy/pods</code> endpoint for each node to see the list of pods its running:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl http://localhost:8001/api/v1/nodes/gke-c-cvknh-default-pool-fc8937e2-h2n9/proxy/pods</span><br><span class="line">curl http://localhost:8001/api/v1/nodes/gke-c-cvknh-default-pool-fc8937e2-hnfl/proxy/pods</span><br><span class="line">curl http://localhost:8001/api/v1/nodes/gke-c-cvknh-default-pool-fc8937e2-r4z1/proxy/pods</span><br></pre></td></tr></table></figure><img src="/kubernetes/understanding-the-kubernetes-node/03-rancher-k8s-node-components-endpoint-pods.png" class="" title="Fig. 4: Kubernetes node" alt="s list of pods"><p>Fig. 4: Kubernetes node’s list of pods</p><p>We can also check the cAdvisor endpoint, which outputs lots of data in Prometheus format. By default, this is available at the <code>/metrics</code> HTTP endpoint:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl http://localhost:8001/api/v1/nodes/gke-c-cvknh-default-pool-fc8937e2-h2n9/proxy/metrics/cadvisor</span><br></pre></td></tr></table></figure><img src="/kubernetes/understanding-the-kubernetes-node/05-rancher-k8s-node-components-endopoint-cAdvisor.png" class="" title="Fig. 5: Kubernetes &#x2F;metrics endpoint"><p>Fig. 5: Kubernetes &#x2F;metrics endpoint</p><p>The same cAdvisor or pod information can also be obtained by SSHing into the node and making direct calls to the <code>kubelet</code> port:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gcloud compute ssh admin@gke-c-cvknh-default-pool-fc8937e2-h2n9 --zone europe-west4-c</span><br><span class="line"></span><br><span class="line">admin@gke-c-cvknh-default-pool-fc8937e2-h2n9 ~ $ curl localhost:10255/metrics/cadvisor</span><br><span class="line">admin@gke-c-cvknh-default-pool-fc8937e2-h2n9 ~ $ curl localhost:10255/pods</span><br></pre></td></tr></table></figure><h3 id="Cleanup"><a href="#Cleanup" class="headerlink" title="Cleanup"></a>Cleanup</h3><p>To clean up the resources we used in this article, let’s delete the Kubernetes cluster from Rancher UI (simply select the cluster and hit the <code>Delete</code> button). This will remove all the nodes our cluster was using, along with the associated IP addresses. If you used a VM in a public cloud to run Rancher, you will need to take care of that too. Find out your instance name, then delete it:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gcloud compute --project=&lt;&lt;<span class="string">YOUR_PROJECT_ID&gt;&gt; instances list</span></span><br><span class="line"><span class="string">$ gcloud compute --project=&lt;&lt;YOUR_PROJECT_ID</span>&gt;&gt; instances delete &lt;&lt;<span class="string">INSTANCE_NAME&gt;&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>In this article, we’ve discussed the key components of Kubernetes node machines. Afterwards, we deployed a Kubernetes cluster using Rancher and made a small deployment to help us play with the <code>kubelet</code> API. To learn more about Kubernetes and its architecture, a great starting point is the official <a href="https://kubernetes.io/docs/concepts/overview/components/">documentation</a>.</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes: Tackling Resource Consumption</title>
      <link href="/kubernetes/kubernetes-tackling-resource-consumption/"/>
      <url>/kubernetes/kubernetes-tackling-resource-consumption/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/kubernetes-tackling-resource-consumption/" target="_blank" title="https://www.xtplayer.cn/kubernetes/kubernetes-tackling-resource-consumption/">https://www.xtplayer.cn/kubernetes/kubernetes-tackling-resource-consumption/</a></p><p>This is the third of a series of three articles focusing on <a href="https://rancher.com/tags/security">Kubernetes security</a>: <a href="https://rancher.com/blog/2019/kubernetes-security-outside-attacks">the outside attack</a>, <a href="https://rancher.com/blog/2019/secure-kubernetes-from-inside-threads">the inside attack</a>, and dealing with resource consumption or noisy neighbors.</p><p>A concern for many administrators setting up a multi-tenant Kubernetes cluster is how to prevent a co-tenant from becoming a “noisy neighbor,” one who monopolizes CPU, memory, storage and other resources. As a result, this noisy neighbor negatively impacts the performance of other users’ resources that share the infrastructure.</p><p>Keeping track of the resource usage of Kubernetes containers and Pods is an important function, not only to keep the container orchestration system running optimally and to reduce operating costs, but also to strengthen the overall security posture of Kubernetes.</p><p>Some operations teams might not consider resource consumption a security issue on par with protecting Kubernetes from internal and external cyberattacks, but they should. That’s because skilled hackers can find ways to exploit a poorly functioning infrastructure to access Kubernetes components, experts say.</p><p>“Security is not just, ‘don’t break into my house,’ but also, ‘how do I keep my house running nicely all the time,’” said Adrian Goins, a Senior Solutions Architect with Rancher Labs, the company that makes Rancher, a complete container management platform for Kubernetes.</p><p>Operations teams need to maximize resources consumed by Kubernetes Pods – a group of one or more containers with shared storage and network resources – to ensure optimal performance for every user and to monitor usage for cost allocations. “Usage equals cost,” Goins says, “because Kubernetes resources run on the underlying compute infrastructure of cloud providers like Amazon Web Services, Google or Microsoft. Even when a cluster runs on bare-metal physical infrastructure in a datacenter, excess usage costs money in hardware, power, and other resources.”</p><p>By default, containers are provisioned without any limits on the amount of resources they can consume. If the container does not operate efficiently, the organization deploying the container will pay for the excess. Thankfully, Kubernetes has features that help operations teams manage and optimize Kubernetes resource utilization capabilities.</p><h2 id="Managing-Resources-in-Pods"><a href="#Managing-Resources-in-Pods" class="headerlink" title="Managing Resources in Pods"></a>Managing Resources in Pods</h2><p>When administrators define a Pod, they can optionally specify how much CPU and memory (RAM) each container needs. When containers have resource requests specified, the scheduler can make better decisions about which nodes to place Pods on. And when containers have their limits specified, contention for resources on a node can be handled in a specified manner, according to the Kubernetes documentation.</p><p>By default, all resources in a Kubernetes cluster are created in a default Namespace. Namespaces are a way to logically group cluster resources and include options for specifying resource quotas.</p><p>Administrators can set resource limits or quotas on a Namespace, stating that a workload or application running in the Namespace is allotted a certain amount of CPU, RAM or storage – the three resources within a Kubernetes cluster. “If launching another resource in the Namespace would exceed the quota, then nothing else gets to launch,” Goins noted.</p><p>“When you apply a resource quota, you are forcing everything that runs in that Namespace to set a resource limit for itself. There are two types of limits: a reservation and a maximum,” Goins explained. For example, with a reservation, an administrator can have a Kubernetes cluster allocate 128 megabytes of RAM for a WordPress site. For every WordPress Pod deployed, there will be a guaranteed 128 megabytes of RAM from the server itself. Consequently, if an administrator combined a resource request with a resource quota of one gigabyte, then users can only run eight WordPress Pods before they exceed their limit. After that, they won’t be able to tap into anymore RAM.</p><p>The second part of resource limitations is a maximum. An administrator can put in a resource request reservation of 128 megabytes and a maximum of 256 megabytes of RAM. “If a Pod exceeds more than 256 megabytes of RAM usage, Kubernetes will kill it and restart it,” Goins said. “Now you’re protected from runaway processes and noisy neighbors.”</p><h2 id="Projects-and-Resource-Quotas"><a href="#Projects-and-Resource-Quotas" class="headerlink" title="Projects and Resource Quotas"></a>Projects and Resource Quotas</h2><p>A platform such as Rancher is designed to make the management of Kubernetes easier by providing an intuitive interface and centralized management of tasks like the implementation of role descriptions at the global layer.</p><p>As mentioned in the previous article on insider threat protection, Rancher goes beyond Namespaces by including a Project resource that helps ease the administrative burden of clusters. Within Rancher, Projects allow administrators to manage multiple namespaces as a single entity. As a result, Rancher can apply resource quotas to Projects.</p><p>In a standard Kubernetes deployment, resource quotas are applied to individual Namespaces. However, administrators cannot apply the quota to namespaces simultaneously with a single action. Instead, the resource quota must be applied multiple times. In Rancher, an administrator applies a resource quota to the Project, and then the quota propagates to each Namespace. Kubernetes then enforces the admin’s limits using the native version of resource quotas. If administrators want to change the quota for a specific Namespace, they can override the previous quota.</p><h2 id="Fortifying-and-Optimizing-Kubernetes"><a href="#Fortifying-and-Optimizing-Kubernetes" class="headerlink" title="Fortifying and Optimizing Kubernetes"></a>Fortifying and Optimizing Kubernetes</h2><p>Kubernetes has become the container orchestration standard, prompting most cloud and virtualization vendors to offer it as standard infrastructure. However, the general lack of awareness of security issues related to the Kubernetes environment can expose various components to attacks from both inside and outside the network clusters.</p><p>The past two articles have offered some actionable steps organizations can take to strengthen Kubernetes from both external and internal cyber threats by using Kubernetes capabilities and container management solutions such as Rancher. Organizations should secure Kubernetes API access from the outside via role-based access control (RBAC) and strong authentication. And for insider protection, because Kubernetes clusters are multi-user, organizations will need to ensure that cross-communication is protected via RBAC, logical isolation and NetworkPolicies.</p><p>To protect against other tenants monopolizing CPU, memory, storage and other resources dragging down the performance of clusters, Kubernetes provides features such as resource limitation and quotas to help operations teams manage and optimize Kubernetes resource utilization capabilities. Finally, there are some very efficient tools that can help with Kubernetes management and cluster protection beyond the available default settings. A platform like Rancher, a highly optimized container management solution built for organizations that deploy multiple clusters into production environments, makes it easier to manage and run Kubernetes everywhere. It can protect Kubernetes clusters from <a href="https://rancher.com/blog/2019/kubernetes-security-outside-attacks">outside hackers</a>, <a href="https://rancher.com/blog/2019/secure-kubernetes-from-inside-threads">insider threats</a>, and even noisy neighbors.</p><h2 id="Online-Training-in-Kubernetes-and-Rancher"><a href="#Online-Training-in-Kubernetes-and-Rancher" class="headerlink" title="Online Training in Kubernetes and Rancher"></a>Online Training in Kubernetes and Rancher</h2><p>To see Rancher and Kubernetes in action, <a href="https://rancher.com/training">join</a> the weekly intro to Rancher and Kubernetes online training sessions. Hosted by a Rancher and Kubernetes expert, these sessions are free to join and provide a great hands-on overview of both Kubernetes and the Kubernetes-management platform, Rancher.</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Announcing Harvester - Open Source Hyperconverged Infrastructure (HCI) Software</title>
      <link href="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/"/>
      <url>/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/" target="_blank" title="https://www.xtplayer.cn/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/">https://www.xtplayer.cn/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/</a></p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/harvester-featured.png" class="" title="Announcing Harvester: Open Source Hyperconverged Infrastructure (HCI) Software"><p>Today, I am excited to announce project <a href="http://harvesterhci.io/">Harvester</a>, open source hyperconverged infrastructure (HCI) software built using Kubernetes. Harvester provides fully integrated virtualization and storage capabilities on bare-metal servers. No Kubernetes knowledge is required to use Harvester.</p><h2 id="Why-Harvester"><a href="#Why-Harvester" class="headerlink" title="Why Harvester?"></a>Why Harvester?</h2><p>In the past few years, we’ve seen many attempts to bring VM management into container platforms, including our own RancherVM, and other solutions like KubeVirt and Virtlet. We’ve seen some demand for solutions like this, mostly for running legacy software side by side with containers. But in the end, none of these solutions have come close to the popularity of industry-standard virtualization products like vSphere and Nutanix.</p><p>We believe the reason for this lack of popularity is that all efforts to date to manage VMs in container platforms require users to have substantial knowledge of container platforms. Despite Kubernetes becoming an industry standard, knowledge of it is not widespread among VM administrators. They are familiar with concepts like ISO images, disk volumes, NICs and VLANS – not concepts like pods and PVCs.</p><p><em>Enter Harvester.</em></p><p>Project Harvester is an open source alternative to traditional proprietary hyperconverged infrastructure software. Harvester is built on top of cutting-edge open source technologies including Kubernetes, KubeVirt and <a href="https://longhorn.io/">Longhorn</a>. We’ve designed Harvester to be easy to understand, install and operate. Users don’t need to understand anything about Kubernetes to use Harvester and enjoy all the benefits of Kubernetes.</p><h2 id="Harvester-v0-1-0"><a href="#Harvester-v0-1-0" class="headerlink" title="Harvester v0.1.0"></a>Harvester v0.1.0</h2><p>Harvester v0.1.0 has the following features:</p><h3 id="Installation-from-ISO"><a href="#Installation-from-ISO" class="headerlink" title="Installation from ISO"></a>Installation from ISO</h3><p>You can download ISO from the release page on Github and install it directly on bare-metal nodes. During the installation, you can choose to create a new cluster or add the current node into an existing cluster. Harvester will automatically create a cluster based on the information you provided.</p><h3 id="Install-as-a-Helm-Chart-on-an-Existing-Kubernetes-Cluster"><a href="#Install-as-a-Helm-Chart-on-an-Existing-Kubernetes-Cluster" class="headerlink" title="Install as a Helm Chart on an Existing Kubernetes Cluster"></a>Install as a Helm Chart on an Existing Kubernetes Cluster</h3><p>For development purposes, you can install Harvester on an existing Kubernetes cluster. The nodes must be able to support KVM through either hardware virtualization (Intel VT-x or AMD-V) or nested virtualization.</p><h3 id="VM-Lifecycle-Management"><a href="#VM-Lifecycle-Management" class="headerlink" title="VM Lifecycle Management"></a>VM Lifecycle Management</h3><p>Powered by KubeVirt, Harvester supports creating&#x2F;deleting&#x2F;updating operations for VMs, as well as SSH key injection and cloud-init.</p><p>Harvester also provides a graphical console and a serial port console for users to access the VM in the UI.</p><h3 id="Storage-Management"><a href="#Storage-Management" class="headerlink" title="Storage Management"></a>Storage Management</h3><p>Harvester has a built-in highly available block storage system powered by Longhorn. It will use the storage space on the node, to provide highly available storage to the VMs inside the cluster.</p><h3 id="Networking-Management"><a href="#Networking-Management" class="headerlink" title="Networking Management"></a>Networking Management</h3><p>Harvester provides several different options for networking.</p><p>By default, each VM inside Harvester will have a management NIC, powered by Kubernetes overlay networking.</p><p>Users can also add additional NICs to the VMs. Currently, VLAN is supported.</p><p>The multi-network functionality in Harvester is powered by Multus.</p><h3 id="Image-Management"><a href="#Image-Management" class="headerlink" title="Image Management"></a>Image Management</h3><p>Harvester has a built-in image repository, allowing users to easily download&#x2F;manage new images for the VMs inside the cluster.</p><p>The image repository is powered by MinIO.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image01.png" class="" title="Image 01"><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><p>To install Harvester, just load the Harvester ISO into your bare-metal machine and boot it up.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image02.png" class="" title="Image 02"><p>For the first node where you install Harvester, select <strong>Create a new Harvester cluster</strong>.</p><p>Later, you will be prompted to enter the password that will be used to enter the console on the host, as well as “Cluster Token.” The Cluster Token is a token that’s needed later by other nodes that want to join the same cluster.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image03.png" class="" title="Image 03"><p>Then you will be prompted to choose the NIC that Harvester will use. The selected NIC will be used as the network for the management and storage traffic.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image04.png" class="" title="Image 04"><p>Once everything has been configured, you will be prompted to confirm the installation of Harvester.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image05.png" class="" title="Image 05"><p>Once installed, the host will be rebooted and boot into the Harvester console.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image06.png" class="" title="Image 06"><p>Later, when you are adding a node to the cluster, you will be prompted to enter the management address (which is shown above) as well as the cluster token you’ve set when creating the cluster.</p><p>See <a href="https://youtu.be/97ADieBX6bE">here</a> for a demo of the installation process.</p><p>Alternatively, you can install Harvester as a Helm chart on your existing Kubernetes cluster, if the nodes in your cluster have hardware virtualization support. See <a href="https://github.com/rancher/harvester/tree/master/deploy/charts/harvester">here</a> for more details. And <a href="https://youtu.be/TG0GaAD_6J4">here</a> is a demo using Digital Ocean which supports nested virtualization.</p><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>Once installed, you can use the management URL shown in the Harvester console to access the Harvester UI.</p><p>The default user name&#x2F;password is documented <a href="https://github.com/rancher/harvester/blob/master/docs/authentication.md">here</a>.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image07.png" class="" title="Image 07"><p>Once logged in, you will see the dashboard.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image08.png" class="" title="Image 08"><p>The first step to create a virtual machine is to import an image into Harvester.</p><p>Select the <strong>Images</strong> page and click the <strong>Create</strong> button, fill in the URL field and the image name will be automatically filled for you.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image09.png" class="" title="Image 09"><p>Then click <strong>Create</strong> to confirm.</p><p>You will see the real-time progress of creating the image on the <strong>Images</strong> page.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image10.png" class="" title="Image 10"><p>Once the image is finished creating, you can then start creating the VM using the image.</p><p>Select the <strong>Virtual Machine</strong> page, and click <strong>Create</strong>.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image11.png" class="" title="Image 11"><p>Fill in the parameters needed for creation, including volumes, networks, cloud-init, etc. Then click <strong>Create</strong>.</p><p>VM will be created soon.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image12.png" class="" title="Image 12"><p>Once created, click the <strong>Console</strong> button to get access to the console of the VM.</p><img src="/hci/announcing-harvester-open-source-hyperconverged-infrastructure-hci-software/image13.png" class="" title="Image 13"><p>See <a href="https://youtu.be/wVBXkS1AgHg">here</a> for a UI demo.</p>]]></content>
      
      
      <categories>
          
          <category> hci </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HCI </tag>
            
            <tag> Harvester </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Using Hybrid and Multi-Cloud Service Mesh Based Applications for Distributed Deployments</title>
      <link href="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/"/>
      <url>/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/" target="_blank" title="https://www.xtplayer.cn/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/">https://www.xtplayer.cn/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/</a></p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/mesh-featured.png" class="" title="Using Hybrid and Multi-Cloud Service Mesh Based Applications for Distributed Deployments"><p>Service Mesh is an emerging architecture pattern gaining traction today. Along with Kubernetes, Service Mesh can form a powerful platform which addresses the technical requirements that arise in a highly distributed environment typically found on a microservices cluster and&#x2F;or service infrastructure. A Service Mesh is a dedicated infrastructure layer for facilitating service-to-service communications between microservices.</p><p>Service Mesh addresses the communication requirements typical in a microservices-based application, including encrypted tunnels, health checks, circuit breakers, load balancing and traffic permission. Leaving the microservices to address these requirements leads to an expensive and time consuming development process.</p><p>In this blog, we’ll provide an overview of the most common microservice communication requirements that the Service Mesh architecture pattern solves.</p><h2 id="Microservices-Dynamics-and-Intrinsic-Challenges"><a href="#Microservices-Dynamics-and-Intrinsic-Challenges" class="headerlink" title="Microservices Dynamics and Intrinsic Challenges"></a>Microservices Dynamics and Intrinsic Challenges</h2><p>The problem begins when you realize that microservices implement a considerable amount of code not related to the business logic they were originally assigned. Additionally, it’s possible you have multiple microservices implementing similar capabilities in a non-standardized process. In other words, the microservices development team should focus on business logic and leave the low-level communication capabilities to a specific layer.</p><p>Moving forward with our scenario, consider the intrinsic dynamics of microservices. In given time, you may (or most likely will) have multiple instances of a microservice due to several reasons, including:</p><ul><li>Throughput: depending on the incoming requests, you might have a higher or lower number of instances of a microservice</li><li>Canary release</li><li>Blue&#x2F;green deployment</li><li>A&#x2F;B testing</li></ul><p>In short, the microservice-to-microservice communication has specific requirements and issues to solve. The illustration below shows this scenario:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_01.png" class="" title="Image 01"><p>The illustration depicts several technical challenges. Clearly, one of the main responsibilities of Microservice 1 is to balance the load among all Microservice 2 instances. As such, Microservice 1 has to figure out how many Microservice 2 instances we have at the request moment. In other words, Microservice 1 must implement service discovery and load balancing.</p><p>On the other hand, Microservice 2 has to implement some service registration capabilities to tell Microservice 1 when a brand-new instance is available.</p><p>In order to have a fully dynamic environment, these other capabilities should be part of the microservices development:</p><ul><li>Traffic control: a natural evolution of load balancing. We want to specify the number of requests that should go to each of the Microservice 2 instances.</li><li>Encrypted communication between the Microservices 1 and 2.</li><li>Circuit breakers and health checks to address and overcome networking problems.</li></ul><p>In conclusion, the main problem is that the development team is spending significant resources writing complex code not directly related to business logic expected to be delivered by the microservices.</p><h2 id="Potential-Solutions"><a href="#Potential-Solutions" class="headerlink" title="Potential Solutions"></a>Potential Solutions</h2><p>How about externalizing all the non-functional and operational capabilities in an external and standardized component that all microservices can call? For example, the diagram below compiles all capabilities that should not be part of a given microservice. So, after identifying all capabilities, we need to decide where to implement them.</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_02.png" class="" title="Image 02"><h3 id="Solution-1-Encapsulating-all-capabilities-in-a-library"><a href="#Solution-1-Encapsulating-all-capabilities-in-a-library" class="headerlink" title="Solution #1 - Encapsulating all capabilities in a library"></a>Solution #1 - Encapsulating all capabilities in a library</h3><p>The developers would be responsible for calling functions provided by the library to address the microservice communication requirements.</p><p>There are a few drawbacks to this solution:</p><ul><li>It’s a tightly coupled solution, meaning that the microservices are highly dependent on the library.</li><li>It’s not an easy model to distribute or upgrade new versions of the library.</li><li>It doesn’t fit the microservice polyglot principle with different programming languages being applied on different contexts</li></ul><h3 id="Solution-2-Transparent-Proxy"><a href="#Solution-2-Transparent-Proxy" class="headerlink" title="Solution #2 - Transparent Proxy"></a>Solution #2 - Transparent Proxy</h3><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_03.png" class="" title="Image 03"><p>This solution implements the same collection of capabilities. However, with a very different approach: each microservice has a specific component, playing a proxy role, taking care of its incoming and outcoming traffic. The proxy solves the library drawbacks we described before as follows:</p><ul><li>The proxy is transparent, meaning the microservice is not aware it is running nearby and implementing all needed capabilities to communicate with other microservices.</li><li>Since it’s a transparent proxy, the developer doesn’t need to change the code to refer to the proxy. Therefore, upgrading the proxy would be a low-impact process from a microservice development perspective.</li><li>The proxy can be developed using different technologies and programming languages used by microservice.</li></ul><p><strong>The Service Mesh Architectural Pattern</strong></p><p>While a transparent proxy approach brings several benefits to the microservice development team and the microservice communication requirements, there are still some missing parts:</p><ul><li>The proxy is just enforcing policies to implement the communication requirements like load balancing, canary, etc.</li><li>What is responsible for defining such policies and publishing them across all running proxies?</li></ul><p>The solution architecture needs another component. Such components would be used by admins for policy definition and it will be responsible for broadcasting the policies to the proxies.</p><p>The following diagram shows the final architecture which is the service mesh pattern:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_04.png" class="" title="Image 04"><p>As you can see, the pattern comprehends the two main components we’ve described:</p><ul><li>The data plane: also known as sidecar, it plays the transparent proxy role. Again, each microservice will have its own data plane intercepting all incoming and outgoing traffic and applying the policies previously described.</li><li>The control plane: used by the admin to define policies and publish them to the data plane.</li></ul><p>Some important things to note:</p><ul><li>It’s “push-based” architecture. The data plane doesn’t do “callouts” to get the policies: that would be a big network consuming architecture.</li><li>The data plane usually reports usage metrics to the control plane or a specific infrastructure.</li></ul><h2 id="Get-Hands-On-with-Rancher-Kong-and-Kong-Mesh"><a href="#Get-Hands-On-with-Rancher-Kong-and-Kong-Mesh" class="headerlink" title="Get Hands-On with Rancher, Kong and Kong Mesh"></a>Get Hands-On with Rancher, Kong and Kong Mesh</h2><p>Kong provides an enterprise-class and comprehensive service connectivity platform that includes an API gateway, a Kubernetes ingress controller and a Service Mesh implementation. The platform allows customers to deploy on multiple environments such as on premises, hybrid, multi-­­­­­­region and multi-cloud.</p><p>Let’s implement a Service Mesh with a canary release running on a cloud-agnostic Kubernetes cluster, which could include a Google Kubernetes Engine (GKE) cluster or any other Kubernetes distribution. The Service Mesh will be implemented by Kong Mesh (and protected by <a href="https://github.com/Kong/kubernetes-ingress-controller">Kong for Kubernetes</a> as the Kubernetes ingress controller. Generically speaking, the ingress controller is responsible for defining entry points to your Kubernetes cluster, exposing the microservices deployed inside of it and applying consumption policies to it.</p><p>First of all, make sure you have <a href="https://rancher.com/docs/rancher/v2.x/en/installation/">Rancher</a> installed, as well as a Kubernetes cluster running and managed by Rancher. After logging into Rancher, choose the Kubernetes cluster we’re going to work on – in our case “kong-rancher”. Click the <strong>Cluster Explorer</strong> link. You will be redirected to a page like this:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_05.png" class="" title="Image 05"><p>Now, let’s start with the Service Mesh:</p><ol><li><p>Kong Mesh Helm Chart</p><p>Go back to Rancher Cluster Manager home page and choose your cluster again. To add a new catalog, pass your mouse over the “Tools” menu option and click on <strong>Catalogs</strong>. Click the <strong>Add Catalog</strong> button and include <a href="https://kong.github.io/kong-mesh-charts">Kong Mesh’s Helm v3 charts</a> .</p><p>Choose <strong>global</strong> as the scope and <strong>Helm v3</strong> as the Helm version.</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_06.png" class="" title="Image 06"><p>Now click on <strong>Apps</strong> and <strong>Launch</strong> to see Kong Mesh available in the Catalog. Notice that Kong, as a Rancher partner, provides Kong for Kubernetes Helm Charts, by default:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_07.png" class="" title="Image 07"></li><li><p>Install Kong Mesh</p><p>Click on the top menu option <strong>Namespaces</strong> and create a “kong-mesh-system” namespace.</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_08.png" class="" title="Image 08"><p>Pass your mouse over the <strong>kong-rancher</strong> top menu option and click on <strong>kong-rancher active cluster</strong>.</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_09.png" class="" title="Image 09"><p>Click on <strong>Launch kubectl</strong></p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_10.png" class="" title="Image 10"><p>Create a file named “license.json” for the Kong Mesh license you received from Kong. The license follows the format:</p><p>{“license”:{“version”:1,“signature”:“6a7c81af4b0a42b380be25c2816a2bb1d761c0f906ae884f93eeca1fd16c8b5107cb6997c958f45d247078ca50a25399a5f87d546e59ea3be28284c3075a9769”,“payload”:{“customer”:“Kong_SE_Demo_H1FY22”,“license_creation_date”:“2020-11-30”,“product_subscription”:“Kong Enterprise Edition”,“support_plan”:“None”,“admin_seats”:“5”,“dataplanes”:“5”,“license_expiration_date”:“2021-06-30”,“license_key”:“XXXXXXXXXXXXX”}}}</p><p>Now, create a Kubernetes generic secret with the following command:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">create</span> <span class="string">secret</span> <span class="string">generic</span> <span class="string">kong-mesh-license</span> <span class="string">-n</span> <span class="string">kong-mesh-system</span> <span class="string">--from-file=./license.json</span></span><br></pre></td></tr></table></figure><p>Close the kubectl session, click on <strong>Default</strong> project and on <strong>Apps</strong> top menu option. Click on <strong>Launch</strong> button and choose the <strong>kong-mesh</strong> Helm charts.</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_11.png" class="" title="Image 11"><p>Click on <strong>Use an existing namespace</strong> and choose the one we just created. There are <a href="https://artifacthub.io/packages/helm/kong-mesh/kong-mesh">several parameters</a> to configure Kong Mesh, but we’re going to keep all the default values. After clicking on <strong>Launch</strong> , you should see the Kong Mesh application deployed:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_12.png" class="" title="Image 12"><p>And you can check the installation using Rancher Cluster Explorer again. Click on <strong>Pods</strong> on the left menu and choose <strong>kong-mesh-system</strong> namespace:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_13.png" class="" title="Image 13"><p>You can use kubectl as well like this:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">NAMESPACE</span>          <span class="string">NAME</span>                                                      <span class="string">READY</span>   <span class="string">STATUS</span>    <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">cattle-system</span>      <span class="string">cattle-cluster-agent-785fd5f54d-r7x8r</span>                     <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">75m</span></span><br><span class="line"><span class="string">fleet-system</span>       <span class="string">fleet-agent-77c78f9c74-f97tv</span>                              <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">75m</span></span><br><span class="line"><span class="string">kong-mesh-system</span>   <span class="string">kuma-control-plane-5b9c6f4598-nvq8q</span>                       <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">16m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">event-exporter-gke-666b7ffbf7-n9lfl</span>                       <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">fluentbit-gke-xqsdv</span>                                       <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">gke-metrics-agent-gjrqr</span>                                   <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">konnectivity-agent-4c4hf</span>                                  <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-dns-66d6b7c877-tq877</span>                                 <span class="number">4</span><span class="string">/4</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-dns-autoscaler-5c78d65cd9-5hcxs</span>                      <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-proxy-gke-c-kpwnf-default-0-be059c1c-49qp</span>            <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">l7-default-backend-5b76b455d-v6dvg</span>                        <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">metrics-server-v0.3.6-547dc87f5f-qntjf</span>                    <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">75m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">prometheus-to-sd-fdf9j</span>                                    <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">stackdriver-metadata-agent-cluster-level-68d94db6-64n4r</span>   <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">1</span>          <span class="string">75m</span></span><br></pre></td></tr></table></figure></li><li><p>Microservices deployment</p><p>Our Service Mesh deployment is based on a simple microservice-to-microservice communication scenario. As we’re running a canary release, the called microservice has two versions.</p><ul><li>“magnanimo”: exposed through Kong for Kubernetes ingress controller.</li><li>“benigno”: provides a “hello” endpoint where it echoes the current datetime. It has a canary release that sends a slightly different response.</li></ul><p>The figure below illustrates the architecture:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_14.png" class="" title="Image 14"><p>Create a namespace with the sidecar injection annotation. You can use the Rancher Cluster Manager again: choose your cluster and click on <strong>Projects&#x2F;Namespaces</strong>. Click on <strong>Add Namespace</strong>. Type “kong-mesh-app” for name and include an annotation with a “kuma.io&#x2F;sidecar-injection” key and “enabled” as its value:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_15.png" class="" title="Image 15"><p>Again, you can use kubectl as an alternative</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">create</span> <span class="string">namespace</span> <span class="string">kong-mesh-app</span></span><br><span class="line"></span><br><span class="line"><span class="string">kubectl</span> <span class="string">annotate</span> <span class="string">namespace</span> <span class="string">kong-mesh-app</span> <span class="string">kuma.io/sidecar-injection=enabled</span></span><br><span class="line"></span><br><span class="line"><span class="string">Submit</span> <span class="string">the</span> <span class="string">following</span> <span class="string">declaration</span> <span class="string">to</span> <span class="string">deploy</span> <span class="string">Magnanimo</span> <span class="string">injecting</span> <span class="string">the</span> <span class="string">Kong</span> <span class="string">Mesh</span> <span class="string">data</span> <span class="string">plane</span></span><br><span class="line"></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF</span> <span class="string">|</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="bullet">-</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">name:</span> <span class="string">magnanimo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">kong-mesh-app</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">matchLabels:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">magnanimo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">template:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">magnanimo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">magnanimo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">image:</span> <span class="string">claudioacquaviva/magnanimo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ports:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">4000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">name:</span> <span class="string">magnanimo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">kong-mesh-app</span></span><br><span class="line"></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">magnanimo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ports:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">port:</span> <span class="number">4000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">name:</span> <span class="string">http</span></span><br><span class="line"></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">magnanimo</span></span><br><span class="line"></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>Check your deployment using Rancher Cluster Manager. Pass the mouse over the <strong>kong-rancher</strong> menu and click on the <strong>Default</strong> project to see the current deployments:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_16.png" class="" title="Image 16"><p>Click on <strong>magnanimo</strong> to check details of the deployment, including its pods:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_17.png" class="" title="Image 17"><p>Click on the <strong>magnanimo</strong> pod to check the containers running inside of it.</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_18.png" class="" title="Image 18"><p>As we can see, the pod has two running containers:</p><ul><li>magnanimo: where the microservice is actually running</li><li>kuma-sidecar: injected during deployment time, playing the Kong Mesh data plane role.</li></ul><p>Similarly, deploy Benigno with its own sidecar:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF</span> <span class="string">|</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="bullet">-</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">name:</span> <span class="string">benigno-v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">kong-mesh-app</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">matchLabels:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">template:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">version:</span> <span class="string">v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">image:</span> <span class="string">claudioacquaviva/benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ports:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">name:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">kong-mesh-app</span></span><br><span class="line"></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ports:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">port:</span> <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">name:</span> <span class="string">http</span></span><br><span class="line"></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">And</span> <span class="string">finally,</span> <span class="attr">deploy Benigno canary release. Notice that the canary release will be abstracted by the same Benigno Kubernetes Service created before:</span></span><br><span class="line"></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF</span> <span class="string">|</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="bullet">-</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">name:</span> <span class="string">benigno-v2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">kong-mesh-app</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">matchLabels:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">template:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">app:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">version:</span> <span class="string">v2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">benigno</span></span><br><span class="line"></span><br><span class="line"><span class="attr">image:</span> <span class="string">claudioacquaviva/benigno\_rc</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ports:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>Check the deployments and pods with:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pod</span> <span class="string">--all-namespaces</span></span><br><span class="line"><span class="string">NAMESPACE</span>          <span class="string">NAME</span>                                                      <span class="string">READY</span>   <span class="string">STATUS</span>    <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">cattle-system</span>      <span class="string">cattle-cluster-agent-785fd5f54d-r7x8r</span>                     <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">75m</span></span><br><span class="line"><span class="string">fleet-system</span>       <span class="string">fleet-agent-77c78f9c74-f97tv</span>                              <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">75m</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">benigno-v1-fd4567d95-drnxq</span>                                <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">110s</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">benigno-v2-b977c867b-lpjpw</span>                                <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">30s</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">magnanimo-658b67fb9b-tzsjp</span>                                <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">5m3s</span></span><br><span class="line"><span class="string">kong-mesh-system</span>   <span class="string">kuma-control-plane-5b9c6f4598-nvq8q</span>                       <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">16m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">event-exporter-gke-666b7ffbf7-n9lfl</span>                       <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">fluentbit-gke-xqsdv</span>                                       <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">gke-metrics-agent-gjrqr</span>                                   <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">konnectivity-agent-4c4hf</span>                                  <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-dns-66d6b7c877-tq877</span>                                 <span class="number">4</span><span class="string">/4</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-dns-autoscaler-5c78d65cd9-5hcxs</span>                      <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-proxy-gke-c-kpwnf-default-0-be059c1c-49qp</span>            <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">l7-default-backend-5b76b455d-v6dvg</span>                        <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">metrics-server-v0.3.6-547dc87f5f-qntjf</span>                    <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">75m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">prometheus-to-sd-fdf9j</span>                                    <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">76m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">stackdriver-metadata-agent-cluster-level-68d94db6-64n4r</span>   <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">1</span>          <span class="string">75m</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">service</span> <span class="string">--all-namespaces</span></span><br><span class="line"><span class="string">NAMESPACE</span>          <span class="string">NAME</span>                   <span class="string">TYPE</span>        <span class="string">CLUSTER-IP</span>    <span class="string">EXTERNAL-IP</span>   <span class="string">PORT(S)</span>                                                <span class="string">AGE</span></span><br><span class="line"><span class="string">default</span>            <span class="string">kubernetes</span>             <span class="string">ClusterIP</span>   <span class="number">10.0</span><span class="number">.16</span><span class="number">.1</span>     <span class="string">&lt;none&gt;</span>        <span class="number">443</span><span class="string">/TCP</span>                                                <span class="string">79m</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">benigno</span>                <span class="string">ClusterIP</span>   <span class="number">10.0</span><span class="number">.20</span><span class="number">.52</span>    <span class="string">&lt;none&gt;</span>        <span class="number">5000</span><span class="string">/TCP</span>                                               <span class="string">4m6s</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">magnanimo</span>              <span class="string">ClusterIP</span>   <span class="number">10.0</span><span class="number">.30</span><span class="number">.251</span>   <span class="string">&lt;none&gt;</span>        <span class="number">4000</span><span class="string">/TCP</span>                                               <span class="string">7m18s</span></span><br><span class="line"><span class="string">kong-mesh-system</span>   <span class="string">kuma-control-plane</span>     <span class="string">ClusterIP</span>   <span class="number">10.0</span><span class="number">.21</span><span class="number">.228</span>   <span class="string">&lt;none&gt;</span>        <span class="number">5681</span><span class="string">/TCP,5682/TCP,443/TCP,5676/TCP,5678/TCP,5653/UDP</span>   <span class="string">18m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">default-http-backend</span>   <span class="string">NodePort</span>    <span class="number">10.0</span><span class="number">.19</span><span class="number">.10</span>    <span class="string">&lt;none&gt;</span>        <span class="number">80</span><span class="string">:32296/TCP</span>                                           <span class="string">79m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-dns</span>               <span class="string">ClusterIP</span>   <span class="number">10.0</span><span class="number">.16</span><span class="number">.10</span>    <span class="string">&lt;none&gt;</span>        <span class="number">53</span><span class="string">/UDP,53/TCP</span>                                          <span class="string">79m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">metrics-server</span>         <span class="string">ClusterIP</span>   <span class="number">10.0</span><span class="number">.20</span><span class="number">.174</span>   <span class="string">&lt;none&gt;</span>        <span class="number">443</span><span class="string">/TCP</span>                                                <span class="string">79m</span></span><br></pre></td></tr></table></figure><p>You can use Kong Mesh console to check the microservices and data planes also. On a terminal run:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">port-forward</span> <span class="string">service/kuma-control-plane</span> <span class="string">-n</span> <span class="string">kong-mesh-system</span> <span class="number">5681</span></span><br></pre></td></tr></table></figure><p>Redirect your browser to <code>http://localhost:5681/gui</code>. Click on <strong>Skip to Dashboard</strong> and <strong>All Data Plane Proxies</strong> :</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_19.png" class="" title="Image 19"><p>Start a loop to see the canary release in action. Notice the service has been deployed as ClusterIP type, so you need to expose them directly with “port-forward”. The next step will show how to expose the service with the Ingress Controller.</p><p>On a local terminal run:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">port-forward</span> <span class="string">service/magnanimo</span> <span class="string">-n</span> <span class="string">kong-mesh-app</span> <span class="number">4000</span></span><br></pre></td></tr></table></figure><p>Open another terminal and start the loop. The request is going to port 4000 provided by Magnanimo. The path “&#x2F;hw2” routes the request to Benigno Service, which has two endpoints behind it related to both Benigno releases:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">while</span> [<span class="number">1</span>]<span class="string">;</span> <span class="string">do</span> <span class="string">curl</span> <span class="string">http://localhost:4000/hw2;</span> <span class="string">echo;</span> <span class="string">done</span></span><br></pre></td></tr></table></figure><p>You should see a result similar to this:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:05.811667</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:06.304731</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:06.789208</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:07.269674</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:07.755884</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:08.240453</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:08.728465</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:09.208588</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:09.689478</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:10.179551</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:10.662465</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:11.145237</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:11.618557</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:12.108586</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:12.596296</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:13.093329</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 12:57:13.593487</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 12:57:14.068870</span></span><br></pre></td></tr></table></figure></li><li><p>Controlling the Canary Release</p><p>As we can see, the request to both Benigno microservice releases is uses a round-robin policy. That is, we’re not in control of the canary release consumption. Service Mesh allows us to define when and how we want to expose the canary release to our consumers (in our case, the Magnanimo microservice).</p><p>To define a policy to control the traffic going to both releases, use this following declaration. It says that 90 percent of the traffic should go to the current release, while only 10 percent should be redirected to the canary release.</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF</span> <span class="string">|</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="bullet">-</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kuma.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">TrafficRoute</span></span><br><span class="line"><span class="attr">mesh:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">route-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">sources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line"><span class="attr">kuma.io/service:</span> <span class="string">magnanimo\_kong-mesh-app\_svc\_4000</span></span><br><span class="line"><span class="attr">destinations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line"><span class="attr">kuma.io/service:</span> <span class="string">benigno\_kong-mesh-app\_svc\_5000</span></span><br><span class="line"><span class="attr">conf:</span></span><br><span class="line"><span class="attr">split:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">90</span></span><br><span class="line"><span class="attr">destination:</span></span><br><span class="line"><span class="attr">kuma.io/service:</span> <span class="string">benigno\_kong-mesh-app\_svc\_5000</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">v1</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">destination:</span></span><br><span class="line"><span class="attr">kuma.io/service:</span> <span class="string">benigno\_kong-mesh-app\_svc\_5000</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">v2</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>After applying the declaration, you should see a result like this:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:02.553389</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:03.041120</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:03.532701</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:04.021804</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:04.515245</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 13:05:05.000644</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:05.482606</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:05.963663</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="string">Benigno,</span> <span class="attr">Canary Release:</span> <span class="number">2020-11-20 13:05:06.446599</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:06.926737</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:07.410605</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:07.890827</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:08.374686</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:08.857266</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:09.337360</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:09.816912</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:10.301863</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:10.782395</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:11.262624</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:11.743427</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:12.221174</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:12.705731</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:13.196664</span></span><br><span class="line"><span class="string">Hello</span> <span class="string">World,</span> <span class="attr">Benigno:</span> <span class="number">2020-11-20 13:05:13.680319</span></span><br></pre></td></tr></table></figure></li><li><p>Install Kong for Kubernetes</p><p>Let’s go back to Rancher to install our Kong for Kubernetes Ingress Controller and control the service mesh exposition. In the Rancher Catalog page, click the <strong>Kong</strong> icon. Accept the default values and click <strong>Launch</strong> :</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_20.png" class="" title="Image 20"><p>You should see both applications, Kong and Kong Mesh, deployed:</p><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_21.png" class="" title="Image 21"><img src="/service-mesh/using-hybrid-and-multi-cloud-service-mesh-based-applications-for-distributed-deployments/kong_22.png" class="" title="Image 22"><p>Again, check the installation with kubectl:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pod</span> <span class="string">--all-namespaces</span></span><br><span class="line"><span class="string">NAMESPACE</span>          <span class="string">NAME</span>                                                      <span class="string">READY</span>   <span class="string">STATUS</span>    <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">cattle-system</span>      <span class="string">cattle-cluster-agent-785fd5f54d-r7x8r</span>                     <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">fleet-system</span>       <span class="string">fleet-agent-77c78f9c74-f97tv</span>                              <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">83m</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">benigno-v1-fd4567d95-drnxq</span>                                <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">10m</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">benigno-v2-b977c867b-lpjpw</span>                                <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">8m47s</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">magnanimo-658b67fb9b-tzsjp</span>                                <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">13m</span></span><br><span class="line"><span class="string">kong-mesh-system</span>   <span class="string">kuma-control-plane-5b9c6f4598-nvq8q</span>                       <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">24m</span></span><br><span class="line"><span class="string">kong</span>               <span class="string">kong-kong-754cd6947-db2j9</span>                                 <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">1</span>          <span class="string">72s</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">event-exporter-gke-666b7ffbf7-n9lfl</span>                       <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">85m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">fluentbit-gke-xqsdv</span>                                       <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">gke-metrics-agent-gjrqr</span>                                   <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">konnectivity-agent-4c4hf</span>                                  <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-dns-66d6b7c877-tq877</span>                                 <span class="number">4</span><span class="string">/4</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-dns-autoscaler-5c78d65cd9-5hcxs</span>                      <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-proxy-gke-c-kpwnf-default-0-be059c1c-49qp</span>            <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">l7-default-backend-5b76b455d-v6dvg</span>                        <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">85m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">metrics-server-v0.3.6-547dc87f5f-qntjf</span>                    <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">prometheus-to-sd-fdf9j</span>                                    <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">84m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">stackdriver-metadata-agent-cluster-level-68d94db6-64n4r</span>   <span class="number">2</span><span class="string">/2</span>     <span class="string">Running</span>   <span class="number">1</span>          <span class="string">84m</span></span><br><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">service</span> <span class="string">--all-namespaces</span></span><br><span class="line"><span class="string">NAMESPACE</span>          <span class="string">NAME</span>                   <span class="string">TYPE</span>           <span class="string">CLUSTER-IP</span>    <span class="string">EXTERNAL-IP</span>     <span class="string">PORT(S)</span>                                                <span class="string">AGE</span></span><br><span class="line"><span class="string">default</span>            <span class="string">kubernetes</span>             <span class="string">ClusterIP</span>      <span class="number">10.0</span><span class="number">.16</span><span class="number">.1</span>     <span class="string">&lt;none&gt;</span>          <span class="number">443</span><span class="string">/TCP</span>                                                <span class="string">85m</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">benigno</span>                <span class="string">ClusterIP</span>      <span class="number">10.0</span><span class="number">.20</span><span class="number">.52</span>    <span class="string">&lt;none&gt;</span>          <span class="number">5000</span><span class="string">/TCP</span>                                               <span class="string">10m</span></span><br><span class="line"><span class="string">kong-mesh-app</span>      <span class="string">magnanimo</span>              <span class="string">ClusterIP</span>      <span class="number">10.0</span><span class="number">.30</span><span class="number">.251</span>   <span class="string">&lt;none&gt;</span>          <span class="number">4000</span><span class="string">/TCP</span>                                               <span class="string">13m</span></span><br><span class="line"><span class="string">kong-mesh-system</span>   <span class="string">kuma-control-plane</span>     <span class="string">ClusterIP</span>      <span class="number">10.0</span><span class="number">.21</span><span class="number">.228</span>   <span class="string">&lt;none&gt;</span>          <span class="number">5681</span><span class="string">/TCP,5682/TCP,443/TCP,5676/TCP,5678/TCP,5653/UDP</span>   <span class="string">24m</span></span><br><span class="line"><span class="string">kong</span>               <span class="string">kong-kong-proxy</span>        <span class="string">LoadBalancer</span>   <span class="number">10.0</span><span class="number">.26</span><span class="number">.38</span>    <span class="number">35.222</span><span class="number">.91</span><span class="number">.194</span>   <span class="number">80</span><span class="string">:31867/TCP,443:31039/TCP</span>                             <span class="string">78s</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">default-http-backend</span>   <span class="string">NodePort</span>       <span class="number">10.0</span><span class="number">.19</span><span class="number">.10</span>    <span class="string">&lt;none&gt;</span>          <span class="number">80</span><span class="string">:32296/TCP</span>                                           <span class="string">85m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">kube-dns</span>               <span class="string">ClusterIP</span>      <span class="number">10.0</span><span class="number">.16</span><span class="number">.10</span>    <span class="string">&lt;none&gt;</span>          <span class="number">53</span><span class="string">/UDP,53/TCP</span>                                          <span class="string">85m</span></span><br><span class="line"><span class="string">kube-system</span>        <span class="string">metrics-server</span>         <span class="string">ClusterIP</span>      <span class="number">10.0</span><span class="number">.20</span><span class="number">.174</span>   <span class="string">&lt;none&gt;</span>          <span class="number">443</span><span class="string">/TCP</span>                                                <span class="string">85m</span></span><br></pre></td></tr></table></figure></li><li><p>Ingress Creation</p><p>With the following declaration, we’re going to expose Magnanimo microservice through an Ingress and its route “&#x2F;route1”.</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF</span> <span class="string">|</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="bullet">-</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">route1</span></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">kong-mesh-app</span></span><br><span class="line"><span class="attr">annotations:</span></span><br><span class="line"><span class="attr">konghq.com/strip-path:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line"><span class="attr">paths:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/route1</span></span><br><span class="line"><span class="attr">backend:</span></span><br><span class="line"><span class="attr">serviceName:</span> <span class="string">magnanimo</span></span><br><span class="line"><span class="attr">servicePort:</span> <span class="number">4000</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>Now the temporary “port-forward” exposure mechanism can be replaced by a formal Ingress. And our loop can start consuming the Ingress with similar results:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">while</span> [<span class="number">1</span>]<span class="string">;</span> <span class="string">do</span> <span class="string">curl</span> <span class="string">http://35.222.91.194/route1/hw2;</span> <span class="string">echo;</span> <span class="string">done</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> service-mesh </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Set up K3s in High Availability using k3d</title>
      <link href="/k3s/set-up-k3s-in-high-availability-using-k3d/"/>
      <url>/k3s/set-up-k3s-in-high-availability-using-k3d/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/k3s/set-up-k3s-in-high-availability-using-k3d/" target="_blank" title="https://www.xtplayer.cn/k3s/set-up-k3s-in-high-availability-using-k3d/">https://www.xtplayer.cn/k3s/set-up-k3s-in-high-availability-using-k3d/</a></p><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d-featured.png" class="" title="Set up K3s in High Availability using k3d"><p>Have you ever wanted to try K3s high availability cluster “mode,” and you either did not have the minimum three “spare nodes” or the time required to set up the same amount of VMs? Then you are in for a good treat: meet k3d!</p><p>If you’re not familiar with k3d, its name gives you a hint to what it’s all about: K3s in Docker. <a href="https://k3d.io/">k3d</a> is a lightweight wrapper to run <a href="https://github.com/rancher/k3s">K3s</a> (a lightweight, single &lt;40MB binary, certified Kubernetes distribution developed by Rancher Labs and now a CNCF sandbox project) in Docker. With k3d, it’s easy to create single- and multi-node <a href="https://github.com/rancher/k3s">K3s</a> clusters in Docker, for local development on Kubernetes.</p><p>k3d allows you to start a K3s cluster in literally no time. Plus, you can learn its few, but powerful, commands very quickly. k3d runs in Docker, which allows you to scale up and scale down nodes without further setup. In this blog, we’ll cover setting up a single-node K3s cluster using k3d and then walk through the steps for setting up K3s in high availability mode using k3d.</p><p>The two main objectives of this blog are to provide an introduction to k3d as a tool for deploying K3s clusters and to show how K3s high availability resists “nodes degradation.” As a bonus, we’ll look at which components K3s deploys in a cluster by default.</p><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>All of you have a preference when it comes to OS (Linux, MacOS, Windows). So, before we check the setup used for this blog post, there are only two mandatory requirements: Docker and a Linux shell.</p><p>If you are on MacOS or Windows, Docker Desktop is the preferred solution for Docker. For Linux, you can get the Docker engine and CLIs as described <a href="https://www.docker.com/products/docker-desktop">here</a>.</p><p>Concerning the Linux shell, MacOS and Linux are covered. For Windows, the easiest and fastest solution is WSL2, which we’ll use in this demo.</p><p>Here is the setup that we’ll use:</p><p>OS: Windows 10 version 2004 (build: 19041)</p><ul><li>OS components: Virtual Machine Platform and Windows Subsystem for Linux<ul><li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Installation steps</a></li></ul></li><li>WSL2 distro: Ubuntu<ul><li><a href="https://www.microsoft.com/en-us/p/ubuntu/9nblggh4msv6">Windows store link</a></li></ul></li><li>[Optional] Console used: Windows Terminal<ul><li><a href="https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701">Windows store link</a></li></ul></li></ul><h3 id="Step-1-Start-with-Installation"><a href="#Step-1-Start-with-Installation" class="headerlink" title="Step 1: Start with Installation"></a>Step 1: Start with Installation</h3><p>The k3d installation process is well documented <a href="https://k3d.io/#installation">here</a>.</p><p>For this blog post, we’ll use the “curl” installation.</p><p><strong>Attention:</strong> <em>running scripts directly from a URL is a big security no-no. So before running any script, ensure that the source is the project’s website and&#x2F;or git online repository. And even if you trust the source, reviewing the script is never too much.</em></p><p>Here are the installation steps:</p><p>Go to <a href="https://k3d.io/#installation">https://k3d.io/#installation</a></p><ul><li>Copy the “curl” installation command and run it inside your terminal <code>curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash</code> <img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_01.png" class="" title="Image 01"></li></ul><p><strong>Note</strong>: this screenshot shows two more commands: <code>k3d version</code>: <em>provides which version of k3d has been installed</em> <code>k3d --help</code>: <em>lists the commands that can be used with k3d</em></p><p>Now k3d is installed and ready to use.</p><h2 id="Step-2-Start-Small-with-a-Single-Node-Cluster"><a href="#Step-2-Start-Small-with-a-Single-Node-Cluster" class="headerlink" title="Step 2: Start Small with a Single-Node Cluster"></a>Step 2: Start Small with a Single-Node Cluster</h2><p>Before we create an HA cluster, let’s start with a one-node cluster in order to understand the commands (“grammar”) and see what k3d deploys by default.</p><p>First, the grammar. In v3, k3d made a big switch in how to use commands. We won’t delve into how commands were done before; we’ll use the syntax of v3.</p><p>The k3s commands follows a “noun + verb” syntax. First specify <em>what</em> we want to use (cluster or node) and then which <em>action</em> we want to apply (create, delete, start, stop).</p><h3 id="Create-a-one-node-cluster"><a href="#Create-a-one-node-cluster" class="headerlink" title="Create a one-node cluster"></a>Create a one-node cluster</h3><p>To create our first cluster with k3d, we’ll create one with no options, using only the defaults:</p><p><code>k3d cluster create</code> <img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_02.png" class="" title="Image 02"></p><p><strong>Note:</strong> the output of the <em>k3d cluster create command suggests running another command to check that the cluster is running and accessible:</em> kubectl cluster-info</p><p>Now the cluster is up and running! That was fast, right?</p><h3 id="Sneak-peek-at-internals"><a href="#Sneak-peek-at-internals" class="headerlink" title="Sneak peek at internals"></a>Sneak peek at internals</h3><p>One optional task that we can do is to see what exactly is deployed from <em>various</em> points of view.</p><p>Let’s start from the top and look at what’s inside the K3s cluster (pods, services, deployments, etc.): <code>kubectl get all --all-namespaces</code> <img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_03.png" class="" title="Image 03"></p><p>We can see that, in addition of the <strong>Kubernetes</strong> service, K3s deploys DNS, metrics and ingress (traefik) services when we use the defaults.</p><p>Now let’s look at the nodes from different points of view.</p><p>First, we’ll check it from a cluster perspective:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get nodes --output wide</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_04.png" class="" title="Image 04"><p>As expected, we only see one node. Now let’s see it from k3d’s perspective:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3d node list</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_05.png" class="" title="Image 05"><p>Now we have two nodes. The (very) smart implementation here is that while the cluster is running on its node <strong>k3d-k3s-default-server-0</strong> , there is another “node” that acts as the load balancer. While this might not be useful for a single-node cluster, it will save us a lot of effort in our HA cluster.</p><p>Finally, we can see the two nodes from Docker: <code>docker ps</code> <img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_06.png" class="" title="Image 06"></p><h3 id="Cleaning-the-resources"><a href="#Cleaning-the-resources" class="headerlink" title="Cleaning the resources"></a>Cleaning the resources</h3><p>Our one-node cluster helped us understand the mechanics and commands of k3d. Let’s clean up the resource before deploying our HA cluster. <code>k3d cluster delete</code> <img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_07.png" class="" title="Image 07"></p><p><strong>Note:</strong> for demo purposes, we added the following commands to the screenshot above:</p><p><code>k3d cluster list</code>: List the active k3d clusters <code>kubectl cluster-info</code>: Check the cluster connectivity</p><p><code>docker ps</code>: Check the active containers</p><p>Now we’ve created, checked and deleted a single-node cluster with k3d. In the next step, we’ll have fun with HA.</p><h2 id="Step-3-Welcome-to-the-HA-World"><a href="#Step-3-Welcome-to-the-HA-World" class="headerlink" title="Step 3: Welcome to the HA World"></a>Step 3: Welcome to the HA World</h2><p>Before we jump into the command line, let’s get a basic understanding of what we’ll launch and mention a few additional requirements.</p><p>First, Kubernetes HA has <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">two possible setups</a>: embedded or external database (DB). We’ll use the <strong>embedded DB</strong> setup.</p><p>Second, K3s has two different technologies for <a href="https://rancher.com/docs/k3s/latest/en/installation/ha-embedded/">HA with an embedded DB</a>: one based on <strong>dqlite</strong> (K3s v1.18) and another on <strong>etcd</strong> (K3S v1.19+).</p><p>This means <strong>etcd</strong> is the default in the current K3s stable release and is the one which will be used in this blog. <strong>dqlite</strong> has been deprecated.</p><p>At the time of this writing, k3d was using K3s version v1.18.9-k3s1 by default. You can check this by running <code>k3d version</code>: <img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_08.png" class="" title="Image 08"></p><p>Does this mean we need to reinstall k3d with a version that supports K3s v1.19? The answer is no!</p><p>We can use our current installed version of k3d and still have K3s v1.19 support, thanks to the following reasons:</p><ul><li>k3d has an option where we can specify a particular K3s docker image to be used.</li><li>All K3s versions are published as container images, too.</li></ul><p>Based on the two reasons above, we can assume a K3s v1.19 exists as a container image stored in Docker Hub.</p><p>As a helper, <a href="https://hub.docker.com/r/rancher/k3s/tags?page=1&name=v1.19">here is the search link</a>.</p><p>Enough with the theory. Let’s create our first K3s HA cluster with k3d.</p><h3 id="The-triumvirate-control-planes"><a href="#The-triumvirate-control-planes" class="headerlink" title="The triumvirate control planes"></a>The triumvirate control planes</h3><p>As Kubernetes HA best practices <a href="https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/#best-practices-for-replicating-masters-for-ha-clusters">strongly recommend</a>, we should create an HA cluster with at least three control plane nodes.</p><p>We can achieve that with k3d in one command:</p><p><code>k3d cluster create --servers 3 --image rancher/k3s:v1.19.3-k3s2</code> <img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_09.png" class="" title="Image 09"></p><p><strong>Learning the command:</strong> <em>Base command:</em> <code>k3d cluster create</code> Options:</p><p>–<code>server 3</code>: <em>requests three nodes to be created with the role server</em></p><p>–<code>image rancher/k3s:v1.19.3-k3s2</code>: <em>specifies the K3S image to be used</em></p><p>We can now check what has been created from the different points of view:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get nodes --output wide</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_10.png" class="" title="Image 10"><p>As seen here, we checked from the different points of view to ensure our nodes are correctly running.</p><p>If we look at the components deployed, our <strong>daemonset</strong> now has three replicas instead of one:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get all --all-namespaces</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_11.png" class="" title="Image 11"><p>One last check is to see on which node the <strong>pods</strong> are running:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get podes --all-namespaces --output wide</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_12.png" class="" title="Image 12"><p>Now we have the base of our HA cluster. Let’s add an additional control plane node, bring some “destruction” and see how the cluster behaves.</p><h3 id="Scale-up-the-cluster"><a href="#Scale-up-the-cluster" class="headerlink" title="Scale up the cluster"></a>Scale up the cluster</h3><p>Thanks to k3d and the fact our cluster runs on top of containers, we can quickly simulate the addition of another control plane node to the HA cluster:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3d node create extraCPnode --role=server --image=rancher/k3s:v1.19.3-k3s2</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_13.png" class="" title="Image 13"><p><strong>Learning the command:</strong> <em>Base command:</em> <code>k3d node create</code> <em>Options:</em></p><p><code>extraCPnode</code>: <em>base name that k3d will use to create the final node name</em></p><p><code>role=server</code>: <em>sets the role for the node to be a control plane</em></p><p><code>image rancher/k3s:v1.19.3-k3s2</code>: <em>specifies the K3s image to be used</em></p><p>As seen here, we checked from the different points of view to ensure our new control plane node is running correctly.</p><p>With this additional node added, we can perform our final test: bring down the <strong>node0</strong>!</p><h3 id="HA-Heavy-Armored-against-crashes"><a href="#HA-Heavy-Armored-against-crashes" class="headerlink" title="HA: Heavy Armored against crashes"></a>HA: Heavy Armored against crashes</h3><p>There are various reasons that this test is one of the most sensible. The <strong>node0</strong> is normally the one that our <em>KUBECONFIG</em> refers to (in terms of IP or hostname), and therefore our kubectl application tries to connect to for running the different commands.</p><p>As we are working with containers, the best way to “crash” a node is to literally stop the container:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker stop k3d-k3s-default-server-0</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_14.png" class="" title="Image 14"><p><strong>Note:</strong> <em>The Docker and k3d commands will show the state change immediately. However, the Kubernetes (read: K8s or K3s) cluster needs a short time to see the state change to NotReady.</em></p><p>And like magic, our cluster still responds to our commands using kubectl.</p><p>Now it is a good time to reference again the <strong>load balancer</strong> k3d uses and how it is critical in allowing us to continue accessing the K3s cluster.</p><p>While the load balancer internally switched to the next available node, from an external connectivity point of view, we still use the same IP&#x2F;host. This abstraction saves us quite some efforts and it’s one of the most useful features of k3d.</p><p>Let’s look at the state of the cluster:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get all --all-namespaces</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_15.png" class="" title="Image 15"><p>Everything looks right. If we look at the pods more specifically, then we will see that K3s automatically self-healed by recreating pods running on the failed node on other nodes:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pods --all-namespaces --output wide</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_16.png" class="" title="Image 16"><p>Finally, to show the power of HA and how K3s manages it, let’s restart the <strong>node0</strong> and see it being re-included into the cluster as if nothing happened:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker start k3d-k3s-default-server-0</span><br></pre></td></tr></table></figure><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_17.png" class="" title="Image 17"><p>Our cluster is stable, and all the nodes are fully operational again.</p><h3 id="Cleaning-the-resources-once-again"><a href="#Cleaning-the-resources-once-again" class="headerlink" title="Cleaning the resources, once again"></a>Cleaning the resources, once again</h3><p>Now we can delete our local HA cluster, as it has served its purpose. Plus, we know that we can create a new one just like that.</p><p>Let’s clean up the resource our HA cluster used : <code>k3d cluster delete</code></p><img src="/k3s/set-up-k3s-in-high-availability-using-k3d/k3d_18.png" class="" title="Image 18"><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>While we created a single node and an HA cluster locally, inside containers, we can still see how K3s behaves with the new etcd embedded DB. If we deployed K3s on bare metal or virtual machines, it would act the same way.</p><p>That being said, k3d helped a lot in terms of management. It created a load balancer by default that allowed permanent connectivity to the K3s cluster while abstracting all the tasks that we would have done manually if it was <a href="https://rancher.com/blog/2020/k3s-high-availability">deployed outside containers</a>.</p><p>In this post, we’ve seen how easy it is to set up high availability K3s clusters using k3d. If you haven’t tried K3s or k3d, what are you waiting for? They are both free and easy to use.</p><p>Thanks for reading!</p>]]></content>
      
      
      <categories>
          
          <category> k3s </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Compute Confidently at the Edge with Rancher and Longhorn</title>
      <link href="/rancher/compute-confidently-at-the-edge-with-rancher-and-longhorn-1-1/"/>
      <url>/rancher/compute-confidently-at-the-edge-with-rancher-and-longhorn-1-1/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/compute-confidently-at-the-edge-with-rancher-and-longhorn-1-1/" target="_blank" title="https://www.xtplayer.cn/rancher/compute-confidently-at-the-edge-with-rancher-and-longhorn-1-1/">https://www.xtplayer.cn/rancher/compute-confidently-at-the-edge-with-rancher-and-longhorn-1-1/</a></p><img src="/rancher/compute-confidently-at-the-edge-with-rancher-and-longhorn-1-1/longhorn-1-1-featured.png" class="" title="Compute Confidently at the Edge with Rancher and Longhorn 1.1"><p>Today’s announcement of <a href="https://longhorn.io/">Longhorn 1.1</a>, a Cloud Native Computing Foundation (CNCF) Sandbox project, is exciting news for users of <a href="https://rancher.com/products/rancher">Rancher</a>, SUSE’s Kubernetes management platform, and the Kubernetes community. Longhorn is an enterprise-grade, cloud native container storage solution that went GA in June 2020. Since then, adoption has increased by 235 percent. Now Longhorn is the first cloud native storage solution designed and built for the edge, with ARM64 support, new self-healing capabilities and increased performance visibility.</p><p>The ability for our users to “Compute Everywhere” has always been a key differentiator between Rancher and other solutions on the market. This strategy cemented the foundations for successful projects like K3s and Longhorn, which were originally built by Rancher product teams to address the infrastructure needs of multinational organizations with globally distributed and often resource-constrained environments. The success of these projects eventually led to their donation to the open source community through the CNCF.</p><p>The latest Longhorn release supports this “Compute Everywhere” legacy and offers the following new features and improvements to Rancher users and the Kubernetes community at large. These include:</p><h2 id="Robust-Kubernetes-Native-Storage-at-the-Edge"><a href="#Robust-Kubernetes-Native-Storage-at-the-Edge" class="headerlink" title="Robust Kubernetes-Native Storage at the Edge"></a>Robust Kubernetes-Native Storage at the Edge</h2><p>The latest Longhorn release gives Rancher access to the first Kubernetes-native edge storage solution. With ARM64 support, Longhorn was designed to help teams store data reliably within even the most hostile and resource-constrained environments. Together, Longhorn and Rancher provide users with a powerful Kubernetes-native management and storage solution designed for the edge.</p><h2 id="Driving-Efficient-Container-Performance"><a href="#Driving-Efficient-Container-Performance" class="headerlink" title="Driving Efficient Container Performance"></a>Driving Efficient Container Performance</h2><p>Longhorn has significantly improved its performance with <strong>ReadWriteMany</strong> support, which was a highly requested feature from the community. This feature gives developers an efficient persistent storage solution that enables volumes to be read and written across multiple containers at any time. It also lets teams share volume storage between different paths on different nodes. ReadWriteMany support adds to Longhorn’s enterprise-grade benefits so you can use Longhorn in production with confidence.</p><h2 id="Enhanced-Visibility-Operations-Support-and-Maintenance-Functionality"><a href="#Enhanced-Visibility-Operations-Support-and-Maintenance-Functionality" class="headerlink" title="Enhanced Visibility, Operations Support and Maintenance Functionality"></a>Enhanced Visibility, Operations Support and Maintenance Functionality</h2><p>Longhorn 1.1 brings better insights and functionality to an organization’s storage infrastructure without compromising on its user-friendly simplicity and reliability. This is through:</p><ul><li>New integrated support for Prometheus monitoring</li><li>New support for CSI Snapshotter, which enables users to easily create and restore backups via kubectl</li><li>Enhanced node maintenance capabilities with support for Kubernetes drain operations</li></ul><p>These improvements give Rancher users complete access to real-time metrics of their storage health and let them maintain their storage volumes alongside managing their Kubernetes clusters. This can all be operated within the Longhorn console, which is easily accessible via the Rancher platform.</p><img src="/rancher/compute-confidently-at-the-edge-with-rancher-and-longhorn-1-1/image-01.png" class="" title="Image 02">*Longhorn’s comprehensive real-time view of performance metrics*<h2 id="Increased-Resilience"><a href="#Increased-Resilience" class="headerlink" title="Increased Resilience"></a>Increased Resilience</h2><p>Longhorn 1.1 also builds resilience into resource-constrained Kubernetes environments like the edge with its new Data Locality feature. Rancher users can have confidence about storage and performance at the edge, as this new Longhorn feature keeps a storage replica local to the workload - ensuring storage will not be lost even if connectivity is lost.</p><h2 id="Rancher-and-Longhorn-Together"><a href="#Rancher-and-Longhorn-Together" class="headerlink" title="Rancher and Longhorn Together"></a>Rancher and Longhorn Together</h2><p>Together with the Rancher Kubernetes management platform, Longhorn makes the deployment of highly available enterprise-grade persistent storage easy, fast and reliable. As the first cloud native storage built for the edge, Longhorn empowers users to do more with their Kubernetes environments.</p><p>Existing Rancher users can download and install Longhorn directly from Rancher’s app catalog. Longhorn remains an open source CNCF project and is free to download and use. Rancher customers can now purchase premium support through <a href="https://www.suse.com/support/">SUSE Support Services</a>. There are no licensing fees, and node-based subscription pricing keeps costs to a minimum.</p><p><a href="https://info.rancher.com/online-meetup-february2021">Join the next community meetup</a> where we will explore how Rancher users can leverage the new Longhorn features across their edge environments.</p><p><a href="https://rancher.com/blog/2021/compute-at-edge-rancher-longhorn">https://rancher.com/blog/2021/compute-at-edge-rancher-longhorn</a></p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rancher </tag>
            
            <tag> Longhorn </tag>
            
            <tag> Edge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 规范中标签的重要性</title>
      <link href="/kubernetes/autoscaling/"/>
      <url>/kubernetes/autoscaling/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/autoscaling/" target="_blank" title="https://www.xtplayer.cn/kubernetes/autoscaling/">https://www.xtplayer.cn/kubernetes/autoscaling/</a></p><p><a href="https://www.magalix.com/blog/the-best-kubernetes-tutorials">Kubernetes</a>的核心是资源管理和编排工具。可以集中精力进行第一天的操作，以探索和试用其很酷的功能来部署，监视和控制 Pod。但是，您还需要考虑第二天的操作。您需要关注以下问题：</p><ul><li>我将如何扩展 Pod 和应用程序？</li><li>如何保持容器健康运行并高效运行？</li><li>随着代码和用户工作量的不断变化，我该如何应对这些变化？</li></ul><p>我在这篇文章中提供了 Kubernetes 内部不同可伸缩性机制的高级概述，以及使它们满足您的需求的最佳方法。请记住，要真正掌握 Kubernetes，您需要掌握不同的方法来管理集群资源的规模，这<a href="https://speakerdeck.com/thockin/everything-you-ever-wanted-to-know-about-resource-scheduling-dot-dot-dot-almost">是 Kubernetes 承诺的核心</a>。</p><p><em>配置 Kubernetes 集群以平衡资源和性能可能是一项挑战，并且需要具备 Kubernetes 内部运作的专业知识。仅仅因为您的应用程序或服务的工作负载不是恒定的，而是整天（甚至不是整小时）都会波动。将其视为一段旅程和一个持续的过程。</em></p><h2 id="Kubernetes-自动扩展构建基块"><a href="#Kubernetes-自动扩展构建基块" class="headerlink" title="Kubernetes 自动扩展构建基块"></a>Kubernetes 自动扩展构建基块</h2><p>有效的<a href="https://www.magalix.com/blog/the-best-kubernetes-tutorials">kubernetes</a>自动缩放需要两层可缩放性之间的协调：（1）荚层自动缩放器，其中包括水平荚式自动缩放器（HPA）和垂直荚式自动缩放器（VPA）；两者都可扩展容器的可用资源，以及（2）由集群自动缩放器（CA）管理的集群级别可扩展性；它可以按比例增加或减少群集内节点的数量。</p><h3 id="Pod-水平自动缩放（HPA）"><a href="#Pod-水平自动缩放（HPA）" class="headerlink" title="Pod 水平自动缩放（HPA）"></a>Pod 水平自动缩放（HPA）</h3><p>顾名思义，HPA 扩展了 Pod 副本的数量。大多数 DevOps 使用 CPU 和内存作为触发来扩展更多或更少的 Pod 副本。但是，您可以对其进行配置，以根据<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics">自定义指标</a>，<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-multiple-metrics">多个指标</a>甚至<a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/external-metrics-autoscaling">外部指标</a>来扩展您的广告连播。</p><h4 id="高级-HPA-工作流程"><a href="#高级-HPA-工作流程" class="headerlink" title="高级 HPA 工作流程"></a>高级 HPA 工作流程</h4><img src="/kubernetes/autoscaling/xfPZh3ir2vgsp246yK9kEg.png" class="" title="空值"><ol><li>HPA 连续检查您在设置过程中配置的指标值，默认间隔为 30 秒</li><li>如果达到 SPECIFIED 阈值，则 HPA 尝试增加 Pod 的数量</li><li>HPA 主要更新部署或复制控制器内的副本数</li><li>然后，<a href="https://www.magalix.com/blog/kubernetes-deployments-101">部署</a>&#x2F;<a href="https://www.magalix.com/blog/kubernetes-replicaset-101">复制</a>控制器将推出任何其他所需的 Pod</li></ol><p><strong>推出 HPA 时请考虑以下几点：</strong></p><ul><li>HPA 的默认检查间隔为 30 秒。可以通过控制器管理器的“ horizontal-pod-autoscaler-sync-period”标志进行配置</li><li>默认 HPA 相对指标公差为 10％</li><li>在最后一次扩展事件发生后，HPA 等待 3 分钟，以使指标稳定下来。也可以通过 “horizontal-pod-autoscaler-upscale-delay” 标志进行配置</li><li>从上一次缩减事件开始，HPA 等待 5 分钟，以避免自动缩放器跳动。可通过以下方式配置：horizontal-pod-autoscaler-downscale-delay 标志</li><li>HPA 与部署对象（而不是复制控制器）一起使用时效果最好。不适用于使用直接操作复制控制器的滚动更新。进行部署时，取决于部署对象来管理基础副本集的大小</li></ul><h3 id="立式豆荚自动定标器"><a href="#立式豆荚自动定标器" class="headerlink" title="立式豆荚自动定标器"></a>立式豆荚自动定标器</h3><p>垂直 Pod Autoscaler（VPA）为现有 Pod 分配更多（或更少）的 CPU 或内存。可以将其视为为豆荚提供一些生长激素，它可以用于<a href="https://www.magalix.com/blog/kubernetes-statefulsets-101-state-of-the-pods">有状态</a>和无状态的豆荚，但是它主要是为有状态服务而构建的。但是，如果您想对最初分配给 Pod 的资源实施自动校正，则也可以将其用于无状态 Pod。VPA 还可以对 OOM（内存不足）事件做出反应。VPA 当前需要重新启动 Pod 才能更改分配的 CPU 和内存。VPA 重新启动 Pod 时，它会遵守<a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">Pod 的分配预算</a>（PDB），以确保始终有最少数量的必要广告连播。您可以设置 VPA 可分配给任何吊舱的资源的最小值和最大值。例如，您可以将最大内存限制限制为不超过 8 GB。当您知道当前节点不能为每个容器分配超过 8 GB 的内存时，此功能特别有用。阅读<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md">VPA 的官方 Wiki 页面</a>以获取详细的规格和设计。</p><p>VPA 还具有一个有趣的功能，称为 VPA 推荐器。它监视所有吊舱的历史资源使用情况和 OOM 事件，以建议<a href="https://www.magalix.com/blog/kubernetes-resource-requests-and-limits-101">“请求”资源</a>规范的新值。推荐器通常使用一些智能算法根据历史指标来计算内存和 cpu 值。它还提供了一个使用 pod 描述符并提供建议资源请求的 API。</p><p>值得一提的是，VPA Recommender 在设置资源 “限制” 上不起作用。这可能会导致 Pod 垄断节点内部的资源。我建议您在命名空间级别设置一个 “限制” 值，以避免疯狂消耗内存或 CPU</p><h4 id="高级-VPA-工作流程"><a href="#高级-VPA-工作流程" class="headerlink" title="高级 VPA 工作流程"></a>高级 VPA 工作流程</h4><img src="/kubernetes/autoscaling/dxjzbfL_tfEdovzlPuZJOA.png" class="" title="空值"><ol><li>VPA 连续检查您在设置过程中配置的指标值，默认间隔为 10 秒</li><li>如果达到阈值，VPA 会尝试更改分配的内存和&#x2F;或 CPU</li><li>VPA 主要更新部署或复制控制器规范中的资源</li><li>重新启动 Pod 后，新资源将全部应用于创建的实例。</li></ol><p><strong>推出 VPA 时需要考虑以下几点：</strong></p><ul><li>如果不重新启动 Pod，将无法更改资源。到目前为止，主要的合理性在于，这样的变化可能会导致很多不稳定。因此，考虑重新启动 Pod 并根据新分配的资源对其进行调度。</li><li>VPA 和 HPA 尚不兼容，不能在同一吊舱上使用。如果要在同一群集中同时使用它们，请确保在设置中分开它们的作用域。</li><li>VPA 仅根据观察到的过去和当前资源使用情况来调整容器的<a href="https://www.magalix.com/blog/kubernetes-resource-requests-and-limits-101">资源请求</a>。它没有设置<a href="https://www.magalix.com/blog/kubernetes-resource-requests-and-limits-101">资源限制</a>。对于行为不当的应用程序可能会出现问题，这些应用程序开始使用越来越多的资源导致 Pod 被 Kubernetes 杀死。</li><li>VPA 尚处于初期阶段。它将在接下来的几个月中发展，为此做好准备：)有关已知限制的详细信息，请参见<a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#known-limitations-of-the-alpha-version">此处</a>和有关未来的工作<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md#future-work">。</a></li></ul><h3 id="集群自动缩放器"><a href="#集群自动缩放器" class="headerlink" title="集群自动缩放器"></a>集群自动缩放器</h3><p>群集自动缩放器（CA）根据挂起的 Pod 扩展群集节点。它会定期检查是否有任何暂挂的 Pod，并在需要更多资源以及扩展后的群集是否仍在用户提供的限制内的情况下增加群集的大小。CA 与云提供商接口以请求更多节点或取消分配空闲节点。它适用于 GCP，AWS 和 Azure。Kubernetes 1.8 发行了 1.0 版（GA）。</p><h4 id="高级-CA-工作流程"><a href="#高级-CA-工作流程" class="headerlink" title="高级 CA 工作流程"></a>高级 CA 工作流程</h4><p><strong><img src="/kubernetes/autoscaling/Workflow.png" class="" title="CA 工作流程"></strong></p><ol><li>CA 以默认间隔 10 秒检查处于挂起状态的 Pod。</li><li>如果由于集群上没有足够的可用资源而无法在集群上分配一个或多个 Pod 处于挂起状态，则它将尝试设置一个或多个其他节点。</li><li>当云提供商授予该节点后，该节点将加入集群并准备为 Pod 服务。</li><li><a href="https://www.magalix.com/blog/kubernetes-scheduler-101">Kubernetes 调度程序</a>将挂起的 Pod 分配给新节点。如果某些吊舱仍处于挂起状态，则将重复该过程并将更多节点添加到群集中。</li></ol><p>*<strong>推出 CA 时请考虑这些因素</strong></p><ul><li>群集自动缩放器可确保群集中的所有 Pod 都有运行的地方，无论是否有 CPU 负载。而且，它试图确保集群中没有不需要的节点。（来源）</li><li>CA 大约 30 秒钟即可实现可伸缩性需求。</li><li>默认情况下，在不需要节点后，CA 会按比例等待 10 分钟，然后再按比例缩小。</li><li>CA 具有扩展器的概念。扩展器提供不同的策略来选择要添加新节点的节点组。</li><li>负责任地使用“ cluster-autoscaler.kubernetes.io&#x2F;safe-to-evict”：“true”。如果您在所有节点上设置了许多 Pod 或足够多的 Pod，则将失去很多伸缩的灵活性。</li><li>使用<a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudgets</a>可以防止 Pod 被删除并最终导致应用程序的一部分完全无法运行。</li></ul><h2 id="Kubernetes-自动定标器如何相互作用"><a href="#Kubernetes-自动定标器如何相互作用" class="headerlink" title="Kubernetes 自动定标器如何相互作用"></a>Kubernetes 自动定标器如何相互作用</h2><p>如果您想达到自动<a href="https://www.magalix.com/blog/the-best-kubernetes-tutorials">扩展 Kubernetes</a>集群的必杀技，则需要在 CA 上使用 Pod 层自动扩展器。彼此之间的工作方式相对简单，如下图所示。</p><img src="/kubernetes/autoscaling/6-M58_pIUhE0HE5f.png" class="" title="空值"><ol><li>HPA 或 VPA 更新分配给现有 Pod 的 Pod 副本或资源。</li><li>如果在伸缩性事件后没有足够的节点运行 Pod，则 CA 会接收到这样的事实，即部分或全部已缩放 Pod 处于挂起状态。</li><li>CA 分配新节点</li><li>在已配置的节点上调度 Pod。</li></ol><h3 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h3><p>我曾在不同的论坛上看到过，例如 Kubernetes 的松弛频道和 StackOverflow 问题，这是一些常见问题，这是由于许多 DevOps 在使用自动缩放器弄湿时会错过一些事实。</p><p>HPA 和 VPA 取决于指标和一些历史数据。如果您没有分配足够的资源，您的 Pod 将被 OOM 杀死，并且永远不会有机会生成指标。在这种情况下，您的规模可能永远不会发生。</p><p>扩大规模主要是对时间敏感的操作。您希望您的 Pod 和集群能够在用户遭受应用程序的任何中断或崩溃之前相当快地扩展。您应该考虑荚和集群扩展所需的平均时间。</p><h4 id="最佳情况-4-分钟"><a href="#最佳情况-4-分钟" class="headerlink" title="最佳情况-4 分钟"></a>最佳情况-4 分钟</h4><ol><li>30 秒—目标指标值已更新：30–60 秒</li><li>30 秒-HPA 检查指标值：30 秒-&gt;</li><li>&lt;2 秒-吊舱已创建并进入待处理状态-1 秒</li><li>&lt;2 秒内-CA 看到挂起的 Pod 并向配置节点发出呼叫-1 秒</li><li>3 分钟-云提供商配置节点和 K8 等待节点准备就绪：最多 10 分钟（取决于多种因素）</li></ol><h4 id="（合理）最坏的情况-12-分钟"><a href="#（合理）最坏的情况-12-分钟" class="headerlink" title="（合理）最坏的情况-12 分钟"></a>（合理）最坏的情况-12 分钟</h4><ol><li>60 秒-目标指标值已更新</li><li>30 秒— HPA 检查指标值</li><li>&lt;2 秒-窗格已创建并进入待处理状态</li><li>不到 2 秒-CA 看到挂起的 Pod 并向配置节点发出呼叫</li><li>10 分钟-云提供商配置节点，而 K8 等待节点准备就绪（取决于多个因素，例如提供商延迟，操作系统延迟，引导捆绑工具等）。</li></ol><p>不要将云提供商的可伸缩性机制与 CA 混淆。CA 在您的群集中运行，而云提供商的可伸缩性机制（例如 AWS 内的 ASG）则根据节点分配进行工作。它不知道 Pod 或应用程序正在发生什么。一起使用它们会使您的群集不稳定并且难以预测行为。</p><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL; DR"></a>TL; DR</h2><ul><li>Kubernetes 是一种资源管理和编排工具。第 2 天的操作来管理您的 Pod 和集群资源是掌握 Kubernetes 旅程中的关键里程碑。</li><li>请牢记正确的思维模式，着重使用 HPA 和 VPA 调整 Pod 的可扩展性。</li><li>如果您对吊舱和容器的需求有充分的了解，建议使用 CA。</li><li>了解不同的自动缩放器如何协同工作将有助于您配置集群。</li><li>确保您的 Pod 和群集放大或缩小需要多长时间，以应对最坏情况和最佳情况。</li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 资源请求和限制</title>
      <link href="/kubernetes/resource-requests-and-limits/"/>
      <url>/kubernetes/resource-requests-and-limits/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/resource-requests-and-limits/" target="_blank" title="https://www.xtplayer.cn/kubernetes/resource-requests-and-limits/">https://www.xtplayer.cn/kubernetes/resource-requests-and-limits/</a></p><p>资源调度是 Kubernetes 的核心功能之一。Kubernetes 调度程序确保容器获得足够的资源以正确执行。此过程由调度策略控制。在深入研究调度程序的工作原理之前，我们确保我们了解 Kubernetes 集群内部资源定义，分配和限制的基本结构。</p><h2 id="资源类型"><a href="#资源类型" class="headerlink" title="资源类型"></a>资源类型</h2><p>Kubernetes 只有两个内置的可管理资源：<strong>CPU</strong>和<strong>内存</strong>。CPU 基本单元是核心，内存以字节为单位，这两种资源在调度器如何将 Pods 分配给节点方面起着关键作用。您将控制您的集群请求，分配和使用内存和 CPU，始终设置正确的 CPU 和内存值，这将确保运行异常的应用程序不会影响群集中其他 Pod 的可用资源容量。</p><h2 id="资源请求和限制"><a href="#资源请求和限制" class="headerlink" title="资源请求和限制"></a>资源请求和限制</h2><p>Kubernetes 使用请求（requests）和限制（limits）结构来控制 CPU 和内存等资源。</p><ul><li><strong>requests</strong>: 请求是保证容器预期需要分配的资源大小，可以理解为所需的最小资源。如果容器配置了 requests，Kubernetes 将仅在可以为其分配足够资源的节点上调度它。</li><li><strong>limits</strong>: 限制是容器永远不会超过的资源阈值，可以理解为最大值。<ul><li>CPU 是 <strong>可压缩的资源</strong>，这意味着一旦您的容器使用资源达到 CPU 限制的极限，它将继续运行，但是操作系统将对其进行限制并阻止使用 CPU 进行调度。</li><li>内存是 <strong>不可压缩的资源</strong>。如果容器中进程使用内存超过 limits 限制的最大值，将会触发系统的 OOM 机制，容器中应用进程将会被强制 KILL。</li><li>limits 永远不能低于 requests，如果您的限制高于请求，Kubernetes 将引发错误，并且不允许您运行容器。</li></ul></li></ul><blockquote><p><strong>提示</strong>：最好在容器级别设置请求和限制，以进行更细化的资源控制和更有效地分配容器。</p></blockquote><h2 id="定义-CPU-请求和限制"><a href="#定义-CPU-请求和限制" class="headerlink" title="定义 CPU 请求和限制"></a>定义 CPU 请求和限制</h2><p>要为容器指定 CPU 请求，请在容器的 YAML 资源清单的 <strong>resources &gt; requests</strong> 字段中添加相应的值。类似地，要指定 CPU 限制，需要在 <strong>resources &gt; limits</strong> 字段中添加相应的值</p><h3 id="不同云提供商的-CPU-单元"><a href="#不同云提供商的-CPU-单元" class="headerlink" title="不同云提供商的 CPU 单元"></a>不同云提供商的 CPU 单元</h3><p>如果您在具有带超线程的英特尔裸金属处理器上运行，Kubernetes 中的 CPU 单元最初相当于一个超线程。重要的是要了解它如何映射到主要云提供商的不同 CPU 容量，误解这些可能会导致 Kubernetes 集群内部的性能下降。</p><table><thead><tr><th align="left"><strong><strong>Infrastructure</strong></strong></th><th align="left"><strong>相对于 1 个 CPU</strong></th></tr></thead><tbody><tr><td align="left">AWS</td><td align="left">1 个 vCPU</td></tr><tr><td align="left">Azure</td><td align="left">1 个 vCore</td></tr><tr><td align="left">GCP</td><td align="left">1 个 vCore</td></tr><tr><td align="left">IBM 公司</td><td align="left">1 个 vCPU</td></tr><tr><td align="left">bare-metal Intel processor with Hyperthreading</td><td align="left">1 Hyperthread</td></tr></tbody></table><h2 id="定义内存请求和限制"><a href="#定义内存请求和限制" class="headerlink" title="定义内存请求和限制"></a>定义内存请求和限制</h2><p>要为容器指定内存请求，请在容器的 YAML 资源清单的 <strong>resources &gt; requests</strong> 字段中添加相应的值。类似地，要指定内存限制，需要在 <strong>resources &gt; limits</strong> 字段中添加相应的值。</p><p>内存限制和请求以字节为单位，您可以将内存表示为普通整数或使用以下后缀之一的定点整数: E, P, T, G, M, K。</p><h2 id="请求和限制配置示例"><a href="#请求和限制配置示例" class="headerlink" title="请求和限制配置示例"></a>请求和限制配置示例</h2><p>看下面的例子。这是一个有两个容器的 Pod。每个容器都有 0.5 CPU 和 300MiB 内存的请求。每个容器限制为 1 个 CPU 和 500MiB 的内存。</p><img src="/kubernetes/resource-requests-and-limits/2.png" class="" title="12qwe2"><p><strong>如果你使用 rancher 进行应用管理，那么配置资源请求和限制将会非常简单。</strong></p><ol><li><p>选择需要配置请求和限制的服务，然后在右侧省略号菜单中选择 <strong>编辑</strong>；</p><img src="/kubernetes/resource-requests-and-limits/image-20210218182743548.png" class="" title="image-20210218182743548"></li><li><p>跳转后的页面往下翻，点击<strong>显示高级选项</strong>；</p></li><li><p>页面往下翻，点击 <strong>安全&#x2F;主机设置</strong>。</p></li><li><p>最后即可配置<strong>请求和限制</strong>。</p><img src="/kubernetes/resource-requests-and-limits/image-20210218183054622.png" class="" title="image-20210218183054622"></li></ol><h2 id="命名空间设定请求和限制"><a href="#命名空间设定请求和限制" class="headerlink" title="命名空间设定请求和限制"></a>命名空间设定请求和限制</h2><p>理想情况下，您希望团队成员始终设置限制和资源。但是在现实世界中，您的团队会忘记这样做。为了防止这些情况发生，您可以在命名空间级别设置 <strong>ResourceQuotas</strong> 和 <strong>LimitRanges</strong>。</p><h3 id="资源配额（ResourceQuotas）"><a href="#资源配额（ResourceQuotas）" class="headerlink" title="资源配额（ResourceQuotas）"></a>资源配额（ResourceQuotas）</h3><p>您可以使用 ResourceQuotas 来限制命名空间资源大小。ResourceQuotas 让您只看如何限制该命名空间内容器的 CPU 和内存资源使用率。资源配额配置示例如下：</p><img src="/kubernetes/resource-requests-and-limits/qweqweq.png" class="" title="qweqweq"><p>可以看到有四个部分，配置这些部分都是可选的。</p><ul><li><strong>requests.cpu:</strong> 命名空间中的所有容器请求 CPU 资源总和的最大值。只要命名空间中请求的总 CPU 资源少于 500m，就可以运行任意多的容器。</li><li><strong>Requests.memory:</strong> 命名空间中的所有容器请求内存资源总和的最大值。只要命名空间中请求的总内存小于 100MiB，就可以运行任意多的容器。</li><li><strong>limits.cpu:</strong> 命名空间中所有容器 CPU 限制总和的最大值。</li><li><strong>limits.memory:</strong> 命名空间中所有容器内存限制总和的最大值。</li></ul><h3 id="限制范围（LimitRanges）"><a href="#限制范围（LimitRanges）" class="headerlink" title="限制范围（LimitRanges）"></a>限制范围（LimitRanges）</h3><p>LimitRange 适用于单个容器。这可以防止您的团队成员在命名空间内创建超小型或超大型容器。一个 LimitRange 配置示例：</p><img src="/kubernetes/resource-requests-and-limits/3.png" class="" title="312asd"><p>可以看到有四个可选部分：</p><ul><li><strong>default</strong> 部分为 Pod 中的容器设置了<strong>默认限制</strong>，未明确设置这些值的容器，默认情况下将被赋给这些值。</li><li><strong>defaultRequest</strong> 部分为 Pod 中的容器设置<strong>默认请求</strong>，未明确设置这些值的容器，默认情况下将被赋给这些值。</li><li><strong>max</strong> 将设置一个 Pod 中的容器可以设置的<strong>最大限制</strong>，<strong>default</strong> 部分的设置不能高于这个值。同样，对容器设置的限制也不能高于这个值。如果容器本身没有明确设置限制值，并且如果设置了这个 <strong>max</strong> 值而没有设置 <strong>default</strong> 值，则会将 <strong>max</strong> 作为限制值分配给容器。</li><li>该 <strong>min</strong> 设置的最小要求，在一个荚的容器可以设置。该<strong>defaultRequest</strong>部分不能低于此值。同样，在容器上设置的请求也不能低于此值。如果设置了该最小值而未设置<strong>defaultRequest</strong>部分，则最小值也将成为<strong>defaultRequest</strong>值，</li><li><strong>min</strong> 部分定义了一个 Pod 容器可以设置的<strong>最小请求</strong>。<strong>defaultRequest</strong> 部分不能低于 <strong>min</strong> 值。同样，容器本身设置的<strong>请求</strong>也不能低于这个 <strong>min</strong> 值。如果设置了这个 <strong>min</strong> 值，而没有设置 <strong>defaultRequest</strong> 部分，则 <strong>min</strong> 也会变成 <strong>defaultRequest</strong> 值设置到容器。</li></ul><h2 id="Kubernetes-Pod-的生命周期"><a href="#Kubernetes-Pod-的生命周期" class="headerlink" title="Kubernetes Pod 的生命周期"></a>Kubernetes Pod 的生命周期</h2><p>了解 Pods 如何获得资源，以及当它们超过了所分配的资源或集群的总体容量时会发生什么，这一点非常重要，这样您就可以正确地调优容器和集群容量。下面是典型的 Pod 调度流程:</p><ol><li><p>您可以通过规范定义资源，并运行 kubectl apply 命令分配给 Pod。</p></li><li><p>Kubernetes 调度器将使用循环调度策略来选择一个节点来运行 Pod。</p></li><li><p>对于每个节点，Kubernetes 会检查该节点是否有足够的资源来满足资源请求。</p></li><li><p>Pod 被调度到第一个有足够资源的节点。</p></li><li><p>如果所有节点都没有足够的资源，则 Pod 将进入挂起状态。</p><ol><li>如果您正在使用 CA autoscaler，您的集群将扩展节点数量以分配更多的容量。</li></ol></li><li><p>如果一个节点正在运行 Pod，而 Pod 的限制总和超过了它的可用容量，那么 Kubernetes 将进入过量使用状态。</p><ol><li>由于 CPU 可以被压缩，Kubernetes 将确保您的容器获得它们请求的 CPU，并将节制其余的 CPU。</li><li>内存无法压缩，因此，如果节点内存不足，Kubernetes 需要开始决定终止哪个容器。</li></ol></li></ol><h2 id="如果-Kubernetes-进入过量使用状态怎么办？"><a href="#如果-Kubernetes-进入过量使用状态怎么办？" class="headerlink" title="如果 Kubernetes 进入过量使用状态怎么办？"></a>如果 Kubernetes 进入过量使用状态怎么办？</h2><p>请记住，Kuberentes 针对整个系统的运行状况和可用性进行了优化。当它进入过量使用状态时，Kubernetes 调度程序可能会做出杀死 Pod 的决定。通常，如果某个 Pod 使用的资源超过了请求的数量，则该 Pod 会成为终止请求的候选对象。在这里，您要记住的条件是：</p><ul><li>Pod 没有定义对 CPU 和内存的请求，Kubernetes 认为这个 Pod 使用的比他们默认要求的要多。</li><li>Pod 定义了一个 <strong>request</strong>，但它们使用的请求多于 <strong>request</strong>，但低于 <strong>limit</strong>。</li><li>如果多个 Pod 超过了它们的请求，Kubernetes 将根据它们的优先级对它们进行排序，最低优先级的 Pod 首先被终止。</li><li>如果所有 Pod 都具有相同的优先级，则 Kubernetes 将从最占用请求资源的 Pod 开始。</li><li>如果关键组件（例如 kubelet 或 docker 引擎）使用的资源比为其保留的资源更多，Kubernetes 可能会杀死其请求中的 Pod。这是罕见的情况。</li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 概念及其重要性</title>
      <link href="/kubernetes/concepts-and-matters/"/>
      <url>/kubernetes/concepts-and-matters/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/concepts-and-matters/" target="_blank" title="https://www.xtplayer.cn/kubernetes/concepts-and-matters/">https://www.xtplayer.cn/kubernetes/concepts-and-matters/</a></p><p>要完全理解该技术，您需要知道为什么使用容器编排工具以及 Kubernetes 是如何出现的。Kubernetes 的故事始于容器，为了解容器的好处，让我们看看软件部署机制是如何随着时间演变的。</p><h2 id="Docker-容器改变了部署软件的方式"><a href="#Docker-容器改变了部署软件的方式" class="headerlink" title="Docker 容器改变了部署软件的方式"></a>Docker 容器改变了部署软件的方式</h2><p>在过去，软件部署相对困难，耗时且容易出错。要安装应用程序，需要购买许多物理机并为 CPU 和内存支付超出实际需要的费用。几年后，虚拟化成为主流。一台功能强大的裸机服务器可以托管多台虚拟机，CPU 和内存可以共享，因此您节省了一些成本。如今，机器可以比虚拟服务器分割成更小的部分 — <strong>容器</strong>。容器仅在几年之间变得如此流行。那么，Linux 容器到底是什么？<a href="https://www.docker.com/">Docker</a> 适合用在什么场景？</p><img src="/kubernetes/concepts-and-matters/machines-1.png" class="" title="虚拟机内部的应用程序"><p>容器就像虚拟机一样提供一种虚拟化的实现，主机管理程序提供了硬件级别的隔离，而容器提供了<em>进程</em>级别的隔离。</p><p>为了理解这种差异，让我们回到示例中。</p><p>您决定使用容器，而不是为 Apache 和 MySQL 创建虚拟机。现在，您的堆栈如下图所示。</p><img src="/kubernetes/concepts-and-matters/containers-1.png" class="" title="Docker 容器中的应用程序"><p>容器也是操作系统上的一组进程，容器通过 Linux 内核功能（例如：<a href="https://en.wikipedia.org/wiki/Cgroups">cgroups</a>，<a href="https://en.wikipedia.org/wiki/Chroot">chroot</a>，<a href="https://en.wikipedia.org/wiki/UnionFS">UnionFS</a> 和 <a href="https://en.wikipedia.org/wiki/Linux_namespaces">命名空间）</a>与其他<strong>进程&#x2F;容器</strong>完全隔离。</p><p>这意味着您只需支付一台物理主机的费用，安装一个操作系统，尽可能多的运行容器从而提高资源利用率。减少在同一物理主机上运行所需的操作系统数量，意味着更少的存储，内存和 CPU 浪费。</p><p>2010 年，Docker 成立。Docker 可以指公司和产品。Docker 让用户和公司非常容易地利用容器进行软件部署。需要注意的一点是，Docker 并不是市场上唯一能够做到这一点的工具。还有其他应用程序，如 rkt、Apache Mesos、LXC 等，但 Docker 是最受欢迎的一个。</p><h2 id="容器和微服务：需要协调器"><a href="#容器和微服务：需要协调器" class="headerlink" title="容器和微服务：需要协调器"></a>容器和微服务：需要协调器</h2><p>在同一个操作系统上以进程(也就是容器)的形式运行完整服务的能力是革命性的。它本身带来了很多可能性:</p><ul><li>由于容器比虚拟机便宜得多，速度也快得多，所以大型应用程序现在可以被分解成小的、相互依赖的组件，每个组件都在自己的容器中运行。这种体系结构被称为微服务。</li><li>随着微服务体系结构越来越占主导地位，应用程序有了更大更丰富的自由。以前，单个应用程序一直在发展，直到达到一定的限制，使其变得笨拙、更难调试，并且非常难以重新部署。然而，随着容器的出现，要向应用程序添加更多特性，所需要做的就是构建更多的<strong>容器&#x2F;服务</strong>。使用 IaC(基础设施即代码)，部署就像对配置文件运行命令一样简单。</li><li>如今，服务不可访问已是不再可接受的。用户根本不在乎您的应用程序是否正在发生网络中断或群集节点崩溃。如果您的系统未运行，则用户将直接切换到您的竞争对手。</li><li>容器是过程，而过程本质上是短暂的。如果容器损坏了怎么办？</li><li>为了实现高可用性，您为每个组件创建多个容器。例如，Apache 的两个容器，每个容器都托管一个 Web 服务器。但是，其中哪一个将响应客户的请求？</li><li>当您需要更新应用程序时，您想利用每个服务的多个容器。您将在容器的一部分上部署新代码，重新创建它们，然后在其余容器上执行相同的操作。但是，手动执行此操作非常困难。更不用说容易出错。</li><li>容器配置。</li><li>维护正在运行的容器的状态（和数量）。</li><li>通过将容器从一个节点移动到另一个节点，将应用程序负载平均分配到硬件节点上。</li><li>承载相同服务的容器之间的负载平衡。</li><li>处理容器持久性<a href="https://www.magalix.com/blog/kubernetes-storage-101">存储</a>。</li><li>确保即使推出更新，该应用程序始终可用。</li></ul><p>以上所有内容都鼓励 IT 专业人员做一件事：创建尽可能多的容器。但是，这也有缺点：</p><p>例如，假设您有一个微服务应用程序，该应用程序具有运行 Apache，Ruby，Python 和 NodeJS 的多个服务。您使用容器来充分利用手头的硬件。但是，由于有很多容器分散在您的节点上而没有被管理，因此基础架构可能如下图所示。</p><img src="/kubernetes/concepts-and-matters/inside-containers-1.png" class="" title="容器内的多个应用程序"><p>因此，您需要一个容器编排引擎！</p><h2 id="欢迎-Kubernetes"><a href="#欢迎-Kubernetes" class="headerlink" title="欢迎 Kubernetes"></a>欢迎 Kubernetes</h2><p><a href="https://kubernetes.io/">Kubernetes</a> 是一个容器编排工具。编排是容器生命周期管理的另一个名称，容器编排引擎执行许多任务。</p><p>就像 Docker 并不是目前唯一的容器平台一样，Kubernetes 并不是市场上唯一的编排工具。还有其他工具，例如 <a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a>，<a href="http://mesos.apache.org/">Apache Mesos</a>，<a href="https://mesosphere.github.io/marathon/">Marathon</a> 等。那么，什么使 Kubernetes 成为最常用的呢？</p><h2 id="Kubernetes-为什么如此受欢迎？"><a href="#Kubernetes-为什么如此受欢迎？" class="headerlink" title="Kubernetes 为什么如此受欢迎？"></a>Kubernetes 为什么如此受欢迎？</h2><p>Kubernetes 最初是由软件和搜索巨头 Google 开发的，Kubernetes 是他们 <a href="https://en.wikipedia.org/wiki/Borg_(cluster_manager)">Borg</a> 项目的一个分支。自成立以来，Kubernetes 受到了开源社区的巨大推动，它是 <a href="https://www.cncf.io/">Cloud Native Computing Foundation</a> 的主要项目。一些最大的市场参与者正在支持它：<a href="https://www.google.com/">Google</a>，<a href="https://aws.amazon.com/">AWS</a>，<a href="https://azure.microsoft.com/en-us/">Azure</a>，<a href="https://www.ibm.com/eg-en/cloud">IBM</a> 和 <a href="https://www.cisco.com/">Cisco</a> 等。</p><h2 id="Kubernetes-体系结构及其环境？"><a href="#Kubernetes-体系结构及其环境？" class="headerlink" title="Kubernetes 体系结构及其环境？"></a>Kubernetes 体系结构及其环境？</h2><p>Kubernetes 是一个希腊单词，代表舵手或船长。它是你们集群的管理者，为了能够完成这项关键的工作，Kubernetes 以高度模块化的方式设计。该技术的每个部分都为依赖它的服务提供了必要的基础。下图展示了应用程序如何工作的概览，每个模块都包含在一个更大的模块中，该模块依赖于它的功能。让我们更深入地研究每一个问题。</p><img src="/kubernetes/concepts-and-matters/ecosystem.png" class="" title="kubernetes 生态系统"><h3 id="Kubernetes-核心功能"><a href="#Kubernetes-核心功能" class="headerlink" title="Kubernetes 核心功能"></a>Kubernetes 核心功能</h3><p>也被称为控制平面，它是整个系统最核心的部分。它提供了许多 RESTful API，使集群能够执行其最基本的操作。核心的另一部分是执行，执行涉及许多控制器，如副本控制器、复制集、部署等。它还包括 kubelet，它是负责与容器运行时通信的模块。</p><p>核心还负责联系其他层(通过 kubelet)来全面管理容器。让我们简要地了解它们:</p><h4 id="容器运行时"><a href="#容器运行时" class="headerlink" title="容器运行时"></a>容器运行时</h4><p>Kubernetes 使用容器运行时接口(CRI)来透明地管理容器，而不必知道(或处理)所使用的运行时。当我们讨论容器时，我们提到了 Docker，尽管它很受欢迎，但并不是唯一可用的容器管理系统。Kubernetes 默认使用 containerd 作为容器运行时。这就是你能够对 Kubernetes 容器发出标准 Docker 命令的方式。它还使用 rkt 作为替代运行时。在这一点上不要太困惑。这是 Kubernetes 的内部工作原理，尽管您需要理解它，但您几乎不必完全处理它。Kubernetes 通过其丰富的 api 对这一层进行了抽象。</p><h4 id="网络插件"><a href="#网络插件" class="headerlink" title="网络插件"></a>网络插件</h4><p>正如我们前面所讨论的，容器编制系统负责管理容器和服务通信所通过的网络。Kubernetes 使用称为容器网络接口(CNI)的库作为集群和各种网络提供商之间的接口。Kubernetes 中可以使用许多网络供应商。这个数字是不断变化的。举几个例子:</p><ul><li><a href="https://www.weave.works/oss/net/">Weave net</a></li><li><a href="https://contiv.io/">Contiv</a></li><li><a href="https://github.com/coreos/flannel/">Flannel</a></li><li><a href="https://www.projectcalico.org/calico-networking-for-kubernetes/">Calico</a></li></ul><p>这个清单太长了，在这里就不再一一列举。你可能会问: 为什么 Kubernetes 需要多个<a href="https://www.magalix.com/blog/kubernetes-network-policies-101">网络</a>提供商来进行选择？Kubernetes 的设计主要用于部署在不同的环境中，Kubernetes 节点可以是裸金属物理服务器、虚拟机或云实例中的任何节点。有了这样的多样性，对于容器如何彼此通信，您实际上有无穷多的选择。这需要不止一个人来选择，这就是 Kubernetes 设计者选择将 CNI 背后的网络提供者层抽象出来的原因。</p><h3 id="Volume-插件"><a href="#Volume-插件" class="headerlink" title="Volume 插件"></a>Volume 插件</h3><p>卷广义上是指将用于 Pod 的存储空间。 Pod 是由 Kubernetes 作为一个单元管理的一个或多个容器。因为 Kubernetes 被设计为部署在多个环境中，所以集群和底层存储之间存在一个抽象层。Kubernetes 还使用 CSI(容器存储接口)来与各种已经可用的存储插件进行交互。</p><h4 id="镜像仓库"><a href="#镜像仓库" class="headerlink" title="镜像仓库"></a>镜像仓库</h4><p>Kubernetes 必须能够访问镜像仓库（无论是公共的还是私有的）才能获取镜像并创建出容器。</p><h4 id="云提供商"><a href="#云提供商" class="headerlink" title="云提供商"></a>云提供商</h4><p>Kubernetes 可以部署在您可能想到的几乎任何平台上。但是，大多数用户喜欢使用 AWS，Azure 或 GCP 之类的云提供商，以节省更多成本。Kubernetes 依靠云提供商的 API 来执行可伸缩性和资源供应任务，例如：供应负载平衡器，访问云存储，利用节点间 VPC 网络等。</p><h4 id="身份提供者"><a href="#身份提供者" class="headerlink" title="身份提供者"></a>身份提供者</h4><p>如果您要在用户数量较少的小型公司中配置 Kubernetes 集群，则身份验证不会成为大问题。您可以为每个用户创建一个帐户。但是，如果您在大型企业中工作，有成百上千的开发人员、操作员、测试人员、安全专业人员等，手动为每个人创建一个帐户可能会变成一场噩梦。Kubernetes 的设计人员在开发身份验证机制时就考虑到了这一点，只要集群使用<a href="https://openid.net/connect/">OpenID connect</a>，您就可以使用自己的身份提供程序系统对集群中的用户进行身份验证。</p><h3 id="控制器层"><a href="#控制器层" class="headerlink" title="控制器层"></a>控制器层</h3><p>这也称为服务结构层。它负责集群的一些更高级别的功能：路由、自我修复、负载平衡、服务发现和基本部署（有关更多信息，请参考 <a href="https://kubernetes.io/docs/concepts/services-networking/">https://kubernetes.io/docs/concepts/services-networking/</a> 和 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a>）。</p><h3 id="管理层"><a href="#管理层" class="headerlink" title="管理层"></a>管理层</h3><p>这就是应用策略执行选项的地方。在这一层中，执行指标收集和自动缩放等功能。它还控制授权和不同资源(如网络和存储)之间的配额。您可以访问<a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">此处</a>了解更多关于资源配额的信息。</p><h3 id="接口层"><a href="#接口层" class="headerlink" title="接口层"></a>接口层</h3><p>在这一层中，有用于与集群交互的面向客户端的工具，目前，kubectl 是最受欢迎的客户端程序。在后台，它向 Kubernetes 发出 RESTful API 请求，并根据提供的选项以 JSON 或 YAML 的形式显示响应。kubectl 可以很容易地与其他高级工具集成，以促进集群管理。</p><p>在同一领域还有 helm，它可以被认为是运行在 Kubernetes 之上的应用程序包管理器。使用 helm-charts，只需在配置文件中定义其属性&#x2F;参数，就可以在 Kubernetes 上运行完整的应用服务。</p><h3 id="DevOps-和基础架构环境"><a href="#DevOps-和基础架构环境" class="headerlink" title="DevOps 和基础架构环境"></a>DevOps 和基础架构环境</h3><p>Kubernetes 是最繁忙的开源项目之一。它有一个庞大的、充满活力的用户社区，它不断地变化以适应新的要求和挑战。Kubernetes 提供了大量的特性。尽管它只有几年的历史，但它能够支持几乎所有类型的环境。Kubernetes 在许多现代软件<strong>构建&#x2F;部署</strong>实践中被使用，包括:</p><ul><li><strong>DevOps</strong>: 为测试和 QA 更容易、更快的提供临时环境；</li><li><a href="https://en.wikipedia.org/wiki/CI/CD">CI&#x2F;CD</a><strong>：</strong>使用 Kubernetes 管理的容器，构建持续的<strong>集成&#x2F;部署</strong>，甚至交付管道也更加无缝连接。您可以轻松地将诸如<a href="https://jenkins.io/">Jenkins</a>，<a href="https://travis-ci.org/">TravisCI</a>，<a href="https://drone.io/">Drone CI 之</a>类的工具与 Kubernetes 集成在一起，以<strong>构建&#x2F;测试&#x2F;部署</strong>应用程序和其他云组件。</li><li><a href="https://www.atlassian.com/blog/software-teams/what-is-chatops-adoption-guide">ChatOps</a><strong>：</strong>像 Slack 这样的聊天应用程序可以轻松地与 Kubernetes 提供的丰富 API 集集成，以监控甚至管理集群。</li><li>云托管的 Kubernetes：大多数云提供商都提供已安装 Kubernetes 的产品。例如，<a href="https://aws.amazon.com/eks/">AWS EKS</a>，<a href="https://cloud.google.com/kubernetes-engine/">Google GKE</a> 和 <a href="https://docs.microsoft.com/en-us/azure/aks/">Azure AKS</a>。</li><li><a href="https://github.com/weaveworks/flux">GitOps</a>：Kubernetes 中的所有内容都通过 YAML 文件进行管理。使用 Git 之类的版本控制系统，您可以轻松管理集群应用，甚至不必使用 <em>kubectl</em>。</li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解 Kubernetes 1.20</title>
      <link href="/kubernetes/about-kubernetes-1.20/"/>
      <url>/kubernetes/about-kubernetes-1.20/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/about-kubernetes-1.20/" target="_blank" title="https://www.xtplayer.cn/kubernetes/about-kubernetes-1.20/">https://www.xtplayer.cn/kubernetes/about-kubernetes-1.20/</a></p><p>Kubernetes 1.20 版本有 43 个增强(从 1.19 版本增加的 34 个)，包括 15 个全新的，11 个升级到稳定的，以及 17 个对现有特性的改进。</p><p>这表明这些增强的作用范围较小。例如，对 kube-apiserver 进行了一些更改，以使其在 HA 集群中更加友好并具有更好的性能。它还可以在升级后更有效地重新启动。</p><h2 id="为什么会这样？"><a href="#为什么会这样？" class="headerlink" title="为什么会这样？"></a>为什么会这样？</h2><p>这些小的改进和新特性为未来的重大变化铺平了道路。然而，最新版本已经有了一个主要的变化(尽管人们期待已久)。</p><h3 id="Kubernetes-弃用-Docker"><a href="#Kubernetes-弃用-Docker" class="headerlink" title="Kubernetes 弃用 Docker"></a>Kubernetes 弃用 Docker</h3><p>从 1.20 版本开始，Kubernetes 将不再支持 Docker 作为容器运行时，而是支持容器运行时接口<a href="https://github.com/kubernetes/kubernetes/blob/242a97307b34076d5d8f5bbeb154fa4d97c9ef1d/docs/devel/container-runtime-interface.md">Container Runtime Interface（CRI）</a>。</p><p>但是不要惊慌！</p><p>这并不意味着 Docker 已经死了，你不必放弃 Docker 工具。对于 Kubernetes 用户来说，不会有太大的改变，因为你仍然可以使用 Docker 构建容器，生成的映像也将继续在 Kubernetes 集群中运行。</p><p>然而，Kubernetes 计划在未来的版本中移除 kubelet 和 dockershim 中的 Docker 引擎支持，很可能在明年晚些时候。但是你可以通过将内置 dockershim 替换为外部 dockershim 来继续使用它。</p><p>Docker 和 Mirantis 还同意合作并维护 Kubernetes 之外的 shim 代码，作为 Docker 引擎的一个一致的 CRI 接口。这确保它通过了所有的一致性测试，并像以前的内置版本一样无缝地工作。</p><p>为了保持优秀的开发人员体验，Docker 计划继续将这个 shim 发布到 Docker 桌面，而 Mirantis 将在 Mirantis Kubernetes 引擎中利用这个功能。此外，对用 Docker 工具构建的容器映像的 net&#x2F;net 支持不会被弃用，而且会像以前一样工作。</p><p>尽管 Docker 是领先的容器解决方案，但并不是为了嵌入到 Kubernetes 中而开发的。它不仅具有容器运行时功能，还具有多种 UX 增强功能，使开发人员能够与之无缝交互。</p><p>Docker 是一个完整的技术堆栈(而不仅仅是一个集装箱化平台)，它也提供了称为“containerd”的高级容器运行时，从现在起，这将是您的容器运行时选项。</p><p>这些更新和增强功能不一定专注于 Kubernetes。取而代之的是，它们旨在克服障碍，使开发人员能够最大程度地利用障碍。例如，目前，Kubernetes 集群需要一个名为 Dockershim 的工具，该工具已容器化。它用于为团队必须维护的另一种工具增加一定程度的复杂性。但是，它是经常产生错误和其他问题的来源。因此，Kubernetes 项目计划在 1.23 版中删除 Dockershim 并终止对 Docker 的支持。</p><p>这意味着问题仅归结为将 Docker 换成 CRI 运行时。但是就目前而言，<a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Docker 开发仍然是相同的</a>没有任何明显的差异。内置 Docker 的映像不是特定于 Docker 的，而是<a href="https://opencontainers.org/">Open Container Initiative (OCI)</a> images。</p><p>OCI 是 Docker 在 2015 年建立的，以支持可互操作的容器标准(以确保容器可以在任何环境下运行)。在过去的五年里，它被证明是一个巨大的成功，在促进创新的同时保持互操作性。要使用这些图像，可以使用 containerd 或<a href="https://cri-o.io/">CRI-O</a>。</p><h2 id="令人兴奋的新功能"><a href="#令人兴奋的新功能" class="headerlink" title="令人兴奋的新功能"></a>令人兴奋的新功能</h2><h3 id="1-CronJobs-和-Kubelet-CRI-支持"><a href="#1-CronJobs-和-Kubelet-CRI-支持" class="headerlink" title="1. CronJobs 和 Kubelet CRI 支持"></a>1. CronJobs 和 Kubelet CRI 支持</h3><p>CronJobs 在 1.4 版中引入，并从 1.5 版开始具有 CRI 支持。但是，尽管被广泛使用，但都没有被认为是稳定的。因此，很高兴看到开发人员所依赖的用于运行生产集群的功能不再被视为 Alpha。</p><h3 id="2-CSIServiceAccountToken"><a href="#2-CSIServiceAccountToken" class="headerlink" title="2. CSIServiceAccountToken"></a>2. CSIServiceAccountToken</h3><p>此更新通过增强身份验证和令牌处理，大大提高了安全性。现在您可以更安全地访问需要身份验证的卷(包括 secret vaults)，设置和部署也要容易得多。</p><h3 id="3-公开关注于资源请求和对-Pod-模型的限制的度量"><a href="#3-公开关注于资源请求和对-Pod-模型的限制的度量" class="headerlink" title="3.公开关注于资源请求和对 Pod 模型的限制的度量"></a>3.公开关注于资源请求和对 Pod 模型的限制的度量</h3><p>现在有了更多指标可以更好地规划集群的容量，当遇到驱逐问题时，它也有助于排除故障。</p><h3 id="4-优雅的节点关闭"><a href="#4-优雅的节点关闭" class="headerlink" title="4.优雅的节点关闭"></a>4.优雅的节点关闭</h3><p>虽然它是一个小特性，但它使开发人员的工作变得简单多了。通过在节点关闭时适当释放资源，您现在可以避免奇怪的行为。</p><h3 id="5-kube-apiserver-身份标识"><a href="#5-kube-apiserver-身份标识" class="headerlink" title="5. kube-apiserver 身份标识"></a>5. kube-apiserver 身份标识</h3><p>每个 kube-apiserver 实例的唯一标识符通常不会被注意到，但这是必要的，因为了解它将有助于确保未来 Kubernetes 版本中的高可用性特性。</p><h3 id="6-系统组件日志清理"><a href="#6-系统组件日志清理" class="headerlink" title="6.系统组件日志清理"></a>6.系统组件日志清理</h3><p>Kubernetes 系统漏洞最近曝光，尤其是凭证泄漏到日志输出中。牢记大局，现在您可以确定泄漏的潜在来源，并建立编辑机制以消除这些泄漏。</p><p>Kubernetes 的系统漏洞最近暴露出来，特别是凭证泄露到日志输出中。现在可以确定泄漏的潜在源，并设置一个修订机制来消除这些漏洞。</p><h2 id="应该了解的关键弃用"><a href="#应该了解的关键弃用" class="headerlink" title="应该了解的关键弃用"></a>应该了解的关键弃用</h2><h3 id="1-kubeadm-主节点角色更名"><a href="#1-kubeadm-主节点角色更名" class="headerlink" title="1. kubeadm 主节点角色更名"></a>1. kubeadm 主节点角色更名</h3><p>阶段：弃用</p><p>功能组：集群生命周期</p><p>现在将<code>node-role.kubernetes.io/master</code> 更改为 <code>node-role.kubernetes.io/control-plane</code>。</p><h3 id="2-弃用和禁用-SelfLink"><a href="#2-弃用和禁用-SelfLink" class="headerlink" title="2.弃用和禁用 SelfLink"></a>2.弃用和禁用 SelfLink</h3><p>阶段：升级到 Beta</p><p>功能组：api-machinery</p><p>每个 Kubernetes 对象中的 SelfLink 字段都包含一个表示给定对象的 URL，但不提供任何新信息。同时，它的创建和维护会影响性能。</p><p>Kubernetes 1.16 开始弃用，从现在开始，特性门（ feature gate）在默认情况下是禁用的，并计划在 Kubernetes 1.21 中删除。</p><h3 id="3-流式代理重定向"><a href="#3-流式代理重定向" class="headerlink" title="3.流式代理重定向"></a>3.流式代理重定向</h3><p>阶段：弃用</p><p>功能组：节点</p><p>在 1.18 版中已标记为弃用，StreamingProxyRedirects 和<code>--redirect-container-streaming</code>标志都不会启用。在 1.22 版本中，它也会被默认禁用，而在 1.24 版本中则完全删除。</p><p>从上面可以看到，Kubernetes 的开发人员和管理员没有什么可担心的。当他们利用 docker 命令和 kubectl 命令来管理 Kubernetes 集群时，本质上就是业务。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cicd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>影响 Kubernetes 调度的决策因素</title>
      <link href="/kubernetes/scheduler/influencing-kubernetes-scheduler-decisions/"/>
      <url>/kubernetes/scheduler/influencing-kubernetes-scheduler-decisions/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/scheduler/influencing-kubernetes-scheduler-decisions/" target="_blank" title="https://www.xtplayer.cn/kubernetes/scheduler/influencing-kubernetes-scheduler-decisions/">https://www.xtplayer.cn/kubernetes/scheduler/influencing-kubernetes-scheduler-decisions/</a></p><p>为了提高节点资源的最大利用率，调度程序使用复杂的算法来确保最有效的 Pod 调度。在本文中，我们讨论调度程序如何选择最佳节点来运行 Pod，以及如何影响其决策。</p><h2 id="哪个节点具有可用资源？"><a href="#哪个节点具有可用资源？" class="headerlink" title="哪个节点具有可用资源？"></a>哪个节点具有可用资源？</h2><p>选择适当的节点时，调度程序会检查每个节点是否有足够的资源满足 Pod 调度。如果您已经声明 Pod 所需的 CPU 和内存量（通过请求和限制），调度程序将使用以下公式来计算给定节点上的可用内存：</p><p><code>调度可用内存 = 节点总内存 - 已预留内存</code></p><p><strong>保留内存</strong>是指：</p><ol><li>Kubernetes 守护进程使用的内存，例如：<strong>kubelet</strong>、<strong>containerd</strong>（一种容器运行时）。</li><li>节点操作系统使用的内存，例如：内核守护程序。</li></ol><p>通过使用此方程式，调度程序可确保由于过多 Pod 竞争消耗节点所有可用资源，从而导致节点资源耗尽引起其他系统异常，比如系统触发 oom。</p><h2 id="影响调度过程"><a href="#影响调度过程" class="headerlink" title="影响调度过程"></a>影响调度过程</h2><p>在不受用户影响的情况下，调度程序在将 Pod 调度到节点时执行以下步骤：</p><ol><li>调度程序检测到已创建新的 Pod，但尚未将其分配给节点；</li><li>它检查 Pod 需求，并相应地筛选出所有不合适的节点；</li><li>根据权重将剩下的节点进行排序，权重最高的排在第一位；</li><li>调度程序选择排序列表中的第一个节点，然后将 Pod 分配给它。</li></ol><p>通常，我们会让调度程序自动选择合适的节点（前提是 Pod 配置了资源请求和限制）。但是，有时可能需要通过强制调度程序选择特定节点或手动向多个节点添加权重来影响此决策，以使其比其他节点<em>更</em>适合 Pod 调度。</p><p>让我们看看我们如何做到这一点：</p><img src="/kubernetes/scheduler/influencing-kubernetes-scheduler-decisions/influncing.jpg" class="" title="influncing"><h3 id="节点名称"><a href="#节点名称" class="headerlink" title="节点名称"></a>节点名称</h3><p>在最简单的节点选择配置中，您只需在 <strong>.spec.nodeName</strong> 中指定其名称，就可以强制 Pod 在指定节点上运行。例如，以下 YAML 定义 Pod 强制在 app-prod01 上进行调度：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">containers:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">   <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line"> <span class="attr">nodeName:</span> <span class="string">app-prod01</span></span><br></pre></td></tr></table></figure><p>请注意，由于以下原因，此方法是最简单但最不推荐的节点选择方法：</p><ul><li>如果由于某种原因无法找到指定名称的节点（例如，更改了主机名），则 Pod 将不会运行。</li><li>如果该节点没有 Pod 运行所需的资源，则 Pod 会运行失败，并且也不会将该 Pod 调度到其他节点。</li><li>这会导致 Pods 与它们的节点紧密耦合，这是一种糟糕的设计实践。</li></ul><h3 id="节点选择器"><a href="#节点选择器" class="headerlink" title="节点选择器"></a>节点选择器</h3><p>覆盖调度程序决策的第一个最简单的方法是使用 Pod 定义或 Pod 模板（如果使用的是 Deployments 之类的控制器）中的 <strong>.spec.nodeSelector</strong> 参数。nodeSelector 接受 <strong>一个或多个</strong> 键-值对标签，这些 <strong>键-值对</strong> 标签必须在节点设置才能正常的调度 Pod。假设您最近购买了两台配备 SSD 磁盘的计算机，您希望数据库相关所有的 Pod 在 SSD 支持的节点上进行调度，以获得最佳的数据库性能。DB Pod 的 Pod YAML 可能如下所示：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">db</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">containers:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mongodb</span></span><br><span class="line">   <span class="attr">image:</span> <span class="string">mongo</span></span><br><span class="line"> <span class="attr">nodeSelector:</span></span><br><span class="line">   <span class="attr">disktype:</span> <span class="string">ssd</span></span><br></pre></td></tr></table></figure><p>根据该定义，当调度程序选择合适的 Pod 分配节点时，将仅考虑具有 <strong>disktype&#x3D;ssd</strong> 标签的节点。</p><p>此外，您可以使用自动分配给节点的任何<a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#built-in-node-labels">内置标签</a>来操纵选择决策。例如，节点的主机名（kubernetes.io&#x2F;hostname），体系结构（kubernetes.io&#x2F;arch），操作系统（kubernetes.io&#x2F;os）等均可用于节点选择。</p><h3 id="节点亲和性"><a href="#节点亲和性" class="headerlink" title="节点亲和性"></a>节点亲和性</h3><p>当您需要选择特定的节点来运行我们的 Pod 时，节点选择非常有用。但是选择节点的方式是有限的，只有与所有定义的标签匹配的节点才被考虑用于 Pod 放置。Node Affinity 通过允许您定义<strong>硬节点和软节点需求</strong>，为您提供了更大的灵活性。硬性要求必须在要选择的节点上匹配。另一方面，软条件允许您为具有特定标签的节点增加更多权重，以使它们<em>在列表中的位置</em>比对等节点<em>更高</em>。没有软需求标签的节点将不被忽略，但它们权重更小。</p><p>让我们举个例子：我们的数据库是 I&#x2F;O 密集型的。我们需要数据库 Pods 始终在 SSD 支持的节点上运行。此外，如果 Pod 部署在区域 zone1 或 zone2 中的节点上，因为它们在物理上更靠近应用程序节点，那么它们的延迟会更短。满足我们需求的 Pod 定义可能如下所示：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">db</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">affinity:</span></span><br><span class="line">   <span class="attr">nodeAffinity:</span></span><br><span class="line">     <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">       <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">         <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">disk-type</span></span><br><span class="line">           <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">           <span class="attr">values:</span></span><br><span class="line">           <span class="bullet">-</span> <span class="string">ssd</span></span><br><span class="line">     <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">       <span class="attr">preference:</span></span><br><span class="line">         <span class="attr">matchExpressions:</span></span><br><span class="line">         <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">zone</span></span><br><span class="line">           <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">           <span class="attr">values:</span></span><br><span class="line">           <span class="bullet">-</span> <span class="string">zone1</span></span><br><span class="line">           <span class="bullet">-</span> <span class="string">zone2</span></span><br><span class="line"> <span class="attr">containers:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">db</span></span><br><span class="line">   <span class="attr">image:</span> <span class="string">mongo</span></span><br></pre></td></tr></table></figure><p>nodeAffinity 节使用以下参数来定义硬性要求和软性要求：</p><ul><li><strong>requiredDuringSchedulingIgnoredDuringExecution：</strong>部署 DB Pod 时，节点必须具有 disk-type&#x3D;ssd。</li><li><strong>preferredDuringSchedulingIgnoredDuringExecution：</strong> 当对节点进行排序时，调度器会给予标签为<strong>zone&#x3D;zone1</strong>或<strong>zone&#x3D;zone2</strong>的节点更高的权重。如果有<strong>disk-type&#x3D;ssd</strong>和<strong>zone&#x3D;zone1</strong>的节点，则优先选择 disk-type&#x3D;ssd 且无 zone 标签的节点或指向其他 zone 的节点。权重可以是 1 到 100 之间的任意值，权重号赋予匹配节点相对于其他节点更高的权重。数字越大，权值越高。</li></ul><p><strong>注意</strong>，在进行选择时，节点亲和性允许您在选择目标节点上应该存在(或不存在)哪些标签时拥有更多的自由。在本例中，我们使用 In 操作符定义了多个标签，目标节点上存在任何一个标签即可。其他运算符是 NotIn、Exists、doesnoexistists、Lt(小于)和 Gt(大于)。值得注意的是，NotIn 和 doesnot existist 实现了所谓的节点反亲和性。</p><p>节点亲和性和节点选择器不是互斥的，它们可以共存于同一个定义文件中。但是，在这种情况下，节点选择器和节点亲和性硬要求必须匹配。</p><h3 id="Pod-亲和性"><a href="#Pod-亲和性" class="headerlink" title="Pod 亲和性"></a>Pod 亲和性</h3><p>节点选择器和节点亲和性(以及反亲和性)帮助我们影响调度器关于在何处放置 Pods 的决策。但是，它只允许您基于节点上的标签进行选择。它不关心 Pod 本身的标签。您可能需要在以下情况下根据 Pod 标签进行选择:</p><ul><li>需要将所有中间件 Pod 放在同一个物理节点上，与那些具有 role&#x3D;front 标签的 Pod 一起，以减少它们之间的网络延迟。</li><li>作为一种安全最佳实践，我们不希望中间件 Pod 与处理用户身份验证的 Pod 共存(role&#x3D;auth)。这不是一个严格的要求。</li></ul><p>如您所见，这些要求不能用节点选择器或亲和性来满足，因为在选择过程中不考虑 Pod 标签——只考虑节点标签。</p><p>为了满足这些需求，我们使用 Pod 亲和性和反亲和性。本质上，它们的工作方式与节点亲和性和反亲和性相同。必须满足硬性要求来选择目标节点，而软条件增加了拥有所选节点的机会(权重)，但不是严格要求。让我们举个例子:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">middleware</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">affinity:</span></span><br><span class="line">   <span class="attr">podAffinity:</span></span><br><span class="line">     <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">         <span class="attr">matchExpressions:</span></span><br><span class="line">         <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">role</span></span><br><span class="line">           <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">           <span class="attr">values:</span></span><br><span class="line">           <span class="bullet">-</span> <span class="string">frontend</span></span><br><span class="line">       <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">   <span class="attr">podAntiAffinity:</span></span><br><span class="line">     <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">100</span></span><br><span class="line">       <span class="attr">podAffinityTerm:</span></span><br><span class="line">         <span class="attr">labelSelector:</span></span><br><span class="line">           <span class="attr">matchExpressions:</span></span><br><span class="line">           <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">role</span></span><br><span class="line">             <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">             <span class="attr">values:</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">auth</span></span><br><span class="line">         <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line"> <span class="attr">containers:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">middleware</span></span><br><span class="line">   <span class="attr">image:</span> <span class="string">redis</span></span><br></pre></td></tr></table></figure><p>在上面的 Pod 定义文件中，我们对硬性要求和软性要求进行了如下设置：</p><p><strong>requiredDuringSchedulingIgnoredDuringExecution：</strong>我们的 Pod 必须在具有标签为 app-front 的 Pod 节点上调度。</p><p><strong>preferredDuringSchedulingIgnoredDuringExecution：</strong>我们的 Pod 不应该(但它可以)被调度到运行带有标签为 role&#x3D;auth 的 Pod 的节点上。与节点亲和性一样，soft requirement 将权重从 1 设置为 100，以增加节点相对于其他节点的概率。在我们的示例中，软需求被放置在 poantiaffinity 中，导致运行具有标签为 role&#x3D;auth 的 Pod 的节点在调度程序做出决定时被选中的可能性更小。</p><p><strong>topologyKey</strong> 用于对规则将应用于哪个领域做出更细粒度的决策。topologyKey 接受一个标签键，该标签键必须出现在选择过程中考虑的节点上。在我们的示例中，我们使用了自动填充的标签，该标签在默认情况下自动添加到所有节点，并引用节点的主机名。但是您可以使用其他自动填充的标签，甚至是自定义的标签。例如，您可能需要只在具有 rack 或 zone 标签的节点上应用 Pod 亲和规则。</p><h2 id="关于-IgnoredDuringExecution-的注释"><a href="#关于-IgnoredDuringExecution-的注释" class="headerlink" title="关于 IgnoredDuringExecution 的注释"></a>关于 IgnoredDuringExecution 的注释</h2><p>您可能已经注意到，硬需求和软需求都有 <strong>IgnoredDuringExecution</strong> 后缀。这意味着在做出调度决策之后，调度程序将不会尝试更改已经放置的 Pods，即使条件发生了变化。例如，根据节点亲和性规则，将一个 Pod 调度到一个具有标签为 app&#x3D;prod 的节点上。如果该标签被更改为 app&#x3D;dev, 旧 Pod 不会被终止，并在另一个有 app&#x3D;prod 标签的节点上启动新的 Pod。这个行为在将来可能会改变，以允许调度程序在部署后继续检查节点和 Pod 的关联性(和反关联性)规则。</p><h2 id="污点与容忍"><a href="#污点与容忍" class="headerlink" title="污点与容忍"></a>污点与容忍</h2><p>在某些场景中，您可能希望阻止将 Pod 调度到特定节点。可能您正在运行测试或扫描此节点以查找威胁，而您不希望应用程序受到影响。节点反亲和性可以实现这一目标。但是，这是一个重大的管理负担，因为您需要向部署到集群的每个新 Pod 添加反关联规则。对于这种场景，您应该使用污点。</p><p>当一个节点配置了污点时，除非 Pod 能够容忍这种污点，否则不能对它调度 Pod。容忍只是一个与污点匹配的键-值对。让我们举个例子来说明:</p><p>需要对主机 web01 进行污染，以使其不接受更多 Pod。taint 命令如下：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">taint</span> <span class="string">nodes</span> <span class="string">web01</span> <span class="string">locked=true:NoSchedule</span></span><br></pre></td></tr></table></figure><p>上面的命令在名为 web01 的节点上放置了一个污点，该节点具有以下属性:</p><ul><li><p>标签 locked&#x3D;true，该标签必须配置在想要运行在该节点的 Pod 上。</p></li><li><p>NoSchedule 的污点类型。污点类型定义了应用污点的行为，它有以下几种可能:</p><ul><li><strong>NoSchedule：</strong>此节点一定不要调度。</li><li><strong>PreferNoSchedule：</strong>尽量不要调度，类似软亲和性。</li><li><strong>NoExecute：</strong>不仅不会调度，还会驱逐 Node 上已有的 Pod。</li></ul></li></ul><p>在带有污点的节点上，Pod 的定义文件可能如下所示:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">mypod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">containers:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mycontainer</span></span><br><span class="line">   <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line"> <span class="attr">tolerations:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;locked&quot;</span></span><br><span class="line">   <span class="attr">operator:</span> <span class="string">&quot;Equal&quot;</span></span><br><span class="line">   <span class="attr">value:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">   <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure><p>让我们仔细看看这个定义的容忍部分:</p><ul><li><p>为了具有正确的容忍，我们需要指定键（locked），值（true）和运算符。</p></li><li><p>运算符可以是两个值之一：</p><ul><li>Equal：当使用 equal 操作符时，键、值和污染效果必须与节点的污染匹配。</li><li>Exists：在使用 exists 操作符时，不需要将污点与容忍匹配，只需匹配建即可。</li></ul></li><li><p>如果使用 Exists 运算符，则可以忽略容忍键、值和效果。具有这种容忍的 Pod 可以被调度到具有任何受污点的节点。</p></li></ul><p>注意，在 Pod 上放置容忍并不能保证它被部署到受污染的节点上。它只允许行为发生。如果要强制 Pod 加入受污染的节点，还必须像前面讨论的那样向其定义添加节点亲和性。</p><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL; DR"></a>TL; DR</h2><p>在节点上自动放置 Pod 是 Kubernetes 诞生的原因之一。作为管理员，只要您对 Pod 需求做出了良好的声明，您就不用担心节点是否有足够的空闲资源来运行这些 Pod。但是，有时您必须手动干预和覆盖系统关于在何处放置 Pods 的决定。在本文中，我们讨论了几种方法，在决定部署 Pods 时，您可以通过这些方法对特定节点的调度器产生更大的影响。让我们快速回顾一下这些方法:</p><ul><li><strong>节点名称：</strong>通过将节点的主机名添加到 Pod 定义的 <strong>.spec.nodeName</strong> 参数中，可以强制此 Pod 在该特定节点上运行。调度程序使用的任何选择算法都将被忽略。不建议使用此方法。</li><li><strong>节点选择器：</strong>通过在节点上放置指定的标签，Pod 可以使用 nodeelector 参数指定一个或多个键-值标签，这些标签必须存在于目标节点上才能被选中以运行 Pod。推荐使用这种方法，因为它增加了很多灵活性，并建立了松耦合的 node-Pod 关系。</li><li><strong>节点亲和性：</strong>在选择应该考虑哪个节点来调度特定的 Pod 时，这种方法增加了更多的灵活性。使用节点亲和性，Pod 可能严格要求在具有特定标签的节点上调度。它还可以通过影响调度程序为特定节点赋予更大的权重来表示对特定节点的某种程度的偏好。</li><li><strong>Pod 亲和性和反亲和性：</strong>当 Pod 与同一节点上的其他 Pod 共存(或不共存)时，可以使用此方法。Pod 亲和性允许将 Pod 部署在具有特定标签的 Pod 运行的节点上。相反，Pod 可能会强制调度程序不将其调度到具有特定标签的 Pod 运行的节点上。</li><li><strong>污点和容忍：</strong>在这种方法中，您不需要决定将 Pod 调度到哪些节点，而是决定哪些节点是否接受 所有 Pod 调度，或者只接受选定的 Pod 调度。通过污染一个节点，调度程序将不考虑将这个节点作为任何 Pod 的调度节点，除非 Pod 配置了容忍。容忍由键、值和受染的效果组成。</li></ul><blockquote><p>原文链接：<a href="https://www.magalix.com/blog/influencing-kubernetes-scheduler-decisions">https://www.magalix.com/blog/influencing-kubernetes-scheduler-decisions</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> scheduler </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubernetes scheduler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初步了解 Kubernetes 基础功能</title>
      <link href="/kubernetes/kubernetes-basics/"/>
      <url>/kubernetes/kubernetes-basics/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/kubernetes-basics/" target="_blank" title="https://www.xtplayer.cn/kubernetes/kubernetes-basics/">https://www.xtplayer.cn/kubernetes/kubernetes-basics/</a></p><h2 id="什么是-Kubernetes？"><a href="#什么是-Kubernetes？" class="headerlink" title="什么是 Kubernetes？"></a>什么是 Kubernetes？</h2><p>随着越来越多的组织开始采用<a href="https://cloud.google.com/containers">容器</a>，以容器为中心的管理软件 Kubernetes 已成为部署和操作容器化应用的通行标准。Google Cloud 是 Kubernetes 诞生的地方 - Kubernetes 最初在 Google 开发，然后在 2014 年开源发布。Kubernetes 的构建以 15 年来运行 Google 的容器化工作负载的经验以及开源社区的宝贵贡献为依托。Kubernetes 受 Google 内部集群管理系统 <a href="https://research.google.com/pubs/pub43438.html">Borg</a> 的启发，能够简化与部署和管理应用相关的所有工作。Kubernetes 可提供自动化容器编排，因此能够提高可靠性，同时节省日常运营所需的时间和资源。</p><h2 id="Kubernetes-的定义"><a href="#Kubernetes-的定义" class="headerlink" title="Kubernetes 的定义"></a>Kubernetes 的定义</h2><blockquote><p>Kubernetes（有时简写为“K8s”，其中“8”代表“K”和“s”之间的 8 个字母）是一个开源系统，支持在任何地方部署、扩缩和管理容器化应用。</p></blockquote><p>Kubernetes 可自动执行容器管理的操作任务，其内置了用于部署应用、更改应用、根据不断变化的需求扩缩应用、监控应用等的命令，以便更轻松地管理应用。</p><h2 id="Kubernetes-有哪些优势？"><a href="#Kubernetes-有哪些优势？" class="headerlink" title="Kubernetes 有哪些优势？"></a>Kubernetes 有哪些优势？</h2><h3 id="自动化运营"><a href="#自动化运营" class="headerlink" title="自动化运营"></a>自动化运营</h3><p>Kubernetes 具有许多内置命令，可用于处理应用管理中繁重的工作，从而自动化日常操作，帮助您确保应用始终按照预期的方式运行。</p><h3 id="基础架构抽象"><a href="#基础架构抽象" class="headerlink" title="基础架构抽象"></a>基础架构抽象</h3><p>安装 Kubernetes 后，它将代表您的工作负载处理计算、网络和存储。这使开发者可以专注于应用，而不必担心底层环境。</p><h3 id="服务运行状况监控"><a href="#服务运行状况监控" class="headerlink" title="服务运行状况监控"></a>服务运行状况监控</h3><p>Kubernetes 会对您的服务不间断地执行运行状况检查，重新启动有故障或停滞的容器，且只会在确认服务正常运行时向用户提供服务。</p><h2 id="Kubernetes-与-Docker-的区别"><a href="#Kubernetes-与-Docker-的区别" class="headerlink" title="Kubernetes 与 Docker 的区别"></a>Kubernetes 与 Docker 的区别</h2><p>Kubernetes 和 Docker 通常被误认为只能二选一，其实它们都是用于运行容器化应用的技术，彼此不同但相互补充。Docker 可以将运行应用所需的一切资源放入一个箱子中，这个箱子可以存储起来并在需要的时间和位置打开。将应用装箱后，就需要一种方法来管理它们，这就是 Kubernetes 的作用。Kubernetes 是希腊语，意思是 “船长”。就像船长负责船舶在海上的安全航行一样，Kubernetes 负责安全运送这些箱子并将其交付到可以使用的地点。Kubernetes 可以和 Docker 搭配使用，也可以独立使用 Docker 并不是 Kubernetes 的替代品，因此其实也不存在 “Kubernetes 与 Docker 的区别” 这个问题。将 Kubernetes 与 Docker 结合使用可以容器化您的应用，并大规模运行它们 Docker 和 Kubernetes 之间的差异与它们在容器化和运行应用中所扮演的角色有关 Docker 是在容器中打包和分发应用的开放式业界标准 Kubernetes 使用 Docker 来部署、管理和扩缩容器化应用</p><h2 id="Kubernetes-有哪些用途？"><a href="#Kubernetes-有哪些用途？" class="headerlink" title="Kubernetes 有哪些用途？"></a>Kubernetes 有哪些用途？</h2><p>Kubernetes 用于创建易于在任何地方管理和部署的应用。当作为代管式服务时，Kubernetes 可为您提供一系列解决方案以满足您的需求。以下是一些常见使用场景。</p><h3 id="提高开发速度"><a href="#提高开发速度" class="headerlink" title="提高开发速度"></a>提高开发速度</h3><p>Kubernetes 可帮助您构建基于微服务的云原生应用。它还支持容器化现有应用，为应用现代化改造奠定基础，帮助您更快地开发应用。</p><h3 id="在任何地方部署应用"><a href="#在任何地方部署应用" class="headerlink" title="在任何地方部署应用"></a>在任何地方部署应用</h3><p>Kubernetes 可以在任何地方使用，让您可以在本地部署、公有云部署以及混合部署之间运行应用。因此，您可以在任何需要的地方运行您的应用。</p><h3 id="运行高效的服务"><a href="#运行高效的服务" class="headerlink" title="运行高效的服务"></a>运行高效的服务</h3><p>Kubernetes 可以自动调整运行服务所需集群的大小。这使您可以根据需求自动扩缩并高效运行应用。</p><h2 id="Kubernetes-能为您做什么？"><a href="#Kubernetes-能为您做什么？" class="headerlink" title="Kubernetes 能为您做什么？"></a>Kubernetes 能为您做什么？</h2><p>通过现代的 Web 服务，用户希望应用程序能够 24&#x2F;7 全天候使用，开发人员希望每天可以多次发布部署新版本的应用程序。 容器化可以帮助软件包达成这些目标，使应用程序能够以简单快速的方式发布和更新，而无需停机。Kubernetes 帮助您确保这些容器化的应用程序在您想要的时间和地点运行，并帮助应用程序找到它们需要的资源和工具。Kubernetes 是一个可用于生产的开源平台，根据 Google 容器集群方面积累的经验，以及来自社区的最佳实践而设计。</p><h2 id="Kubernetes-基础模块"><a href="#Kubernetes-基础模块" class="headerlink" title="Kubernetes 基础模块"></a>Kubernetes 基础模块</h2><table><thead><tr><th align="left"><strong>1. 创建 k8s 集群</strong></th><th align="left"><strong>2. 部署应用</strong> &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</th></tr></thead><tbody><tr><td align="left"><img src="/kubernetes/kubernetes-basics/module_01.svg" class="" title="img"></td><td align="left"><img src="/kubernetes/kubernetes-basics/module_02.svg" class="" title="img"></td></tr><tr><td align="left"><strong>3. 探索您的应用</strong></td><td align="left"><strong>4. 发布应用</strong> &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</td></tr><tr><td align="left"><img src="/kubernetes/kubernetes-basics/module_03-20210213225613100.svg" class="" title="img"></td><td align="left"><img src="/kubernetes/kubernetes-basics/module_04.svg" class="" title="img"></td></tr><tr><td align="left"><strong>5. 伸缩应用</strong></td><td align="left"><strong>6. 更新应用</strong> &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</td></tr><tr><td align="left"><img src="/kubernetes/kubernetes-basics/module_05.svg" class="" title="img"></td><td align="left"><img src="/kubernetes/kubernetes-basics/module_06.svg" class="" title="img"></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Windows 10X 初步了解</title>
      <link href="/windows/windows-10x/"/>
      <url>/windows/windows-10x/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/windows/windows-10x/" target="_blank" title="https://www.xtplayer.cn/windows/windows-10x/">https://www.xtplayer.cn/windows/windows-10x/</a></p><p>Windows 10X 是为新 PC 完全构建的 Windows 的新版本，将于 2021 年开始在硬件上交付。它基于新的称为 <code>Windows Core OS</code> 的现代 Windows 版本构建，该版本可破坏旧组件，以及支持现代用户体验和增强安全性的功能。</p><p>这意味着从 <code>Windows Shell</code> 到底层操作系统的所有功能都已使用现代技术进行了重建。因此，Windows 10X 在启动时不支持旧版 Win32 应用程序。2021 年的 Windows 10X PC 将能够运行 Microsoft Edge，UWP 和 Web 应用程序。</p><p>但是，旧版 Win32 应用程序支持将在以后提供。这样做时，默认情况下，Win32 应用程序将在安全容器中运行，这意味着这些传统应用程序在关闭时不会影响系统性能和电池寿命。因此，Windows 10X 是一个更加安全和稳定的操作系统，因为传统应用程序没有机会引起比特币腐烂。</p><h2 id="新的用户体验"><a href="#新的用户体验" class="headerlink" title="新的用户体验"></a>新的用户体验</h2><img src="/windows/windows-10x/windows-10x-laptop.jpg" class="" title="Windows 10X"><p>Windows 10X 具有使用现代技术构建的新外壳（即用户界面）。这是一种自适应的用户体验，可以根据设备的“姿势”进行调整。例如，对于可折叠的 PC，用户可能希望以几种不同的方式使用它。作为笔记本电脑或平板电脑，或在帐篷模式下观看电影。因此，无论使用哪种设备，用户界面都必须适应提供最佳体验。</p><p>这也意味着 Windows 10X 上的遗留外壳程序元素（如控制面板，文件资源管理器以及错误对话框和图标）已消失。由于 Microsoft 重建了整个外壳，因此它不包含任何使 Windows 10 在 UI 方面如此不一致的遗留事物。Windows 10X 上的 Windows Shell 应该更加一致。</p><p>在发布时，Windows 10X 仅适用于主要面向教育和企业市场的传统翻盖式 PC。该平台最终将以可折叠 PC 等新设备形式交付，但这种情况不会在 2021 年发生。</p><h3 id="新的开始菜单"><a href="#新的开始菜单" class="headerlink" title="新的开始菜单"></a>新的开始菜单</h3><img src="/windows/windows-10x/windows-10x-start.jpg" class="" title="Windows 10X 开始"><p>Microsoft 正在重新设计 Windows 10X 上的“开始”菜单体验，重点是提高生产力。它的顶部有一个系统范围的搜索栏，它也可以搜索网络，而在其下方是已安装应用程序的网格，以代替实时磁贴。</p><p>它还具有“最近活动”区域，该区域动态更新用户可能希望直接跳转的内容，例如最近的 Office 文档和访问的网站。可以自定义应用程序列表，用户可以重新排列在前几行中显示的应用程序。</p><h3 id="新的任务栏"><a href="#新的任务栏" class="headerlink" title="新的任务栏"></a>新的任务栏</h3><img src="/windows/windows-10x/10x-taskbar-preview.jpg" class="" title="10x 任务栏预览"><p>Windows 10X 还具有一个新的自适应任务栏，该任务栏具有居中设计。“开始”和“任务视图”按钮出现在中间，正在运行和固定的应用程序出现在两者之间。当您打开一个应用程序时，“开始”和“任务视图”按钮会逐渐分开，使任务栏外观更加流畅。</p><p>有一些新的动画。单击“开始”和“任务视图”按钮时，它们都有自己的动画；当您最小化正在任务栏上运行的应用程序时，应用程序图标会有轻微的反弹。除了新设计之外，还有多达三种不同的任务栏大小：小，中和大。大号适合平板电脑，而大号和小号则模仿了我们今天在 Windows 10 上已经拥有的通常尺寸。</p><p>在平板电脑上，用户现在可以在任务栏上的任意位置向上滑动以访问“开始”菜单，从而使触摸用户可以更轻松地访问其应用程序列表。您不再需要点击特定的“开始”按钮来访问“开始”菜单。</p><h3 id="一个新的操作中心"><a href="#一个新的操作中心" class="headerlink" title="一个新的操作中心"></a>一个新的操作中心</h3><img src="/windows/windows-10x/windows-10x-ac.jpg" class="" title="Windows 10X AC"><p>除了新的 <strong>开始</strong> 和<strong>任务栏</strong>体验之外，还有一个新的 <strong>操作中心</strong> 来对它们进行补充。这个新的<strong>操作中心</strong>更加强调快速启动，能够跳入特定的快速启动进行进一步控制，而无需离开操作中心。</p><p>它的设计也模仿了控制中心，并在一个单独的盒子中放置了通知。这个新的 Action Center 包括音量控制，电源选项和电池百分比之类的东西。从支持的应用程序播放音乐时，操作中心还会显示一个新的音乐控制 UI。</p><h3 id="新的设置经验"><a href="#新的设置经验" class="headerlink" title="新的设置经验"></a>新的设置经验</h3><img src="/windows/windows-10x/windows-10x-oobe.jpg" class="" title="Windows 10x Oobe"><p>由于 Windows 10X 的每个部分都经过了重新设计，因此开箱即用的体验也得到了现代化的外观和感觉更新。它仍然会引导您完成 Windows 设置过程，选择语言，使用 Microsoft 帐户登录并同意条款和条件，但是 Cortana 在整个设置过程中不再存在。这是一种较为传统的设置体验，已在 10X 上进行了美化。</p><h2 id="新的文件资源管理器"><a href="#新的文件资源管理器" class="headerlink" title="新的文件资源管理器"></a>新的文件资源管理器</h2><img src="/windows/windows-10x/fileexplorer2.jpg" class="" title="文件浏览器 10X"><p>由于 Windows 10X 具有现代内核，因此不再存在诸如经典文件资源管理器之类的旧组件。这意味着微软已经建立了一个专门针对 Windows 10X 的新文件资源管理器，并且它是围绕 OneDrive 构建的。Windows 10X 是 Web 优先的操作系统，其中包括在 Windows 10X PC 上存储和管理文件的方式。默认情况下，所有文件都与云中的 OneDrive 帐户同步，同时也可以在设备上本地访问。</p><h2 id="改进的-Windows-Update"><a href="#改进的-Windows-Update" class="headerlink" title="改进的 Windows Update"></a>改进的 Windows Update</h2><img src="/windows/windows-10x/windows-update-hero-2019.jpg" class="" title="Windows 更新"><p>微软还以一种使 Windows Update 在 Windows 10X 上更快的方式改进了 Windows Update。功能更新的安装时间无需像 Windows 10 上那样长，因为这些功能更新现在已安装在后台，无需重启即可<em>完成</em>更新<em>。</em>因此，就像在 Android 和 Chrome 操作系统上一样，当更新准备好重新启动 PC 时，它将像正常情况一样重新启动，并且不需要 15 分钟就可以完成安装，然后再备份并运行。</p><p>这将导致更新花费少于 90 秒的时间来重新引导。内部测试表明它甚至比这还要快。这是对 Windows 10 今天更新方式的巨大改进，根据设备的不同，重新启动可能需要 5 分钟到 20 分钟之间的任何时间。</p><h2 id="默认安全"><a href="#默认安全" class="headerlink" title="默认安全"></a>默认安全</h2><img src="/windows/windows-10x/10x-windows-defender.png" class="" width="10" title="个 Windows Defender"><p>与 Windows 10 不同，Windows 10X 具有称为“状态分离”的功能，这是操作系统在驱动器上的布局方式。如今，Windows 10 将所有内容安装在一个分区中，这意味着用户以及应用程序和潜在的攻击者都可以访问系统文件。在 Windows 10X 上，所有内容都进入其自己的只读分区。因此，OS 文件，应用程序文件，驱动程序和注册表都被锁定。用户和应用程序只能访问用户分区。</p><p>这意味着恶意软件或病毒无法进入并影响系统，因为这些程序只能在单个分区中运行，并且假定它们能够脱离 Microsoft 内置的应用程序容器系统。Windows 10X 上的所有应用程序都在容器中运行，并且需要显式权限才能访问该容器外部的内容。这已经是 UWP 应用程序在 Windows 10 上的工作方式，Microsoft 将在支持 Win32 应用程序时将其扩展到 Windows 10X 上的 Win32 应用程序。</p><h2 id="动态壁纸"><a href="#动态壁纸" class="headerlink" title="动态壁纸"></a>动态壁纸</h2><img src="/windows/windows-10x/dynamicwallpaper10x.gif" class="" title="动态壁纸"><p>我了解 Windows 10X 具有动态墙纸，可根据不同因素更改内容。例如，内部 Windows 10X 构建具有山景墙纸，该墙纸具有早上，下午，晚上和夜晚的变体，这些变体会根据设备的实际时间出现。这种墙纸看似还经常出现动感的云朵和小鸟。</p><p>我不知道 Windows 10X 上会出现多少这些动态壁纸，也不知道它们有多复杂。不过，很高兴为用户看到更多自定义选项。</p><h2 id="2021-年春季"><a href="#2021-年春季" class="headerlink" title="2021 年春季"></a>2021 年春季</h2><p>Windows 10X 将于今年春季首次面向商业市场推出。商业市场包括教育和企业行业，他们希望为教室中的学生或一线员工提供低于 600 美元的 PC。Windows 10X 不会在 2021 年在家用 PC 上启动，这意味着您不会在旗舰 Dell 或 HP 设备上找到它。它也仅适用于翻盖式 PC，可折叠，平板电脑和其他外形尺寸支持将于 2022 年及以后推出。</p><p>Windows 10X 将在没有内置邮件和日历应用程序的情况下启动。它已从 Windows 10X 的第一版中删除，因为该平台面向可能使用 Outlook Web 或通过 Windows 虚拟桌面流 Outlook 的商业市场。用户可以根据需要选择从 Microsoft Store 重新安装 Mail 和 Calendar 应用程序。</p><p>直到 2022 年，面向主流市场的 Windows 10X 才会面世。届时，作为计划于 2022 年上半年发布的 <code>Windows 10 Nickel</code> 版本的一部分，Win32 应用程序对其他功能的支持将进入操作系统。</p><p>由于 Windows 10X 是新的操作系统，因此不会作为现有 Windows 10 PC 的更新发布。用户将无法在 Windows 10X 最初不附带的设备上安装 Windows 10X。不会有任何官方的 ISO 媒体，您将无法自行购买 Windows 10X 并安装在现有设备上。仅适用于新 PC。</p>]]></content>
      
      
      <categories>
          
          <category> windows </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows10X </tag>
            
            <tag> windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>运用 DevOps 之力实现基础设施自动化</title>
      <link href="/devops/automation-of-infrastructure/"/>
      <url>/devops/automation-of-infrastructure/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/devops/automation-of-infrastructure/" target="_blank" title="https://www.xtplayer.cn/devops/automation-of-infrastructure/">https://www.xtplayer.cn/devops/automation-of-infrastructure/</a></p><p>GitOps 提供一种自动化基础设施管理方法，已经在众多团队中得到应用的 DevOps 最佳实践——包括版本控制、代码审查以及 CI&#x2F;CD 流水线——都将被囊括于其中。目前，许多公司都在采用 DevOps，看中的正是它在提高生产率和软件质量方面拥有的巨大潜力。在这一过程中，我们已经找到了自动化软件开发生命周期的方法。但是，当涉及到基础设施的设置和部署时，手动操作的比重仍然相当可观。有了 GitOps，团队就可以自动化基础设施配置过程。这是由于在 GitOps 方法中，我们能够使用声明将基础设施编写为代码（IaC），而后像存储应用程序开发代码一样将基础设施即代码存储在 Git repo 当中。</p><h2 id="GitOps-如何发挥作用？"><a href="#GitOps-如何发挥作用？" class="headerlink" title="GitOps 如何发挥作用？"></a>GitOps 如何发挥作用？</h2><p>GitOps 的概念最初是由 Kubernetes 管理公司 Weaveworks 所提出，因此关于 GitOps 的讨论主要是在 Kubernetes 的背景下进行的。随着整体设施转向运行在容器内的微服务架构，我们自然需要更多可行的编排平台作为支撑。事实上，基于容器的应用程序也往往拥有极为复杂且难以管理的配置体系。GitOps 则通过应用在 DevOps 领域已经得到实际验证的技术，帮助我们简化了这一过程。如今，这一思路已经在 DevOps 支持者中得到广泛认可，也代表着 IaC 概念的升级模型。其中包含三大主要组成部分：</p><ul><li>基础设施即代码</li><li>Pull 请求</li><li>CI&#x2F;CD</li></ul><p>下面具体来看。</p><h3 id="基础设施即代码"><a href="#基础设施即代码" class="headerlink" title="基础设施即代码"></a>基础设施即代码</h3><p>IaC 是一种将基础设施以声明文件的形式进行配置和管理，并将其存储为代码的实践。通过利用 IaC 和版本控制，团队即可轻松优化所有的运营过程。<strong>GitOps 以 IaC 的声明性模型为核心，同时也为 Kubernetes 提供了良好的施展平台。</strong>声明性意味着配置更多关注指向预期状态的声明，而不是一组具体命令。例如，在 Kubernetes 中，你可以在 manifest 中定义服务所需的 Pod 数量。以此为基础，系统将根据服务的运行状态自动为其提供 Pod，而不再由工程师编写固定的 Pod 配置数量。任何符合声明式模型的云原生软件都可以被视为代码。我们使用 AWS CloudFormation（一种声明性工具）来编写 AWS 基础设施，借此实现基础设施即代码原则。所需的状态将被声明为代码形式，系统则应用更改以自动达到这一目标状态。当然，声明式模型并不是实现 GitOps 的唯一途径。大家也可以使用命令式定义环境实现相同的运营效果。</p><h3 id="Pull-请求"><a href="#Pull-请求" class="headerlink" title="Pull 请求"></a>Pull 请求</h3><p>GitOps 概念背后的核心思路，是将版本控制系统视为单一的客观来源。我们使用 Git 作为应用程序代码的变更管理系统，也可以将其用于基础架构代码。所以所有的声明文件都托管在统一位置以供协作使用。在此基础之上，我们得以使用 Git 的关键概念——<strong>操作更改的 pull 请求</strong>。在应用程序开发工作流中，我们使用一个主分支作为发布分支。开发人员在主分支内创建功能分支。在开发一项特定的功能或故事之后，我们创建一个 pull 请求以将其合并回主分支。同样的方法也能在基础设施代码中便捷起效。通过创建 pull 请求，我们可以保证代码在被集成至代码库的另一个分支之前，首先经过完整的代码审查流程。代码审查可以阻止低质量代码进入测试或生产环境，这一点对于基础架构代码来说尤为重要。通过代码审查获得正式的批准，也将有助于后续的审核和故障排查工作。</p><h3 id="Git-组织"><a href="#Git-组织" class="headerlink" title="Git 组织"></a>Git 组织</h3><p>GitOps 的部署过程至少需要两个 repo：应用程序 repo 与环境配置 repo。前者包含应用程序的源代码及其部署 manifest；后者则包含了整个系统所需的状态，该状态使用声明性规范来对环境中的各项要素加以描述。你可以在代码 repo 中将环境描述为开发、测试和生产环境，同时包含可以在该环境的特定版本中运行的应用程序和基础设施服务。在基础设施的情况下，主分支可以表示一个环境。我们可以在功能分支中实现这些更改，而后创建一个 pull 请求来合并主分支中的变更。通过这种方式，我们可以在实现协作的同时，以更加透明的方式了解谁执行了哪些更改。因为所有的更改都是在 Git 中提交完成，因此这也有利于跟踪引发问题的根本原因。GitOps 适用于任何基于 Git 的系统，包括 GitHub、BitBucket 或 GitLab。其不依赖于任何特定工具或技术。</p><h3 id="CI-x2F-CD"><a href="#CI-x2F-CD" class="headerlink" title="CI&#x2F;CD"></a>CI&#x2F;CD</h3><p>为了建立完整的 GitOps 实现，你还需要一条 CI&#x2F;CD 流水线。通过使用自动化的交付流水线，每当 Git 存储库中发生更改时，你都可以将基础设施更改交付到指定环境当中。这条流水线将你的 Git pull 请求连接到业务流程系统。当你使用 pull 请求触发流水线时，业务流程系统将相应执行该任务。GitOps 的部署策略有两种方式：push 与 pull 流水线。二者的区别，主要体现在构建基础设施时所采取的环境部署方式之上。</p><p>许多流行的 CI&#x2F;CD 工具都在使用这种策略。我们将应用程序的源代码及其部署 manifest 存储在一个 repo 当中。当应用程序代码中发生新的更新时，构建流水线将触发。流水线将构建容器镜像并将更改推送到环境。这种策略带来了更高的灵活性，足以支持任意类型的基础设施。当然，这种方法也有缺点，即允许 CI&#x2F;CD 工具直接访问你的环境。</p><img src="/devops/automation-of-infrastructure/640-20210210235914739" class="" title="图片">社区普遍认为，pull 流水线方法对 GitOps 来说是一种更为安全的实践方案。这种方法引入引入了操作符。操作符属于流水线和业务流程工具之间的组件，它会不断将环境 repo 中的目标状态与已部署基础设施中的实际状态进行比较。一旦检测到任何更改，则操作符会更改基础设施以适应环境 repo。此外，它还可以监控镜像仓库，识别待部署的新版本镜像。正是这一切，让 GitOps 变得如此特别。在 GitOps 中，只有在环境 repo 中发生了更改时，才会引发环境更新。如果实现的基础设施以环境 repo 中未经定义的任何其他方式发生更改，系统将恢复所做的任何修改。大多数应用程序可能需要同时使用多个环境。GitOps 允许您创建多个可以更改环境 repo 的流水线。您可以在环境 repo 中使用单独的分支以管理更多环境。面对分支变更，运维人员可以在响应中将此项变更部署到生产环境当中，同时将来自另一分支的其他变更部署到测试环境。<img src="/devops/automation-of-infrastructure/640" class="" title="图片"><h2 id="GitOps-的优势是什么？"><a href="#GitOps-的优势是什么？" class="headerlink" title="GitOps 的优势是什么？"></a>GitOps 的优势是什么？</h2><h3 id="DevOps-最佳实践"><a href="#DevOps-最佳实践" class="headerlink" title="DevOps 最佳实践"></a>DevOps 最佳实践</h3><p>GitOps 是一套专注于现有 Git 工作流、IaC、CI&#x2F;CD 流水线、不可变服务器、跟踪与可观察性最佳实践的模型，也代表着 Kubernetes 在云原生应用程序管理领域的先进的理念。因此，其技术栈与操作体验能够切实为企业用户带来诸多助益。</p><h3 id="持续部署——简化"><a href="#持续部署——简化" class="headerlink" title="持续部署——简化"></a>持续部署——简化</h3><p>持续部署意味着更快、更频繁的部署节奏。出于多种不同考量，例如系统的有状态性、宕机弹性、上游&#x2F;下游的依赖关系，以及组织内常见的其他过程与依赖项，很多朋友可能发现越来越难以建立适当的持续部署机制。GitOps 不仅能够实现持续部署，同时也让大家摆脱了对大量工具方案的单独管理——这是因为所有操作都发生在版本控制系统之内。作为另一大助力，部署操作符则负责提供结构和自动化支持。这也提高了生产力并带来更快的 MTTD（平均部署时间）。自动化持续部署确保团队每天可以交付 30-100 倍以上的更改，将平均生产效能提高 2-3 倍。</p><p>Rancher 2.5 通过 Rancher 持续交付（Continuous Delivery）简化了部署和管理。这是一项全新的功能，通过使用 Git 仓库自动存储和管理应用程序和配置信息，以确保部署的一致性，大大减轻了客户的负担，从而简化跨私有云、公有云、混合云或多云环境的部署流程。</p><p>Rancher 于 2020 年推出了<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247495011&idx=1&sn=f7cbcf9c2ace03aaa71dcf3713634bf4&chksm=e8396ba5df4ee2b38631ed2c7af2afb9ccd4e5477321b061c422b57370a5f7a0ca79c1304d19&scene=21#wechat_redirect">海量集群管理项目 Fleet</a>，这个项目成为了 Rancher 持续交付的引擎。Fleet 是一个 Kubernetes 集群控制器，旨在解决全球内成千上万集群的挑战。</p><h3 id="低-MTTR（平均修复时间）"><a href="#低-MTTR（平均修复时间）" class="headerlink" title="低 MTTR（平均修复时间）"></a>低 MTTR（平均修复时间）</h3><p>MTTR 是 DevOps 团队需要衡量的关键指标之一。在微服务架构中，即使是极微小的问题也可能难以修复。由于 GitOps 将所有更改保存在版本控制系统中，同时辅以自动化管理手段，因此有望显著缩短 MTTR。你可以全面了解环境的变化进程，同时极大降低错误恢复难度。</p><h3 id="简化-Kubernetes-管理"><a href="#简化-Kubernetes-管理" class="headerlink" title="简化 Kubernetes 管理"></a>简化 Kubernetes 管理</h3><p>即使对 Kubernetes 不甚了解，开发人员可以使用熟悉的工具(如 Git)轻松获取 Kubernetes 升级与功能实现。新手嵌入式开发人员能够很快跟上进度，将原本需要数月的适应期压缩到几天时间。</p><h3 id="改进企业整体的标准化水平"><a href="#改进企业整体的标准化水平" class="headerlink" title="改进企业整体的标准化水平"></a>改进企业整体的标准化水平</h3><p>你可以在整个企业中建立起透明的端到端工作流，这要归功 GitOps 提供的用于呈现应用程序、软件和 Kubernetes 附加组件修改的呈现框架。Git 还能够全面重现你的各项操作活动。</p><h2 id="应用-GitOps-的先决条件"><a href="#应用-GitOps-的先决条件" class="headerlink" title="应用 GitOps 的先决条件"></a>应用 GitOps 的先决条件</h2><h3 id="建立稳定的代码审查与测试过程"><a href="#建立稳定的代码审查与测试过程" class="headerlink" title="建立稳定的代码审查与测试过程"></a>建立稳定的代码审查与测试过程</h3><p>深入检查代码更改将帮助我们准确识别某些重要操作，例如添加全局变量，借此防止低质量代码被发布到测试甚至生产环境当中。以此为基础，您可以通过 pull 请求提交验证过的代码，且严格禁止开发人员直接提交更改。一旦 pull 请求完成审查与合并，即可触发流水线。这是也维护高标准代码、进而增强系统稳定性的第一步。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>GitOps 的介入意味着整个自动化水平都将提升到新的高度，这也要求我们对流水线发布的应用程序进行彻底测试。尽管 GitOps 能帮助我们相对轻松地完成回滚，但发布经过良好测试的高质量代码才是真正提升进程可靠性的最佳途径。</p><h3 id="监控为王"><a href="#监控为王" class="headerlink" title="监控为王"></a>监控为王</h3><p>GitOps 能够重播操作过程，持续跟踪系统状态并加以改进，最终据此执行发布与回滚。严格的监控体系可以帮助你识别并防止配置中出现任何非预期的漂移与系统更改。因此，在开始使用 GitOps 之前，请检查你的监控技能并着手加强，确保其有能力处理这种变化。</p><h3 id="拥抱新文化"><a href="#拥抱新文化" class="headerlink" title="拥抱新文化"></a>拥抱新文化</h3><p>传统的流程约束以及较长的发布时间只会拖慢业务节奏。全面拥抱 DevOps 文化，意味着我们应当全面利用最佳战略并帮助团队理解开发和运维行动的价值。与此同时，开发与运维团队必须联手协作，建立起整体稳定的基础设施，更快速、更顺畅地运行应用程序，进而提升系统管理效率。而 DevOps 文化的欠缺将严重阻碍我们享受 GitOps 带来的好处。</p><h2 id="为什么采用-GitOps？"><a href="#为什么采用-GitOps？" class="headerlink" title="为什么采用 GitOps？"></a>为什么采用 GitOps？</h2><p>GitOps 是一种强大的工作流模式，可以帮助您高效治理云基础设施。GitOps 可以为工程团队带来诸多优势，极大增强系统的协调能力、透明度、稳定性与持久性。</p><p>原文链接：<a href="https://mp.weixin.qq.com/s/pe1SlHH5fFKWPS0kLtAWJQ">https://mp.weixin.qq.com/s/pe1SlHH5fFKWPS0kLtAWJQ</a></p>]]></content>
      
      
      <categories>
          
          <category> devops </category>
          
      </categories>
      
      
        <tags>
            
            <tag> devops </tag>
            
            <tag> DevOps </tag>
            
            <tag> 自动化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ETCD 常见问题集锦</title>
      <link href="/etcd/etcd-problem-sets/"/>
      <url>/etcd/etcd-problem-sets/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/etcd/etcd-problem-sets/" target="_blank" title="https://www.xtplayer.cn/etcd/etcd-problem-sets/">https://www.xtplayer.cn/etcd/etcd-problem-sets/</a></p><h3 id="客户端必须向-etcd-leader-发送请求吗"><a href="#客户端必须向-etcd-leader-发送请求吗" class="headerlink" title="客户端必须向 etcd leader 发送请求吗?"></a>客户端必须向 etcd leader 发送请求吗?</h3><p>Raft is leader-based， leader 处理所有需要一致性的客户机请求。但客户端不需要知道哪个节点是 leader，所有发送给跟随者的一致性请求都会自动转发给 leader。不需要协商一致的请求(例如，序列化读取)可以由任何集群成员处理。</p><h3 id="系统要求"><a href="#系统要求" class="headerlink" title="系统要求"></a>系统要求</h3><p>由于 etcd 将数据写入磁盘，因此强烈建议使用 SSD 或者超高速磁盘来运行 etcd 服务。为防止性能下降或无意中存储空间耗尽，etcd 强制设置 2GB 默认存储大小配额，可以通过<code>--quota-backend-bytes</code>配置配额，最高可配置为 8GB。空间配额用来保障集群可靠运行，如果没有限制配额，当键空间变大之后，直到耗尽磁盘空间。当任意节点超出空间配额，那么当前 etcd 服务将进入维护状态，只接受<code>读/删</code>操作。只有释放了足够空间、去碎片化了后端数据库并且清理了空间配额之后，集群才能继续正常操作。</p><h3 id="etcd-集群大小"><a href="#etcd-集群大小" class="headerlink" title="etcd 集群大小"></a>etcd 集群大小</h3><p>从理论上讲，没有硬性限制。但是，一个 etcd 集群建议不超过七个节点。</p><h3 id="Error-from-server-etcdserver-mvcc-database-space-exceeded"><a href="#Error-from-server-etcdserver-mvcc-database-space-exceeded" class="headerlink" title="Error from server: etcdserver: mvcc: database space exceeded"></a>Error from server: etcdserver: mvcc: database space exceeded</h3><p>etcd 默认不会自动进行数据压缩，etcd 保存了 keys 的历史信息，数据频繁的改动会导致数据版本越来越多，相对应的数据库就会越来越大。etcd 数据库大小默认 2GB，当在 etcd 容器或者 Rancher ui 出现以下日志时，说明数据库空间占满，需要进行数据压缩腾出空间。</p><h4 id="释放空间"><a href="#释放空间" class="headerlink" title="释放空间"></a>释放空间</h4><ol><li><p>登录 etcd 容器</p><p>在 etcd 主机上，执行以下命令登录 etcd 容器</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -ti etcd sh</span><br></pre></td></tr></table></figure></li><li><p>获取历史版本号:</p><p>在 etcd 容器执行以下命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ver=$(etcdctl endpoint status --write-out=<span class="string">&quot;json&quot;</span> | egrep -o <span class="string">&#x27;&quot;revision&quot;:[0-9]*&#x27;</span> | egrep -o <span class="string">&#x27;[0-9].*&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>压缩旧版本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">etcdctl compact <span class="variable">$ver</span></span><br></pre></td></tr></table></figure></li><li><p>清理碎片</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">etcdctl defrag</span><br></pre></td></tr></table></figure><blockquote><p>以上 2-4 步，操作需在每个 etcd 容器中执行。</p></blockquote></li></ol><h4 id="忽略-etcd-告警"><a href="#忽略-etcd-告警" class="headerlink" title="忽略 etcd 告警"></a>忽略 etcd 告警</h4><p>通过执行<code>etcdctl alarm list</code>可以查看 etcd 的告警情况，如果存在告警，即使释放了 etcd 空间，etcd 也处于只读状态。</p><p>在确定以上的操作均执行完毕后，在任意一个 etcd 容器中执行以下命令忽略告警:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">etcdctl alarm disarm</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> etcd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> etcd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kubernetes 常见问题集锦</title>
      <link href="/kubernetes/kubernetes-problem-sets/"/>
      <url>/kubernetes/kubernetes-problem-sets/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/kubernetes-problem-sets/" target="_blank" title="https://www.xtplayer.cn/kubernetes/kubernetes-problem-sets/">https://www.xtplayer.cn/kubernetes/kubernetes-problem-sets/</a></p><h3 id="为什么在节点出现故障时重新调度-Pod-需要-5-分钟以上？"><a href="#为什么在节点出现故障时重新调度-Pod-需要-5-分钟以上？" class="headerlink" title="为什么在节点出现故障时重新调度 Pod 需要 5 分钟以上？"></a>为什么在节点出现故障时重新调度 Pod 需要 5 分钟以上？</h3><p>这是由于以下默认 Kubernetes 设置的组合：</p><ul><li>kubelet<ul><li><code>node-status-update-frequency</code>：指定 kubelet 将节点状态发布到 master 的频率（默认为 10 秒）</li></ul></li><li>kube-controller-manager<ul><li><code>node-monitor-period</code>：在 NodeController 中同步 NodeStatus 的时间段（默认 5 秒）</li><li><code>node-monitor-grace-period</code>：在标记运行节点不健康之前允许运行节点无响应的时间（默认为 40 秒）</li><li><code>pod-eviction-timeout</code>：删除失败节点上的 Pod 的宽限期（默认为 5m0）</li></ul></li></ul><p>有关这些设置的更多信息，请访问文档：<a href="/kubernetes/fast-migration-pod-when-node-unavailable/">节点不可用时快速迁移 Pods</a></p><h3 id="network-stat-x2F-var-x2F-lib-x2F-calico-x2F-nodename-no-such-file-or-directory"><a href="#network-stat-x2F-var-x2F-lib-x2F-calico-x2F-nodename-no-such-file-or-directory" class="headerlink" title="network: stat &#x2F;var&#x2F;lib&#x2F;calico&#x2F;nodename: no such file or directory"></a>network: stat &#x2F;var&#x2F;lib&#x2F;calico&#x2F;nodename: no such file or directory</h3><p>calico 服务启动的时候，会在 <code>/var/lib/calico/</code> 目录下生成 <code>nodename</code> 文件，主机上也是对应这个路径。在 <strong>calico pod</strong> 中有多个容器，有容器负责生成 <code>nodename</code> 这个文件。如果出现这个问题，有可能是容器未能正常运行，可以删除 Pod 让其重新运行，或者按以下文档对节点进行初始化之后，再重新添加到集群：<a href="https://www.xtplayer.cn/rancher/node-init/">https://www.xtplayer.cn/rancher/node-init/</a></p><h3 id="controlPlane-Failed-to-bring-up-Control-Plane：Failed-to-verify-healthcheck"><a href="#controlPlane-Failed-to-bring-up-Control-Plane：Failed-to-verify-healthcheck" class="headerlink" title="[controlPlane] Failed to bring up Control Plane：Failed to verify healthcheck"></a>[controlPlane] Failed to bring up Control Plane：Failed to verify healthcheck</h3><p>这是通用的错误提示，出现这个错误一般是 <code>kube-apiserver</code> 或 <code>kube-controller-manager</code> 服务没有正常运行，导致无法通过 rke 的健康检查。</p><p>在 <strong>rke1</strong> 中，<code>kube-apiserver</code> 或 <code>kube-controller-manager</code> 是以 <code>docker run</code> 容器的方式运行，可以通过 <code>docker logs kube-apiserver --tail 100 -f</code> 或者 <code>docker logs kube-controller-manager --tail 100 -f</code> 来查看容器日志，从而进一步判断问题原因。</p><h3 id="failed-to-bring-up-worker-plane"><a href="#failed-to-bring-up-worker-plane" class="headerlink" title="failed to bring up worker plane"></a>failed to bring up worker plane</h3><p>这是通用的错误提示，出现这个错误一般是 <code>kubelet</code> 服务没有正常运行，导致无法通过 <strong>rke</strong> 的健康检查。</p><p>在 <strong>rke1</strong> 中，<strong>kubelet</strong> 是以 <code>docker run</code> 容器的方式运行，可以通过 <code>docker logs kubelet --tail 100 -f</code> 来查看容器日志，从而进一步判断问题原因。</p><h3 id="Runtime-network-not-ready-NetworkReady-x3D-false"><a href="#Runtime-network-not-ready-NetworkReady-x3D-false" class="headerlink" title="Runtime network not ready: NetworkReady&#x3D;false"></a>Runtime network not ready: NetworkReady&#x3D;false</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Runtime network not ready: NetworkReady=<span class="literal">false</span> reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</span><br></pre></td></tr></table></figure><p>rancher kubernetes 网络驱动均以容器方式运行，如果部署环境网络缓慢，则会导致需要很长时间下载镜像，从而影响驱动的部署。因为系统超时，所以会提示以上错误信息。一般等待 5 到 10 分钟即可自动恢复正常。</p><p>如果等了很长还未能恢复正常，那么这应该是驱动运行异常导致。可以通过以下命令删除 网络驱动 Pod 使其重新创建：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | grep -E <span class="string">&#x27;canal|calico|flannel&#x27;</span> | xargs kubectl -n kube-system delete pod</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RKE 常见问题集锦</title>
      <link href="/rke/rke-problem-sets/"/>
      <url>/rke/rke-problem-sets/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rke/rke-problem-sets/" target="_blank" title="https://www.xtplayer.cn/rke/rke-problem-sets/">https://www.xtplayer.cn/rke/rke-problem-sets/</a></p><h3 id="如何添加额外的-arguments-x2F-binds-x2F-environment？"><a href="#如何添加额外的-arguments-x2F-binds-x2F-environment？" class="headerlink" title="如何添加额外的 arguments&#x2F;binds&#x2F;environment？"></a>如何添加额外的 arguments&#x2F;binds&#x2F;environment？</h3><h3 id="无法为主机创建-SSH-隧道"><a href="#无法为主机创建-SSH-隧道" class="headerlink" title="无法为主机创建 SSH 隧道"></a>无法为主机创建 SSH 隧道</h3><ul><li><p>Failed to set up SSH tunneling for host [xxx.xxx.xxx.xxx]: Can’t retrieve Docker Info ，Failed to dial to &#x2F;var&#x2F;run&#x2F;docker.sock: ssh: rejected: administratively prohibited (open failed)</p><p>当使用 <code>RedHat/CentOS</code> 作为操作系统时, 不能使用<code>root</code>用户去登录主机，具体原因可以查看<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1527565">Bugzilla #1527565</a>。需要添加一个非 root 用户并添加访问 <strong>docker.sock</strong> 的权限，配置方法请参考 <a href="https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user">通过非 root 用户管理 Docker</a>。</p><p>如上的报错，指定连接的用户没有权限访问 <strong>docker.sock</strong>，可以通过登录主机并运行 <code>docker ps</code> 命令来检查 。如果 <code>docker ps</code> 命令无法执行，则需要执行命令 <code>sudo usermod -aG docker $USER;</code> 将当前用户添加到 <code>docker</code> 组。</p></li><li><p>Failed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: no key found</p><p>在 RKE的<code>node</code>配置参数中，<code>ssh_key_path</code>参数需要指定访问<code>node</code>节点的私钥文件。此问题可能是因为没有正确指定<code>ssh_key_path</code>文件路径或者没有权限访问该文件，或者指定的文件非正确的私钥文件。</p></li><li><p>Failed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain</p><p>指定的<code>ssh_key_path</code>文件对应的<code>node</code>主机不正确，或者对应的用户名不正确。</p></li><li><p>Failed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: cannot decode encrypted private keys.</p><p>如果要使用加密的私钥，则应使用<code>ssh-agent</code>密码来加载密钥。如果在运行 <strong>rke</strong> 命令的环境中找到 <code>SSH_AUTH_SOCK</code> 环境变量，那么它将自动用于连接到节点，否则需要在 rke 配置文件中设置 <code>ssh_agent_auth: false</code>。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> rke </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rke </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher 常见问题集锦</title>
      <link href="/rancher/rancher-problem-sets/"/>
      <url>/rancher/rancher-problem-sets/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/rancher-problem-sets/" target="_blank" title="https://www.xtplayer.cn/rancher/rancher-problem-sets/">https://www.xtplayer.cn/rancher/rancher-problem-sets/</a></p><h3 id="Rancher-server-管理员密码重置"><a href="#Rancher-server-管理员密码重置" class="headerlink" title="Rancher server 管理员密码重置"></a>Rancher server 管理员密码重置</h3><p>请查参考链接：<a href="/rancher/password-reset/">Rancher server 管理员密码重置</a></p><h3 id="Rancher-server-如何数据持久？"><a href="#Rancher-server-如何数据持久？" class="headerlink" title="Rancher server 如何数据持久？"></a>Rancher server 如何数据持久？</h3><p>Rancher server 不管是单节点单容器安装，或者是多节点 Local kubernetes HA 安装，Rancher server 都是通过 kubernetes API 去写入数据，最后数据都是保存在 kubernetes 后端的数据存储服务中（etcd）。</p><p>在 v2.3.x 之前的 Rancher server 版本中，Rancher server 容器中运行了一个定制化的微型 kubernetes 集群。在 v2.3.x 及之后的 Rancher server 版本中，定制化的微型 kubernetes 集群更换成了 K3S 集群。</p><ul><li>Rancher server 单节点单容器安装</li></ul><p>对于单节点单容器安装的 Rancher server，容器中的定制化微型 kubernetes 集群或者 K3S 集群会一并启动，然后 Rancher server 会去访问微型 kubernetes 集群或者 K3S 集群的 API 地址进行数据的读写，默认数据文件保存在容器的 <code>/var/lib/Rancher</code> 路径下。可以将此目录挂载到主机主机的某个位置，以保证数据不会丢失。</p><ul><li>Rancher server HA 安装</li></ul><p>Local kubernetes 集群 Rancher server HA 安装，这种安装模式下 Rancher server 也是通过 Local kubernetes 集群的 API 地址进行数据读写，通过 Local kubernetes，数据被直接写入到了 kubernetes 后端的 ETCD 服务中。</p><h3 id="Rancher-server-单节点单容器安装场景下，如何在同一个主机上运行-Rancher-server-和-Rancher-Agent？"><a href="#Rancher-server-单节点单容器安装场景下，如何在同一个主机上运行-Rancher-server-和-Rancher-Agent？" class="headerlink" title="Rancher server 单节点单容器安装场景下，如何在同一个主机上运行 Rancher server 和 Rancher-Agent？"></a>Rancher server 单节点单容器安装场景下，如何在同一个主机上运行 Rancher server 和 Rancher-Agent？</h3><p>默认情况下，Rancher 中创建的 kubernetes 集群，会在所有可调度的节点上安装 ingress 控制器服务。ingress 控制器服务默认是 <code>host</code> 网络模式，并且 ingress 控制器服务默认监听了 <code>80</code> 和 <code>443</code> 端口。</p><p>因此，如果 Rancher server 单节点单容器安装时是通过 <code>-p 80:80 -p 443:443</code> 映射的容器端口，那么如果把 ranche server 所在的主机添加到 kubernetes 后将会出现端口冲突的问题。</p><p>针对这个问题，我们有两种方法处理：</p><ol><li><p>修改 Rancher server 容器映射的端口。</p><p> <strong>注意：</strong>如果集群已经创建好，此时修改映射端口将会导致 agent 服务无法访问 Rancher server。如果必须要修改映射端口，请参考文章进行处理：<a href="https://www.xtplayer.cn/rancher/replace-ip-domain/">https://www.xtplayer.cn/rancher/replace-ip-domain/</a></p></li><li><p>修改 ingress 控制器的默认监听端口</p><p> 修改 ingress 控制器工作负载的 YAML 文件，在 <code>args</code> 参数中插入 <code>--http-port=8880 --https-port=8443</code>，或者在 Rancher ui 上修改，具体参考文档：<a href="https://www.xtplayer.cn/kubernetes/ingress-configuration-demo/#%E8%87%AA%E5%AE%9A%E4%B9%89-http-%E5%92%8C-https-%E7%AB%AF%E5%8F%A3">自定义 http 和 https 端口</a> 。</p><ul><li><p>ingress 控制器中是用 Nginx 来提供负载均衡服务，Nginx 默认监听 <code>80</code> 和 <code>443</code> 分别对应 <code>http</code> 和 <code>https</code>，通过域名访问服务时候可以省略端口号，比如：<code>http://wwww.test.com/a</code> 或者 <code>https://wwww.test.com/b</code>。</p></li><li><p>更换端口后，访问则需要添加上具体的端口，比如：<code>http://wwww.test.com:8880/a</code> 或者 <code>https://wwww.test.com:8443/b</code></p></li></ul></li></ol><h3 id="删除或者停用了管理员，该如何恢复？"><a href="#删除或者停用了管理员，该如何恢复？" class="headerlink" title="删除或者停用了管理员，该如何恢复？"></a>删除或者停用了管理员，该如何恢复？</h3><ol><li><p>单节点安装</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -ti &lt;container_id&gt; ensure-default-admin</span><br><span class="line">New default admin user (user-xxxxx)</span><br><span class="line">New password <span class="keyword">for</span> default admin user (user-xxxxx):</span><br><span class="line">&lt;new_password&gt;</span><br></pre></td></tr></table></figure></li><li><p>HA 安装</p><p>HA 安装恢复方法与单节点安装方法相同，只是在 HA 安装架构下，一般是有多个 Rancher server Pod，所以需要先把 Rancher server Pod 数量缩减为 1，然后再按单节点安装恢复方法处理即可。</p></li></ol><h3 id="怎么样开启-Debug-模式？"><a href="#怎么样开启-Debug-模式？" class="headerlink" title="怎么样开启 Debug 模式？"></a>怎么样开启 Debug 模式？</h3><ul><li><p>单节点安装</p><ol><li><p>启用</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -ti &lt;container_id&gt; loglevel --<span class="built_in">set</span> debug</span><br><span class="line">OK</span><br><span class="line">docker logs -f &lt;container_id&gt;</span><br></pre></td></tr></table></figure></li><li><p>禁用</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -ti &lt;container_id&gt; loglevel --<span class="built_in">set</span> info</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li></ol></li><li><p>HA 安装</p><p>HA 安装启用 Debug 与 单节点安装启用 Debug 模式相同，只需在要启用的 Pod 中运行命令。</p></li></ul><h3 id="ClusterIP-无法-ping-通？"><a href="#ClusterIP-无法-ping-通？" class="headerlink" title="ClusterIP 无法 ping 通？"></a>ClusterIP 无法 ping 通？</h3><ul><li><p>在 <strong>kube-proxy</strong> 使用 <strong>iptables</strong> 的 kubernetes 集群中，ClusterIP 是一个虚拟 IP，不会响应 ping，它仅作为 iptables 转发的目标规则。测试 ClusterIP 配置是否正确的最好方法是使用<code>curl</code>来访问 Pod IP 和端口以查看它是否响应。</p></li><li><p>在 <strong>kube-proxy</strong> 使用 <strong>ipvs</strong> 的 kubernetes 集群中，ClusterIP 可以 ping 通。</p></li></ul><h3 id="为什么-L4-层负载均衡-svc-处于“挂起”状态？"><a href="#为什么-L4-层负载均衡-svc-处于“挂起”状态？" class="headerlink" title="为什么 L4 层负载均衡 svc 处于“挂起”状态？"></a>为什么 L4 层负载均衡 svc 处于“挂起”状态？</h3><p>L4 层负载均衡器创建为 <code>type:LoadBalancer</code>，在 Kubernetes 中，这需要云提供商或控制器能够满足这些请求，否则这些将永远处于“挂起”状态。 了解更多 <a href="https://docs2.rancher.cn/rancher2x/concepts/clusters/cloud-providers/">云提供商</a> 或者 <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">Create External Load Balancer</a></p><h3 id="节点的-IP-地址发生了变化，该如何恢复？"><a href="#节点的-IP-地址发生了变化，该如何恢复？" class="headerlink" title="节点的 IP 地址发生了变化，该如何恢复？"></a>节点的 IP 地址发生了变化，该如何恢复？</h3><p>节点需要配置静态 IP（或通过 DHCP 保留 IP）。如果节点的 IP 已更改，则必须将其从集群中删除。删除后，Rancher 会将集群更新为正确的状态。如果集群不再处于 <code>Provisioning</code> 状态，则已经从集群中删除该节点。当节点的 IP 地址发生变化时，Rancher 失去了与节点的连接，因此无法正常清理节点。请参阅 <a href="https://www.xtplayer.cn/rancher/node-init/">清理集群节点</a> 以清除节点。</p><p>从集群中删除节点并清除节点后，您可以将节点重新添加到集群。</p><h3 id="如何查询-ssl-证书与域名或者-IP-的对应关系"><a href="#如何查询-ssl-证书与域名或者-IP-的对应关系" class="headerlink" title="如何查询 ssl 证书与域名或者 IP 的对应关系?"></a>如何查询 ssl 证书与域名或者 IP 的对应关系?</h3><p>通过以下命令可以查看到 ssl 证书绑定的域名或者 ip。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl x509 -noout -<span class="keyword">in</span> cert.pem -text | grep DNS</span><br></pre></td></tr></table></figure><h3 id="404-default-backend"><a href="#404-default-backend" class="headerlink" title="404 - default backend"></a>404 - default backend</h3><ul><li>在通过 helm 安装 Rancher server 后，可能通过域名访问会提示 <code>404 - default backend</code>。出现此问题后，请检查安装 Rancher server 时指定的域名与创建 ssl 证书时绑定的域名时候相同。</li><li>检查创建的 ssl 证书密文对应的证书是否正确。</li><li>如果是权威 CA 机构颁发的授信 SSL 证书，请检查中间证书是否与 cert 证书放在一起，参考文档：<a href="https://www.xtplayer.cn/rancher/install/ha-authority-ssl-ingress/#%E5%88%9B%E5%BB%BA-k8s-ssl-%E5%AF%86%E6%96%87%E3%80%82">https://www.xtplayer.cn/rancher/install/ha-authority-ssl-ingress/#%E5%88%9B%E5%BB%BA-k8s-ssl-%E5%AF%86%E6%96%87。</a></li></ul><h3 id="导入集群-Rancher-ui-无法查看到原有集群中的-Pod"><a href="#导入集群-Rancher-ui-无法查看到原有集群中的-Pod" class="headerlink" title="导入集群 Rancher ui 无法查看到原有集群中的 Pod"></a>导入集群 Rancher ui 无法查看到原有集群中的 Pod</h3><p>Rancher UI 显示应用 Pod，是通过<strong>项目与命名空间</strong>绑定来进行过滤显示的。所以，如果命名空间不在某个项目下，则 Rancher UI 不会显示对应的应用 Pod。</p><p>项目与命名空间的关联，是通过命名空间上的<strong>注释</strong>来实现的。如果命名空间不是通过 Rancher UI 创建的，那么创建的命名空间不会带有对应的<strong>注释</strong>。比如导入的集群，原有的命名空间是通过 cli 命令行或者其他工具创建的，即使导入 Rancher 也不会自动创建对应的 <strong>注释</strong>。</p><p>为了可以正常显示应用 Pod，需要通过 Rancher UI 把命名空间移动到对应的项目下，移动后则会自动为命名空间创建对应的注释。</p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> NetworkReady </tag>
            
            <tag> runtime network not ready </tag>
            
            <tag> rancher password reset </tag>
            
            <tag> rancher debug </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cgroup 常见问题集锦</title>
      <link href="/linux/cgroup/cgroup-problem-sets/"/>
      <url>/linux/cgroup/cgroup-problem-sets/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/cgroup/cgroup-problem-sets/" target="_blank" title="https://www.xtplayer.cn/linux/cgroup/cgroup-problem-sets/">https://www.xtplayer.cn/linux/cgroup/cgroup-problem-sets/</a></p><h3 id="applying-cgroup-configuration-for-process-caused-“No-such-device-or-address””-unknown"><a href="#applying-cgroup-configuration-for-process-caused-“No-such-device-or-address””-unknown" class="headerlink" title="applying cgroup configuration for process caused “No such device or address””: unknown"></a>applying cgroup configuration for process caused “No such device or address””: unknown</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error: failed to start container <span class="string">&quot;default-http-backend&quot;</span>: Error response from daemon: OCI runtime create failed: container_linux.go:265: starting container process caused <span class="string">&quot;process_linux.go:264: applying cgroup configuration for process caused \&quot;No such device or address\&quot;&quot;</span>: unknown</span><br><span class="line">Error: failed to start container <span class="string">&quot;kubedns&quot;</span>: Error response from daemon: OCI runtime create failed: container_linux.go:265: starting container process caused <span class="string">&quot;process_linux.go:264: applying cgroup configuration for process caused \&quot;No such device or address\&quot;&quot;</span>: unknown</span><br></pre></td></tr></table></figure><p>如果出现以上错误信息，并且你使用的是 RKE 1 版本创建 kubernetes 集群。那么请检查 <code>cgroup driver</code> 配置，确保 <code>cgroup driver</code> 为默认配置，即 <code>cgroupdriver=cgroupfs</code>，docker 和 kubelet 需要保持相同配置。docker 和 kubelet 默认为 <code>cgroupfs</code>，所以不建议对 <code>cgroup driver</code> 做修改。</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> cgroup </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux Netfilter 调优</title>
      <link href="/linux/netfilter/linux-netfilter-optimization/"/>
      <url>/linux/netfilter/linux-netfilter-optimization/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/netfilter/linux-netfilter-optimization/" target="_blank" title="https://www.xtplayer.cn/linux/netfilter/linux-netfilter-optimization/">https://www.xtplayer.cn/linux/netfilter/linux-netfilter-optimization/</a></p><p>如果您正在为高流量的 <strong>Web&#x2F;DNS</strong> 服务器提供服务，并且最近使该服务器 <strong>PING</strong> 丢失并且并非所有 <strong>HTTP</strong> 请求都成功。您可以开始检查系统日志。并且如果您看到类似下面的内容，那么下面的指南将帮助您调整 Linux 服务器以正确处理流量负载。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Mar 22 21:25:55 localhost kernel: nf_conntrack: table full, dropping packet.</span><br><span class="line">Mar 22 21:26:00 localhost kernel: printk: 11 messages suppressed.</span><br><span class="line">Mar 22 21:26:00 localhost kernel: nf_conntrack: table full, dropping packet.</span><br><span class="line">Mar 22 21:26:05 localhost kernel: printk: 16 messages suppressed.</span><br></pre></td></tr></table></figure><h2 id="状态查看"><a href="#状态查看" class="headerlink" title="状态查看"></a>状态查看</h2><ul><li><p>buckets 哈希表大小，max 最大记录的连接条数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dmesg | grep conntrack</span><br><span class="line"></span><br><span class="line">[    8.782060] nf_conntrack version 0.5.0 (16384 buckets, 65536 max)</span><br></pre></td></tr></table></figure></li><li><p>哈希表使用情况</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grep conntrack /proc/slabinfo</span><br><span class="line"></span><br><span class="line">nf_conntrack_1       102    102    320   51    4 : tunables    0    0    0 : slabdata      2      2      0</span><br></pre></td></tr></table></figure></li><li><p>当前跟踪的连接数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo sysctl net.netfilter.nf_conntrack_count</span><br></pre></td></tr></table></figure></li><li><p>跟踪连接详细信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># centos</span></span><br><span class="line"><span class="built_in">cat</span> /proc/net/nf_conntrack</span><br><span class="line"><span class="comment"># ubuntu，可能需要安装 conntrack 工具，yum install -y conntrack 或者 apt-getinstall -y conntrack</span></span><br><span class="line">conntrack -L</span><br></pre></td></tr></table></figure></li></ul><h2 id="最大连接跟踪数"><a href="#最大连接跟踪数" class="headerlink" title="最大连接跟踪数"></a>最大连接跟踪数</h2><p>为了完成任务，NAT-server（一般指的是 iptables） 需要记录所有通过它的连接。无论是 “ping” 还是某人的 “ICQ”，NAT-server 都会记录在一个特殊的表中并跟踪所有会话。当会话关闭时，相关记录将从连接跟踪表中删除。这个记录表的大小是固定的，所以如果通过服务器的流量很大，但表太小，那么 NAT-server 就会开始丢弃数据包，中断会话。为了避免这样的麻烦，有必要适当增加连接跟踪表的大小。</p><ul><li><p>最大连接跟踪数默认为 <strong>nf_conntrack_buckets * 4</strong>，可以通过以下命令查看当前值：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sysctl net.netfilter.nf_conntrack_buckets</span><br><span class="line">sysctl net.netfilter.nf_conntrack_max</span><br></pre></td></tr></table></figure></li><li><p><strong>CONNTRACK_MAX</strong> 默认计算公式</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CONNTRACK_MAX = 内存个数*1024*1024*1024/16384/(ARCH/32)</span><br></pre></td></tr></table></figure><ul><li>其中 <code>ARCH</code> 为 CPU 架构，值为 32 或 64。</li><li>比如：64 位 8G 内存的机器：<code>(8*1024^3)/16384/(64/32) = 262144</code></li></ul></li></ul><h3 id="临时调整"><a href="#临时调整" class="headerlink" title="临时调整"></a>临时调整</h3><p>  临时调整是临时性的，重启节点好配置值将会丢失。</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sysctl -w net.netfilter.nf_conntrack_max=1048576</span><br><span class="line">sysctl -w net.nf_conntrack_max=1048576</span><br></pre></td></tr></table></figure><h3 id="永久调整"><a href="#永久调整" class="headerlink" title="永久调整"></a>永久调整</h3><p>  要使其配置在重新启动后永久存在，需要将这些值添加到 <strong>sysctl.conf</strong> 中</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;net.netfilter.nf_conntrack_max&#x27;</span> = 1048576 &gt;&gt; /etc/sysctl.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;net.nf_conntrack_max = 1048576&#x27;</span> &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl  -p</span><br></pre></td></tr></table></figure><blockquote><p>如果服务器中的 RAM 少于 1 GB，建议不要设置太大的值。</p></blockquote><h2 id="哈希表-hash-table"><a href="#哈希表-hash-table" class="headerlink" title="哈希表(hash-table)"></a>哈希表(hash-table)</h2><p>哈希表大小是只读的，不能在 <strong>&#x2F;etc&#x2F;sysctl.conf</strong> 文件中设置值。64 位 Linux 系统中，4G 内存默认 16384，8G 内存默认 65536，16G 翻倍，以此类推。</p><h3 id="给哈希表扩容的影响"><a href="#给哈希表扩容的影响" class="headerlink" title="给哈希表扩容的影响"></a>给哈希表扩容的影响</h3><p>主要是内存使用增加，32 位系统还要关心内核态的地址空间够不够。</p><p>netfilter 的哈希表存储在内核态的内存空间，这部分内存不能 swap，操作系统为了兼容 32 位，默认值往往比较保守。</p><ul><li><p>32 位系统的虚拟地址空间最多 4G，其中内核态最多 1G，通常能用的只有前 896M。</p><p>给 netfilter 分配太多地址空间可能会导致其他内核进程不够分配。1 条跟踪记录 300 字节左右，因此当年 <code>nf_conntrack_max</code> 默认 65535 条，占 20 多 MB。</p></li><li><p>64 位系统的虚拟地址空间有 256 TB，内核态能用一半，只需要关心物理内存的使用情况。</p></li><li><p>计算内存使用的公式</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">size_of_mem_used_by_conntrack (<span class="keyword">in</span> bytes) = CONNTRACK_MAX * sizeof(struct ip_conntrack) + HASHSIZE * sizeof(struct list_head)</span><br></pre></td></tr></table></figure><ul><li><p><code>sizeof(struct ip_conntrack)</code> 在不同架构、内核版本、编译选项下不一样。这里按 352 字节算。</p></li><li><p><code>sizeof(struct list_head) = 2 * size_of_a_pointer</code>（32 位系统的指针大小是 4 字节，64 位是 8 字节）</p></li><li><p>64 位系统，8G 内存的机器，按默认 <code>CONNTRACK_MAX</code> 为 262144，<code>HASHSIZE</code> 为 65536 时：<code>262144 * 352 + 65536 * 8 = 92798976</code>（88.5 MB）</p></li></ul></li><li><p>互联网公司的服务器通常内存没那么紧张，可以放开点：</p><ul><li><code>CONNTRACK_MAX</code> 为 1048576，<code>HASHSIZE</code> 为 262144 ，内存大概使用：<code>1048576 * 352 + 262144 * 8 = 371195904</code>（354 MB）</li></ul></li></ul><h3 id="哈希表大小调整"><a href="#哈希表大小调整" class="headerlink" title="哈希表大小调整"></a>哈希表大小调整</h3><p>需要通过内核模块的方式修改：</p><ul><li><p>临时生效：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 262144 &gt; /sys/module/nf_conntrack/parameters/hashsize</span><br></pre></td></tr></table></figure></li><li><p>永久生效</p><p>将以下内容添加到文件：<code>/etc/modprobe.d/iptables.conf</code>（如果没有则新建）</p><p><code>echo &#39;options nf_conntrack hashsize=262144&#39; &gt;&gt; /etc/modprobe.d/iptables.conf</code></p></li></ul><h2 id="减少超时时间"><a href="#减少超时时间" class="headerlink" title="减少超时时间"></a>减少超时时间</h2><p>NAT-server 只跟踪通过它的 <strong>活动</strong> 的会话。如果一个会话很长时间是空闲的，不活跃，它将会因为超值而被关闭。当会话关闭时，关于它的信息将被删除，以便连接跟踪表不会溢出。</p><p>但是，如果超时的默认值很大，流量较大时候，即使将 <strong>nf_conntrack_max</strong> 扩展到了极限，跟踪表仍然有溢出的风险。为此，必须在 NAT-server 上正确设置连接跟踪超时。</p><p>可以执行以下命令查看默认值：</p><p><strong>sysctl -a | grep conntrack | grep timeout</strong></p><ul><li><p>Ubuntu 16.04</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">net.netfilter.nf_conntrack_generic_timeout = 600</span><br><span class="line">net.netfilter.nf_conntrack_icmp_timeout = 30</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_close = 10</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_established = 432000</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_last_ack = 30</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300</span><br><span class="line">net.netfilter.nf_conntrack_udp_timeout = 30</span><br><span class="line">net.netfilter.nf_conntrack_udp_timeout_stream = 180</span><br></pre></td></tr></table></figure></li><li><p>centos 7.8</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">net.netfilter.nf_conntrack_dccp_timeout_closereq = 64</span><br><span class="line">net.netfilter.nf_conntrack_dccp_timeout_closing = 64</span><br><span class="line">net.netfilter.nf_conntrack_dccp_timeout_open = 43200</span><br><span class="line">net.netfilter.nf_conntrack_dccp_timeout_partopen = 480</span><br><span class="line">net.netfilter.nf_conntrack_dccp_timeout_request = 240</span><br><span class="line">net.netfilter.nf_conntrack_dccp_timeout_respond = 480</span><br><span class="line">net.netfilter.nf_conntrack_dccp_timeout_timewait = 240</span><br><span class="line">net.netfilter.nf_conntrack_events_retry_timeout = 15</span><br><span class="line">net.netfilter.nf_conntrack_generic_timeout = 600</span><br><span class="line">net.netfilter.nf_conntrack_icmp_timeout = 30</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_closed = 10</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_cookie_echoed = 3</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_cookie_wait = 3</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_established = 432000</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_heartbeat_acked = 210</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_heartbeat_sent = 30</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_shutdown_ack_sent = 3</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_shutdown_recd = 0</span><br><span class="line">net.netfilter.nf_conntrack_sctp_timeout_shutdown_sent = 0</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_close = 10</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_established = 86400</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_last_ack = 30</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300</span><br><span class="line">net.netfilter.nf_conntrack_udp_timeout = 30</span><br><span class="line">net.netfilter.nf_conntrack_udp_timeout_stream = 180</span><br></pre></td></tr></table></figure></li></ul><p>以上均是以秒为单位的超时值。</p><p>对于通外网的服务器，考虑调整以下参数，减少 DDoS 的危害：</p><ul><li><p>net.netfilter.nf_conntrack_tcp_timeout_established：默认 432000（5 天）</p><ul><li>这个值对应的场景是 “双方建立了连接后一直不发包，直到 5 天后才发”</li><li>但默认 keep-alive 超时时间只有 2 小时 11 分（<code>net.ipv4.tcp_keepalive_time + net.ipv4.tcp_keepalive_intvl * net.ipv4.tcp_keepalive_probes</code>），由于超时关 socket 不发包，conntrack 无法根据包头的标识知道状态的变化，记录会一直处于 ESTABLISHED 状态，直到 5 天后倒计时结束才删掉。</li><li>空连接攻击的最佳目标。攻击者把 IP 包头的源地址改成随机 IP，握完手就关 socket，用一台机发请求就能把你的哈希表填满。</li></ul></li><li><p>net.netfilter.nf_conntrack_tcp_timeout_syn_recv：默认 60</p><ul><li>类似，故意不发握手的 ACK 即可。但这个超时时间没那么夸张，系统也有 syn cookie 机制来缓解 syn flood 攻击。</li></ul></li></ul><p>其他值得注意的参数：</p><ul><li><p>net.netfilter.nf_conntrack_tcp_timeout_syn_sent：默认 120</p><ul><li>你的程序的 connect timeout 有这么长吗？</li></ul></li><li><p>net.netfilter.nf_conntrack_tcp_timeout_fin_wait：默认 120</p><ul><li><code>net.ipv4.tcp_fin_timeout</code> 默认 60 秒，通常还会参考 BSD 和 macOS 设成更小的值。这里往往也没必要这么大。</li></ul></li><li><p>net.netfilter.nf_conntrack_icmp_timeout：默认 30</p><ul><li>哪里的 ping 会等 30 秒才超时？</li></ul></li></ul><p>这几个倒是比较合理，小于等于可能遇到的极端情况，但如果不想半关闭的连接的记录继续占着宝贵的哈希表，提早清了似乎也没什么问题：</p><ul><li><p>net.netfilter.nf_conntrack_tcp_timeout_time_wait：默认 120</p><ul><li>Linux 里的 MSL 写死 60 秒（而不是 TCP 标准里拍脑袋的 120 秒），TIME_WAIT 要等 2MSL，这里 120 算是个合理的值。</li><li>但现在默认有 PAWS（<code>net.ipv4.tcp_timestamps</code>），不会出现标准制定时担心的迷途报文回来碰巧污染了序列号相同的新连接的数据的情况。互联网公司基本都开 <code>net.ipv4.tcp_tw_reuse</code>，既然半连接都不留这么久，记录似乎也不需要留这么久。</li></ul></li><li><p>net.netfilter.nf_conntrack_tcp_timeout_close_wait：默认 60</p><ul><li>CLOSE_WAIT 状态是让被动关闭方把该传的数据传完。如果程序写得不好，这里抛了未捕捉的异常，也许就走不到发 FIN 那步了，一直停在这里。</li></ul></li><li><p>net.netfilter.nf_conntrack_tcp_timeout_last_ack：默认 30</p><ul><li>被动关闭方发 FIN 后如果一直收不到对面的 ACK 或 RST，会不断重发，直到超时才 CLOSE。<code>net.ipv4.tcp_retries2</code> 的默认值是 15，最多要等 924.6 秒……不过一般都会调小这个值。</li></ul></li></ul><h3 id="调整参数"><a href="#调整参数" class="headerlink" title="调整参数"></a>调整参数</h3><p>添加以下配置参数到 <code>/etc/sysctl.conf</code> 文件，最后执行 <code>sysctl -p</code>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">net.netfilter.nf_conntrack_icmp_timeout=10</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_syn_recv=5</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_syn_sent=5</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_established=600</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_fin_wait=10</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_time_wait=10</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_close_wait=10</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_last_ack=10</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><code>https://testerhome.com/topics/15824</code><br><code>https://www.cnblogs.com/xiangsikai/p/9525287.html</code></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> netfilter </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Netfilter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>将博客搬至 CSDN</title>
      <link href="/csdn/blog-sync-to-csdn/"/>
      <url>/csdn/blog-sync-to-csdn/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/csdn/blog-sync-to-csdn/" target="_blank" title="https://www.xtplayer.cn/csdn/blog-sync-to-csdn/">https://www.xtplayer.cn/csdn/blog-sync-to-csdn/</a></p><p>我的博客即将同步至 CSDN 社区，这是我的 CSDN 主页：<a href="https://blog.csdn.net/hongxiaolu">https://blog.csdn.net/hongxiaolu</a></p>]]></content>
      
      
      <categories>
          
          <category> csdn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSDN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4 步将 Rancher2.5.x 迁移至任意 K8S 发行版</title>
      <link href="/rancher/rancher2-x-migration/"/>
      <url>/rancher/rancher2-x-migration/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/rancher2-x-migration/" target="_blank" title="https://www.xtplayer.cn/rancher/rancher2-x-migration/">https://www.xtplayer.cn/rancher/rancher2-x-migration/</a></p><p>Rancher v2.5 之前的版本是不支持将 Rancher 迁移到其他集群的，但可以利用一些“黑科技”实现将 Rancher 迁移到新的集群，在之前发布的文章「<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247494021&idx=1&sn=0d7e0d106e54336046a2cc2b32d740a6&chksm=e8396f43df4ee6550b5e6c77c9adabf357db068deb59bdd12dc67f5d1df4b59c8acf184a3a4b&scene=21#wechat_redirect">多场景解析如何迁移 Rancher Server</a>」有过介绍。</p><p>从 Rancher v2.5 开始，可以使用 rancher-backup operator 来备份和恢复 Rancher，rancher-backup 工作时不需要暴露 etcd，因为 operator 通过调用 kube-apiserver 来收集资源。我们可以利用此特性将 Rancher 迁移到任何标准 Kubernetes 发行版的集群中。</p><p><strong>迁移过程概要：</strong></p><ol><li>创建 Rancher 备份，并将备份上传到备份存储位置</li><li>创建 Rancher local 集群</li><li>使用 Restore 自定义资源从备份中还原到 local 集群</li><li>使用 Helm 安装 Rancher</li></ol><p><strong>迁移先决条件:</strong></p><ul><li>Rancher 版本必须是 <strong>v2.5.0</strong> 及以上。</li><li>如果你要将 Rancher 迁移到一个新的 Kubernetes 集群，你不需要先在新集群上安装 Rancher。如果将 Rancher 还原到一个已经安装了 Rancher 的新集群上，可能会引起问题。</li><li>要求使用与第一个集群中设置的服务器 URL 相同的 hostname，本例为：<code>rancher.kingsd.top</code>。</li></ul><p><strong>备份存储位置:</strong></p><p>Rancher v2.5 备份支持将备份文件推送到兼容 S3 的对象存储（比如：MinIO 或阿里云 OSS），也可以存储在一个 Persistent Volumes 中。考虑到一些用户是离线环境，而且跨集群使用 Persistent Volumes 的方式比较麻烦，所以本文采用将备份推送到 MinIO 的方式。</p><p>MinIO 安装参考 MinIO 官网(<a href="https://docs.min.io/">https://docs.min.io/</a>)即可。本文已安装的 MinIO 地址为：<code>https://rancher.kingsd.top</code>。</p><h2 id="安装单节点-Rancher"><a href="#安装单节点-Rancher" class="headerlink" title="安装单节点 Rancher"></a>安装单节点 Rancher</h2><p>为了更好的展现迁移效果，本文将演示 “单节点” 迁移到 “高可用” 的场景，当然，也支持 “高可用” 迁移到 “高可用” 的场景，步骤基本相同。</p><p>由于在上面 “迁移先决条件” 中提到的 “要求使用与第一个集群中设置的服务器 URL 相同的 hostname”，所以原集群不能是“<strong>使用 Rancher 默认的自签名证书</strong>”的方式安装，因为该方式将自动为 Rancher 签发证书。本文采用“<strong>使用已有的可信证书</strong>”方式安装 Rancher：</p><p><strong>Rancher 单节点安装指南：</strong></p><p><a href="https://docs.rancher.cn/docs/rancher2/installation_new/other-installation-methods/single-node-docker/_index">https://docs.rancher.cn/docs/rancher2/installation_new&#x2F;other-installation-methods&#x2F;single-node-docker&#x2F;_index</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run -d --privileged --restart=unless-stopped \</span><br><span class="line">    -p 80:80 -p 443:443 \</span><br><span class="line">    -v /opt/rancher.kingsd.top.pem:/etc/rancher/ssl/cert.pem \</span><br><span class="line">    -v /opt/rancher.kingsd.top.key:/etc/rancher/ssl/key.pem \</span><br><span class="line">    rancher/rancher:v2.5.5 \</span><br><span class="line">    --no-cacerts</span><br></pre></td></tr></table></figure><p>然后在 DNS 服务器上将域名 <code>rancher.kingsd.top</code> 映射到你的 Rancher 服务器的 IP，随后你就可以通过 <code>https://rancher.kingsd.top</code> 访问到你的单节点 Rancher UI 了。为了测试迁移之后依然可以管理下游业务集群，我们需要创建一个自定义集群，并创建几个测试 workload，以便迁移之后做验证。</p><img src="/rancher/rancher2-x-migration/640-20210203210548381" class="" title="图片"><img src="/rancher/rancher2-x-migration/640-20210203210548380" class="" title="图片"><h2 id="备份-Rancher"><a href="#备份-Rancher" class="headerlink" title="备份 Rancher"></a>备份 Rancher</h2><h3 id="创建-MinIO-Secret"><a href="#创建-MinIO-Secret" class="headerlink" title="创建 MinIO Secret"></a>创建 MinIO Secret</h3><p>将备份上传到 MinIO 需要设置 MinIO 的用户名和密码，在 Rancher 中是以 Secret 的形式存储的，所以需要在 local 集群中提前创建：</p><img src="/rancher/rancher2-x-migration/640-20210203210548304" class="" title="图片"><h3 id="安装-rancher-backup-operator"><a href="#安装-rancher-backup-operator" class="headerlink" title="安装 rancher-backup operator"></a>安装 rancher-backup operator</h3><ol><li>在 Rancher UI 的 Cluster Manager 中，选择名为 local 的集群</li><li>在右上角单击 Cluster Explorer</li><li>单击 Apps</li><li>单击 Rancher Backup operator</li><li>所有选项默认即可，直接点击 Install 创建 Rancher Backup operator</li></ol><p>  rancher-backup 和 rancher-backup-crd 状态为 Deployed，代表 rancher-backup operator 成功部署。</p>  <img src="/rancher/rancher2-x-migration/640-20210203210548321" class="" title="图片"><h3 id="创建备份"><a href="#创建备份" class="headerlink" title="创建备份"></a>创建备份</h3><ol><li><p>在 Cluster Explorer 中，进入左上角的下拉菜单，单击 Rancher Backups</p></li><li><p>选择 Backups，然后点击右侧 Create</p></li><li><p>输入 Backups 的配置参数：</p></li></ol>  <img src="/rancher/rancher2-x-migration/640-20210203210548323" class="" title="图片"><ol><li>备份创建成功后，备份状态为 Completed，备份文件名称为 rancher-backup-1-8f21c185-3caf-4a82-ab8c-8ba425a6667b-2021-01-19T07-12-30Z.tar.gz</li></ol>  <img src="/rancher/rancher2-x-migration/640-20210203210548337" class="" title="图片"><ol><li>MinIO 页面也会显示对应的备份文件：</li></ol>  <img src="/rancher/rancher2-x-migration/640-20210203210548336" class="" title="图片"><p>  至此，Rancher 的备份已经创建成功，并且将备份文件推送到了 MinIO。</p><h2 id="迁移-Rancher"><a href="#迁移-Rancher" class="headerlink" title="迁移 Rancher"></a>迁移 Rancher</h2><p>迁移 Rancher，其实就是利用在 MinIO 上的备份，将 Rancher 恢复到新的 Kubernetes 集群上，所以我们需要先创建一个 Kubernetes 集群做为 Rancher 的 local 集群，本例使用 K3s 作为 local 集群。</p><h3 id="创建-K3s-集群作为-Local-集群"><a href="#创建-K3s-集群作为-Local-集群" class="headerlink" title="创建 K3s 集群作为 Local 集群"></a>创建 K3s 集群作为 Local 集群</h3><p>由于在撰写文章使用的 Rancher 版本为 v2.5.5，此版本不支持 &gt;&#x3D;v1.20 版本的 Kubernetes 集群作为 local 集群，所以需要指定 K3s 版本为 v1.19.7+k3s1：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=<span class="string">&quot;v1.19.7+k3s1&quot;</span> sh -</span><br></pre></td></tr></table></figure><h3 id="安装-rancher-backup-Helm-chart"><a href="#安装-rancher-backup-Helm-chart" class="headerlink" title="安装 rancher-backup Helm chart"></a>安装 rancher-backup Helm chart</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">helm repo add rancher-charts https://charts.rancher.io</span><br><span class="line">helm repo update</span><br><span class="line">helm install rancher-backup-crd rancher-charts/rancher-backup-crd -n cattle-resources-system --create-namespace</span><br><span class="line">helm install rancher-backup rancher-charts/rancher-backup -n cattle-resources-system</span><br></pre></td></tr></table></figure><h3 id="使用-Restore-自定义资源从备份中还原"><a href="#使用-Restore-自定义资源从备份中还原" class="headerlink" title="使用 Restore 自定义资源从备份中还原"></a>使用 Restore 自定义资源从备份中还原</h3><p>本例使用兼容 S3 的对象存储 MinIO 作为备份源，并且需要使用你的 MinIO 凭证进行还原，所以需要在这个集群中创建一个 MinIO Secret。Secret 数据必须有两个 key，accessKey 和 secretKey，包含 MinIO 凭证，像这样：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">minio-creds</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">accessKey:</span> <span class="string">&lt;Enter</span> <span class="string">your</span> <span class="string">access</span> <span class="string">key&gt;</span></span><br><span class="line">  <span class="attr">secretKey:</span> <span class="string">&lt;Enter</span> <span class="string">your</span> <span class="string">secret</span> <span class="string">key&gt;</span></span><br></pre></td></tr></table></figure><p>这个 secret 可以在任何命名空间中创建，上面的例子中，它将在 default 的命名空间中创建。</p><p>在 Restore 自定义资源中，prune 必须设置为 false。创建一个像下面例子一样的 Restore 自定义资源：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># migrationResource.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">resources.cattle.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Restore</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">restore-migration</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">backupFilename:</span> <span class="string">rancher-backup-1-8f21c185-3caf-4a82-ab8c-8ba425a6667b-2021-01-19T07-12-30Z.tar.gz</span></span><br><span class="line">  <span class="attr">prune:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># encryptionConfigSecretName: encryptionconfig</span></span><br><span class="line">  <span class="attr">storageLocation:</span></span><br><span class="line">    <span class="attr">s3:</span></span><br><span class="line">      <span class="attr">credentialSecretName:</span> <span class="string">minio-creds</span></span><br><span class="line">      <span class="attr">credentialSecretNamespace:</span> <span class="string">default</span></span><br><span class="line">      <span class="attr">bucketName:</span> <span class="string">rancher-backup</span></span><br><span class="line">      <span class="comment"># folder: ecm1</span></span><br><span class="line">      <span class="comment"># region: us-west-2</span></span><br><span class="line">      <span class="attr">endpoint:</span> <span class="string">minio.kingsd.top</span></span><br></pre></td></tr></table></figure><p>查看 Restore 结果：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get restore</span><br><span class="line">NAME                BACKUP-SOURCE   BACKUP-FILE                                                                         AGE   STATUS</span><br><span class="line">restore-migration   S3              rancher-backup-1-8f21c185-3caf-4a82-ab8c-8ba425a6667b-2021-01-19T07-12-30Z.tar.gz   52s   Completed</span><br></pre></td></tr></table></figure><p>如果 Restore 有异常，可以通过 cattle-resources-system 命名空间下的 rancher-backup-xxx 查看日志。</p><h3 id="使用-Helm-安装-Rancher"><a href="#使用-Helm-安装-Rancher" class="headerlink" title="使用 Helm 安装 Rancher"></a>使用 Helm 安装 Rancher</h3><p>从 ResourceSets（<a href="https://github.com/rancher/backup-restore-operator/blob/master/crds/resourceset.yaml">https://github.com/rancher/backup-restore-operator/blob/master/crds/resourceset.yaml</a>） 中可以看到在备份和恢复过程中，并没有将 Rancher 的 Pod 备份和恢复，所以需要使用 Helm 安装与第一个集群相同版本的 Rancher。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果是从 HA 迁移到 HA，不需要重新创建此 secret</span></span><br><span class="line">kubectl -n cattle-system create secret tls tls-rancher-ingress \</span><br><span class="line">  --cert=/opt/rancher.kingsd.top.pem \</span><br><span class="line">  --key=/opt/rancher.kingsd.top.key</span><br><span class="line"></span><br><span class="line">helm repo add rancher-latest https://releases.rancher.com/server-charts/latest</span><br><span class="line">helm install rancher rancher-latest/rancher \</span><br><span class="line">  --namespace cattle-system \</span><br><span class="line">  --<span class="built_in">set</span> hostname=rancher.kingsd.top \</span><br><span class="line">  --<span class="built_in">set</span> ingress.tls.source=secret \</span><br><span class="line">  --<span class="built_in">set</span> rancherImageTag=v2.5.5</span><br></pre></td></tr></table></figure><p>此时，你需要在 DNS 服务器上将域名 <code>rancher.kingsd.top</code> 映射到新的 Rancher 服务器的 IP，本例为 K3s master 所在的服务器 IP，你也可以映射到 LB 的 IP。等待 DNS 配置生效，再次使用 <code>https://rancher.kingsd.top</code> 访问 Rancher，可以看到 local 集群已经替成刚才安装的 v1.19.7+k3s1 版本的 K3s，并且创建的测试 workload 正常工作，迁移成功。</p><img src="/rancher/rancher2-x-migration/640-20210203210548360" class="" title="图片"><img src="/rancher/rancher2-x-migration/640-20210203210548370" class="" title="图片"><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><p>Rancher v2.5 中的备份和恢复：</p><p><a href="https://docs.rancher.cn/docs/rancher2/backups/2.5/_index">https://docs.rancher.cn/docs/rancher2/backups/2.5/_index</a></p></li><li><p>将 Rancher 迁移到新集群：</p><p><a href="https://docs.rancher.cn/docs/rancher2/backups/2.5/migrating-rancher/_index">https://docs.rancher.cn/docs/rancher2/backups/2.5/migrating-rancher/_index</a></p></li><li><p>Rancher 高可用安装：</p><p><a href="https://docs.rancher.cn/docs/rancher2/installation_new/install-rancher-on-k8s/_index">https://docs.rancher.cn/docs/rancher2/installation_new/install-rancher-on-k8s/_index</a></p></li><li><p>K3s 安装：</p><p><a href="https://docs.rancher.cn/docs/k3s/quick-start/_index/">https://docs.rancher.cn/docs/k3s/quick-start/_index/</a></p></li><li><p>MinIO：</p><p><a href="https://docs.min.io/docs/">https://docs.min.io/docs/</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> 集群迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一通百通，一文实现灵活的 K8S 基础架构！</title>
      <link href="/kubernetes/k8s-infrastructure/"/>
      <url>/kubernetes/k8s-infrastructure/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/k8s-infrastructure/" target="_blank" title="https://www.xtplayer.cn/kubernetes/k8s-infrastructure/">https://www.xtplayer.cn/kubernetes/k8s-infrastructure/</a></p><p>Kubernetes 是当前最为流行的开源容器编排平台，成为众多企业构建基础架构的首选。在本文中，我们将探讨针对你的用例构建基础设施的最佳方式，以及你可能要根据各种限制条件做出的各种决定。</p><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><p>你的架构应该在很大程度上围绕你的用例来设计，因此在设计过程中你需要非常仔细以确保该基础架构能够支撑你的用例，在必要的时候也可以寻求外部专业团队的帮助。在架构设计的开始保证方向正确十分重要，但是这并不意味着不会发生错误，而且随着每天都有新的技术或研究横空出世，你可以看到变革已经成为常态，并且你的架构设计思维有可能过时。</p><p>这就是为什么我强烈建议你采用 Architect for Chang 的原则，让你的架构成为一个模块化的架构以便在未来有需要的时候你可以灵活地在内部进行改变。</p><p>让我们看看在考虑 client-server 模型的情况下如何实现系统架构的目标。</p><h2 id="切入点：DNS"><a href="#切入点：DNS" class="headerlink" title="切入点：DNS"></a>切入点：DNS</h2><p>在任何典型的基础架构中（无论是否是云原生架构），一个消息请求必须先由 DNS 服务器解析，并返回服务器的 IP 地址。设置你的 DNS 应该基于你所需要的可用性。如果你需要更高的可用性，你可能想要将你的服务器分布到多个区域或者云提供程序上，具体的实现要基于你想要达到的可用性等级。</p><h2 id="内容分发网络（CDN）"><a href="#内容分发网络（CDN）" class="headerlink" title="内容分发网络（CDN）"></a>内容分发网络（CDN）</h2><p>在某些情况下，你可能需要尽可能地以最小的延迟为用户提供服务，同时减少服务器的负载。这就是内容分发网络（CDN）发挥重要作用的地方。</p><p>Client 是否经常从服务器上请求一组静态资产？你是否希望提高向用户交付内容的速度，同时减少服务器的负载？在这种情况下，采用边缘的 CDN 为一组静态资产提供服务，实际上可能有助于降低用户的延迟和服务器的负载。</p><p>你所有的内容都是动态的吗？你是否可以在一定程度上为用户提供延迟的内容，以减少复杂性？或者你的应用程序接收很低的流量吗？在这种情况下，使用 CDN 可能没有太大的意义，你可以将所有的流量直接发送到全局负载均衡器。但要注意的是，拥有 CDN 也确实有分配流量的优势，这在你的服务器受到 DDOS 攻击时是很有帮助的。</p><p>CDN 提供程序包括 Cloudfare CDN、Fastly、Akamai CDN、Stackpath，此外你的云提供商也有可能会提供 CDN 服务，比如谷歌云平台的 Cloud CDN、AWS 的 CloudFront、微软 Azure 的 Azure CDN 等。</p><img src="/kubernetes/k8s-infrastructure/640-20210202213931292" class="" title="img"><h2 id="Load-Balancer"><a href="#Load-Balancer" class="headerlink" title="Load Balancer"></a>Load Balancer</h2><p>如果有一个请求不能被你的 CDN 服务，这个请求下一步会传送到你的负载均衡器上。而这些可以是区域性的 IP，也可以是全局性的 Anycast IP。在某些情况下，你也可以使用负载均衡器来管理内部流量。</p><p>除了路由和代理流量到合适的后端服务，负载均衡还能够承担 SSL 终止、与 CDN 集成，甚至管理网络流量的某些方面等职责。</p><p>虽然存在硬件负载均衡器，但软件负载均衡器提供了强大的灵活性、减少了成本开支以及弹性伸缩性。</p><p>与 CDN 类似，你的云提供程序应该也能够为你提供一个负载均衡器（如 GCP 的 GLB、AWS 的 ELB、Azure 的 ALB 等），但更有趣的是你可以直接从 Kubernetes 中调配这些负载均衡器。例如，在 GKE 中创建一个 Ingress 也会在后端为你创建一个 GLB 来接收流量，其他功能如 CDN、SSL 重定向等也可以通过配置你的 ingress 来设置，访问以下链接查看详情：</p><p><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features">https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features</a></p><p>虽然一开始总是从小开始，但是负载均衡器可以让你逐步扩展至具有以下规模的架构：</p><img src="/kubernetes/k8s-infrastructure/640-20210202213931394-2273171." class="" title="img"><h2 id="网络及安全架构"><a href="#网络及安全架构" class="headerlink" title="网络及安全架构"></a>网络及安全架构</h2><p>下一件需要关注的事情是网络。如果你想要提高安全性，你可能需要一个私有集群。在那里，你可以调节入站和出站流量，在 NATs 后面屏蔽 IP 地址，在多个 VPC 上隔离多个子网的网络等。</p><p>如何设置网络通常取决于你所追求的灵活性程度以及如何实现它。设置正确的网络就是要尽可能地减少攻击面，同时还能保持正常的运转。</p><p>通过设置正确的网络来保护你的基础设施通常还涉及到使用正确规则和限制条件设置的防火墙，以便限制来自各后端服务的流量的进出，包括入站和出站。</p><p>在很多情况下，可以通过设置堡垒主机并通过隧道进行集群中的所有操作来保护这些私有集群，因为你需要向公共网络公开的就是堡垒（又称 Jump host），通常是在与集群相同的网络中设置。</p><p>一些云提供商在实现零信任安全的方法上也提供了定制化的解决方案。例如，GCP 为其用户提供身份意识代理（IAP），可用于代替典型的 VPN 实现。</p><p>所有都处理好之后，下一步是根据你的用例在集群本身内设置网络。</p><p><strong>这牵涉到以下任务：</strong></p><ul><li>设置集群内的服务发现（可由 CoreDNS 处理）</li><li>如果需要的话，设置一个服务网格（如 LinkerD、Istio、Consul 等）</li><li>设置 Ingress controller 和 API 网关（例如：Nginx、Ambassador、Kong、Gloo 等）</li><li>设置使用 CNI 的网络插件，方便集群内的联网</li><li>设置网络策略，调节服务间的通信，并根据需要使用各种服务类型暴露服务</li><li>使用 GRPC、Thrift 或 HTTP 等协议和工具，设置不同服务之间的服务间通信</li><li>设置 A&#x2F;B 测试，如果你使用像 Istio 或 Linkerd 这样的服务网格，实现起来可以更容易</li></ul><p>如果你想看一些示例实现，我建议你看看这个 repo（<a href="https://github.com/terraform-google-modules/cloud-foundation-fabric">https://github.com/terraform-google-modules/cloud-foundation-fabric</a>），它可以帮助用户在 GCP 中设置所有这些不同的网络模型，包括通过 VPN 的 hub 和 spoke、用于内部的 DNS 和 Google Private Access、支持 GKE 的共享 VPC 等等，所有这些都使用 Terraform。</p><p>而云计算中网络的有趣之处在于，它不局限于你所在地区的云服务商，而是可以根据需要跨越多个地区的多个服务商。这就是 Kubefed 或 Crossplane 这样的项目可以提供帮助的地方。</p><p>如果你想探索更多关于设置 VPC、子网和整体网络时的一些最佳实践，我建议你访问下方网页，同样的概念也适用于你加入的任何云提供商：</p><p><a href="https://cloud.google.com/solutions/best-practices-vpc-design">https://cloud.google.com/solutions/best-practices-vpc-design</a></p><h2 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h2><p>如果你使用的是 GKE、EKS、AKS 这样的托管集群，Kubernetes 是自动管理的，从而降低了用户操作的复杂程度。</p><p>如果你自己管理 Kubernetes，你需要处理很多事情，比如，备份和加密 etcd 存储，在集群中的各个节点之间建立网络，定期为你的节点打上最新版本的操作系统补丁，管理集群升级以与上游的 Kubernetes 版本保持一致。基于此，只有当你拥有一个专门的团队来维护这些事情的时候，才建议这样做。</p><h2 id="Site-Reliability-Engineering-SRE"><a href="#Site-Reliability-Engineering-SRE" class="headerlink" title="Site Reliability Engineering (SRE)"></a>Site Reliability Engineering (SRE)</h2><p>当你维护一个复杂的基础设施时，拥有合适的可观察性堆栈是非常重要的，这样你就可以在用户注意到错误之前就检测到错误以及预测可能的变化，进而识别异常，并有余力深入钻研问题到底在哪里。</p><p>现在，这就需要你有代理，将指标暴露为特定的工具或应用来收集分析（可以遵循 pull 或 push 机制）。而如果你使用的是带有 sidecars 的服务网格，它们往往会自带指标，而不需要自定义配置。</p><p>在任意场景下，都可以使用 Prometheus 这样的工具作为时序数据库，为你收集所有的指标，以及借助类似于 OpenTelemetry 的工具，使用内置的 exporter 从应用程序和各种工具中公开指标。借助 Alertmanager 之类的工具可以向多个渠道发送通知和告警， Grafana 将提供可视化仪表板，给用户提供整个基础设施的完整可见性。</p><p>综上，这就是 Prometheus 的可观察性的解决方案：</p><img src="/kubernetes/k8s-infrastructure/640-20210202213931355" class="" title="img"><blockquote><p>来源：<a href="https://prometheus.io/docs/introduction/overview/">https://prometheus.io/docs/introduction/overview/</a></p></blockquote><p>拥有这样复杂的系统，还需要使用日志聚合系统，这样所有的日志就可以流到一个地方，便于调试。大部分企业倾向于使用 ELK 或 EFK 堆栈，Logstash 或 FluentD 根据你的限制条件为你做日志聚合和过滤。但日志领域也有新的玩家，比如 Loki 和 Promtail。</p><p>下图说明了类似 FluentD 的日志聚合系统如何简化你的架构：</p><img src="/kubernetes/k8s-infrastructure/640-20210202213931338" class="" title="img"><blockquote><p>来源：<a href="https://www.fluentd.org/architecture">https://www.fluentd.org/architecture</a></p></blockquote><p>但是，如果要追踪跨越多个微服务和工具的请求呢？这是分布式跟踪开始发挥作用的地方，特别是考虑到微服务的复杂性。像 Zipkin 和 Jaeger 这样的工具一直是这个领域的先驱，最近进入这个领域的新兴工具是 Tempo。</p><p>虽然日志聚合会给出各种来源的信息，但它不一定能给出请求的上下文，这才是做跟踪真正有帮助的地方。但是请记住，在你的堆栈中添加跟踪会给你的请求增加很大的开销，因为上下文必须和请求一起在服务之间传播。</p><p>下图是一个典型的分布式跟踪架构：</p><img src="/kubernetes/k8s-infrastructure/640-20210202213931364" class="" title="img"><blockquote><p>来源：<a href="https://www.jaegertracing.io/docs/1.21/architecture/">https://www.jaegertracing.io/docs/1.21/architecture/</a></p></blockquote><p>但是，网站的可靠性并不仅仅止于监控、可视化和告警。你必须准备好处理系统任何部分的任何故障，并定期进行备份和故障切换，这样至少可以将数据损失的程度降到最低。你可以借助类似 Velero 的工具实现。</p><p>Velero 通过利用你使用的相同 Kubernetes 架构，帮助你维护集群中各种组件的定期备份，包括你的工作负载、存储等。Velero 的架构如下：</p><p>正如你所观察到的，有一个备份 controller，它定期对对象进行备份，根据你设置的计划将它们推送到特定的目的地，其频率是基于你设置的计划。这可以用于故障转移和迁移，因为几乎所有的对象都有备份。</p><h2 id="存-储"><a href="#存-储" class="headerlink" title="存 储"></a>存 储</h2><p>有许多不同的存储程序和文件系统可用，这在云提供程序之间可能存在很大的不同。这就需要像容器存储接口（CSI）这样的标准，该标准可以帮助大部分 volume 的外置插件，从而使其易于维护和发展而不会成为核心瓶颈。</p><p>下图是 CSI 架构，通常可以支持各种 volume 插件：</p><img src="/kubernetes/k8s-infrastructure/640-20210202214440475" class="" title="img"><blockquote><p>来源：<a href="https://kubernetes.io/blog/2018/08/02/dynamically-expand-volume-with-csi-and-kubernetes/">https://kubernetes.io/blog/2018/08/02/dynamically-expand-volume-with-csi-and-kubernetes/</a></p></blockquote><ul><li>分布式存储带来的集群、扩展等各种问题怎么办？</li></ul><p>这时 Ceph 这样的文件系统已经证明了自己的能力，不过考虑到 Ceph 并不是以 Kubernetes 为中心构建的，部署和管理起来存在一些难度，此时可以考虑 Rook 这样的项目。</p><p>虽然 Rook 没有和 Ceph 耦合，也支持其他文件系统，比如 EdgeFS、NFS 等，但 Rook 与 Ceph CSI 就像是天作之合。Rook 与 Ceph 的架构如下：</p><img src="/kubernetes/k8s-infrastructure/640-20210202213931401" class="" title="img"><blockquote><p>来源：<a href="https://rook.io/docs/rook/v1.5/ceph-storage.html">https://rook.io/docs/rook/v1.5/ceph-storage.html</a></p></blockquote><p>如你所见，Rook 承担了 Kubernetes 集群中的 Ceph 安装、配置和管理的功能。根据用户的喜好，自动分配下面的存储。这一切的发生，都不会让应用暴露在任何复杂的情况下。</p><h2 id="镜像仓库"><a href="#镜像仓库" class="headerlink" title="镜像仓库"></a>镜像仓库</h2><p>镜像仓库为你提供了一个用户界面，你可以在这里管理各种用户账户、推送&#x2F;拉取镜像、管理配额、通过 webhook 获得事件通知、进行漏洞扫描、签署推送的镜像，还可以处理镜像或在多个镜像仓库中复制镜像等操作。</p><p>如果你使用的是云提供商，他们很有可能已经提供了镜像仓库作为一项服务（例如 GCR、ECR、ACR 等），这就消除了很多复杂性。如果你的云提供商没有提供，你也可以选择第三方的镜像仓库，比如 Docker Hub、Quay 等。</p><ul><li>但如果你想托管自己的镜像仓库呢？</li></ul><p>如果你想在企业内部部署镜像仓库，想对其本身有更多的控制权，或者想降低漏洞扫描等操作的相关成本，那么可能需要进行托管。</p><p>如果是这种情况，那么选择像 Harbor 这样的私有镜像仓库会对你有所帮助。</p><p><strong>Harbor 架构如下：</strong></p><img src="/kubernetes/k8s-infrastructure/640-20210202213931394" class="" title="img"><p>来源：<a href="https://goharbor.io/docs/1.10/install-config/harbor-ha-helm/">https://goharbor.io/docs/1.10/install-config/harbor-ha-helm/</a></p><p>Harbor 是一个符合 OCI 的镜像仓库，由各种开源组件组成，包括 Docker 镜像仓库 V2、Harbor UI、Clair 和 Notary。</p><h2 id="CI-x2F-CD-架构"><a href="#CI-x2F-CD-架构" class="headerlink" title="CI&#x2F;CD 架构"></a>CI&#x2F;CD 架构</h2><p>Kubernetes 可以在任何规模下托管所有的工作负载，但这也需要一个标准的方式来部署应用程序，并采用精简的 CI&#x2F;CD 工作流程。下图为典型的 CI&#x2F;CD 流水线：</p><img src="/kubernetes/k8s-infrastructure/640-20210202213931432" class="" title="img"><p>一些第三方服务如 Travis CI、Circle CI、Gitlab CI 或 Github Actions 都包含了自己的 CI 运行器。你只需定义你要构建的流水线中的步骤。这通常包括：构建镜像，扫描镜像以查找可能的漏洞，运行测试并将其推送到镜像仓库，在某些情况下还需要提供一个预览环境以进行审批。</p><p>现在，虽然如果你管理自己的 CI 运行器，步骤通常保持不变，但你需要将它们配置为在集群内部或外部设置，并具有适当的权限，以便将资产推送到镜像仓库。</p><h2 id="总-结"><a href="#总-结" class="headerlink" title="总 结"></a>总 结</h2><p>我们已经介绍了基于 Kubernetes 的云原生基础设施的架构。正如我们上面所看到的，各种工具解决了基础设施的不同问题。它们就像乐高积木一样，每一个都专注于当前的一个特定问题，为你抽象掉了很多复杂的东西。</p><p>这使得用户可以以渐进的方式逐渐上手 Kubernetes。并且你可以根据你的用例，只使用整个堆栈中你需要的工具。</p><blockquote><p>本文转自 RancherLabs 公众号，原文链接：<a href="https://mp.weixin.qq.com/s/Q1S9LXjpmFZu8kitxMKDhg">https://mp.weixin.qq.com/s/Q1S9LXjpmFZu8kitxMKDhg</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Longhorn 1.1 ARM 支持</title>
      <link href="/longhorn/longhorn-arm/"/>
      <url>/longhorn/longhorn-arm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>当 DevOps 团队同时使用 Rancher 和 Longhorn 1.1 时，他们可以轻松管理位于任意位置的持久化数据卷，无论这些持久化数据卷位于云端、数据中心抑或边缘。</p></blockquote><p>2021 年 1 月 27 日，全球开源创新领导者及 Rancher 容器管理平台所有者 SUSE 正式宣布发布 Longhorn 1.1。2019 年 10 月，Longhorn 成为了<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247492467&idx=1&sn=27d14bc5caf61f784e20c3d22de10871&chksm=e83965b5df4eeca35556f2dfd594f27d23aa27abfc09d8569022c6b5972d838c4901cd1207b7&scene=21#wechat_redirect">云原生计算基金会（CNCF）的沙箱（Sandbox）项目</a>。2020 年 6 月，<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247493937&idx=1&sn=b63c19b151cc696bf21105a582e53c49&chksm=e8396ff7df4ee6e1f66eca13c5c75539add6cc94b7562b4b981cb9a5b9e27886c55570fc11a8&scene=21#wechat_redirect">Longhorn 正式 GA</a>。自此，<strong>Longhorn 的采用率增长了 235%<strong>，并成为了 Kubernetes 存储领域的中流砥柱。随着 Longhorn 1.1 的发布，Rancher 用户现已可以</strong>在边缘的低功耗硬件中使用 Kubernetes 原生存储解决方案</strong>。</p><p>Longhorn 1.1 帮助 DevOps 团队在任何 Kubernetes 环境中轻松管理持久化数据卷，同时为云原生存储带来<strong>企业级的避免供应商锁定（Lock- in）的解决方案</strong>。通过扩展对 ARM64 的支持、自我修复功能以及增强的性能可见性，最新版本为 Rancher 用户在边缘环境中提供更高的弹性。</p><p>Gartner 预测，到 2025 年，75% 的企业生成的数据将在传统的集中式数据中心或云之外的边缘创建和处理。与 2018 年的 <strong>10%</strong> 相比，这一数据有了极大的增长。基于对爆炸式增长的企业数据的考量，<strong>Longhorn 1.1 帮助开发人员放心地构建应用程序</strong>，并将数据存储在资源受限的边缘环境当中。</p><h2 id="Longhorn-1-1-全新特性"><a href="#Longhorn-1-1-全新特性" class="headerlink" title="Longhorn 1.1 全新特性"></a>Longhorn 1.1 全新特性</h2><p>Longhorn 以微服务为中心，是用于 Kubernetes 部署的 100%开源的云原生存储项目。Longhorn 1.1 为 DevOps 团队带来了一系列的全新特性及功能增强，包括：</p><ul><li><p>位于边缘的强大的 K8S 原生存储</p><p>Longhorn 1.1 扩展了 Kubernetes 原生存储功能，以支持边缘部署，旨在帮助 DevOps 团队在非理想的环境和边缘受限的环境中可靠地存储数据。除此之外，Longhorn 1.1 扩展了对 ARM 64 的支持，除了可以在数据中心 ARM 64 Server 上运行，还可以让以 ARM 为主流的边缘部署成为可能。</p></li><li><p>更全面的访问模式支持</p><p>提升效率一直是 Kubernetes 用户的首要考量。Longhorn 现已提供跨容器的 <strong>ReadWriteMany</strong> 支持，为开发人员提供更全面有效的持久化存储解决方案。与 <strong>ReadWriteOnce</strong> 的方法有所区别的是，<strong>Longhorn 1.1 允许团队在不同节点的不同路径之间共享存储卷。</strong></p></li><li><p>增强的可见性和运维支持</p>  <img src="/longhorn/longhorn-arm/640-20210201195621090" class="" title="img"><p>  Longhorn 1.1 为企业的存储基础设施中带来了更好的洞察力和功能。<strong>有了对 Prometheus 的新集成支持</strong>，用户可以实时了解诸如监控、资源使用、追踪等存储健康状况的指标。通过支持 Prometheus，用户还可以更详细地了解集群性能。最终，Longhorn 1.1 新增了对 CSI Snapshotter 支持，用户可以通过 “<strong>kubectl</strong>” 创建或恢复备份。</p></li><li><p>节点维护功能升级</p><p>  Longhorn 1.1 的另一个全新功能是增强的节点维护能力。Longhorn 现已支持 Kubernetes drain operations，以帮助用户安全地执行节点维护。Longhorn 1.1 还具有识别新节点上现有磁盘的功能，从而为云供应商提供更好的操作环境。</p></li><li><p>弹性增强</p><p>  对于大多数公司来说，最重要的事情就是减轻网络问题。Longhorn 1.1 引入了新的<strong>数据本地功能，以提高在边缘环境等不稳定的的网络条件下的弹性</strong>。这项新功能将使存储副本保持在工作负载所在的位置，确保即使节点暂时失去网络连接，也不会丢失对存储的访问。</p><p>  相比起其他软件定义的存储解决方案，Longhorn 1.1 非常简单。Longhorn 为大多数用例提供了快速且可靠的存储，而不会像传统的存储解决方案一样复杂和臃肿。</p><p>  当前，Rancher 的用户可以直接从 Rancher Catalog 轻松安装 Longhorn，其他用户可以从 GitHub 免费下载及使用 Longhorn。如果你希望获得 SUSE 对 Longhorn 的官方支持，您可以购买 SUSE 所提供的高级支持服务。您可以基于节点数向 SUSE 购买订阅服务，无需单独支付 License 费用。</p></li></ul><h2 id="一切开源，欢迎体验"><a href="#一切开源，欢迎体验" class="headerlink" title="一切开源，欢迎体验"></a>一切开源，欢迎体验</h2><p>Longhorn GitHub:</p><p><a href="https://github.com/longhorn/longhorn">https://github.com/longhorn/longhorn</a></p>]]></content>
      
      
      <categories>
          
          <category> longhorn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Longhorn ARM </tag>
            
            <tag> longhorn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kubernetes secret device or resource busy</title>
      <link href="/kubernetes/kubernetes-secret-device-or-resource-busy/"/>
      <url>/kubernetes/kubernetes-secret-device-or-resource-busy/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/kubernetes-secret-device-or-resource-busy/" target="_blank" title="https://www.xtplayer.cn/kubernetes/kubernetes-secret-device-or-resource-busy/">https://www.xtplayer.cn/kubernetes/kubernetes-secret-device-or-resource-busy/</a></p><img src="/kubernetes/kubernetes-secret-device-or-resource-busy/image-20210129165812618.png" class="" title="image-20210129165812618"><p>如上图，有时候升级业务 Pod 时，旧业务 Pod 一直处于 <strong>removing</strong> 状态。在主机上执行 <code>docker ps -a | grep &lt;pod 名称&gt;</code> 也未能查询到残留容器。登录 Pod 所在的主机，执行 <code>docker logs kubelet --tail 100</code> 查看 <strong>kubelet</strong>  的运行日志，可以看到以下错误日志：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error: <span class="string">&quot;UnmountVolume.TearDown failed for volume \&quot;default-token-r9sxw\&quot;</span></span><br><span class="line"><span class="string"> (UniqueName: \&quot;kubernetes.io/secret/bc895c3f-2fbf-11eb-a93a-4cd98f444b4d-default-token-r9sxw\&quot;) pod \&quot;bc895c3f-2fbf-11eb-a93a-4cd98f444b4d\&quot; (UID: \&quot;bc895c3f-2fbf-11eb-a93a-4cd98f444b4d\&quot;) :</span></span><br><span class="line"><span class="string"> remove /var/lib/kubelet/pods/bc895c3f-2fbf-11eb-a93a-4cd98f444b4d/volumes/kubernetes.io~secret/default-token-r9sxw: device or resource busy&quot;</span></span><br></pre></td></tr></table></figure><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><ol><li><p>指定占用的路径</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">path=/var/lib/kubelet/pods/bc895c3f-2fbf-11eb-a93a-4cd98f444b4d/volumes/kubernetes.io~secret/default-token-r9sxw</span><br></pre></td></tr></table></figure></li><li><p>查找占用路径的进程 pid 号。</p><p> 通过以下命令可以查询到占用上面路径的 pid 号。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find /proc/*/mounts -<span class="built_in">exec</span> grep <span class="variable">$&#123;path&#125;</span> &#123;&#125; +</span><br></pre></td></tr></table></figure><p> 应该会得到类似以下的结果，&#x2F;proc&#x2F; 与 &#x2F;mounts 中间则为进程 pid 号。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/proc/9706/mounts /var/lib/kubelet/pods/bc895c3f-2fbf-11eb-a93a-4cd98f444b4d/volumes/kubernetes.io~secret/default-token-r9sxw</span><br></pre></td></tr></table></figure></li><li><p>根据上一步中查询到的 pid 号，接着执行以下命令查看具体是什么进程占用</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ps -ef | grep &lt;pid 号&gt;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> device or resource busy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher2 OpenLDAP 认证</title>
      <link href="/rancher/authentication/rancher2-openldap-authentication/"/>
      <url>/rancher/authentication/rancher2-openldap-authentication/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/authentication/rancher2-openldap-authentication/" target="_blank" title="https://www.xtplayer.cn/rancher/authentication/rancher2-openldap-authentication/">https://www.xtplayer.cn/rancher/authentication/rancher2-openldap-authentication/</a></p><p>版本支持: <code>Rancher v2.0.5+</code></p><blockquote><p><strong>注意</strong> 在开始之前，请熟悉 <a href="/rancher/authentication/rancher2-authentication/">外部身份验证配置和主要用户</a> 的概念。</p></blockquote><p>如果您的组织使用 LDAP 进行用户身份验证，则可以将 Rancher 与 OpenLDAP 服务集成，以提供统一的用户身份验证。</p><h2 id="OpenLDAP-身份验证流程"><a href="#OpenLDAP-身份验证流程" class="headerlink" title="OpenLDAP 身份验证流程"></a>OpenLDAP 身份验证流程</h2><ol><li>当用户尝试使用 LDAP 账号登录 Rancher 时，Rancher 使用具有 <code>搜索目录和读取用户/组权限</code> 的服务帐户创建对 LDAP 服务器的初始绑定(账号初始化)。</li><li>然后，Rancher 使用基于提供的用户名和配置的属性映射的搜索过滤器在目录中搜索用户。</li><li>找到用户后，使用用户的 DN 和提供的密码对另一个 LDAP 绑定请求进行身份验证。</li><li>验证成功后，Rancher 将从用户对象的成员资格属性中解析组成员资格，并根据配置的用户映射属性执行组搜索。</li></ol><blockquote><p><strong>注意</strong> 在配置之前请先熟悉 <a href="/rancher/authentication/rancher2-authentication/">外部身份验证配置和主要用户的概念</a>。</p></blockquote><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><p>必须使用 LDAP 绑定帐户(也称为服务帐户)配置 Rancher，以搜索和检索用户和组相关的 LDAP 条目。建议不要使用管理员帐户或个人帐户，而是在 OpenLDAP 中创建一个专用帐户，对配置的搜索路径下的用户和组只具有只读访问权限(见下文)。</p><h2 id="配置步骤"><a href="#配置步骤" class="headerlink" title="配置步骤"></a>配置步骤</h2><h3 id="打开-OpenLDAP-配置页面"><a href="#打开-OpenLDAP-配置页面" class="headerlink" title="打开 OpenLDAP 配置页面"></a>打开 OpenLDAP 配置页面</h3><ol><li>使用系统默认的 <code>admin</code> 帐户登录 Rancher UI。</li><li>从 <code>全局</code> 视图中，导航到 <code>安全 &gt; 认证</code>页面</li><li>选择 OpenLDAP，将显示 <code>配置 OpenLDAP 服务器</code> 表单。</li></ol><h3 id="OpenLDAP-服务器配置"><a href="#OpenLDAP-服务器配置" class="headerlink" title="OpenLDAP 服务器配置"></a>OpenLDAP 服务器配置</h3><blockquote><p><strong>使用 TLS？</strong> 如果 OpenLDAP 服务器使用的是自签名证书，或不是来自权威的证书颁发机构，请确保有 PEM 格式的 CA 证书(与所有的中间证书连接)。您必须在配置期间设置证书，以便 Rancher 能够验证证书链。</p></blockquote><h4 id="OpenLDAP-服务器参数"><a href="#OpenLDAP-服务器参数" class="headerlink" title="OpenLDAP 服务器参数"></a>OpenLDAP 服务器参数</h4><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">Hostname</td><td align="left">指定 OpenLDAP 服务器的主机名或 IP 地址</td></tr><tr><td align="left">端口</td><td align="left">指定 OpenLDAP 服务器正在侦听的端口，未加密的 LDAP 通常使用标准端口 <strong>389</strong>，而 LDAPS 使用端口 <strong>636</strong>。</td></tr><tr><td align="left">TLS</td><td align="left">选中此框以启用基于 <strong>SSL&#x2F;TLS</strong> 的 LDAP（通常称为 LDAPS）。如果服务器使用 自签名&#x2F;企业签名 的 <strong>SSL</strong> 证书，则还需要粘贴 CA 证书。</td></tr><tr><td align="left">服务器连接超时</td><td align="left">Rancher 在考虑服务器不可达之前等待的持续时间（以秒为单位）。</td></tr><tr><td align="left">服务帐户</td><td align="left">用于绑定、搜索和检索 LDAP 条目的服务帐户（DN）。</td></tr><tr><td align="left">服务帐号密码</td><td align="left">服务帐户密码。</td></tr><tr><td align="left">用户搜索起点</td><td align="left">用户搜索起点，所有用户都基于此 DN 以及子目录进行搜索。例如：<code>ou=people,dc=acme,dc=com</code>。</td></tr><tr><td align="left">用户组搜索起点</td><td align="left">用户组搜索起点，所有用户组都基于此 DN 以及子目录进行搜索。如果留空，将会基于 <strong>用户搜索起点</strong> 进行搜索。例如：<code>ou=groups,dc=acme,dc=com</code>。</td></tr></tbody></table><h3 id="自定义架构配置"><a href="#自定义架构配置" class="headerlink" title="自定义架构配置"></a>自定义架构配置</h3><p>如果您的 OpenLDAP 不是标准 OpenLDAP 架构，则必须自定义架构以匹配相应字段。</p><p>请注意，Rancher 使用本节中配置的属性映射来构造搜索过滤器并解析组成员身份。因此，始终建议您验证此处的配置是否与您的 OpenLDAP 架构中使用的字段匹配。</p><p>如果您不熟悉 OpenLDAP 服务器中使用的 <strong>用户&#x2F;组</strong> 架构，请咨询 LDAP 管理员，或参阅 Active Directory 身份验证文档中的<a href="https://rancher.com/docs/rancher/v2.x/en/admin-settings/authentication/ad/#annex-identify-search-base-and-schema-using-ldapsearch">使用 ldapsearch 识别搜索库和架构</a>部分。</p><h4 id="用户架构配置"><a href="#用户架构配置" class="headerlink" title="用户架构配置"></a>用户架构配置</h4><p>下表详细介绍了用户架构配置的参数。</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">对象类别</td><td align="left">域中用于用户对象的对象类的名称。如果已定义，则只指定对象类的名称——不要将其包含在 LDAP 包装器中，例如&amp;(objectClass&#x3D;xxxx)</td></tr><tr><td align="left">用户名属性</td><td align="left">用户属性，其值适合作为显示名称。</td></tr><tr><td align="left">登录属性</td><td align="left">该属性的值与用户登录 Rancher 时输入的 <strong>用户名</strong> 匹配，通常是 <code>uid</code>。</td></tr><tr><td align="left">用户成员属性</td><td align="left">包含用户所属组的专有名称的用户属性，通常这是 <code>memberOf</code> 或 <code>isMemberOf</code>。</td></tr><tr><td align="left">搜索属性</td><td align="left">当用户输入文本以在 UI 中添加 <strong>用户或组</strong> 时，Rancher 会查询 LDAP 服务器并尝试通过此设置中提供的属性来匹配用户。通过使用竖线（“ |”）符号将多个属性分开，可以指定多个属性。</td></tr><tr><td align="left">用户启用的属性</td><td align="left">如果您的 OpenLDAP 服务器的架构支持用户属性，可以对其值进行评估以确定该帐户是禁用还是锁定，请输入该属性的名称。默认的 OpenLDAP 模式不支持此功能，并且该字段通常应留空。</td></tr><tr><td align="left">禁用状态位掩码</td><td align="left">这是 禁用&#x2F;锁定 的用户帐户的值。如果 <code>User Enabled Attribute</code> 为空，则忽略该参数。</td></tr></tbody></table><h4 id="用户组架构配置"><a href="#用户组架构配置" class="headerlink" title="用户组架构配置"></a>用户组架构配置</h4><p>下表详细说明了组架构配置的参数。</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">对象类别</td><td align="left">域中用于分组条目的对象类的名称。如果已定义，则只指定对象类的名称——不要将其包含在 LDAP 包装器中，例如 <code>&amp;(objectClass=xxxx)</code></td></tr><tr><td align="left">名称属性</td><td align="left">组属性，其值适合于显示名称。</td></tr><tr><td align="left">组成员用户属性</td><td align="left"><strong>user attribute</strong> 的名称，其格式与 <code>Group Member Mapping Attribute</code> 中的组成员匹配 。</td></tr><tr><td align="left">组成员映射属性</td><td align="left">包含组成员的组属性的名称。</td></tr><tr><td align="left">搜索属性</td><td align="left">将组添加到 UI 中的集群或项目 时，用于构造搜索过滤器的属性。请参阅用户架构 <code>Search Attribute</code> 说明 。</td></tr><tr><td align="left">组 DN 属性</td><td align="left">组属性的名称，其格式与用户的组成员资格属性中的值匹配。请参阅 <code>User Member Attribute</code>。</td></tr><tr><td align="left">嵌套组成员</td><td align="left">此设置定义 Rancher 是否搜索嵌套的组成员。仅当您的组织使用这些嵌套成员时才使用（即您具有包含其他组作为成员的组）。如果使用 <strong>Shibboleth</strong>，则禁用此选项。</td></tr></tbody></table><h3 id="配置脚本"><a href="#配置脚本" class="headerlink" title="配置脚本"></a>配置脚本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">export</span> RANCHER_DOMAIN=<span class="string">&quot;rancher.yourdomain.com&quot;</span></span><br><span class="line"><span class="built_in">export</span> RANCHER_TOKEN=<span class="string">&quot;token-xxxxx:xxxx&quot;</span></span><br><span class="line"><span class="built_in">export</span> ACCESS_MODE=<span class="string">&quot;unrestricted&quot;</span></span><br><span class="line"><span class="built_in">export</span> CONNECTION_TIMEOUT=<span class="string">&quot;5000&quot;</span></span><br><span class="line"><span class="built_in">export</span> LDAP_HOST=<span class="string">&quot;ldap.yourdomain.com&quot;</span></span><br><span class="line"><span class="built_in">export</span> LDAP_PORT=<span class="string">&quot;636&quot;</span></span><br><span class="line"><span class="built_in">export</span> TLS=<span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="built_in">export</span> SA_DN=<span class="string">&quot;uid=x,ou=x,o=x,dc=yourdomain,dc=com&quot;</span></span><br><span class="line"><span class="built_in">export</span> SA_PW=<span class="string">&quot;sa_password&quot;</span></span><br><span class="line"><span class="built_in">export</span> USER_SEARCHBASE=<span class="string">&quot;ou=x,o=x,dc=yourdomain,dc=com&quot;</span></span><br><span class="line"><span class="built_in">export</span> USERNAME=<span class="string">&quot;username&quot;</span></span><br><span class="line"><span class="built_in">export</span> PASSWORD=<span class="string">&quot;password&quot;</span></span><br><span class="line"></span><br><span class="line">curl -u <span class="variable">$RANCHER_TOKEN</span> <span class="string">&quot;https://<span class="variable">$&#123;RANCHER_DOMAIN&#125;</span>/v3/openLdapConfigs/openldap?action=testAndApply&quot;</span> \</span><br><span class="line">     -H <span class="string">&#x27;content-type: application/json&#x27;</span> \</span><br><span class="line">     -H <span class="string">&#x27;accept: application/json&#x27;</span> \</span><br><span class="line">     --data-binary <span class="string">&#x27;&#123;&quot;ldapConfig&quot;:&#123;&quot;accessMode&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;ACCESS_MODE&#125;</span>&quot;</span><span class="string">&#x27;&quot;,&quot;baseType&quot;:&quot;authConfig&quot;,&quot;connectionTimeout&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;CONNECTION_TIMEOUT&#125;</span>&quot;</span><span class="string">&#x27;&quot;,&quot;enabled&quot;:true,&quot;groupDNAttribute&quot;:&quot;entryDN&quot;,&quot;groupMemberMappingAttribute&quot;:&quot;member&quot;,&quot;groupMemberUserAttribute&quot;:&quot;entryDN&quot;,&quot;groupNameAttribute&quot;:&quot;cn&quot;,&quot;groupObjectClass&quot;:&quot;groupOfNames&quot;,&quot;groupSearchAttribute&quot;:&quot;cn&quot;,&quot;id&quot;:&quot;openldap&quot;,&quot;labels&quot;:&#123;&quot;cattle.io/creator&quot;:&quot;norman&quot;&#125;,&quot;name&quot;:&quot;openldap&quot;,&quot;nestedGroupMembershipEnabled&quot;:false,&quot;port&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;LDAP_PORT&#125;</span>&quot;</span><span class="string">&#x27;&quot;,&quot;servers&quot;:[&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;LDAP_HOST&#125;</span>&quot;</span><span class="string">&#x27;&quot;],&quot;serviceAccountDistinguishedName&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;SA_DN&#125;</span>&quot;</span><span class="string">&#x27;&quot;,&quot;tls&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;TLS&#125;</span>&quot;</span><span class="string">&#x27;&quot;,&quot;type&quot;:&quot;openLdapConfig&quot;,&quot;userDisabledBitMask&quot;:0,&quot;userLoginAttribute&quot;:&quot;uid&quot;,&quot;userMemberAttribute&quot;:&quot;memberOf&quot;,&quot;userNameAttribute&quot;:&quot;cn&quot;,&quot;userObjectClass&quot;:&quot;inetOrgPerson&quot;,&quot;userSearchAttribute&quot;:&quot;uid|sn|givenName&quot;,&quot;userSearchBase&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;USER_SEARCHBASE&#125;</span>&quot;</span><span class="string">&#x27;&quot;,&quot;serviceAccountPassword&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;SA_PW&#125;</span>&quot;</span><span class="string">&#x27;&quot;,&quot;groupSearchBase&quot;:null&#125;,&quot;enabled&quot;:true,&quot;username&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;USERNAME&#125;</span>&quot;</span><span class="string">&#x27;&quot;,&quot;password&quot;:&quot;&#x27;</span><span class="string">&quot;<span class="variable">$&#123;PASSWORD&#125;</span>&quot;</span><span class="string">&#x27;&quot;&#125;&#x27;</span> --compressed --insecure</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> authentication </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher2 Azure AD 认证</title>
      <link href="/rancher/authentication/rancher2-azure-ad-authentication/"/>
      <url>/rancher/authentication/rancher2-azure-ad-authentication/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/authentication/rancher2-azure-ad-authentication/" target="_blank" title="https://www.xtplayer.cn/rancher/authentication/rancher2-azure-ad-authentication/">https://www.xtplayer.cn/rancher/authentication/rancher2-azure-ad-authentication/</a></p><p>版本支持: <code>Rancher v2.0.3+</code></p><p>如果您在 Azure 中启用了 Active Directory(AD) 服务，则可以配置 Rancher 以允许您的用户使用 Azure AD 帐户登录。</p><blockquote><p><strong>注意</strong> 在开始之前，请熟悉 <a href="/rancher/authentication/rancher2-authentication/">外部身份验证配置和主要用户</a> 的概念。</p></blockquote><h2 id="在-Azure-中注册-Rancher"><a href="#在-Azure-中注册-Rancher" class="headerlink" title="在 Azure 中注册 Rancher"></a>在 Azure 中注册 Rancher</h2><p>在 Rancher 中启用 Azure AD 之前，必须向 Azure 注册 Rancher。</p><p>Azure 分 Global 区和中国区：</p><ul><li>中国区 Portal 地址：<code>https://portal.azure.cn</code></li><li>Global 区 Portal 地址：<code>https://portal.azure.com</code></li></ul><h2 id="本文配置以中国区为例，Global-区方法类似"><a href="#本文配置以中国区为例，Global-区方法类似" class="headerlink" title="本文配置以中国区为例，Global 区方法类似"></a>本文配置以中国区为例，Global 区方法类似</h2><ol><li><p>步骤中的配置需要管理访问权限，所以需要以管理用户身份登录 <a href="https://portal.azure.cn/">Microsoft Azure portal</a>。</p></li><li><p>搜索<code>应用注册</code>并打开</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823164708129.346f6012.png" class="" title="image-20180823164708129"></li><li><p>点击<code>新应用程序注册</code>， 并完成表单填写，最后点击右下角的创建。</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823165210404.291fc0cf.png" class="" title="image-20180823165210404"><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823165630846.fb222c1b.png" class="" title="image-20180823165630846"><p><strong>注意</strong> 登录 URL 需要填写为 Rancher 设置中配置的 <code>server_url</code>，但是中国区需要在 <code>rancher_server_url</code> 地址后面添加 <code>verify-auth-azure</code> 后缀，例如：<code>https://demo.rancher.com/verify-auth-azure</code>，Azure 为 Global 区不用添加。</p></li></ol><h2 id="创建-Azure-API-密钥"><a href="#创建-Azure-API-密钥" class="headerlink" title="创建 Azure API 密钥"></a>创建 Azure API 密钥</h2><p>从 Azure 门户中创建 API 密钥，Rancher 将使用此密钥对 Azure AD 进行身份验证。</p><ol><li><p>搜索<code>应用注册</code>服务，然后打开上一个过程中创建的 <code>rancher-test</code>。可能会提示<code>您不是此目录中任何应用程序的所有者</code>,直接点击查看所有应用程序。</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823170226581.be32d96b.png" class="" title="image-20180823170226581"><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823170250818.5545941b.png" class="" title="image-20180823170250818"></li><li><p>点击 <code>rancher-test</code> 后弹出新的窗口</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823170455895.2510fec2.png" class="" title="image-20180823170455895"></li><li><p>单击<strong>设置</strong>，从<strong>设置</strong>边栏中选择<code>密钥</code>。</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823170710676.caeda445.png" class="" title="image-20180823170710676"></li><li><p>输入<strong>密钥描述</strong>,比如 <code>rancher-test</code>，选择密钥的<strong>有效期</strong>，最后点击保存。</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823171126876.3582b096.png" class="" title="image-20180823171126876"></li></ol><blockquote><p><strong>注意</strong> 因为密钥只显示一次，所以需要复制密钥并保存到一个安全的地方。</p></blockquote><h2 id="设置-Rancher-的必需权限"><a href="#设置-Rancher-的必需权限" class="headerlink" title="设置 Rancher 的必需权限"></a>设置 Rancher 的必需权限</h2><p>接下来，在 Azure 中为 Rancher 设置 API 权限。</p><ol><li><p>紧接上一步，从<code>设置</code>边栏选择<code>所需权限</code>。</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823171753402.a8eae7c1.png" class="" title="image-20180823171753402"></li><li><p>单击<strong>Windows Azure Active Directory</strong>。</p></li><li><p>从<strong>启用访问权限</strong>边栏选项卡中，勾选以下<strong>委派权限</strong>：</p><ul><li>以登录用户身份访问该目录</li><li>读取目录数据</li><li>读取所有群组</li><li>读取所有用户的完整个人资料</li><li>读取所有用户的基本配置文件</li><li>登录并读取用户个人资料</li></ul><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823172306171.75cf100b.png" class="" title="image-20180823172306171"><p><strong>注意:</strong></p><p>必须以 Azure 管理员身份登录才能成功保存权限设置。</p></li></ol><h2 id="复制-Azure-应用程序数据"><a href="#复制-Azure-应用程序数据" class="headerlink" title="复制 Azure 应用程序数据"></a>复制 Azure 应用程序数据</h2><p>Azure 配置的最后一步，复制用于配置 Rancher 进行 Azure AD 身份验证的相关配置参数到空文本文件中。</p><ol><li><p>获取<code>目录 ID</code></p><ul><li><p>搜索 Azure Active Directory 服务</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823172714211.60b84d15.png" class="" title="image-20180823172714211"></li><li><p>从 Azure Active Directory 菜单中，打开<code>属性</code></p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823172749770.d883586e.png" class="" title="image-20180823172749770"></li><li><p>复制<code>目录 ID</code> 并将其粘贴到文本文件中</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823172910904.e7f68436.png" class="" title="image-20180823172910904"></li></ul></li><li><p>获取<code>应用 ID</code>。</p><ul><li><p>搜索<code>应用注册</code></p></li><li><p>找到创建的 <code>rancher-test</code> 应用</p></li><li><p>复制应用程序 ID 并将其粘贴到您的文本文件中</p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823173342700.1cba640c.png" class="" title="image-20180823173342700"></li><li><p>获取 <code>MICROSOFT AZURE AD GRAPH API 终结点</code>、<code>OAUTH 2.0 令牌终结点</code>和 <code>OAUTH 2.0 授权终结点</code>。</p></li><li><p>搜索<code>应用注册</code>，并点击<code>终结点</code></p><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823173646887.4cc08d47.png" class="" title="image-20180823173646887"><img src="/rancher/authentication/rancher2-azure-ad-authentication/image-20180823173925497.e19f69d2.png" class="" title="image-20180823173925497"></li></ul></li><li><p>将以下端点复制到剪贴板并将其粘贴到文本文件中</p><ul><li>MICROSOFT AZURE AD GRAPH API 终结点</li><li>OAUTH 2.0 令牌终结点</li><li>OAUTH 2.0 授权终结点</li></ul></li></ol><h2 id="在-Rancher-中配置-Azure-AD"><a href="#在-Rancher-中配置-Azure-AD" class="headerlink" title="在 Rancher 中配置 Azure AD"></a>在 Rancher 中配置 Azure AD</h2><p>在 Rancher UI 中，输入在 Azure 中获取到的 AD 配置信息以完成配置。</p><ol><li><p>登录 RancherUI,从<code>全局</code>视图中，选择<code>安全&gt;认证</code>。</p></li><li><p>选择 Azure AD。</p></li><li><p>输入对应的配置信息:</p><p>下表是 Azure 门户配置与 Rancher 认证配置的字段对应表:</p><table><thead><tr><th>Rancher</th><th>Azure AD</th></tr></thead><tbody><tr><td>租户 ID(Tenant ID)</td><td>目录 ID(Directory ID)</td></tr><tr><td>应用 ID(Application ID)</td><td>应用 ID(Application ID)</td></tr><tr><td>Application Secret</td><td>密钥</td></tr><tr><td>Endpoint</td><td><a href="https://login.chinacloudapi.cn/">https://login.chinacloudapi.cn</a></td></tr><tr><td>Graph Endpoint</td><td><a href="https://graph.chinacloudapi.cn/">https://graph.chinacloudapi.cn</a></td></tr><tr><td>Token Endpoint</td><td>OAUTH 2.0 令牌终结点</td></tr><tr><td>Auth Endpoint</td><td>OAUTH 2.0 授权终结点</td></tr></tbody></table><blockquote><p>重要提示</p><p>Global 区 Endpoint 地址: <code>https://login.windows.net/</code><br>Global 区 Graph Endpoint 地址：<code>https://graph.windows.net/</code><br>具体信息请查阅：<a href="https://docs.microsoft.com/en-us/azure/china/china-get-started-developer-guide#Check-endpoints-in-Azure">Check-endpoints-in-Azure</a></p></blockquote></li><li><p>最后点击<code>启用 Azure AD</code>。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> authentication </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher2 Github 认证</title>
      <link href="/rancher/authentication/rancher2-github-authentication/"/>
      <url>/rancher/authentication/rancher2-github-authentication/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/authentication/rancher2-github-authentication/" target="_blank" title="https://www.xtplayer.cn/rancher/authentication/rancher2-github-authentication/">https://www.xtplayer.cn/rancher/authentication/rancher2-github-authentication/</a></p><p>版本支持: <code>Rancher v2.0.0+</code></p><blockquote><p><strong>注意</strong> 在开始之前，请熟悉 <a href="/rancher/authentication/rancher2-authentication/">外部身份验证配置和主要用户</a> 的概念。</p></blockquote><p>在使用 GitHub 的环境中，您可以配置 Rancher 以允许使用 GitHub 凭据登录。</p><ol><li><p>使用具有<code>管理员角色</code>的本地用户登录 Rancher。</p></li><li><p>在全局视图中，从主菜单中选择 <code>安全&gt;认证</code>。</p></li><li><p>选择 <strong>GitHub</strong>。</p></li><li><p>在 <code>1.设置 Github 应用</code>中，点击 <code>点击此处</code>。</p>   <img src="/rancher/authentication/rancher2-github-authentication/image-20180823233039970.4129262d.png" class="" title="image-20180823233039970"></li><li><p>接着弹出 Github 的登录页面，输入账号和密码进行登录。</p>   <img src="/rancher/authentication/rancher2-github-authentication/image-20180823233159575.34ee6fde.png" class="" title="image-20180823233159575"><p>   如果您使用的是本地部署的企业版 Github 服务器，请通过浏览器打开新的窗口访问 <code>https://&lt;github_server_url&gt;/settings/developers</code>。</p></li><li><p>登录 Github 后，点击 <code>new OAuth app</code>，并填写相关参数。</p>   <img src="/rancher/authentication/rancher2-github-authentication/image-20180824101207978.f6fa02d6.png" class="" title="image-20180824101207978"><ul><li>Homepage URL:  主页 URL,查看 Rancher Github 配置页面；</li><li>Application description: 可选；</li><li>Application description: 授权回调 URL,查看 Rancher Github 配置页面；</li><li>最后点击 <code>Register application</code><img src="/rancher/authentication/rancher2-github-authentication/image-20180823234233080.c2df9e73.png" class="" title="image-20180823234233080"><blockquote><p>什么是授权回调网址?</p><p>使用外部身份验证时，实际上不会在您的应用程序中进行身份验证。相反，身份验证在外部进行(在本例中为 GitHub)。在外部身份验证成功完成后，用户将通过<code>授权回调 URL</code> 重新进入应用程序。</p></blockquote></li></ul></li><li><p>点击 <code>Register application</code> 后，自动创建应用。复制 <code>Client ID</code> 和 <code>Client Secret</code></p>   <img src="/rancher/authentication/rancher2-github-authentication/image-20180823234536788.795351c2.png" class="" title="image-20180823234536788"></li><li><p>在 Rancher Github 配置页面，输入复制的 <code>Client ID</code> 和 <code>Client Secret</code></p>   <img src="/rancher/authentication/rancher2-github-authentication/image-20180823234735787.fbe5d79e.png" class="" title="image-20180823234735787"></li><li><p>如果是本地部署的私有 Github 企业版服务器，需要勾选<code>使用私有 GitHub 企业版部署</code>,根据实际情况是否勾选<code>使用安全连接</code>。</p>   <img src="/rancher/authentication/rancher2-github-authentication/image-20180823235000477.6826e67e.png" class="" title="image-20180823235000477"></li><li><p>单击<code>启用 GitHub</code> 进行身份验证。</p><p>  点击<code>启用 GitHub</code> 后会弹出新的窗口镜像认证授权，如果未弹出窗口请坚持浏览器安全设置是否阻止了弹窗。</p>  <img src="/rancher/authentication/rancher2-github-authentication/image-20180823235553820.e07a8c73.png" class="" title="image-20180823235553820"></li><li><p>授权通过后会自动回到 Rancher Github 配置页面， 在<code>站点访问</code>选项中配置用户授权范围</p><ul><li><p><strong>允许所有有效用户</strong></p><p> 所有的 GitHub 用户，以及本地用户，还有其他第三方认证用户都可以访问 Rancher。我们通常不鼓励使用此设置！</p></li><li><p>允许集群、项目以及授权用户和组成员</p><p> 配置为集群成员或项目成员的所有 GitHub 用户或组，以及本地用户都可以登录到 Rancher。此外，您添加到<code>授权用户和组织</code>列表的任何 GitHub 用户或组都可以登录到 Rancher。</p></li><li><p>仅限授权用户和组织的访问权限</p><p> 只有添加到授权用户和组织的 <code>GitHub 用户或组</code>才能登录 Rancher。相对安全性更高，但是灵活性较低。</p></li></ul></li><li><p>点击<strong>保存</strong>。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> authentication </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher 创建自定义 k8s 集群</title>
      <link href="/rancher/clusters/rancher-create-custom-cluster/"/>
      <url>/rancher/clusters/rancher-create-custom-cluster/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/clusters/rancher-create-custom-cluster/" target="_blank" title="https://www.xtplayer.cn/rancher/clusters/rancher-create-custom-cluster/">https://www.xtplayer.cn/rancher/clusters/rancher-create-custom-cluster/</a></p><p>Rancher 创建自定义 kubernetes 集群，适用于拥有内部虚拟机、内部物理主机，或者没有提供 kubernetes 云服务的云服务器，通过自定义安装方式来快速安装 kubernetes 集群。</p><h2 id="主机和端口需求"><a href="#主机和端口需求" class="headerlink" title="主机和端口需求"></a>主机和端口需求</h2><p>访问 <a href="https://docs.rancher.cn/docs/rancher2/installation_new/requirements/_index/">https://docs.rancher.cn/docs/rancher2/installation_new&#x2F;requirements&#x2F;_index&#x2F;</a> 了解基础环境硬件配置和端口需求的具体信息。</p><h2 id="添加集群"><a href="#添加集群" class="headerlink" title="添加集群"></a>添加集群</h2><ol><li><p>进入<code>全局/集群</code>视图，再点击<code>添加集群</code></p><img src="/rancher/clusters/rancher-create-custom-cluster/image-20180820133743148.5cfca8d5.png" class="" title="image-20180820133743148"><img src="/rancher/clusters/rancher-create-custom-cluster/image-20180820133126613.0aca68db.png" class="" title="image-20180820133126613"></li><li><p>选择<code>自定义</code></p><img src="/rancher/clusters/rancher-create-custom-cluster/image-20180820133900819.3174075c.png" class="" title="image-20180820133900819"></li><li><p>设置<code>集群名称</code></p><img src="/rancher/clusters/rancher-create-custom-cluster/image-20180820133931984.f2379b48.png" class="" title="image-20180820133931984"></li><li><p><code>成员与角色</code></p><p>集群成员对应了 Rancher 中的一个真实的用户，角色则表示此用户具有的集群权限。要想在创建集群时添加成员和对应的成员角色，需要首先在全局下添加用户和集群角色。</p></li><li><p>集群选项</p><ul><li><p><code>kubernetes 版本</code></p><p>每个 Rancher 发行版对应了不同的 kubernetes 版本，可根据需求进行选择；</p></li><li><p><code>网络组件</code></p><p>目前 Rancher 支持三种网络组件：flannel、calico、canal。canal 支持基于<code>项目</code>的网络隔离，可根据际需求选择是否开启，flannel 支持 Windows(实验阶段)。</p></li><li><p><code>云提供商</code></p><p>根据主机所属的云平台，选择对应的云提供商。选择对应的云提供商，可以对接公有云上的一些基础设施，比 4 层负载均衡、存储。</p></li></ul></li><li><p><code>显示高级选项</code></p><p>在集群选项页右下角可以看到高级选项按钮</p></li><li><p><code>私有镜像仓库</code></p><p>在全局系统设置中有个默认私有仓库，那个私有仓库是全局性的。如果设置后，安装任何集群都将从那个仓库拉取镜像。这个私有镜像仓库是集群层的，只作用于当前创建的集群。</p></li><li><p><code>授权集群访问地址</code></p><p>k8s 通过 ssl 来通信认证，而生成 ssl 证书时需要与域名绑定。所以在创建集群的时候为 K8S 集群预先设置一个访问地址，当 Rancher Server 无法访问时，可以通过 kubectl 连接此地址去管理 K8S 集群。</p></li><li><p><code>高级集群选项</code></p><ul><li><p><code>Nginx Ingress</code></p><p>Rancher 中默认支持 Nginx Ingress，v2.0.7 开始支持根据自定义是否开启，默认开启。</p></li><li><p><code>NodePort 端口范围</code></p><p>采用 <strong>NodePort</strong> 网络模式时，Pod 映射的宿主机端口，默认 <strong>30000-32767</strong></p></li><li><p><code>Metrics 服务监控</code></p><p>服务监控指标，<code>v2.0.7</code> 开始支持根据自定义是否开启，默认开启。</p></li><li><p><code>Pod 安全策略</code></p><p>根据需求选择启用或者禁止，如要启用，需现在 <code>全局| 安全 |Pod 安全策略</code> 中创建策略，默认禁止。</p></li><li><p><code>主机 Docker 版本</code></p><p>当前版本，经过严格测试的 Docker 有三个版本：18.06.x、18.09.x、19.03.x，如果设置为<code>需要支的版本</code>，主机的 Docker 版本需要为三个版本其中之一，如果版本不一致将无法安装 K8S 集群。默认设置<code>允不受支持的版本</code>,对于生产环境建议选择<code>需要支持的版本</code>。</p></li><li><p><code>Docker 根目录</code></p><p>如果 docker root 目录非默认，需在此指定；</p></li><li><p><code>ETCD 备份存储</code></p><p>可以选择 ETCD 备份存储路径，默认只存本地，也可以存储到 s3 存储；</p></li><li><p><code>ETCD 备份轮换</code></p><p>开启 ETCD 备份轮换后，将控制 ETCD 备份副本数据与快照创建周期；</p></li></ul></li><li><p>最后点击<code>下一步</code>。</p></li><li><p>自定义主机运行命令</p><ul><li><p><code>主机角色</code></p><p>在 K8S 的架构中，必须至少有一个 etcd、Control、Worker，三种角色可以运行在同一台主机上。要保证集群的高可用，那么需要保证有多个 etcd、Control 实例并且运行在不同主机上。因为 etcd 数据同步机制，etcd 节点数需要为奇数个，比如 1、3、5，具体查看[etcd 集群容错表](<a href="https://docs2.rancher.cn/rancher2x/install-prepare/basic-environment-configuration/#10-etcd">https://docs2.rancher.cn/rancher2x/install-prepare/basic-environment-configuration/#10-etcd</a> 集群容错表)。所以要保证 ETCD 高可用运行，那至少需要有三个节点来运行 etcd 服务。</p></li><li><p><code>高级选项</code></p><p>在高级选项中，可以指定节点的 IP 地址。对于内网环境的单 IP 主机，可以忽略此设置；如果是多 IP 主机，以通过此设置来指定主机的访问 IP。</p></li><li><p><code>主机标签</code></p><p>标签可用于主机的识别和应用的调度，可以在添加节点的时候为其指定。</p><p>假设目前只有一台主机，可按以下方式选择：</p><img src="/rancher/clusters/rancher-create-custom-cluster/image-20180820172356088.c9312996.png" class="" title="image-20180820172356088"></li><li><p><code>最后点击点击右侧的复制按钮</code></p><img src="/rancher/clusters/rancher-create-custom-cluster/image-20180820172514578.af804f51.png" class="" title="image-20180820172514578"></li></ul></li><li><p>ssh 登录到准备添加到 K8S 集群的节点，粘贴并运行上一步复制的命令。</p></li><li><p>最后点击<code>完成</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> clusters </category>
          
      </categories>
      
      
        <tags>
            
            <tag> clusters </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>macOS Big Sur 11.2 rc 候选版更新</title>
      <link href="/macos/macos-big-sur-11-2-rc/"/>
      <url>/macos/macos-big-sur-11-2-rc/</url>
      
        <content type="html"><![CDATA[<p>今日凌晨，苹果面向开发人员推送了 macOS Big Sur 11.2 发布候选版（RC）更新。</p><p>macOS Big Su11.2 beta 2 消除了一项功能，该功能允许 Apple 应用程序绕过第三方防火墙，安全工具和 VPN 应用程序。macOS Big Su11 包含一个 ContentFilterExclusionList 列表，该列表使 Apple 的应用程序（例如 App Store，Maps，iCloud 等）可以避免用户安装的防火墙和 VPN 应用程序。这些应用程序无法筛选或检查某些内置 Apple 应用程序的流量。该功能已 macOS Big Su11.2 中删除。</p><img src="/macos/macos-big-sur-11-2-rc/macOS-Big-sur-features0004-1.jpg" class="" title="macOS-Big-sur-features0004-1"><p><strong>11.2 RC</strong> 版本更新还提高了蓝牙的可靠性，并修复了以下问题：</p><ul><li>使用 HDMI 到 DVI 转换器连接到 Mac mini（M1，2020）时，外部显示器可能会显示黑屏。</li><li>在照片应用中对 Apple ProRAW 照片进行编辑可能无法保存。</li><li>禁用 iCloud Drive 的 “桌面和文档文件夹”选项后，iCloud Drive 可能会关闭。</li><li>输入管理员密码时，系统偏好设置可能无法解锁。</li><li>按下 Globe 键可能不会显示 “表情和符号” 窗口。</li></ul>]]></content>
      
      
      <categories>
          
          <category> macos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> macOS Big Sur </tag>
            
            <tag> macos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记一次 request_sock_tcp possible syn flooding on port 事件处理</title>
      <link href="/linux/network/request-sock-tcp-possible-syn-flooding-on-port/"/>
      <url>/linux/network/request-sock-tcp-possible-syn-flooding-on-port/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/network/request-sock-tcp-possible-syn-flooding-on-port/" target="_blank" title="https://www.xtplayer.cn/linux/network/request-sock-tcp-possible-syn-flooding-on-port/">https://www.xtplayer.cn/linux/network/request-sock-tcp-possible-syn-flooding-on-port/</a></p><h2 id="TCP-三次握手"><a href="#TCP-三次握手" class="headerlink" title="TCP 三次握手"></a>TCP 三次握手</h2><ol><li><p>客户端发送 SYN（进入 SYNC_SENT 状态）</p></li><li><p>服务端返回 SYN+ACK（进入 SYNC_RECV 状态）</p></li><li><p>客户端发送 ACK（进入 ESTABLISHED 状态）</p></li></ol><p>如果客户端在第 3 步时不发送 ACK 给服务端，那么服务端的 socket 就会处于 SYNC_RECV 状态。</p><h2 id="TCP-x2F-IP-backlog-参数"><a href="#TCP-x2F-IP-backlog-参数" class="headerlink" title="TCP&#x2F;IP backlog 参数"></a>TCP&#x2F;IP backlog 参数</h2><p>backlog 其实是一个连接队列，在 Linux kernel 2.2 之前，backlog 大小包括半连接状态和全连接状态两种队列大小。</p><ul><li><p>半连接状态为：服务器处于 Listen 状态时收到客户端 SYN 报文时放入半连接队列中，即 SYN queue（服务器端口状态为：SYN_RCVD）。</p></li><li><p>全连接状态为：TCP 的连接状态从服务器（SYN+ACK）响应客户端后，到客户端的 ACK 报文到达服务器之前，则一直保留在半连接状态中。当服务器接收到客户端的 ACK 报文后，该条目将从半连接队列搬到全连接队列尾部，即 accept queue（服务器端口状态为：ESTABLISHED）。</p></li></ul><p>在 Linux kernel 2.2 之后，分离为两个 <strong>backlog</strong> 来分别限制半连接（SYN_RCVD 状态）队列大小和全连接（ESTABLISHED 状态）队列大小。</p><p><code>SYN queue</code> 队列长度由 <code>/proc/sys/net/ipv4/tcp_max_syn_backlog</code> 指定，默认为 2048。</p><p><code>Accept queue</code> 队列长度由 <code>/proc/sys/net/core/somaxconn</code> 和使用 <strong>listen</strong> 函数时传入的参数，二者取最小值，默认为 128。在 Linux kernel 2.4.25 之前，是写死在代码常量 <strong>SOMAXCONN</strong> 。在 Linux kernel 2.4.25 之后，在配置文件 <code>/proc/sys/net/core/somaxconn</code> 中直接修改，或者在 <code>/etc/sysctl.conf</code> 中配置 <code>net.core.somaxconn = 128</code>。</p><img src="/linux/network/request-sock-tcp-possible-syn-flooding-on-port/927655-20161215133843776-605308204.png" class="" title="img"><ul><li><strong>可以通过 ss 命令来显示</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># ss -l</span></span><br><span class="line">State       Recv-Q Send-Q                                     Local Address:Port                                         Peer Address:Port     </span><br><span class="line">LISTEN      0      128                                                    *:http                                                    *:*       </span><br><span class="line">LISTEN      0      128                                                   :::ssh                                                    :::*       </span><br><span class="line">LISTEN      0      128                                                    *:ssh                                                     *:*       </span><br><span class="line">LISTEN      0      100                                                  ::1:smtp                                                   :::*       </span><br><span class="line">LISTEN      0      100                                            127.0.0.1:smtp                                                    *:*       </span><br></pre></td></tr></table></figure><p>在 <strong>LISTEN</strong> 状态，其中 Send-Q 即为 Accept queue 的最大值，Recv-Q 则表示 Accept queue 中等待被服务器 accept()。</p><p>另外客户端 <strong>connect()</strong> 返回不代表 TCP 连接建立成功，有可能此时 accept queue 已满，系统会直接丢弃后续 ACK 请求。客户端误以为连接已建立，开始调用等待至超时，服务器则等待 ACK 超时，会重传 SYN+ACK 给客户端，重传次数由 <code>net.ipv4.tcp_synack_retries</code> 限制，默认为 5 ，表示重发 5 次，每次等待 30~40 秒，即半连接默认时间大约为 180 秒，可以在 tcp 被洪水攻击是临时启用这个参数。</p><ul><li>查看 SYN queue 溢出</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># netstat -s | grep LISTEN</span></span><br><span class="line">102324 SYNs to LISTEN sockets dropped</span><br></pre></td></tr></table></figure><ul><li>查看 Accept queue 溢出</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># netstat -s | grep TCPBacklogDrop</span></span><br><span class="line">TCPBacklogDrop: 2334</span><br></pre></td></tr></table></figure><h2 id="排查步骤"><a href="#排查步骤" class="headerlink" title="排查步骤"></a>排查步骤</h2><p>此处省略排查步骤。</p><h2 id="内核调优"><a href="#内核调优" class="headerlink" title="内核调优"></a>内核调优</h2><p>根据 <code>TCP 三次握手</code> 和 <code>TCP/IP backlog 参数</code> 可以知道通过调节内核的一些参数可以解决队列溢出的问题。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /etc/sysctl.conf</span></span><br><span class="line">net.core.somaxconn = 2048</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 81920</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">net.core.netdev_max_backlog = 1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存退出后，执行：sysctl -p</span></span><br></pre></td></tr></table></figure><h2 id="问题重现"><a href="#问题重现" class="headerlink" title="问题重现"></a>问题重现</h2><p>正常情况，当内核参数经过调优后，SYN flooding 的问题即可解决。但是当内核参数经过调优后，查看系统日志依然有 <code>request_sock_tcp possible syn flooding on port</code>。</p><p>后期经过查阅资料发现，<code>The application&#39;s socket listen backlog is applied when the application makes the listen() system call against its socket.</code>，也就是在程序通过 <code>listen()</code> 系统调用时，可以对 socket listen backlog 做限制。</p><p>会不会是程序限制的 backlog 太小，导致队列溢出呢？通过查看程序代码，果然是 backlog 太小导致的。通过调整程序的 backlog 大小 <code>request_sock_tcp possible syn flooding on port</code> 不再出现。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><code>https://access.redhat.com/solutions/30453</code></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> network </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Rancher hosted Kubernetes AKS</title>
      <link href="/rancher/clusters/rancher-hosted-kubernetes-aks/"/>
      <url>/rancher/clusters/rancher-hosted-kubernetes-aks/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/clusters/rancher-hosted-kubernetes-aks/" target="_blank" title="https://www.xtplayer.cn/rancher/clusters/rancher-hosted-kubernetes-aks/">https://www.xtplayer.cn/rancher/clusters/rancher-hosted-kubernetes-aks/</a></p><blockquote><p><strong>注意：</strong> 因为 rancher 版本的更新， rancher 中有些参数可能与截图不一致。</p></blockquote><h2 id="Azure-部分操作"><a href="#Azure-部分操作" class="headerlink" title="Azure 部分操作"></a>Azure 部分操作</h2><ul><li>Rancher 对接 AKS 需要获取一下资源信息或操作<ul><li>订阅 ID(Subscription ID)</li><li>应用 ID(Client ID)</li><li>应用密钥(Client Secret)</li><li>RBAC 角色授权</li></ul></li></ul><h3 id="获取订阅-ID-Subscription-ID"><a href="#获取订阅-ID-Subscription-ID" class="headerlink" title="获取订阅 ID (Subscription ID)"></a>获取订阅 ID (Subscription ID)</h3><ol><li><p>主页 -&gt; 订阅 -&gt; 查看订阅 ID (Subscription ID)</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1CmtLe3aH3KVjSZFjq6AFWpXag.a3a215d0.jpg" class="" title="image"></li></ol><h3 id="获取应用-ID-Client-ID"><a href="#获取应用-ID-Client-ID" class="headerlink" title="获取应用 ID (Client ID)"></a>获取应用 ID (Client ID)</h3><ol><li><p>首先需要注册一个应用 (App resgistrations)</p><p>主页 -&gt; Azure Active Directory -&gt; 应用注册(App resgistrations) -&gt; 新注册(New registration)</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1I4NMe3mH3KVjSZKzq6z2OXXa8.dc81d572.jpg" class="" title="image"></li><li><p>输入应用名称，其余选项默认即可</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB18TdMe2WG3KVjSZFPq6xaiXXaR.09bb445a.jpg" class="" title="image"></li><li><p>得到应用 ID (Client ID)</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1_FNWeWWs3KVjSZFxq6yWUXXaB.1b25aee6.jpg" class="" title="image"></li></ol><h3 id="获取应用密钥-Client-Secret"><a href="#获取应用密钥-Client-Secret" class="headerlink" title="获取应用密钥 (Client Secret)"></a>获取应用密钥 (Client Secret)</h3><ol><li><p>进去应用页面 -&gt; 证书与密码页面</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1wqJOe79E3KVjSZFGq6A19XXa4.234d31d5.jpg" class="" title="image"></li><li><p>添加一个新客户端密码</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1obXNe25G3KVjSZPxq6zI3XXaP.6c3ccfa6.jpg" class="" title="image"></li><li><p>得到应用密钥 (Client Secret)，该密钥仅会显示一次，请注意保存</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB16ThSe8Gw3KVjSZFwq6zQ2FXaP.04aa7ca6.jpg" class="" title="image"></li></ol><h3 id="对应用进行角色授权"><a href="#对应用进行角色授权" class="headerlink" title="对应用进行角色授权"></a>对应用进行角色授权</h3><p>通过以上操作后已经获取到了创建 Azure 主机的一些凭证 ID，但是还没有对应用进行角色授权，在创建的时候可能会出现 400、403 的错误信息，下面操作对应用进行角色绑定。</p><ol><li><p>添加角色分配</p><p>主页 -&gt; 订阅 -&gt; 访问控制 (IAM) -&gt; 添加角色分配。</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1h_lWeW1s3KVjSZFAq6x_ZXXa9.5352e690.jpg" class="" title="image"></li><li><p>添加角色</p><p>搜索应用 -&gt; 根据所需要权限添加角色 -&gt; 选中应用 -&gt; 添加。</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1knJNe.GF3KVjSZFmq6zqPXXa7.9d666be9.jpg" class="" title="image"></li><li><p>至此，Azure 上面的操作完成，接下来进行 Rancher 页面操作。</p></li></ol><h2 id="Rancher-部分操作"><a href="#Rancher-部分操作" class="headerlink" title="Rancher 部分操作"></a>Rancher 部分操作</h2><ol><li><p>添加集群，选择 Azure AKS</p></li><li><p>输入集群名称，以及订阅 ID(Subscription ID)、应用 ID(Client ID)、应用密钥 (Client Secret)，选择订阅可创建的可用区。</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1PSVfXhv1gK0jSZFFq6z0sXXaN.28fe3f82.jpg" class="" title="image"></li><li><p>选择 kubernets 版本、节点数量、资源组(可使用已有资源组，也可以新建)、上传公共 CA 证书等选项。</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1ucXgXkY2gK0jSZFgq6A5OFXaF.61e31c6a.jpg" class="" title="image"></li><li><p>点击创建即可完成 AKS 集群的创建 (创建集群需要一定时间，可以通过 Azure 资源组查看新建资源)。</p><img src="/rancher/clusters/rancher-hosted-kubernetes-aks/HTB1qY0hXXY7gK0jSZKzq6yikpXaJ.073ef23f.jpg" class="" title="image"></li></ol><p>PS: 创建过程中 ，有可能会出现失败状态，可以查看 Azure 资源组里面资源创建是否失败。如果遇到失败，删除集群，再来一次，有可能遇到连续失败两次或者更多，删除集群，删除资源组所有资源，重新创建即可。</p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> clusters </category>
          
      </categories>
      
      
        <tags>
            
            <tag> clusters </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher 轮换证书</title>
      <link href="/rancher/cert/rancher-rotate-cert/"/>
      <url>/rancher/cert/rancher-rotate-cert/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/cert/rancher-rotate-cert/" target="_blank" title="https://www.xtplayer.cn/rancher/cert/rancher-rotate-cert/">https://www.xtplayer.cn/rancher/cert/rancher-rotate-cert/</a></p><blockquote><p><strong>警告</strong> 如果您的证书已经过期，请先不要升级 <code>Rancher Server</code>，根据 <a href="/rancher/cert/rancher-rotate-cert/#%E8%AF%81%E4%B9%A6%E5%B7%B2%E8%BF%87%E6%9C%9F%E5%AF%BC%E8%87%B4%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5-K8S-%E8%BF%9B%E8%A1%8C%E8%AF%81%E4%B9%A6%E8%BD%AE%E6%8D%A2">证书已过期导致无法连接 k8s</a> 进行处理。</p></blockquote><p>默认情况下，Kubernetes 集群使用 ssl 证书来加密通信，Rancher 自动为集群生成证书。在 <code>Rancher v2.0.14、v2.1.9</code> 之前的版本，Rancher 创建的集群 ssl 证书默认有效期为 1 年 (CA 证书默认 10 年)，在 <code>Rancher v2.0.14、v2.1.9</code> 以及更高的版本中，Rancher 创建的集群 ssl 证书默认为 10 年 (CA 证书默认 10 年)。</p><h2 id="通过-UI-轮换证书-业务集群"><a href="#通过-UI-轮换证书-业务集群" class="headerlink" title="通过 UI 轮换证书 (业务集群)"></a>通过 UI 轮换证书 (业务集群)</h2><p><em>可用版本: Rancher v2.2.0 +</em></p><p>在 Rancher v2.2.0 以及更高版本，可通过 UI 的证书轮换功能对集群证书进行更新，此功能适用于 <code>自定义安装的集群</code>。证书轮换之后，Kubernetes 组件将自动重新启动，重启不影响应用 Pod，重启时间需要 3 到 5 分钟。</p><ul><li>证书轮换可用于下列服务:<ul><li>etcd</li><li>kubelet</li><li>kube-apiserver</li><li>kube-proxy</li><li>kube-scheduler</li><li>kube-controller-manager</li></ul></li><li>通过 UI 轮换证书，目前支持:<ul><li>批量更新所有服务证书 (CA 证书不变)</li><li>更新某个指定服务 (CA 证书不变)</li></ul></li></ul><h3 id="重要-集群更新"><a href="#重要-集群更新" class="headerlink" title="(重要) 集群更新"></a>(重要) 集群更新</h3><p>如果 Rancher 版本是从 <code>v2.x.x 升级到 2.2.x</code>，则需要先做一次 <code>集群更新</code> 操作。</p><ol><li>进入<code>全局\集群</code>视图；</li><li>选择<code>目标集群</code>右侧的<code>省略号</code>菜单，选择升级；<img src="/rancher/cert/rancher-rotate-cert/image-20190423132857924.f6e38a3c.png" class="" title="image-20190423132857924"></li><li>点击右侧<code>显示高级选项</code>，检查 <code>ETCD 备份轮换</code>功能是否开启，建议开启此功能；<img src="/rancher/cert/rancher-rotate-cert/image-20190423133224065.bfafe04c.png" class="" title="image-20190423133224065"></li><li>在<code>授权集群访问地址</code>中，检查功能是否已开启，建议开始此功能，下边的域名可以不用填写；<img src="/rancher/cert/rancher-rotate-cert/image-20190527112849881.44c90721.png" class="" title="image-20190527112849881"></li><li>最后点击<code>保存</code>，集群将自动进行更新<img src="/rancher/cert/rancher-rotate-cert/image-20190423133309672.e0aade98.png" class="" title="image-20190423133309672"></li></ol><h3 id="轮换证书"><a href="#轮换证书" class="headerlink" title="轮换证书"></a>轮换证书</h3><ol><li><p>进入 <code>全局\集群</code> 视图；</p></li><li><p>选择对应集群右侧的<code>省略号</code>菜单，选择更新证书有效期；</p><img src="/rancher/cert/rancher-rotate-cert/image-20190423112648449.7d8e6ccc.png" class="" title="image-20190423112648449"></li><li><p>选择更新所有服务证书，并点击保存</p><img src="/rancher/cert/rancher-rotate-cert/image-20190423132218317.3d3387a0.png" class="" title="image-20190423132218317"></li><li><p>集群将自动更新证书；</p><img src="/rancher/cert/rancher-rotate-cert/image-20190423132305491.f0957394.png" class="" title="image-20190423132305491"></li><li><p>因为证书改变，相应的 <code>token</code> 也会变化，在集群证书更新完成后，需要对连接 <code>API SERVER</code> 的 Pod 进行重建，以获取新的 <code>token</code></p><ul><li>cattle-system&#x2F;cattle-cluster-agent</li><li>cattle-system&#x2F;cattle-node-agent</li><li>cattle-system&#x2F;kube-api-auth</li><li>ingress-nginx&#x2F;nginx-ingress-controller</li><li>kube-system&#x2F;canal</li><li>kube-system&#x2F;kube-dns</li><li>kube-system&#x2F;kube-dns-autoscaler</li><li>其他应用 Pod</li></ul></li></ol><h2 id="通过-UI-API-轮换证书-业务集群"><a href="#通过-UI-API-轮换证书-业务集群" class="headerlink" title="通过 UI API 轮换证书 (业务集群)"></a>通过 UI API 轮换证书 (业务集群)</h2><p><em>可用版本: Rancher v2.0.14+ v2.1.9+</em></p><p>对于 <code>Rancher v2.0.14、v2.1.9</code> 以及更高版本，可通过 API 对集群证书进行更新。API 证书轮换将会同时对所有组件证书进行更新，不支持指定组件更新证书。</p><ol><li><p>在<code>全局</code>视图中，定位到需要更新证书的集群，然后点击右侧省略号菜单，然后点击 <code>API 查看</code></p><img src="/rancher/cert/rancher-rotate-cert/image-20190527122402294.33393d5f.png" class="" title="image-20190527122402294"></li><li><p>点击右上方的 <code>RotateCertificates</code></p> <img src="/rancher/cert/rancher-rotate-cert/image-20190527122838260.efe768bd.png" class="" title="image-20190527122838260"></li><li><p>点击<strong>Show Request</strong></p></li><li><p>点击 <strong>Send Request</strong></p><img src="/rancher/cert/rancher-rotate-cert/image-20190527123018599.66fd0aa8.png" class="" title="image-20190527123018599"></li><li><p>因为证书改变，相应的 <code>token</code> 也会变化，在集群证书更新完成后，需要对连接 <code>API SERVER</code> 的 Pod 进行重建，以获取新的 <code>token</code>。</p><ul><li>cattle-system&#x2F;cattle-cluster-agent</li><li>cattle-system&#x2F;cattle-node-agent</li><li>cattle-system&#x2F;kube-api-auth</li><li>ingress-nginx&#x2F;nginx-ingress-controller</li><li>kube-system&#x2F;canal</li><li>kube-system&#x2F;kube-dns</li><li>kube-system&#x2F;kube-dns-autoscaler</li><li>其他应用 Pod</li></ul></li></ol><h2 id="RKE-集群证书轮换-local-集群和业务集群通用"><a href="#RKE-集群证书轮换-local-集群和业务集群通用" class="headerlink" title="RKE 集群证书轮换 (local 集群和业务集群通用)"></a>RKE 集群证书轮换 (local 集群和业务集群通用)</h2><p><em>可用版本: rke v0.2.0+</em></p><blockquote><p><strong>注意</strong> 如果以前是通过 <code>rke v0.2.0</code> 之前的版本创建的 Kubernetes 集群，在轮换证书前先执行 <code>rke up</code> 操作。</p></blockquote><ul><li>通过 RKE 轮换证书，目前支持:<ul><li>批量更新所有服务证书 (CA 证书不变)</li><li>更新某个指定服务 (CA 证书不变)</li><li>轮换 CA 和所有服务证书</li></ul></li></ul><ol><li><p>批量更新所有服务证书 (CA 证书不变)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke cert rotate</span><br><span class="line"></span><br><span class="line">INFO[0000] Initiating Kubernetes cluster</span><br><span class="line">INFO[0000] Rotating Kubernetes cluster certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kubernetes API server certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Controller certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Scheduler certificates</span><br><span class="line">INFO[0001] [certificates] Generating Kube Proxy certificates</span><br><span class="line">INFO[0001] [certificates] Generating Node certificate</span><br><span class="line">INFO[0001] [certificates] Generating admin certificates and kubeconfig</span><br><span class="line">INFO[0001] [certificates] Generating Kubernetes API server proxy client certificates</span><br><span class="line">INFO[0001] [certificates] Generating etcd-xxxxx certificate and key</span><br><span class="line">INFO[0001] [certificates] Generating etcd-yyyyy certificate and key</span><br><span class="line">INFO[0002] [certificates] Generating etcd-zzzzz certificate and key</span><br><span class="line">INFO[0002] Successfully Deployed state file at [./cluster.rkestate]</span><br><span class="line">INFO[0002] Rebuilding Kubernetes cluster with rotated certificates</span><br><span class="line">.....</span><br><span class="line">INFO[0050] [worker] Successfully restarted Worker Plane..</span><br></pre></td></tr></table></figure></li><li><p>更新指定服务 (CA 证书不变)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke cert rotate --service kubelet</span><br><span class="line">INFO[0000] Initiating Kubernetes cluster</span><br><span class="line">INFO[0000] Rotating Kubernetes cluster certificates</span><br><span class="line">INFO[0000] [certificates] Generating Node certificate</span><br><span class="line">INFO[0000] Successfully Deployed state file at [./cluster.rkestate]</span><br><span class="line">INFO[0000] Rebuilding Kubernetes cluster with rotated certificates</span><br><span class="line">.....</span><br><span class="line">INFO[0033] [worker] Successfully restarted Worker Plane..</span><br></pre></td></tr></table></figure></li><li><p>轮换 CA 和所有服务证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke cert rotate --rotate-ca</span><br><span class="line"></span><br><span class="line">INFO[0000] Initiating Kubernetes cluster</span><br><span class="line">INFO[0000] Rotating Kubernetes cluster certificates</span><br><span class="line">INFO[0000] [certificates] Generating CA kubernetes certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kubernetes API server certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Controller certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Scheduler certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Proxy certificates</span><br><span class="line">INFO[0000] [certificates] Generating Node certificate</span><br><span class="line">INFO[0001] [certificates] Generating admin certificates and kubeconfig</span><br><span class="line">INFO[0001] [certificates] Generating Kubernetes API server proxy client certificates</span><br><span class="line">INFO[0001] [certificates] Generating etcd-xxxxx certificate and key</span><br><span class="line">INFO[0001] [certificates] Generating etcd-yyyyy certificate and key</span><br><span class="line">INFO[0001] [certificates] Generating etcd-zzzzz certificate and key</span><br><span class="line">INFO[0001] Successfully Deployed state file at [./cluster.rkestate]</span><br><span class="line">INFO[0001] Rebuilding Kubernetes cluster with rotated certificates</span><br></pre></td></tr></table></figure></li><li><p>因为证书改变，相应的 <code>token</code> 也会变化，在集群证书更新完成后，需要对连接 <code>API SERVER</code> 的 Pod 进行重建，以获取新的 <code>token</code>。</p><ul><li>cattle-system&#x2F;cattle-cluster-agent</li><li>cattle-system&#x2F;cattle-node-agent</li><li>cattle-system&#x2F;kube-api-auth</li><li>ingress-nginx&#x2F;nginx-ingress-controller</li><li>kube-system&#x2F;canal</li><li>kube-system&#x2F;kube-dns</li><li>kube-system&#x2F;kube-dns-autoscaler</li><li>其他应用 Pod</li></ul></li></ol><h2 id="单容器-Rancher-Server-证书更新"><a href="#单容器-Rancher-Server-证书更新" class="headerlink" title="单容器 Rancher Server 证书更新"></a>单容器 Rancher Server 证书更新</h2><h3 id="证书未过期"><a href="#证书未过期" class="headerlink" title="证书未过期"></a>证书未过期</h3><ul><li>v2.0.14+ 、v2.1.9+</li></ul><ol><li><p>正常升级 rancher 版本到 v2.0.14+ 、v2.1.9+；</p></li><li><p>执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rancher_server_id=xxx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/certs/bundle.json /var/lib/rancher/management-state/certs/bundle.json-bak</span><br><span class="line"></span><br><span class="line">docker restart <span class="variable">$&#123;rancher_server_id&#125;</span></span><br></pre></td></tr></table></figure></li></ol><ul><li>v2.2.0+</li></ul><ol><li><p>正常升级 rancher 版本到 v2.2.0+</p></li><li><p>执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rancher_server_id=xxx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/tls/localhost.crt /var/lib/rancher/management-state/tls/localhost.crt-bak</span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/tls/localhost.key /var/lib/rancher/management-state/tls/localhost.key-bak</span><br><span class="line">docker restart <span class="variable">$&#123;rancher_server_id&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="证书已过期"><a href="#证书已过期" class="headerlink" title="证书已过期"></a>证书已过期</h3><p>如果证书已过期，那么 rancher server 无法正常运行。即使升级到 Rancher v2.0.14+ 、v2.1.9+、v2.2.0+ 也不会更新证书。如果出现这种情况，可以把主机时间往后调整一些，ssl 证书有效时间验证是基于主机时间来验证。</p><ul><li><p>执行以下命令调整主机时间：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 关闭 ntp 同步，防止时间自动更新回来</span></span><br><span class="line">timedatectl set-ntp <span class="literal">false</span></span><br><span class="line"><span class="comment"># 修改节点时间</span></span><br><span class="line">timedatectl set-time <span class="string">&#x27;2019-01-01 00:00:00&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>注意: 有的虚拟机安装了 <code>vmtool</code> 工具并开启了虚拟机与主机时间同步功能，这种情况下需要停止 <code>vmtool</code> 进程，不然时间会自动更新回来。</p></blockquote></li><li><p>v2.0.14+ 、v2.1.9+</p></li></ul><ol><li><p>正常升级 rancher 版本到 v2.0.14+ 、v2.1.9+；</p></li><li><p>执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rancher_server_id=xxx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/certs/bundle.json /var/lib/rancher/management-state/certs/bundle.json-bak</span><br><span class="line"></span><br><span class="line">docker restart <span class="variable">$&#123;rancher_server_id&#125;</span></span><br></pre></td></tr></table></figure></li></ol><ul><li>v2.2.0+</li></ul><ol><li><p>正常升级 rancher 版本到 v2.2.0+</p></li><li><p>执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rancher_server_id=xxx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/tls/localhost.crt /var/lib/rancher/management-state/tls/localhost.crt-bak</span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/tls/localhost.key /var/lib/rancher/management-state/tls/localhost.key-bak</span><br><span class="line">docker restart <span class="variable">$&#123;rancher_server_id&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="故障处理"><a href="#故障处理" class="headerlink" title="故障处理"></a>故障处理</h2><h3 id="提示-CA-证书为空"><a href="#提示-CA-证书为空" class="headerlink" title="提示 CA 证书为空"></a>提示 CA 证书为空</h3><p>如果执行更新证书后出现如下错误提示，因为没有执行集群更新操作</p><img src="/rancher/cert/rancher-rotate-cert/image-20190423133555060.b403ce72.png" class="" title="image-20190423133555060"><p><strong>解决方法</strong></p><ol><li><p>选择对应问题集群，然后查看浏览器的集群 ID，如下图：</p><img src="/rancher/cert/rancher-rotate-cert/image-20190423133810076.db389f73.png" class="" title="ran" alt="chimage-20190423133810076"></li><li><p>执行命令 <code>kubectl edit clusters &lt;clusters_ID&gt;</code></p><ul><li>如果 Rancher 是 HA 安装，直接在 local 集群中，通过 <code>rke</code> 生成的 <code>kube</code> 配置文件执行以上命令；</li><li>如果 Rancher 是单容器运行，通过 <code>docker exec -ti &lt;容器 ID&gt; bash</code> 进入容器中，然后执行 <code>apt install vim -y</code> 安装 vim 工具，然后再执行以上命令；</li></ul></li><li><p>删除 <code>spec.rancherKubernetesEngineConfig.rotateCertificates</code> 层级下的配置参数:</p><img src="/rancher/cert/rancher-rotate-cert/image-20190423135522178.d83c0677.png" class="" title="image-20190423135522178"><p><strong>修改为</strong></p><img src="/rancher/cert/rancher-rotate-cert/image-20190423135604503.04e188d0.png" class="" title="image-20190423135604503"></li><li><p>输入 <code>:wq</code> 保存 yaml 文件后集群将自动更新，更新完成后再进行证书更新。</p></li></ol><h3 id="证书已过期导致无法连接-K8S-进行证书轮换"><a href="#证书已过期导致无法连接-K8S-进行证书轮换" class="headerlink" title="证书已过期导致无法连接 K8S 进行证书轮换"></a>证书已过期导致无法连接 K8S 进行证书轮换</h3><p>如果集群证书已经过期，那么即使升级到 <code>Rancher v2.0.14、v2.1.9</code> 以及更高版本也无法轮换证书。rancher 是通过 <code>Agent</code> 去更新证书，如果证书过期将无法与 <code>Agent</code> 连接。</p><p><strong>解决方法</strong></p><p>可以手动设置节点的时间，把时间往后调整一些。因为 <code>Agent</code> 只与 <code>K8S master</code> 和 <code>Rancher Server</code> 通信，如果 Rancher Server 证书未过期，那就只需调整 <code>K8S master</code> 节点时间。</p><p>调整命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 关闭 ntp 同步，不然时间会自动更新</span></span><br><span class="line">timedatectl set-ntp <span class="literal">false</span></span><br><span class="line"><span class="comment"># 修改节点时间</span></span><br><span class="line">timedatectl set-time <span class="string">&#x27;2019-01-01 00:00:00&#x27;</span></span><br></pre></td></tr></table></figure><p>然后再对 Rancher Server 进行升级，接着按照证书轮换步骤进行证书轮换，等到证书轮换完成后再把时间同步回来。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">timedatectl set-ntp <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>检查证书有效期</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl x509 -<span class="keyword">in</span> /etc/kubernetes/ssl/kube-apiserver.pem -noout -dates</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> cert </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cert </tag>
            
            <tag> rancher 轮换证书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>删除状态为 Failed、Evicted 的 Pod</title>
      <link href="/kubernetes/kubectl-delete-evicted-pods/"/>
      <url>/kubernetes/kubectl-delete-evicted-pods/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/kubectl-delete-evicted-pods/" target="_blank" title="https://www.xtplayer.cn/kubernetes/kubectl-delete-evicted-pods/">https://www.xtplayer.cn/kubernetes/kubectl-delete-evicted-pods/</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">kubectl get pods --all-namespaces -o go-template=<span class="string">&#x27;&#123;&#123;range .items&#125;&#125; &#123;&#123;if (or (eq .status.phase &quot;Evicted&quot;) (eq .status.phase &quot;Failed&quot; ))&#125;&#125; &#123;&#123;.metadata.name&#125;&#125;&#123;&#123;&quot; &quot;&#125;&#125; &#123;&#123;.metadata.namespace&#125;&#125; &#123;&#123;&quot;\n&quot;&#125;&#125;&#123;&#123;end&#125;&#125; &#123;&#123;end&#125;&#125;&#x27;</span> | <span class="keyword">while</span> <span class="built_in">read</span> epod namespace; <span class="keyword">do</span> kubectl -n <span class="variable">$namespace</span> delete pod <span class="variable">$epod</span>; <span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Evicted </tag>
            
            <tag> Failed </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tcpdump 手册</title>
      <link href="/linux/network/tcpdump-man-page/"/>
      <url>/linux/network/tcpdump-man-page/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/network/tcpdump-man-page/" target="_blank" title="https://www.xtplayer.cn/linux/network/tcpdump-man-page/">https://www.xtplayer.cn/linux/network/tcpdump-man-page/</a></p><p>本文翻译至：<a href="https://www.tcpdump.org/manpages/tcpdump.1.html">https://www.tcpdump.org/manpages/tcpdump.1.html</a></p><h2 id="基本参数"><a href="#基本参数" class="headerlink" title="基本参数"></a>基本参数</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tcpdump  -h</span></span><br><span class="line"></span><br><span class="line">tcpdump version 4.9.3</span><br><span class="line">libpcap version 1.9.1 (with TPACKET_V3)</span><br><span class="line">OpenSSL 1.1.1g FIPS  21 Apr 2020</span><br><span class="line">Usage: tcpdump [-aAbdDefhHIJKlLnNOpqStuUvxX<span class="comment">#] [ -B size ] [ -c count ]</span></span><br><span class="line">    [ -C file_size ] [ -E algo:secret ] [ -F file ] [ -G seconds ]</span><br><span class="line">    [ -i interface ] [ -j tstamptype ] [ -M secret ] [ --number ]</span><br><span class="line">    [ -Q <span class="keyword">in</span>|out|inout ]</span><br><span class="line">    [ -r file ] [ -s snaplen ] [ --time-stamp-precision precision ]</span><br><span class="line">    [ --immediate-mode ] [ -T <span class="built_in">type</span> ] [ --version ] [ -V file ]</span><br><span class="line">    [ -w file ] [ -W filecount ] [ -y datalinktype ] [ -z postrotate-command ]</span><br><span class="line">    [ -Z user ] [ expression ]</span><br></pre></td></tr></table></figure><h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Tcpdump 打印出与布尔表达式匹配的网络接口上的数据包的描述内容，该描述之前有一个时间戳，默认打印为小时、分钟、秒以及从午夜开始的几分之一秒。它还可以使用 <strong>-w</strong> 参数运行，将把数据包保存到一个文件中供以后分析，<strong>和&#x2F;或</strong>使用 <strong>-r</strong> 参数从保存的数据包文件中读取，也可以使用 <strong>-V</strong> 参数读取已保存的包文件列表。在所有情况下，tcpdump 只处理匹配表达式的数据包。</p><p>如果不使用 <code>-c</code> 参数指定捕获数据包数量，它会一直捕捉数据包，一直到收到 <code>SIGINT</code>（control + c）或者 <code>SIGTERM</code>(<strong>kill</strong> 终止命令)信号时才会停止。 如果使用 <code>-c</code> 参数，它会捕捉包到收到 <code>SIGINT</code> 或者 <code>SIGTERM</code> 信号，或者收到指定数量的包之后才会停止。</p><p>当 <code>tcpdump</code> 完成捕捉包，它会报告以下的计数：</p><ul><li><code>captured</code> <em>tcpdump</em> 已接收和处理的数据包数量；</li><li><code>received by filter</code> 过滤器接收的数据包数量；</li><li><code>dropped by kernel</code> 丢弃的包，一般由于 buffer 空间不够引起；</li></ul><p>对于支持 <code>SIGINFO</code> 信号的平台，比如大多数的 BSD(包括 mac osx)和 Digital&#x2F;Tru64 UNIX， 它收到 <code>SIGINFO</code> 信号时会报告这些计数，然后继续捕捉包。从网络设备上读取包需要 root 权限，从文件上读取不需要。使用<strong>SIGUSR2</strong>信号和 <strong>-w</strong> 参数将强制把数据包缓冲区刷新到输出文件中。</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><ul><li><p><strong>-A</strong></p><p>以 ASCII 码方式显示每一个数据包(不会显示数据包中链路层头部信息). 在抓取包含网页数据的数据包时， 可方便查看数据。</p></li><li><p><strong>-b</strong></p><p>在 BGP 报文中打印 AS 号，使用 ASDOT 代替 ASPLAIN。</p></li><li><p><strong>-B</strong> <em>buffer_size</em></p></li><li><p><em>–buffer-size&#x3D;buffer_size</em></p><p>将操作系统捕获缓冲区大小设置为 <em>buffer_size</em>，单位为 KiB(1024 字节)。</p></li><li><p><strong>-c</strong> <em>count</em></p><p>接收到 <em>count</em> 个数据包后退出。</p></li><li><p><strong>–count</strong></p><p>读取捕获文件时仅在标准错误上打印数据包计数，而不是解析&#x2F;打印数据包。如果在命令行中指定了过滤器，<em>tcpdump</em> 只对匹配该过滤器表达式的数据包进行统计。</p></li><li><p><strong>-C</strong> <em>file_size</em> （此选项用于配合 -w file 选项使用)）</p><p>在将原始数据包写入 savefile 之前，检查该文件当前是否大于 <em>file_size</em>，如果大于，则关闭当前 savefile 并打开一个新的 savefile。在第一个保存文件之后的保存文件将具有用 <strong>-w</strong> 标志指定的名称，后面有一个数字，从 1 开始递增。<em>file_size</em> 的单位是百万字节(1，000，000 字节，而不是 1，048，576 字节)。</p></li><li><p><strong>-d</strong></p><p>以容易阅读的形式在标准输出上打印编译后的包匹配代码并停止。</p><p>请注意，尽管代码编译始终是特定于 DLT 的，但是通常不可能（且不必要）指定用于转储的 DLT，因为 tcpdump 使用 -r 指定的输入 pcap 文件的 DLT 或默认的 DLT。-i 指定的网络接口，或 -y 和 -i 分别指定的网络接口的特定 DLT。在这些情况下，转储显示代码完全相同，可以过滤输入文件或不带-d 的网络接口。</p><p>但是，当没有指定 <strong>-r</strong> 和 <strong>-i</strong> 时，指定 <strong>-d</strong> 可防止 <em>tcpdump</em> 猜测合适的网络接口(参见 <strong>-i</strong> )。在本例中，DLT 默认为 EN10MB，可以使用 <strong>-y</strong> 手动将其设置为另一个有效值。</p></li><li><p><strong>-dd</strong></p><p>使用 c 语言片段的方式打印 packet-matching code。</p></li><li><p><strong>-ddd</strong></p><p>使用十进制数字的方式打印 packet-matching code。</p></li><li><p><strong>-D</strong></p></li><li><p><strong>–list-interfaces</strong></p><p>打印系统上可用的网络接口列表，tcpdump 可以在该网络接口上捕获数据包。对于每个网络接口，将打印一个数字和一个接口名称，并可能在其后显示该接口的文本描述。可以将接口名称或编号传递给 -i 参数，以指定要捕获的接口。</p><p>这对于没有命令列出网络接口的操作系统（例如 Windows 系统或缺少 ifconfig -a 的 UNIX 系统）很有用，该数字在 Windows 2000 和更高版本的系统上非常有用，在 Windows 2000 和更高版本的系统上，接口名称是一个有点复杂的字符串。</p><p>如果<em>tcpdump</em>是用<em>libpcap</em>的旧版本构建的，缺少 <strong><a href="https://www.tcpdump.org/manpages/pcap_findalldevs.3pcap.html">pcap_findalldevs</a>(3PCAP)</strong> 函数，则不支持 <strong>-D</strong> 参数。</p></li><li><p><strong>-e</strong></p><p>在每个转储行上打印链接级标题。例如，它可以用于打印以太网和 IEEE 802.11 等协议的 MAC 层地址。</p></li><li><p><strong>-E</strong></p><p>Use <em><a href="mailto:spi@ipaddr">spi@ipaddr</a> algo:secret</em> for decrypting IPsec ESP packets that are addressed to <em>addr</em> and contain Security Parameter Index value <em>spi</em>. This combination may be repeated with comma or newline separation.</p><p>注意，目前仅支持对 IPv4 ESP 报文设置 secret。</p><p>可用于加密的算法包括 <code>des-cbc， 3des-cbc， blowfish-cbc， rc3-cbc， cast128-cbc， 或者没有(none)</code>，默认为 des-cbc。注意：只有在 <em>tcpdump</em>编译时启用了加密技术，才具有解密数据包的能力。</p><p>secret 为用于 ESP 的密钥， 使用 ASCII 字符串方式表达. 如果以 0x 开头， 该密钥将以 16 进制方式读入.</p><p>该选项中 ESP 的定义遵循 RFC2406， 而不是 RFC1827. 并且， 此选项只是用来调试的， 不推荐以真实密钥(secret)来使用该选项， 因为这样不安全: 在命令行中输入的 secret 可以被其他人通过 ps 等命令查看到.</p><p>除了上述语法之外，语法 <em>file name</em> 可用于让 tcpdump 读取提供的文件。该文件在收到第一个 ESP 数据包时被打开，因此 tcpdump 可能已经给予的任何特殊权限都应该已经被放弃。</p></li><li><p><strong>-f</strong></p><p>以数字而不是符号的方式打印 “外部” IPv4 地址(这个选项是为了避免 Sun 的 NIS 服务器的严重脑损伤——通常在转换非本地互联网号码时它会永远挂起)。</p><p>对“外部”IPv4 地址的测试是使用正在捕获的接口的 IPv4 地址和 netmask 完成的。如果该地址或 netmask 不可用，或者因为正在进行捕获的接口没有地址或 netmask，或者因为捕获是在 Linux “any”接口上进行的，该接口可以在多个接口上进行捕获，那么该选项将不能正确工作。</p></li><li><p><strong>-F</strong> <em>file</em></p><p>使用 <em>file</em> 作为过滤表达式的输入。在命令行中给出的附加表达式将被忽略。</p></li><li><p><strong>-G</strong> <em>rotate_seconds</em></p><p>如果指定该参数，将以 <strong>-w</strong> 选项指定的转储文件每 <em>rotate_seconds</em> 秒旋转一次。保存文件的名称将由 <strong>-w</strong> 指定，其中应包含<strong>strftime</strong>定义的时间格式。如果没有指定时间格式，每个新文件将覆盖之前的文件。每当生成的文件名不是唯一的，tcpdump 就会覆盖已存在的数据。因此，不建议提供比捕获周期更粗的时间规范。</p><p>如果与 <strong>-C</strong> 选项一起使用，文件名将采取 ‘<em>file</em><count>‘ 的形式。</p></li><li><p><strong>-h</strong></p></li><li><p><strong>–help</strong></p><p>打印 tcpdump 和 libpcap 版本字符串，输出帮助信息并退出。</p></li><li><p><strong>–version</strong></p><p>打印 tcpdump 和 libpcap 版本字符串并退出。</p></li><li><p><strong>-H</strong></p><p>尝试检测 802.11s 网络头。</p></li><li><p><strong>-i</strong> <em>interface</em></p></li><li><p><em>–interface&#x3D;interface</em></p><p>监听 interface 接口。如果没指定，tcpdump 会搜索系统中编号最少的接口(不包括回环地址)。</p><p>对于 linux 2.2 及以后的系统，可以使用 any 来指定捕捉所有的接口。对于 promiscuous 模式不适用。</p><p>如果支持-D 参数，可以使用接口数字来进行指定。</p></li><li><p><strong>-I</strong></p></li><li><p><strong>–monitor-mode</strong></p><p>将接口设置为 monitor mode，这只支持 ieee802.11 Wi-Fi 接口，并且只支持某些操作系统。</p><p>注意，在监控模式下，适配器可能会与它所关联的网络解除关联，因此您将无法使用该适配器的任何无线网络。如果您在监控模式下进行捕获，并且没有通过另一个适配器连接到另一个网络，那么这可能会阻止访问网络服务器上的文件，或解析主机名或网络地址。</p><p>此标志将影响**-L** 标志的输出。如果没有指定 <strong>-I</strong>，则只显示那些在监控模式下可用的链路层类型;如果指定了 <strong>-I</strong>，则只显示在监控模式下可用的链路层类型。</p></li><li><p><strong>–immediate-mode</strong></p><p>捕获在 “immediate mode”。在这种模式下，数据包一到达就被发送到 tcpdump，而不是为了提高效率而进行缓冲。如果数据包被打印到终端而不是保存信息包到“savefile”时，这是默认值。</p></li><li><p><strong>-j</strong> <em>tstamp_type</em></p></li><li><p><em>–time-stamp-type&#x3D;tstamp_type</em></p><p>将捕获的时间戳类型设置为<em>tstamp_type</em>。时间戳类型使用的名称见**<a href="https://www.tcpdump.org/manpages/pcap-tstamp.7.html">pcap-tstamp</a>**(7);并不是所有列出的类型对任何给定的接口都是有效的。</p></li><li><p><strong>-J</strong></p></li><li><p><strong>–list-time-stamp-types</strong></p><p>列出接口和出口支持的时间戳类型。如果不能设置接口的时间戳类型，则不列出时间戳类型。</p></li><li><p><em>–time-stamp-precision&#x3D;tstamp_precision</em></p><p>When capturing， set the time stamp precision for the capture to <em>tstamp_precision</em>. Note that availability of high precision time stamps (nanoseconds) and their actual accuracy is platform and hardware dependent. Also note that when writing captures made with nanosecond accuracy to a savefile， the time stamps are written with nanosecond resolution， and the file is written with a different magic number， to indicate that the time stamps are in seconds and nanoseconds; not all programs that read pcap savefiles will be able to read those captures.</p><p>When reading a savefile， convert time stamps to the precision specified by <em>timestamp_precision</em>， and display them with that resolution. If the precision specified is less than the precision of time stamps in the file， the conversion will lose precision.</p><p>The supported values for <em>timestamp_precision</em> are <strong>micro</strong> for microsecond resolution and <strong>nano</strong> for nanosecond resolution. The default is microsecond resolution.</p></li><li><p><strong>–micro</strong></p></li><li><p><strong>–nano</strong></p><p>Shorthands for <strong>–time-stamp-precision&#x3D;micro</strong> or <strong>–time-stamp-precision&#x3D;nano</strong>， adjusting the time stamp precision accordingly. When reading packets from a savefile， using <strong>–micro</strong> truncates time stamps if the savefile was created with nanosecond precision. In contrast， a savefile created with microsecond precision will have trailing zeroes added to the time stamp when <strong>–nano</strong> is used.</p></li><li><p><strong>-K</strong></p></li><li><p><strong>–dont-verify-checksums</strong></p><p>Don’t attempt to verify IP， TCP， or UDP checksums. This is useful for interfaces that perform some or all of those checksum calculation in hardware; otherwise， all outgoing TCP checksums will be flagged as bad.</p></li><li><p><strong>-l</strong></p><p>缓冲 stdout 行数。如果您想在捕获数据时查看数据，这是非常有用的。例如，</p><p><code>tcpdump -l | tee dat</code> 或者 <code>tcpdump -l &gt; dat &amp; tail -f dat</code></p><p>注意，在 Windows 上，<code>line buffered</code> 意味着 <code>unbuffered</code>，所以如果指定了 <strong>-l</strong>，WinDump 将单独写入每个字符。</p><p><strong>-U</strong> is similar to <strong>-l</strong> in its behavior， but it will cause output to be &#96;&#96;packet-buffered’’， so that the output is written to stdout at the end of each packet rather than at the end of each line; this is buffered on all platforms， including Windows.</p></li><li><p><strong>-L</strong></p></li><li><p><strong>–list-data-link-types</strong></p><p>List the known data link types for the interface， in the specified mode， and exit. The list of known data link types may be dependent on the specified mode; for example， on some platforms， a Wi-Fi interface might support one set of data link types when not in monitor mode (for example， it might support only fake Ethernet headers， or might support 802.11 headers but not support 802.11 headers with radio information) and another set of data link types when in monitor mode (for example， it might support 802.11 headers， or 802.11 headers with radio information， only in monitor mode).</p></li><li><p><strong>-m</strong> <em>module</em></p><p>从 <em>module</em> 文件加载 SMI MIB 模块定义。该选项可多次使用，用于将多个 MIB 模块加载到 <em>tcpdump</em>中。</p></li><li><p><strong>-M</strong> <em>secret</em></p><p>如果 TCP 数据包(TCP segments)有 TCP-MD5 选项(在 RFC 2385 有相关描述)， 则为其摘要的验证指定一个公共的密钥 secret.</p></li><li><p><strong>-n</strong></p><p>不要将地址(即主机地址、端口号等)转换为名称。</p></li><li><p><strong>-N</strong></p><p>-N 不打印出 host 的域名部分。比如，如果设置了此参数， tcpdump 将会打印 ‘nic’ 而不是 ‘nic.ddn.mil’。</p></li><li><p><strong>-#</strong></p></li><li><p><strong>–number</strong></p><p>在行首打印一个可选的数据包号。</p></li><li><p><strong>-O</strong></p></li><li><p><strong>–no-optimize</strong></p><p>不要运行数据包匹配代码优化器。这只有在您怀疑优化器中存在 bug 时才有用。</p></li><li><p><strong>-p</strong></p></li><li><p><strong>–no-promiscuous-mode</strong></p><p><em>不要</em> 将接口设置为混杂模式。请注意，接口可能由于其他原因处于混杂模式，因此，’-p’ 不能用作 ‘ether host {local-hw-addr} or ether broadcast’  的缩写。</p></li><li><p><strong>–print</strong></p><p>即使原始数据包被保存到带有**-w**标志的文件中，也打印输出已解析的数据包。</p></li><li><p><strong>-Q</strong> <em>direction</em></p></li><li><p><em>–direction&#x3D;direction</em></p><p>Choose send&#x2F;receive direction <em>direction</em> for which packets should be captured. Possible values are <code>in&#39;， </code>out’ and &#96;inout’. Not available on all platforms.</p></li><li><p><strong>-q</strong></p><p>快速(安静?)输出。打印更少的协议信息，这样输出行就更短。</p></li><li><p><strong>-r</strong> <em>file</em></p><p>从文件 file 中读取包数据. 如果 file 字段为 ‘-‘ 符号， 则 tcpdump 会从标准输入中读取包数据。</p></li><li><p><strong>-S</strong></p></li><li><p><strong>–absolute-tcp-sequence-numbers</strong></p><p>打印绝对而不是相对的 TCP 序列号。</p><p>相对顺序号可理解为， 相对第一个 TCP 包顺序号的差距，比如， 接受方收到第一个数据包的绝对顺序号为 232323， 对于后来接收到的第 2 个，第 3 个数据包， tcpdump 会打印其序列号为 1， 2 分别表示与第一个数据包的差距为 1 和 2. 而如果此时-S 选项被设置， 对于后来接收到的第 2 个， 第 3 个数据包会打印出其绝对顺序号: 232324， 232325。</p></li><li><p><strong>-s</strong> <em>snaplen</em></p></li><li><p><em>–snapshot-length&#x3D;snaplen</em></p><p>设置 tcpdump 的数据包抓取长度为 snaplen，如果不设置默认将会是 68 字节(而支持网络接口分接头。68 字节对于 IP， ICMP(nt: Internet Control Message Protocol，因特网控制报文协议)， TCP 以及 UDP 协议的报文已足够， 但对于名称服务(nt: 可理解为 dns， nis 等服务)， NFS 服务相关的数据包会产生包截短. 如果产生包截短这种情况， tcpdump 的相应打印输出行中会出现’’[|proto]’’的标志（proto 实际会显示为被截短的数据包的相关协议层次). 需要注意的是， 采用长的抓取长度，会增加包的处理时间， 并且会减少 tcpdump 可缓存的数据包的数量， 从而会导致数据包的丢失. 所以， 在能抓取我们想要的包的前提下， 抓取长度越小越好。把 snaplen 设置为 0 意味着让 tcpdump 自动选择合适的长度来抓取数据包.</p><p>Snarf <em>snaplen</em> bytes of data from each packet rather than the default of 262144 bytes. Packets truncated because of a limited snapshot are indicated in the output with &#96;&#96;[|<em>proto</em>]’’， where <em>proto</em> is the name of the protocol level at which the truncation has occurred.</p><p>Note that taking larger snapshots both increases the amount of time it takes to process packets and， effectively， decreases the amount of packet buffering. This may cause packets to be lost. Note also that taking smaller snapshots will discard data from protocols above the transport layer， which loses information that may be important. NFS and AFS requests and replies， for example， are very large， and much of the detail won’t be available if a too-short snapshot length is selected.</p><p>If you need to reduce the snapshot size below the default， you should limit <em>snaplen</em> to the smallest number that will capture the protocol information you’re interested in. Setting <em>snaplen</em> to 0 sets it to the default of 262144， for backwards compatibility with recent older versions of <em>tcpdump</em>.</p></li><li><p><strong>-T</strong> <em>type</em></p><p>Force packets selected by “<em>expression</em>“ to be interpreted the specified <em>type</em>. Currently known types are <strong>aodv</strong> (Ad-hoc On-demand Distance Vector protocol)， <strong>carp</strong> (Common Address Redundancy Protocol)， <strong>cnfp</strong> (Cisco NetFlow protocol)， <strong>domain</strong> (Domain Name System)， <strong>lmp</strong> (Link Management Protocol)， <strong>pgm</strong> (Pragmatic General Multicast)， <strong>pgm_zmtp1</strong> (ZMTP&#x2F;1.0 inside PGM&#x2F;EPGM)， <strong>ptp</strong> (Precision Time Protocol)， <strong>radius</strong> (RADIUS)， <strong>resp</strong> (REdis Serialization Protocol)， <strong>rpc</strong> (Remote Procedure Call)， <strong>rtcp</strong> (Real-Time Applications control protocol)， <strong>rtp</strong> (Real-Time Applications protocol)， <strong>snmp</strong> (Simple Network Management Protocol)， <strong>someip</strong> (SOME&#x2F;IP)， <strong>tftp</strong> (Trivial File Transfer Protocol)， <strong>vat</strong> (Visual Audio Tool)， <strong>vxlan</strong> (Virtual eXtensible Local Area Network)， <strong>wb</strong> (distributed White Board) and <strong>zmtp1</strong> (ZeroMQ Message Transport Protocol 1.0).</p><p>Note that the <strong>pgm</strong> type above affects UDP interpretation only， the native PGM is always recognised as IP protocol 113 regardless. UDP-encapsulated PGM is often called “EPGM” or “PGM&#x2F;UDP”.</p><p>Note that the <strong>pgm_zmtp1</strong> type above affects interpretation of both native PGM and UDP at once. During the native PGM decoding the application data of an ODATA&#x2F;RDATA packet would be decoded as a ZeroMQ datagram with ZMTP&#x2F;1.0 frames. During the UDP decoding in addition to that any UDP packet would be treated as an encapsulated PGM packet.</p></li><li><p><strong>-t</strong></p><p>不要在每个转储行上打印时间戳。</p></li><li><p><strong>-tt</strong></p><p>在每个转储行上打印时间戳。在每个转储行上显示自 1970 年 1 月 1 日、00:00:00、UTC 以来的秒数，以及自该时间以来的几分之一秒。</p></li><li><p><strong>-ttt</strong></p><p>在每个转储行上的当前行和前一行之间打印一个时间增量，微秒或者纳秒由 <strong>–time-stamp-precision</strong> 选项决定，默认为 微秒。</p></li><li><p><strong>-tttt</strong></p><p>在每个转储行上打印一个时间戳。</p></li><li><p><strong>-ttttt</strong></p><p>在每个转储行的当前行和第一行之间打印一个增量，微秒或者纳秒由 <strong>–time-stamp-precision</strong> 选项决定，默认为 微秒。</p></li><li><p><strong>-u</strong></p><p>打印未解码的 NFS 句柄。</p></li><li><p><strong>-U</strong></p></li><li><p><strong>–packet-buffered</strong></p><p>If the <strong>-w</strong> option is not specified， or if it is specified but the <strong>–print</strong> flag is also specified， make the printed packet output &#96;&#96;packet-buffered’’; i.e.， as the description of the contents of each packet is printed， it will be written to the standard output， rather than， when not writing to a terminal， being written only when the output buffer fills.</p><p>如果指定了 <strong>-w</strong> 选项，则使保存的原始数据包输出为 <strong>packet-buffered</strong>，也就是说，当每个数据包被保存时，它将被写入输出文件，而不是只在输出缓冲区填充时才写入。</p><p>The <strong>-U</strong> flag will not be supported if <em>tcpdump</em> was built with an older version of <em>libpcap</em> that lacks the <strong><a href="https://www.tcpdump.org/manpages/pcap_dump_flush.3pcap.html">pcap_dump_flush</a>(3PCAP)</strong> function.</p></li><li><p><strong>-v</strong></p><p>当分析和打印的时候， 产生详细的输出。</p></li><li><p><strong>-vv</strong></p><p>产生比-v 更详细的输出。</p></li><li><p><strong>-vvv</strong></p><p>产生比 -vv 更详细的输出。</p></li><li><p><strong>-V</strong> <em>file</em></p><p>从 file 中读取一个文件名列表。如果 file 为-，则从标准输入中读取。</p></li><li><p><strong>-w</strong> <em>file</em></p><p>把原生信息写入到 file 文件，而不是进行输出。后续可以使用-r 来读取。如果 file 为 <code>-</code>，则写到标准输出。如果将该输出写入文件或管道，则该输出将被缓冲，因此从文件或管道读取数据包的程序可能在接收数据包后的一段时间内看不到数据包，因为数据还未写入文件。使用 <strong>-U</strong> 标志使数据包在收到后立即被写入文件。</p><p>The MIME type <em>application&#x2F;vnd.tcpdump.pcap</em> has been registered with IANA for <em>pcap</em> files. The filename extension <em>.pcap</em> appears to be the most commonly used along with <em>.cap</em> and <em>.dmp</em>. <em>Tcpdump</em> itself doesn’t check the extension when reading capture files and doesn’t add an extension when writing them (it uses magic numbers in the file header instead). However， many operating systems and applications will use the extension if it is present and adding one (e.g. .pcap) is recommended.</p><p>See <strong><a href="https://www.tcpdump.org/manpages/pcap-savefile.5.html">pcap-savefile</a></strong>(5) for a description of the file format.</p></li><li><p><strong>-W</strong> <em>filecount</em></p><p>此选项与 <code>-C</code> 选项配合使用，这将限制可打开的文件数目，并且当文件数据超过这里设置的限制时，依次循环替代之前的文件， 这相当于一个拥有 filecount 个文件的文件缓冲池。同时，该选项会使得每个文件名的开头会出现足够多并用来占位的 0， 这可以方便这些文件被正确的排序。</p><p>与 <strong>-G</strong> 选项一起使用，将限制所创建的旋转转储文件的数量，当达到限制时将以状态 0 退出。</p><p>如果与 <strong>-C</strong>* 和 <strong>-G</strong> 同时使用，**-W** 选项当前将被忽略，只会影响文件名。</p></li><li><p><strong>-x</strong></p><p>When parsing and printing， in addition to printing the headers of each packet， print the data of each packet (minus its link level header) in hex. The smaller of the entire packet or <em>snaplen</em> bytes will be printed. Note that this is the entire link-layer packet， so for link layers that pad (e.g. Ethernet)， the padding bytes will also be printed when the higher layer packet is shorter than the required padding. In the current implementation this flag may have the same effect as <strong>-xx</strong> if the packet is truncated.</p></li><li><p><strong>-xx</strong></p><p>解析和打印时，除了打印每个数据包的报头外，还打印每个数据包的数据，<em>包括</em>其链路级报头，十六进制。</p></li><li><p><strong>-X</strong></p><p>When parsing and printing， in addition to printing the headers of each packet， print the data of each packet (minus its link level header) in hex and ASCII. This is very handy for analysing new protocols. In the current implementation this flag may have the same effect as <strong>-XX</strong> if the packet is truncated.</p></li><li><p><strong>-XX</strong></p><p>解析和打印时，除了打印每个数据包的报头外，还打印每个数据包的数据，<em>包括</em>其链路级报头，十六进制和 ASCII 格式。</p></li><li><p><strong>-y</strong> <em>datalinktype</em></p></li><li><p><em>–linktype&#x3D;datalinktype</em></p><p>Set the data link type to use while capturing packets (see <strong>-L</strong>) or just compiling and dumping packet-matching code (see <strong>-d</strong>) to <em>datalinktype</em>.</p></li><li><p><strong>-z</strong> <em>postrotate-command</em></p><p>Used in conjunction with the <strong>-C</strong> or <strong>-G</strong> options， this will make <em>tcpdump</em> run “ <em>postrotate-command file</em> “ where <em>file</em> is the savefile being closed after each rotation. For example， specifying <strong>-z gzip</strong> or <strong>-z bzip2</strong> will compress each savefile using gzip or bzip2.</p><p>Note that tcpdump will run the command in parallel to the capture， using the lowest priority so that this doesn’t disturb the capture process.</p><p>And in case you would like to use a command that itself takes flags or different arguments， you can always write a shell script that will take the savefile name as the only argument， make the flags &amp; arguments arrangements and execute the command that you want.</p></li><li><p><strong>-Z</strong> <em>user</em></p></li><li><p><em>–relinquish-privileges&#x3D;user</em></p><p>如果<em>tcpdump</em>以 root 身份运行，在打开捕获设备或输入 savefile 之后，但在打开任何 savefile 输出之前，将用户 ID 更改为<em>user</em>，将组 ID 更改为<em>user</em>的主组。</p><p>这种行为也可以在编译时默认启用。</p></li><li><p><em>expression</em></p><p>selects which packets will be dumped. If no <em>expression</em> is given， all packets on the net will be dumped. Otherwise， only packets for which <em>expression</em> is &#96;true’ will be dumped.For the <em>expression</em> syntax， see <strong><a href="https://www.tcpdump.org/manpages/pcap-filter.7.html">pcap-filter</a></strong>(7).The <em>expression</em> argument can be passed to <em>tcpdump</em> as either a single Shell argument， or as multiple Shell arguments， whichever is more convenient. Generally， if the expression contains Shell metacharacters， such as backslashes used to escape protocol names， it is easier to pass it as a single， quoted argument rather than to escape the Shell metacharacters. Multiple arguments are concatenated with spaces before being parsed.</p></li></ul><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><ol><li>打印所有到达或者离开 sundown 主机的包：</li></ol><p>  <code>tcpdump host sundown</code></p><ol><li>打印 holios 与 hot 或者 ace 通信的包：</li></ol><p>  <code>tcpdump host helios and \( hot or ace \)</code></p><ol><li>打印除了 helios 的所有与 ace 通信的 ip 包：</li></ol><p>  <code>tcpdump ip host ace and not helios</code></p><ol><li>To print all traffic between local hosts and hosts at Berkeley:</li></ol><p>  <code>tcpdump net ucb-ether</code></p><ol><li>To print all ftp traffic through internet gateway <em>snup</em>: (note that the expression is quoted to prevent the shell from (mis-)interpreting the parentheses):</li></ol><p>  <code>tcpdump &#39;gateway snup and (port ftp or ftp-data)&#39;</code></p><ol><li>To print traffic neither sourced from nor destined for local hosts (if you gateway to one other net， this stuff should never make it onto your local net).</li></ol><p>  <code>tcpdump ip and not net** *localnet*</code></p><ol><li>To print the start and end packets (the SYN and FIN packets) of each TCP conversation that involves a non-local host.</li></ol><p>  <code>tcpdump &#39;tcp[tcpflags] &amp; (tcp-syn|tcp-fin) != 0 and not src and dst net** *localnet*&#39;</code></p><ol><li>To print the TCP packets with flags RST and ACK both set. (i.e. select only the RST and ACK flags in the flags field， and if the result is “RST and ACK both set”， match)</li></ol><p>  <code>tcpdump &#39;tcp[tcpflags] &amp; (tcp-rst|tcp-ack) == (tcp-rst|tcp-ack)&#39;</code></p><ol><li>To print all IPv4 HTTP packets to and from port 80， i.e. print only packets that contain data， not， for example， SYN and FIN packets and ACK-only packets. (IPv6 is left as an exercise for the reader.)</li></ol><p>  <code>tcpdump &#39;tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)&#39;</code></p><ol><li>To print IP packets longer than 576 bytes sent through gateway <em>snup</em>:</li></ol><p>  <code>tcpdump &#39;gateway snup and ip[2:2] &gt; 576&#39;</code></p><ol><li>To print IP broadcast or multicast packets that were <em>not</em> sent via Ethernet broadcast or multicast:</li></ol><p>  <code>tcpdump &#39;ether[0] &amp; 1 = 0 and ip[16] &gt;= 224&#39;</code></p><ol><li>To print all ICMP packets that are not echo requests&#x2F;replies (i.e.， not ping packets):</li></ol><p>  <code>tcpdump &#39;icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply&#39;</code></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tcpdump </tag>
            
            <tag> 抓包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>配置 Network Manager 以忽略某些网络设备</title>
      <link href="/linux/network/networkmanager-not-manager-container-network-interface/"/>
      <url>/linux/network/networkmanager-not-manager-container-network-interface/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/network/networkmanager-not-manager-container-network-interface/" target="_blank" title="https://www.xtplayer.cn/linux/network/networkmanager-not-manager-container-network-interface/">https://www.xtplayer.cn/linux/network/networkmanager-not-manager-container-network-interface/</a></p><p>Network Manager 是一个能够动态控制和配置网络的守护进程，对应 <strong>NetworkManager.service</strong> 服务，默认配置文件路径 <strong>&#x2F;etc&#x2F;NetworkManager&#x2F;NetworkManager.conf</strong>。从 <strong>RHEL&#x2F;CentOS 7</strong> 开始，网络功能默认由 NetworkManager 以服务的形式提供。Ubuntu 的 server 版本默认没有启用 NetworkManager。</p><p>默认情况下，NetworkManager 管理除 <code>lo</code>（环回）设备以外的所有设备。但是，在容器环境下很多网络设备是由网络驱动创建，NetworkManager 对容器网络设备的控制可能导致集群网络通信异常。因为需要将容器相关的设备设置为 <code>unmanaged</code>，以使 NetworkManager 忽略这些设备。</p><h2 id="查看设备状态"><a href="#查看设备状态" class="headerlink" title="查看设备状态"></a>查看设备状态</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看设备管理状态</span></span><br><span class="line">nmcli device status</span><br></pre></td></tr></table></figure><h2 id="临时-unmanaged"><a href="#临时-unmanaged" class="headerlink" title="临时 unmanaged"></a>临时 unmanaged</h2><p>如果只是临时测试，那么可以通过 nmcli 命令去设置某个网络设备为 unmanaged 状态。但是只要重启 NetworkManager 服务或者重启了主机，网络接口又会恢复为 managed 状态。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nmcli device <span class="built_in">set</span> xxx managed no</span><br></pre></td></tr></table></figure><h2 id="永久-unmanaged"><a href="#永久-unmanaged" class="headerlink" title="永久 unmanaged"></a>永久 unmanaged</h2><p>要想永久的排除 NetworkManager 管理某些网络设备，则需要通过配置文件去设置。</p><ol><li><p>启用插件</p><p> 在 <code>/etc/NetworkManager/NetworkManager.conf</code> 配置文件的[main] 层级下启用插件 keyfile。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[main]</span><br><span class="line">plugins=keyfile</span><br></pre></td></tr></table></figure></li><li><p>创建 <code>/etc/NetworkManager/conf.d/99-unmanaged-devices.conf</code> 配置文件，包含以下内容：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[keyfile]</span><br><span class="line">unmanaged-devices=interface-name:eth*,except:interface-name:eth0;interface-name:docker0;interface-name:flannel*;interface-name:flannel*;interface-name:cni0;;mac:66:77:88:99:00:aa</span><br></pre></td></tr></table></figure><ul><li>以分号隔开；</li><li>可以使用通配符来匹配接口；</li><li>interface-name:eth*,except:interface-name:eth0; 表示：除了 eth0，其他以 eth 开头的接口全部 unmanaged；</li><li>可以通过 mac 地址来排除接口；</li></ul></li><li><p>重新加载配置</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl reload NetworkManager</span><br></pre></td></tr></table></figure></li></ol><h2 id="禁用-Network-Manager"><a href="#禁用-Network-Manager" class="headerlink" title="禁用 Network Manager"></a>禁用 Network Manager</h2><p>配置的错误，或者配置更新不及时都可能导致不必要的问题，如果不是必须使用 NetworkManager 服务，一般情况还是建议禁用 Network Manager 服务。</p><p>在 centos 8.x 环境中，默认没有安装 <code>network-scripts</code>，禁止 Network Manager 服务后则无法重启网络，可以通过手动执行 <code>yum -y install network-scripts</code> 来安装。</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> network </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>权威 ssl 证书 + ingress + 安装 Rancher HA</title>
      <link href="/rancher/install/ha-authority-ssl-ingress/"/>
      <url>/rancher/install/ha-authority-ssl-ingress/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/ha-authority-ssl-ingress/" target="_blank" title="https://www.xtplayer.cn/rancher/install/ha-authority-ssl-ingress/">https://www.xtplayer.cn/rancher/install/ha-authority-ssl-ingress/</a></p><h2 id="节点软硬件要求"><a href="#节点软硬件要求" class="headerlink" title="节点软硬件要求"></a>节点软硬件要求</h2><p>节点硬件要求请参考：<a href="https://docs.rancher.cn/docs/rancher2/installation_new/requirements/_index/">https://docs.rancher.cn/docs/rancher2/installation_new&#x2F;requirements&#x2F;_index&#x2F;</a></p><h2 id="节点基础环境配置"><a href="#节点基础环境配置" class="headerlink" title="节点基础环境配置"></a>节点基础环境配置</h2><p>请参考 <a href="/kubernetes/k8s-basic-environment-configuration/">基础环境配置</a></p><h2 id="同步镜像"><a href="#同步镜像" class="headerlink" title="同步镜像"></a>同步镜像</h2><p>如果你是在离线环境安装，请先访问<a href="https://www.xtplayer.cn/rancher/rancher-install-offline-images/">rancher 离线安装镜像同步</a>，按照方法同步所有镜像到离线私有镜像仓库。如果主机能够直接拉取镜像，则跳过此步骤。</p><h2 id="创建-rke-配置文件"><a href="#创建-rke-配置文件" class="headerlink" title="创建 rke 配置文件"></a>创建 rke 配置文件</h2><p>使用下面的示例创建 <code>rancher-cluster.yml</code> 文件，使用创建的 3 个节点的 IP 地址或域名替换列表中的 IP 地址。</p><blockquote><p><strong>注意:</strong> 如果节点有<code>公网地址 和 内网地址</code>地址，建议手动设置 <code>internal_address:</code>以便 Kubernetes 将内网地址用于集群内部通信。如果需要开启自动配置安全组或防火墙，某些服务(如 AWS EC2)需要设置 <code>internal_address:</code>。</p></blockquote><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.114</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.22</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.116</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.32</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.127</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.42</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果要使用私有仓库中的镜像，配置以下参数来指定默认私有仓库地址。</span></span><br><span class="line"><span class="comment">##private_registries:</span></span><br><span class="line"><span class="comment">##    - url: registry.com</span></span><br><span class="line"><span class="comment">##      user: Username</span></span><br><span class="line"><span class="comment">##      password: password</span></span><br><span class="line"><span class="comment">##      is_default: true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">etcd:</span></span><br><span class="line">    <span class="comment"># 扩展参数 https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">auto-compaction-mode:</span> <span class="string">periodic</span></span><br><span class="line">      <span class="attr">auto-compaction-retention:</span> <span class="string">60m</span></span><br><span class="line">      <span class="comment"># 修改空间配额为$((6*1024*1024*1024))，默认 2G,最大 8G</span></span><br><span class="line">      <span class="attr">quota-backend-bytes:</span> <span class="string">&#x27;6442450944&#x27;</span></span><br><span class="line">      <span class="attr">snapshot-count:</span> <span class="number">50000</span></span><br><span class="line">    <span class="comment"># 自动备份</span></span><br><span class="line">    <span class="comment">## rke 版本小于 0.2.x 或 rancher 版本小于 v2.2.0 时使用</span></span><br><span class="line">    <span class="attr">snapshot:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">creation:</span> <span class="string">6h</span></span><br><span class="line">    <span class="attr">retention:</span> <span class="string">24h</span></span><br><span class="line">    <span class="comment">## rke 版本大于等于 0.2.x 或 rancher 版本大于等于 v2.2.0 时使用(两段配置二选一)</span></span><br><span class="line">    <span class="attr">backup_config:</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">true</span>         <span class="comment"># 设置 true 启用 ETCD 自动备份，设置 false 禁用；</span></span><br><span class="line">      <span class="attr">interval_hours:</span> <span class="number">12</span>    <span class="comment"># 快照创建间隔时间，不加此参数，默认 5 分钟；</span></span><br><span class="line">      <span class="attr">retention:</span> <span class="number">6</span>          <span class="comment"># etcd 备份保留份数；</span></span><br></pre></td></tr></table></figure><blockquote><p>更多 etcd 配置请参考: <a href="https://www.xtplayer.cn/etcd/etcd-optimize/">etcd 优化</a></p></blockquote><h3 id="RKE-节点参数说明"><a href="#RKE-节点参数说明" class="headerlink" title="RKE 节点参数说明"></a>RKE 节点参数说明</h3><table><thead><tr><th>参数</th><th>必须</th><th>描述</th></tr></thead><tbody><tr><td><code>address</code></td><td>yes</td><td>公共域名或 IP 地址</td></tr><tr><td><code>user</code></td><td>yes</td><td>可以运行 docker 命令的用户</td></tr><tr><td><code>role</code></td><td>yes</td><td>分配给节点的 Kubernetes 角色列表</td></tr><tr><td><code>internal_address</code></td><td>no</td><td>内部集群通信的私有域名或 IP 地址</td></tr><tr><td><code>ssh_key_path</code></td><td>no</td><td>用于对节点进行身份验证的 SSH 私钥的路径（默认为~&#x2F;.ssh&#x2F;id_rsa）</td></tr></tbody></table><h3 id="镜像仓库配置"><a href="#镜像仓库配置" class="headerlink" title="镜像仓库配置"></a>镜像仓库配置</h3><p>如果需要使用自己镜像仓库的镜像或者是离线安装，那么可以在 <code>private_registries</code> 字段中配置私有镜像仓库信息。</p><table><thead><tr><th>选项</th><th>值</th><th>描述</th></tr></thead><tbody><tr><td><code>url</code></td><td>‘’</td><td>镜像仓库地址</td></tr><tr><td><code>user</code></td><td>‘’</td><td>镜像仓库用户名</td></tr><tr><td><code>password</code></td><td>‘’</td><td>镜像仓库密码</td></tr><tr><td><code>is_default</code></td><td><code>true/false</code></td><td>这个参数很重要，当配置这个参数后，<br/>rke 会自动为系统镜像添加镜像仓库前缀，<br/>比如 rancher&#x2F;rancher 会变为 192.168.1.1&#x2F;rancher&#x2F;rancher，<br/>用于离线环境或者使用自己的私有仓库构建集群。</td></tr></tbody></table><blockquote><p>完整的配置示例，请参考<a href="https://docs.rancher.cn/docs/rke/example-yamls/_index/">完整-cluster-yml-示例</a></p></blockquote><h3 id="高级配置"><a href="#高级配置" class="headerlink" title="高级配置"></a>高级配置</h3><p>RKE 有许多配置选项可用于自定义安装以适合您的特定环境，有关选项和功能的完整列表，请查看<a href="https://docs.rancher.cn/docs/rke/config-options/_index/">RKE 文档</a> 。</p><h2 id="安装-rke、kubectl、helm-工具"><a href="#安装-rke、kubectl、helm-工具" class="headerlink" title="安装 rke、kubectl、helm 工具"></a>安装 rke、kubectl、helm 工具</h2><p>需要选择一个节点用来运行 rke、kubectl、helm 等命令，你可以叫它为 <strong>操作节点</strong>。这个节点可以是 local 集群中的任意一台，也可以是专门准备的独立的一个节点。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rke</span></span><br><span class="line">sudo curl -o /usr/local/bin/rke http://rancher-mirror.cnrancher.com/rke/v1.2.4/rke_linux-amd64</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/rke</span><br><span class="line"><span class="comment"># kubectl</span></span><br><span class="line">sudo curl -o /usr/local/bin/kubectl http://rancher-mirror.cnrancher.com/kubectl/v1.20.0/linux-amd64-v1.20.0-kubectl</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/kubectl</span><br><span class="line"><span class="comment"># helm</span></span><br><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">sudo curl -O http://rancher-mirror.cnrancher.com/helm/v3.5.0/helm-v3.5.0-linux-amd64.tar.gz;</span><br><span class="line">sudo tar -zxf helm*tar.gz;</span><br><span class="line">sudo <span class="built_in">cp</span> -rf /tmp/linux-amd64/helm /usr/local/bin/helm</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/helm</span><br></pre></td></tr></table></figure><p>或者访问 <a href="http://mirror.cnrancher.com/">http://mirror.cnrancher.com/</a> 下载 rke、kubectl、helm 二进制文件，然后放在<strong>操作节点</strong>的 <code>/usr/local/bin</code> 目录下，并给与执行权限。</p><h2 id="创建-Kubernetes-集群"><a href="#创建-Kubernetes-集群" class="headerlink" title="创建 Kubernetes 集群"></a>创建 Kubernetes 集群</h2><p>运行 RKE 命令创建 Kubernetes 集群</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke up --config ./rancher-cluster.yml</span><br></pre></td></tr></table></figure><p>完成后，它应显示：<code>Finished building Kubernetes cluster successfully</code>。</p><h2 id="测试集群"><a href="#测试集群" class="headerlink" title="测试集群"></a>测试集群</h2><p>RKE 会自动创建 <code>kube_config_rancher-cluster.yml</code>。这个文件包含 kubectl 和 helm 访问 K8S 的凭据，请妥善保管。</p><blockquote><p><strong>注意:</strong> 如果您使用的文件不叫 <code>rancher-cluster.yml</code>, 那么这个 <code>kubeconfig</code> 配置文件将被命名为 <code>kube_config_&lt;FILE_NAME&gt;.yml</code>。</p></blockquote><p>通过 <code>kubectl</code> 测试您的连接，并查看您的所有节点是否处于 <code>Ready</code> 状态。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_xxx.yml get nodes</span><br><span class="line"></span><br><span class="line">NAME                          STATUS    ROLES                      AGE       VERSION</span><br><span class="line">165.227.114.63                Ready     controlplane,etcd,worker   11m       v1.10.1</span><br><span class="line">165.227.116.167               Ready     controlplane,etcd,worker   11m       v1.10.1</span><br><span class="line">165.227.127.226               Ready     controlplane,etcd,worker   11m       v1.10.1</span><br></pre></td></tr></table></figure><h2 id="检查集群-Pod-的运行状况"><a href="#检查集群-Pod-的运行状况" class="headerlink" title="检查集群 Pod 的运行状况"></a>检查集群 Pod 的运行状况</h2><ul><li>Pods 是 <code>Running</code> 或者 <code>Completed</code> 状态。</li><li><code>READY</code> 列显示所有正在运行的容器 (i.e. <code>3/3</code>)，<code>STATUS</code> 显示 POD 是 <code>Running</code>。</li><li>Pods 的 <code>STATUS</code> 是 <code>Completed</code> 为 <code>run-one Jobs</code>,这些 pods<code>READY</code> 应该为 <code>0/1</code>。</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_xxx.yml get pods -A</span><br><span class="line"></span><br><span class="line">NAMESPACE       NAME                                      READY     STATUS      RESTARTS   AGE</span><br><span class="line">ingress-nginx   nginx-ingress-controller-tnsn4            1/1       Running     0          30s</span><br><span class="line">ingress-nginx   nginx-ingress-controller-tw2ht            1/1       Running     0          30s</span><br><span class="line">ingress-nginx   nginx-ingress-controller-v874b            1/1       Running     0          30s</span><br><span class="line">kube-system     canal-jp4hz                               3/3       Running     0          30s</span><br><span class="line">kube-system     canal-z2hg8                               3/3       Running     0          30s</span><br><span class="line">kube-system     canal-z6kpw                               3/3       Running     0          30s</span><br><span class="line">kube-system     kube-dns-7588d5b5f5-sf4vh                 3/3       Running     0          30s</span><br><span class="line">kube-system     kube-dns-autoscaler-5db9bbb766-jz2k6      1/1       Running     0          30s</span><br><span class="line">kube-system     metrics-server-97bc649d5-4rl2q            1/1       Running     0          30s</span><br><span class="line">kube-system     rke-ingress-controller-deploy-job-bhzgm   0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-kubedns-addon-deploy-job-gl7t4        0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-metrics-addon-deploy-job-7ljkc        0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-network-plugin-deploy-job-6pbgj       0/1       Completed   0          30s</span><br></pre></td></tr></table></figure><h2 id="Helm-安装-Rancher"><a href="#Helm-安装-Rancher" class="headerlink" title="Helm 安装 Rancher"></a>Helm 安装 Rancher</h2><h3 id="准备-Rancher-Chart"><a href="#准备-Rancher-Chart" class="headerlink" title="准备 Rancher Chart"></a>准备 Rancher Chart</h3><p>访问 <a href="http://mirror.cnrancher.com/">rancher mirror</a> 下载 Rancher Chart 压缩包，解压后应该有 rancher 文件夹。</p><h3 id="创建-k8s-ssl-密文"><a href="#创建-k8s-ssl-密文" class="headerlink" title="创建 k8s ssl 密文"></a>创建 k8s ssl 密文</h3><p>此步骤需要将权威 ssl 证书以密文的形式导入 K8S 集群。</p><ol><li><p>将服务的 crt 文件重命名为 <code>tls.crt</code>，将 key 文件重命名为 <code>tls.key</code>。</p><blockquote><p><strong>特别提示</strong></p></blockquote><p> 在权威机构申请的授信 ssl 证书，一般由三个部分组成：key 文件，crt 文件，还有一个<strong>中间证书</strong>。这个<strong>中间证书</strong>是类似于 CA 的文件，有的平台会把 <strong>中间证书</strong> 直接合并在 crt 文件中，放在 crt 文件内容的最后。比如以下为一个合并了 <strong>中间证书</strong> 的 crt 文件，可以看到有两段 <code>-----BEGIN CERTIFICATE——</code> 和 <code>-----END CERTIFICATE-----</code></p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIIFqTCCBJGgAwIBAgIQAl+S5ainrDPnletBrRbcTzANBgkqhkiG9w0BAQsFADBy</span><br><span class="line">MQswCQYDVQQGEwJDTjElMCMGA1UEChMcVHJ1c3RBc2lhIFRlY2hub2xvZ2llcywg</span><br><span class="line">SW5jLjEdMBsGA1UECxMURG9tYWluIFZhbGlkYXRlZCBTU0wxHTAbBgNVBAMTFFRy</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">AHpHYC2oLh082nOzqEzNyNpnOx9camPK191EDiuAEcfJhp2/pvtzsdOa1BGpatx/</span><br><span class="line">MCiAxDHKFt0flpq1xS7tNqiCwJNhkwBKw6BEDXjMPndN1gueTBWa30FRcJ7PdU4p</span><br><span class="line">g4FMXAjUgRa5zwvcXemscpNrL6Cf56Yr88ToOHDXTtHMvaV+d81i/xeBTWho9Fl9</span><br><span class="line">oYk+/dkctvq0knSLVA==</span><br><span class="line">-----END CERTIFICATE-----</span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIIErjCCA5agAwIBAgIQBYAmfwbylVM0jhwYWl7uLjANBgkqhkiG9w0BAQsFADBh</span><br><span class="line">MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3</span><br><span class="line">d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBD</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">ISLNdMRiDrXntcImDAiRvkh5GJuH4YCVE6XEntqaNIgGkRwxKSgnU3Id3iuFbW9F</span><br><span class="line">UQ9Qqtb1GX91AJ7i4153TikGgYCdwYkBURD8gSVe8OAco6IfZOYt/TEwii1Ivi1C</span><br><span class="line">qnuUlWpsF1LdQNIdfbW3TSe0BhQa7ifbVIfvPWHYOu3rkg1ZeMo6XRU9B4n5VyJY</span><br><span class="line">RmE=</span><br><span class="line">-----END CERTIFICATE-----</span><br></pre></td></tr></table></figure><p> 但是有的平台是把各部分单独保存为一个文件，如下图：</p> <img src="/rancher/install/ha-authority-ssl-ingress/image-20210114225943743.png" class="" title="image-20210114225943743"><p> 这种情况下需要手动把 中间证书 合并到 crt 文件中，可以通过命令 <code>cat xxx &gt;&gt; xxx.crt</code> 把内容合并过去，或者手动打开文件复制粘贴过去。</p></li><li><p>使用 <code>kubectl</code> 创建 <code>tls</code> 类型的 <code>secrets</code>。</p><blockquote><p>注意：文件的名称一定要与下面的名称相同。</p></blockquote> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定配置文件</span></span><br><span class="line"><span class="built_in">export</span> kubeconfig=xxx/xxx/xx.kubeconfig.yml</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> create namespace cattle-system</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> \</span><br><span class="line">    -n cattle-system create \</span><br><span class="line">    secret tls tls-rancher-ingress \</span><br><span class="line">    --cert=./tls.crt \</span><br><span class="line">    --key=./tls.key</span><br></pre></td></tr></table></figure></li></ol><h3 id="安装-Rancher-Server"><a href="#安装-Rancher-Server" class="headerlink" title="安装 Rancher Server"></a>安装 Rancher Server</h3><blockquote><p>更多的 chart 参数可访问 <a href="https://docs.rancher.cn/docs/rancher2/installation_new/install-rancher-on-k8s/chart-options/_index">https://docs.rancher.cn/docs/rancher2/installation_new&#x2F;install-rancher-on-k8s&#x2F;chart-options&#x2F;_index</a></p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定配置文件</span></span><br><span class="line"><span class="built_in">export</span> kubeconfig=xxx/xxx/xx.kubeconfig.yml</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> create namespace cattle-system</span><br><span class="line">helm --kubeconfig=<span class="variable">$kubeconfig</span> install rancher ./rancher \</span><br><span class="line">    --namespace cattle-system \</span><br><span class="line">    --<span class="built_in">set</span> hostname=&lt;您自己的域名&gt; \</span><br><span class="line">    --<span class="built_in">set</span> ingress.tls.source=secret</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意:</strong> 申请证书对应的<code>域名</code>需要与 <code>hostname</code> 选项匹配，否则 <code>ingress</code> 将无法代理访问 Rancher。</p></blockquote><h2 id="可选-为-Agent-Pod-添加主机别名-x2F-etc-x2F-hosts"><a href="#可选-为-Agent-Pod-添加主机别名-x2F-etc-x2F-hosts" class="headerlink" title="(可选)为 Agent Pod 添加主机别名(&#x2F;etc&#x2F;hosts)"></a>(可选)为 Agent Pod 添加主机别名(&#x2F;etc&#x2F;hosts)</h2><p>如果你的环境中没有 DNS 服务器做域名解析，那么这个时候通过 <code>kubectl --kubeconfig=xxx.yaml -n cattle-system get pod</code> 应该是可以看到 agent pod 一直无法正常运行，查看 agent pod 日志会发现是因为连接不上 rancher server url。</p><ul><li><p>解决方法</p><p>可以通过给 <code>cattle-cluster-agent Pod</code> 和 <code>cattle-node-agent pod</code> 添加主机别名(&#x2F;etc&#x2F;hosts)，让其可以正常通过 <code>Rancher Server URL</code> 与 Rancher Server 通信<code>(前提是 IP 地址可以互通)</code>。</p><ol><li><p>cattle-cluster-agent pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> kubeconfig=xxx/xxx/xx.kubeconfig.yml</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">patch deployments cattle-cluster-agent --patch <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">    &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;template&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;hostAliases&quot;: [</span></span><br><span class="line"><span class="string">                    &#123;</span></span><br><span class="line"><span class="string">                        &quot;hostnames&quot;:</span></span><br><span class="line"><span class="string">                        [</span></span><br><span class="line"><span class="string">                            &quot;demo.cnrancher.com&quot;</span></span><br><span class="line"><span class="string">                        ],</span></span><br><span class="line"><span class="string">                            &quot;ip&quot;: &quot;192.168.1.100&quot;</span></span><br><span class="line"><span class="string">                    &#125;</span></span><br><span class="line"><span class="string">                ]</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p>cattle-node-agent pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> kubeconfig=xxx/xxx/xx.kubeconfig.yml</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">patch  daemonsets cattle-node-agent --patch <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">    &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;template&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;hostAliases&quot;: [</span></span><br><span class="line"><span class="string">                    &#123;</span></span><br><span class="line"><span class="string">                        &quot;hostnames&quot;:</span></span><br><span class="line"><span class="string">                        [</span></span><br><span class="line"><span class="string">                            &quot;xxx.rancher.com&quot;</span></span><br><span class="line"><span class="string">                        ],</span></span><br><span class="line"><span class="string">                            &quot;ip&quot;: &quot;192.168.1.100&quot;</span></span><br><span class="line"><span class="string">                    &#125;</span></span><br><span class="line"><span class="string">                ]</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong> 1、替换其中的域名和 IP<br>2、别忘记 json 中的引号。</p></blockquote></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>权威 ssl 证书 + Nodeport +  外部 L7 + 安装 Rancher HA</title>
      <link href="/rancher/install/ha-nodeport-external-l7/"/>
      <url>/rancher/install/ha-nodeport-external-l7/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/ha-nodeport-external-l7/" target="_blank" title="https://www.xtplayer.cn/rancher/install/ha-nodeport-external-l7/">https://www.xtplayer.cn/rancher/install/ha-nodeport-external-l7/</a></p><h2 id="架构说明"><a href="#架构说明" class="headerlink" title="架构说明"></a>架构说明</h2><img src="/rancher/install/ha-nodeport-external-l7/http-rancher.26f8b994.26f8b994.png" class="" title="Rancher HA"><h2 id="节点软硬件要求"><a href="#节点软硬件要求" class="headerlink" title="节点软硬件要求"></a>节点软硬件要求</h2><p>节点硬件要求请参考：<a href="https://docs.rancher.cn/docs/rancher2/installation_new/requirements/_index/">https://docs.rancher.cn/docs/rancher2/installation_new&#x2F;requirements&#x2F;_index&#x2F;</a></p><h2 id="节点基础环境配置"><a href="#节点基础环境配置" class="headerlink" title="节点基础环境配置"></a>节点基础环境配置</h2><p>请参考 <a href="/kubernetes/k8s-basic-environment-configuration/">基础环境配置</a></p><h2 id="同步镜像"><a href="#同步镜像" class="headerlink" title="同步镜像"></a>同步镜像</h2><p>如果你是在离线环境安装，请先访问<a href="https://www.xtplayer.cn/rancher/rancher-install-offline-images/">rancher 离线安装镜像同步</a>，按照方法同步所有镜像到离线私有镜像仓库。如果主机能够直接拉取镜像，则跳过此步骤。</p><h2 id="创建-rke-配置文件"><a href="#创建-rke-配置文件" class="headerlink" title="创建 rke 配置文件"></a>创建 rke 配置文件</h2><p>使用下面的示例创建 <code>rancher-cluster.yml</code> 文件，使用创建的 3 个节点的 IP 地址或域名替换列表中的 IP 地址。</p><blockquote><p><strong>注意:</strong> 如果节点有<code>公网地址 和 内网地址</code>地址，建议手动设置 <code>internal_address:</code>以便 Kubernetes 将内网地址用于集群内部通信。如果需要开启自动配置安全组或防火墙，某些服务(如 AWS EC2)需要设置 <code>internal_address:</code>。</p></blockquote><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.114</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.22</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.116</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.32</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.127</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.42</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果要使用私有仓库中的镜像，配置以下参数来指定默认私有仓库地址。</span></span><br><span class="line"><span class="comment">##private_registries:</span></span><br><span class="line"><span class="comment">##    - url: registry.com</span></span><br><span class="line"><span class="comment">##      user: Username</span></span><br><span class="line"><span class="comment">##      password: password</span></span><br><span class="line"><span class="comment">##      is_default: true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">etcd:</span></span><br><span class="line">    <span class="comment"># 扩展参数 https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">auto-compaction-mode:</span> <span class="string">periodic</span></span><br><span class="line">      <span class="attr">auto-compaction-retention:</span> <span class="string">60m</span></span><br><span class="line">      <span class="comment"># 修改空间配额为$((6*1024*1024*1024))，默认 2G,最大 8G</span></span><br><span class="line">      <span class="attr">quota-backend-bytes:</span> <span class="string">&#x27;6442450944&#x27;</span></span><br><span class="line">      <span class="attr">snapshot-count:</span> <span class="number">50000</span></span><br><span class="line">    <span class="comment"># 自动备份</span></span><br><span class="line">    <span class="comment">## rke 版本小于 0.2.x 或 rancher 版本小于 v2.2.0 时使用</span></span><br><span class="line">    <span class="attr">snapshot:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">creation:</span> <span class="string">6h</span></span><br><span class="line">    <span class="attr">retention:</span> <span class="string">24h</span></span><br><span class="line">    <span class="comment">## rke 版本大于等于 0.2.x 或 rancher 版本大于等于 v2.2.0 时使用(两段配置二选一)</span></span><br><span class="line">    <span class="attr">backup_config:</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">true</span>         <span class="comment"># 设置 true 启用 ETCD 自动备份，设置 false 禁用；</span></span><br><span class="line">      <span class="attr">interval_hours:</span> <span class="number">12</span>    <span class="comment"># 快照创建间隔时间，不加此参数，默认 5 分钟；</span></span><br><span class="line">      <span class="attr">retention:</span> <span class="number">6</span>          <span class="comment"># etcd 备份保留份数；</span></span><br></pre></td></tr></table></figure><blockquote><p>更多 etcd 配置请参考: <a href="https://www.xtplayer.cn/etcd/etcd-optimize/">etcd 优化</a></p></blockquote><h3 id="RKE-节点参数说明"><a href="#RKE-节点参数说明" class="headerlink" title="RKE 节点参数说明"></a>RKE 节点参数说明</h3><table><thead><tr><th>参数</th><th>必须</th><th>描述</th></tr></thead><tbody><tr><td><code>address</code></td><td>yes</td><td>公共域名或 IP 地址</td></tr><tr><td><code>user</code></td><td>yes</td><td>可以运行 docker 命令的用户</td></tr><tr><td><code>role</code></td><td>yes</td><td>分配给节点的 Kubernetes 角色列表</td></tr><tr><td><code>internal_address</code></td><td>no</td><td>内部集群通信的私有域名或 IP 地址</td></tr><tr><td><code>ssh_key_path</code></td><td>no</td><td>用于对节点进行身份验证的 SSH 私钥的路径（默认为~&#x2F;.ssh&#x2F;id_rsa）</td></tr></tbody></table><h3 id="镜像仓库配置"><a href="#镜像仓库配置" class="headerlink" title="镜像仓库配置"></a>镜像仓库配置</h3><p>如果需要使用自己镜像仓库的镜像或者是离线安装，那么可以在 <code>private_registries</code> 字段中配置私有镜像仓库信息。</p><table><thead><tr><th>选项</th><th>值</th><th>描述</th></tr></thead><tbody><tr><td><code>url</code></td><td>‘’</td><td>镜像仓库地址</td></tr><tr><td><code>user</code></td><td>‘’</td><td>镜像仓库用户名</td></tr><tr><td><code>password</code></td><td>‘’</td><td>镜像仓库密码</td></tr><tr><td><code>is_default</code></td><td><code>true/false</code></td><td>这个参数很重要，当配置这个参数后，<br/>rke 会自动为系统镜像添加镜像仓库前缀，<br/>比如 rancher&#x2F;rancher 会变为 192.168.1.1&#x2F;rancher&#x2F;rancher，<br/>用于离线环境或者使用自己的私有仓库构建集群。</td></tr></tbody></table><blockquote><p>完整的配置示例，请参考<a href="https://docs.rancher.cn/docs/rke/example-yamls/_index/">完整-cluster-yml-示例</a></p></blockquote><h3 id="高级配置"><a href="#高级配置" class="headerlink" title="高级配置"></a>高级配置</h3><p>RKE 有许多配置选项可用于自定义安装以适合您的特定环境，有关选项和功能的完整列表，请查看<a href="https://docs.rancher.cn/docs/rke/config-options/_index/">RKE 文档</a> 。</p><h2 id="安装-rke、kubectl、helm-工具"><a href="#安装-rke、kubectl、helm-工具" class="headerlink" title="安装 rke、kubectl、helm 工具"></a>安装 rke、kubectl、helm 工具</h2><p>需要选择一个节点用来运行 rke、kubectl、helm 等命令，你可以叫它为 <strong>操作节点</strong>。这个节点可以是 local 集群中的任意一台，也可以是专门准备的独立的一个节点。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rke</span></span><br><span class="line">sudo curl -o /usr/local/bin/rke http://rancher-mirror.cnrancher.com/rke/v1.2.4/rke_linux-amd64</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/rke</span><br><span class="line"><span class="comment"># kubectl</span></span><br><span class="line">sudo curl -o /usr/local/bin/kubectl http://rancher-mirror.cnrancher.com/kubectl/v1.20.0/linux-amd64-v1.20.0-kubectl</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/kubectl</span><br><span class="line"><span class="comment"># helm</span></span><br><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">sudo curl -O http://rancher-mirror.cnrancher.com/helm/v3.5.0/helm-v3.5.0-linux-amd64.tar.gz;</span><br><span class="line">sudo tar -zxf helm*tar.gz;</span><br><span class="line">sudo <span class="built_in">cp</span> -rf /tmp/linux-amd64/helm /usr/local/bin/helm</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/helm</span><br></pre></td></tr></table></figure><p>或者访问 <a href="http://mirror.cnrancher.com/">http://mirror.cnrancher.com/</a> 下载 rke、kubectl、helm 二进制文件，然后放在<strong>操作节点</strong>的 <code>/usr/local/bin</code> 目录下，并给与执行权限。</p><h2 id="创建-Kubernetes-集群"><a href="#创建-Kubernetes-集群" class="headerlink" title="创建 Kubernetes 集群"></a>创建 Kubernetes 集群</h2><p>运行 RKE 命令创建 Kubernetes 集群</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke up --config ./rancher-cluster.yml</span><br></pre></td></tr></table></figure><p>完成后，它应显示：<code>Finished building Kubernetes cluster successfully</code>。</p><h2 id="测试集群"><a href="#测试集群" class="headerlink" title="测试集群"></a>测试集群</h2><p>RKE 会自动创建 <code>kube_config_rancher-cluster.yml</code>。这个文件包含 kubectl 和 helm 访问 K8S 的凭据，请妥善保管。</p><blockquote><p><strong>注意:</strong> 如果您使用的文件不叫 <code>rancher-cluster.yml</code>, 那么这个 <code>kubeconfig</code> 配置文件将被命名为 <code>kube_config_&lt;FILE_NAME&gt;.yml</code>。</p></blockquote><p>通过 <code>kubectl</code> 测试您的连接，并查看您的所有节点是否处于 <code>Ready</code> 状态。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_xxx.yml get nodes</span><br><span class="line"></span><br><span class="line">NAME                          STATUS    ROLES                      AGE       VERSION</span><br><span class="line">165.227.114.63                Ready     controlplane,etcd,worker   11m       v1.10.1</span><br><span class="line">165.227.116.167               Ready     controlplane,etcd,worker   11m       v1.10.1</span><br><span class="line">165.227.127.226               Ready     controlplane,etcd,worker   11m       v1.10.1</span><br></pre></td></tr></table></figure><h2 id="检查集群-Pod-的运行状况"><a href="#检查集群-Pod-的运行状况" class="headerlink" title="检查集群 Pod 的运行状况"></a>检查集群 Pod 的运行状况</h2><ul><li>Pods 是 <code>Running</code> 或者 <code>Completed</code> 状态。</li><li><code>READY</code> 列显示所有正在运行的容器 (i.e. <code>3/3</code>)，<code>STATUS</code> 显示 POD 是 <code>Running</code>。</li><li>Pods 的 <code>STATUS</code> 是 <code>Completed</code> 为 <code>run-one Jobs</code>,这些 pods<code>READY</code> 应该为 <code>0/1</code>。</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_xxx.yml get pods -A</span><br><span class="line"></span><br><span class="line">NAMESPACE       NAME                                      READY     STATUS      RESTARTS   AGE</span><br><span class="line">ingress-nginx   nginx-ingress-controller-tnsn4            1/1       Running     0          30s</span><br><span class="line">ingress-nginx   nginx-ingress-controller-tw2ht            1/1       Running     0          30s</span><br><span class="line">ingress-nginx   nginx-ingress-controller-v874b            1/1       Running     0          30s</span><br><span class="line">kube-system     canal-jp4hz                               3/3       Running     0          30s</span><br><span class="line">kube-system     canal-z2hg8                               3/3       Running     0          30s</span><br><span class="line">kube-system     canal-z6kpw                               3/3       Running     0          30s</span><br><span class="line">kube-system     kube-dns-7588d5b5f5-sf4vh                 3/3       Running     0          30s</span><br><span class="line">kube-system     kube-dns-autoscaler-5db9bbb766-jz2k6      1/1       Running     0          30s</span><br><span class="line">kube-system     metrics-server-97bc649d5-4rl2q            1/1       Running     0          30s</span><br><span class="line">kube-system     rke-ingress-controller-deploy-job-bhzgm   0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-kubedns-addon-deploy-job-gl7t4        0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-metrics-addon-deploy-job-7ljkc        0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-network-plugin-deploy-job-6pbgj       0/1       Completed   0          30s</span><br></pre></td></tr></table></figure><h2 id="Helm-安装-Rancher"><a href="#Helm-安装-Rancher" class="headerlink" title="Helm 安装 Rancher"></a>Helm 安装 Rancher</h2><p>有的场景，外部有七层负载均衡器作为 ssl 终止，常见用法是把负载均衡器的 <code>443</code> 端口代理到内部应用的<code>非 https 端口上，比如 80</code>。</p><p>为了保证网络转发性能，这里禁用了内置的 ingress 服务，以 <code>NodePort 方式</code> 把 Rancher Server 容器的 <code>80</code> 端口映射到宿主机 <code>30303</code> 端口上。再把外部七层负载均衡器的 <code>443</code> 端口反向代理到 Rancher Server 对应的 NodePort 端口上。访问外部七层负载均衡器的 <code>443</code> 端口的流量将转发到 Rancher Server 容器的 <code>80</code> 端口。</p><h3 id="准备-chart"><a href="#准备-chart" class="headerlink" title="准备 chart"></a>准备 chart</h3><p>Chart 地址: <a href="https://gitee.com/rancher/server-chart.git">https://gitee.com/rancher/server-chart.git</a></p><ul><li>本 Chart 基于 <a href="https://github.com/rancher/server-chart/">https://github.com/rancher/server-chart/</a> 修改，当前支持版本为 <code>rancher v2.1.7、v2.1.8、v2.1.9、v2.2.0、v2.2.1、v2.2.2、v2.2.3、v2.2.4、v2.2.5、v2.2.6 v2.2.7 v2.2.8 v2.2.9 v2.3.3</code>。</li><li>不支持 LetsEncrypt、cert-manager 提供证书，需手动通过 Secret 导入证书， 默认开启审计日志功能。</li></ul><h3 id="制作自签名证书或重命名权威认证证书"><a href="#制作自签名证书或重命名权威认证证书" class="headerlink" title="制作自签名证书或重命名权威认证证书"></a>制作自签名证书或重命名权威认证证书</h3><ul><li>仓库根目录有一键创建自签名证书脚本，会自动创建 <code>cacerts.pem</code>、<code>tls.key</code>、<code>tls.crt</code>；</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--ssl-domain: 生成 ssl 证书需要的主域名，如不指定则默认为 www.rancher.local，如果是 ip 访问服务，则可忽略；</span><br><span class="line">--ssl-trusted-ip: 一般 ssl 证书只信任域名的访问请求，有时候需要使用 ip 去访问 server，那么需要给 ssl 证书添加扩展 IP，多个 IP 用逗号隔开；</span><br><span class="line">--ssl-trusted-domain: 如果想多个域名访问，则添加扩展域名（TRUSTED_DOMAIN）,多个 TRUSTED_DOMAIN 用逗号隔开；</span><br><span class="line">--ssl-size: ssl 加密位数，默认 2048；</span><br><span class="line">--ssl-cn: 国家代码(2 个字母的代号)，默认 CN；</span><br><span class="line">使用示例:</span><br><span class="line">./create_self-signed-cert.sh --ssl-domain=www.test.com --ssl-trusted-domain=www.test2.com \</span><br><span class="line">--ssl-trusted-ip=1.1.1.1,2.2.2.2,3.3.3.3 --ssl-size=2048 --ssl-date=3650</span><br></pre></td></tr></table></figure><ul><li>如果使用权威认证证书，需要重命名 crt 和 key 为 <code>tls.crt</code> 和 <code>tls.key</code>。</li></ul><h3 id="准备-SSL-证书密文"><a href="#准备-SSL-证书密文" class="headerlink" title="准备 SSL 证书密文"></a>准备 SSL 证书密文</h3><p>把 CA 证书作为密文导入 K8S，如果是使用权威认证证书，可以跳过此步骤。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定 kubeconfig 配置文件路径</span></span><br><span class="line">kubeconfig=xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 cattle-system 命名空间</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> \</span><br><span class="line">  create namespace cattle-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CA 证书密文</span></span><br><span class="line"><span class="comment">## 如果是使用权威认证证书，可以跳过此步骤。</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> \</span><br><span class="line">  -n cattle-system create \</span><br><span class="line">  secret generic tls-ca \</span><br><span class="line">  --from-file=cacerts.pem</span><br></pre></td></tr></table></figure><h3 id="helm-安装-Rancher"><a href="#helm-安装-Rancher" class="headerlink" title="helm 安装 Rancher"></a>helm 安装 Rancher</h3><ul><li>helm 3.x 安装</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b v2.4.11 https://gitee.com/rancher/server-chart.git</span><br><span class="line"></span><br><span class="line">kubeconfig=xxx.yml</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> create namespace cattle-system</span><br><span class="line"></span><br><span class="line">helm --kubeconfig=<span class="variable">$kubeconfig</span> \</span><br><span class="line">  install rancher \</span><br><span class="line">  --namespace cattle-system \</span><br><span class="line">  --<span class="built_in">set</span> rancherImage=rancher/rancher \</span><br><span class="line">  --<span class="built_in">set</span> busyboxImage=rancher/busybox \</span><br><span class="line">  --<span class="built_in">set</span> service.type=NodePort \</span><br><span class="line">  --<span class="built_in">set</span> service.ports.nodePort=30303 \</span><br><span class="line">  --<span class="built_in">set</span> tls=external \</span><br><span class="line">  --<span class="built_in">set</span> privateCA=<span class="literal">true</span> \</span><br><span class="line">  --<span class="built_in">set</span> useBundledSystemChart=<span class="literal">true</span> \</span><br><span class="line">  server-chart/rancher</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意:</strong></p></blockquote><ol><li>通过 <code>--kubeconfig=</code>指定 kubectl 配置文件;</li><li>如果使用权威 ssl 证书，则去除 <code>--set privateCA=true</code>;</li><li>如果为离线安装，设置 <code>--set privateRegistry=true</code> 使用私有仓库，并且使用 <code>--set systemDefaultRegistry=</code>指定离线私有仓库地址，注意不要添加协议头（http 或者 https）;</li><li>如果镜像名非标准 rancher 镜像名，可通过 <code>--set rancherImage=</code>指定镜像名称，不要指定镜像版本，系统会根据 chart 版本自动获取镜像版本;</li><li>默认自动获取 chart 版本号作为 Rancher 镜像版本号，如果想指定镜像版本号，可通过配置 <code>--set rancherImageTag=v2.3.x</code> 来指定;</li><li>点击查看更多<a href="https://docs.rancher.cn/docs/rancher2/installation_new/install-rancher-on-k8s/chart-options/_index">Chart 设置参数</a>;</li><li>如果 rancher server 要访问自签名 ssl 证书的服务，比如自签名证书的 git 服务，那么需要提前把 CA 文件映射到 rancher server 容器从，rancher server 可以通过此 CA 去校验自签名 ssl 证书。</li></ol><h2 id="配置外部七层负载均衡器"><a href="#配置外部七层负载均衡器" class="headerlink" title="配置外部七层负载均衡器"></a>配置外部七层负载均衡器</h2><h3 id="NGINX-负载均衡器"><a href="#NGINX-负载均衡器" class="headerlink" title="NGINX 负载均衡器"></a>NGINX 负载均衡器</h3><p>默认情况下，rancher 容器会将 80 端口上的请求重定向到 443 端口上。如果 Rancher Server 通过负载均衡器来代理，这个时候请求是通过负载均衡器发送给 Rancher Server，而并非客户端直接访问 Rancher Server。在非全局 <code>https</code> 的环境中，如果以外部负载均衡器作为 ssl 终止，这个时候通过负载均衡器的 <code>https</code> 请求将需要被反向代理到 Rancher Server http(80)上。在负载均衡器上配置 <code>X-Forwarded-Proto: https</code> 参数，Rancher Server http(80)上收到负载均衡器的请求后，就不会再重定向到 https(443)上。</p><p>负载均衡器或代理必须支持以下参数:</p><ul><li><strong>WebSocket</strong> 连接</li><li><strong>SPDY</strong>&#x2F;<strong>HTTP&#x2F;2</strong>协议</li><li>传递&#x2F;设置以下 headers:</li></ul><table><thead><tr><th>Header</th><th>Value</th><th align="left">描述</th></tr></thead><tbody><tr><td><code>Host</code></td><td>传递给 Rancher 的主机名</td><td align="left">识别客户端请求的主机名。</td></tr><tr><td><code>X-Forwarded-Proto</code></td><td><code>https</code></td><td align="left">识别客户端用于连接负载均衡器的协议。<strong>注意：</strong>如果存在此标头，<code>rancher/rancher</code> 不会将 HTTP 重定向到 HTTPS。</td></tr><tr><td><code>X-Forwarded-Port</code></td><td>Port used to reach Rancher.</td><td align="left">识别客户端用于连接负载均衡器的端口。</td></tr><tr><td><code>X-Forwarded-For</code></td><td>IP of the client connection.</td><td align="left">识别客户端的原始 IP 地址。</td></tr></tbody></table><h4 id="nginx-配置示例"><a href="#nginx-配置示例" class="headerlink" title="nginx 配置示例"></a>nginx 配置示例</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">worker_processes 4;</span><br><span class="line">worker_rlimit_nofile 40000;</span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 8192;</span><br><span class="line">&#125;</span><br><span class="line">http &#123;</span><br><span class="line">    upstream rancher &#123;</span><br><span class="line">        server IP_NODE_1:30303;</span><br><span class="line">        server IP_NODE_2:30303;</span><br><span class="line">        server IP_NODE_3:30303;</span><br><span class="line">    &#125;</span><br><span class="line">    map <span class="variable">$http_upgrade</span> <span class="variable">$connection_upgrade</span> &#123;</span><br><span class="line">        default Upgrade;</span><br><span class="line">        <span class="string">&#x27;&#x27;</span>      close;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 443 ssl http2; <span class="comment"># 如果是升级或者全新安装 v2.2.2,需要禁止 http2，其他版本不需修改。</span></span><br><span class="line">        server_name FQDN;</span><br><span class="line">        ssl_certificate &lt;tls.crt&gt;;  <span class="comment"># 这里使用安装 Rancher 时候创建的自签名或者权威 ssl 证书。</span></span><br><span class="line">        ssl_certificate_key &lt;tls.key&gt;;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_set_header Host <span class="variable">$host</span>;</span><br><span class="line">            proxy_set_header X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">            proxy_set_header X-Forwarded-Port <span class="variable">$server_port</span>;</span><br><span class="line">            proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">            proxy_pass http://rancher;</span><br><span class="line">            proxy_http_version 1.1;</span><br><span class="line">            proxy_set_header Upgrade <span class="variable">$http_upgrade</span>;</span><br><span class="line">            proxy_set_header Connection <span class="variable">$connection_upgrade</span>;</span><br><span class="line">            <span class="comment"># This allows the ability for the execute shell window to remain open for up to 15 minutes. </span></span><br><span class="line">            <span class="comment">## Without this parameter, the default is 1 minute and will automatically close.</span></span><br><span class="line">            proxy_read_timeout 900s;</span><br><span class="line">            proxy_buffering off;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 80;</span><br><span class="line">        server_name FQDN;</span><br><span class="line">        <span class="built_in">return</span> 301 https://$server_name<span class="variable">$request_uri</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>为了减少网络传输的数据量，可以在七层代理的 <code>http</code> 定义中添加 <code>GZIP</code> 功能。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Gzip Settings</span></span><br><span class="line">gzip on;</span><br><span class="line">gzip_disable <span class="string">&quot;msie6&quot;</span>;</span><br><span class="line">gzip_disable <span class="string">&quot;MSIE [1-6]\.(?!.*SV1)&quot;</span>;</span><br><span class="line">gzip_vary on;</span><br><span class="line">gzip_static on;</span><br><span class="line">gzip_proxied any;</span><br><span class="line">gzip_min_length 0;</span><br><span class="line">gzip_comp_level 8;</span><br><span class="line">gzip_buffers 16 8k;</span><br><span class="line">gzip_http_version 1.1;</span><br><span class="line">gzip_types</span><br><span class="line">  text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml application/font-woff</span><br><span class="line">  text/javascript application/javascript application/x-javascript</span><br><span class="line">  text/x-json application/json application/x-web-app-manifest+json</span><br><span class="line">  text/css text/plain text/x-component</span><br><span class="line">  font/opentype application/x-font-ttf application/vnd.ms-fontobject font/woff2</span><br><span class="line">  image/x-icon image/png image/jpeg;</span><br></pre></td></tr></table></figure><h3 id="F5-七层代理"><a href="#F5-七层代理" class="headerlink" title="F5 七层代理"></a>F5 七层代理</h3><p>本文档基于 <code>BIGIP-15.0.1-0.0.11.ALL-vmware</code> 虚拟机版本编写。</p><h4 id="配置健康检查"><a href="#配置健康检查" class="headerlink" title="配置健康检查"></a>配置健康检查</h4><p>点击 <code>Local Traffice</code>，然后点击 <code>Monitors</code> 旁边的<code>➕</code>。Type 中选择类型为 <code>http</code>，在 <code>Send String</code> 中填写参数为:<code>GET /healthz \r\n</code>。</p><img src="/rancher/install/ha-nodeport-external-l7/image-20191230132530250.94d111c1.png" class="" title="image-20191230132530250"><h4 id="添加节点（可选）"><a href="#添加节点（可选）" class="headerlink" title="添加节点（可选）"></a>添加节点（可选）</h4><p>可以在添加 <code>node pool</code> 的时候添加节点，所以当前这一步可以跳过。也可以在这一步配置好节点，添加 <code>node pool</code> 的时候直接选择节点。</p><ol><li><p>点击 <code>Local Traffice</code>，然后点击 <code>Nodes &gt; Nodes List</code> 旁边的<code>➕</code>。</p><img src="/rancher/install/ha-nodeport-external-l7/image-20191230140934898.46869448.png" class="" title="image-20191230140934898"></li><li><p>配置节点信息</p><blockquote><p>注意 <code>Health Monitors</code> 设置为 <code>None</code></p></blockquote><img src="/rancher/install/ha-nodeport-external-l7/image-20191230143159443.8e72cd71.png" class="" title="image-20191230143159443"></li></ol><h4 id="添加-pool"><a href="#添加-pool" class="headerlink" title="添加 pool"></a>添加 pool</h4><ol><li><p>点击 <code>Local Traffice &gt; Pools &gt; Pool List</code> 旁边的<code>➕</code></p><img src="/rancher/install/ha-nodeport-external-l7/image-20191230143703891.a2d01bd4.png" class="" title="image-20191230143703891"></li><li><p>配置节点池</p><ol><li>设置节点池名称；</li><li><code>Health Monitors</code> 选择前面步骤创建的健康检查；</li><li><code>New Members</code> 中选择 <code>Node List</code>，选择开始添加的节点，点击下方的 <code>Add</code>，端口以实际 Rancher 暴露的端口为准；</li></ol><img src="/rancher/install/ha-nodeport-external-l7/image-20191230143942453.04ea9f96.png" class="" title="image-20191230143942453"></li></ol><h4 id="添加-irule"><a href="#添加-irule" class="headerlink" title="添加 irule"></a>添加 irule</h4><ol><li><p>待添加内容</p> <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">when HTTP_REQUEST <span class="punctuation">&#123;</span></span><br><span class="line">  HTTP<span class="punctuation">:</span><span class="punctuation">:</span>header insert “X-Forwarded-Proto” “https”;</span><br><span class="line">  HTTP<span class="punctuation">:</span><span class="punctuation">:</span>header insert “X-Forwarded-Port” <span class="number">443</span>;</span><br><span class="line">  HTTP<span class="punctuation">:</span><span class="punctuation">:</span>header insert “X-Forwarded-For” <span class="punctuation">[</span>IP<span class="punctuation">:</span><span class="punctuation">:</span>client_addr<span class="punctuation">]</span>;</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>点击 <code>Local Traffice &gt; iRules &gt; iRules List</code> 旁边的<code>➕</code><img src="/rancher/install/ha-nodeport-external-l7/image-20191230150056645.f3dc0e06.png" class="" title="image-20191230150056645"></p></li><li><p>设置 irule 名称，粘贴以下内容 <img src="/rancher/install/ha-nodeport-external-l7/image-20191227125418437.cb6be378.png" class="" title="image-20191227125418437"></p></li></ol><h4 id="添加证书"><a href="#添加证书" class="headerlink" title="添加证书"></a>添加证书</h4><ol><li><p>访问 <code>system &gt; Certificate Management &gt; Traffic Certificate Management &gt; SSL Certificate List &gt; Import</code></p><img src="/rancher/install/ha-nodeport-external-l7/image-20191230153056971.511089ab.png" class="" title="image-20191230153056971"></li><li><p>分别选择导入类型为 <code>key 和 certificate</code>，然后选择导入文件。</p><img src="/rancher/install/ha-nodeport-external-l7/image-20191230160624221.5e36c0c0.png" class="" title="image-20191230160624221"><img src="/rancher/install/ha-nodeport-external-l7/image-20191230160708555.4175d0d2.png" class="" title="image-20191230160708555"><blockquote><p><strong>注意:</strong> 权威证书会有中间链 CA 证书，所以这里会多一个 CA 证书，如果是自签名证书则可以忽略这个 CA 证书。</p></blockquote><img src="/rancher/install/ha-nodeport-external-l7/image-20191230160808182.c5ed6bba.png" class="" title="image-20191230160808182"></li></ol><h4 id="添加-SSL-Profile"><a href="#添加-SSL-Profile" class="headerlink" title="添加 SSL Profile"></a>添加 SSL Profile</h4><ol><li><p>点击 <code>Local Traffice &gt; Profiles &gt; SSL &gt; client</code> 旁边的<code>➕</code><img src="/rancher/install/ha-nodeport-external-l7/image-20191230161142099.15f53ddd.png" class="" title="image-20191230161142099"></p></li><li><p>配置 SSL 相关参数</p><ol><li><p>设置 <code>Name</code>；</p></li><li><p><code>Configuration</code> 选择<code>高级</code>，并点击右侧<code>自定义</code>；</p></li><li><p><code>Certificate Key Chain</code> 处点击 <code>Add</code>，然后选择对应的证书和私钥；</p><img src="/rancher/install/ha-nodeport-external-l7/image-20191230161452425.621ccb6f.png" class="" title="image-20191230161452425"><blockquote><p>添加之后</p></blockquote><img src="/rancher/install/ha-nodeport-external-l7/image-20191230161524914.647d0f23.png" class="" title="image-20191230161524914"></li><li><p>其他参数保持默认。</p></li></ol></li></ol><h4 id="配置-Rancher-Virtual-Servers"><a href="#配置-Rancher-Virtual-Servers" class="headerlink" title="配置 Rancher Virtual Servers"></a>配置 Rancher Virtual Servers</h4><ol><li><p>点击 <code>Local Traffice &gt; Virtual Servers &gt; Virtual Server List</code> 旁边的<code>➕</code>；</p></li><li><p>配置 <code>Name</code>;</p></li><li><p>保持 <code>Type</code> 为默认；</p></li><li><p>配置 <code>Source Address</code> 为 <code>0.0.0.0/0</code>;</p></li><li><p>根据实际情况配置 <code>Destination Address/Mask;</code></p></li><li><p><code>Service Port</code> 设置为 <code>443/https</code>;</p></li><li><p><code>Configuration</code> 选择高级；</p></li><li><p><code>HTTP Profile (Client)</code>设置为 <code>http</code>；</p></li><li><p><code>SSL Profile (Client)</code>选择之前创建的 SSL Profile；</p></li><li><p><code>WebSocket Profile</code> 选择 <code>WebSocket</code>；</p></li><li><p><code>Source Address Translation</code> 选择 <code>auto map</code>；</p></li><li><p>在 <code>Resources\iRules</code> 中选择之前创建 iRules 规则；</p></li><li><p><code>Default Pool</code> 选择之前创建的主机池；</p></li><li><p>完整配置</p><img src="/rancher/install/ha-nodeport-external-l7/image-20191230164316342.50b55ff5.png" class="" title="image-20191230164316342"><img src="/rancher/install/ha-nodeport-external-l7/image-20191230164413097.682978b8.png" class="" title="image-20191230164413097"><img src="/rancher/install/ha-nodeport-external-l7/image-20191230164433577.723ba726.png" class="" title="image-20191230164433577"><img src="/rancher/install/ha-nodeport-external-l7/image-20191230164450851.8c98a0ed.png" class="" title="image-20191230164450851"><img src="/rancher/install/ha-nodeport-external-l7/image-20191230164501560.8e9a8de8.png" class="" title="image-20191230164501560"></li></ol><h4 id="HTTP-重定向-HTTPS"><a href="#HTTP-重定向-HTTPS" class="headerlink" title="HTTP 重定向 HTTPS"></a>HTTP 重定向 HTTPS</h4><p>点击 <code>Local Traffice &gt; Virtual Servers &gt; Virtual Server List</code> 旁边的<code>➕</code>；</p><ol><li>配置 <code>Name</code>;</li><li>保持 <code>Type</code> 为默认；</li><li>配置 <code>Source Address</code> 为 <code>0.0.0.0/0</code>;</li><li>根据实际情况配置 <code>Destination Address/Mask;</code></li><li><code>Service Port</code> 设置为 <code>80/http</code>;</li><li><code>Configuration</code> 选择<code>基础</code>；</li><li><code>HTTP Profile (Client)</code>设置为 <code>http</code>；</li><li>在 <code>Resources\iRules</code> 中选择<code>_sys_https_redirect</code>；</li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自签名 ssl 证书 + ingress + 安装 Rancher HA</title>
      <link href="/rancher/install/ha-self-signed-ssl-ingress/"/>
      <url>/rancher/install/ha-self-signed-ssl-ingress/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/ha-self-signed-ssl-ingress/" target="_blank" title="https://www.xtplayer.cn/rancher/install/ha-self-signed-ssl-ingress/">https://www.xtplayer.cn/rancher/install/ha-self-signed-ssl-ingress/</a></p><h2 id="节点软硬件要求"><a href="#节点软硬件要求" class="headerlink" title="节点软硬件要求"></a>节点软硬件要求</h2><p>节点硬件要求请参考：<a href="https://docs.rancher.cn/docs/rancher2/installation_new/requirements/_index/">https://docs.rancher.cn/docs/rancher2/installation_new&#x2F;requirements&#x2F;_index&#x2F;</a></p><h2 id="节点基础环境配置"><a href="#节点基础环境配置" class="headerlink" title="节点基础环境配置"></a>节点基础环境配置</h2><p>请参考 <a href="/kubernetes/k8s-basic-environment-configuration/">基础环境配置</a></p><h2 id="同步镜像"><a href="#同步镜像" class="headerlink" title="同步镜像"></a>同步镜像</h2><p>如果你是在离线环境安装，请先访问 <a href="https://www.xtplayer.cn/rancher/rancher-install-offline-images/">rancher 离线安装镜像同步</a>，按照方法同步所有镜像到离线私有镜像仓库。如果主机能够直接拉取镜像，则跳过此步骤。</p><h2 id="创建-rke-配置文件"><a href="#创建-rke-配置文件" class="headerlink" title="创建 rke 配置文件"></a>创建 rke 配置文件</h2><p>使用下面的示例创建 <code>rancher-cluster.yml</code> 文件，使用创建的 3 个节点的 IP 地址或域名替换列表中的 IP 地址。</p><blockquote><p><strong>注意:</strong> 如果节点有<code>公网地址 和 内网地址</code>地址，建议手动设置 <code>internal_address:</code>以便 Kubernetes 将内网地址用于集群内部通信。如果需要开启自动配置安全组或防火墙，某些服务(如 AWS EC2)需要设置 <code>internal_address:</code>。</p></blockquote><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.114</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.22</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.116</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.32</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">165.227</span><span class="number">.127</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">internal_address:</span> <span class="number">172.16</span><span class="number">.42</span><span class="string">.x</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">role:</span> [<span class="string">controlplane</span>,<span class="string">worker</span>,<span class="string">etcd</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果要使用私有仓库中的镜像，配置以下参数来指定默认私有仓库地址。</span></span><br><span class="line"><span class="comment">##private_registries:</span></span><br><span class="line"><span class="comment">##    - url: registry.com</span></span><br><span class="line"><span class="comment">##      user: Username</span></span><br><span class="line"><span class="comment">##      password: password</span></span><br><span class="line"><span class="comment">##      is_default: true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">etcd:</span></span><br><span class="line">    <span class="comment"># 扩展参数 https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">auto-compaction-mode:</span> <span class="string">periodic</span></span><br><span class="line">      <span class="attr">auto-compaction-retention:</span> <span class="string">60m</span></span><br><span class="line">      <span class="comment"># 修改空间配额为$((6*1024*1024*1024))，默认 2G,最大 8G</span></span><br><span class="line">      <span class="attr">quota-backend-bytes:</span> <span class="string">&#x27;6442450944&#x27;</span></span><br><span class="line">      <span class="attr">snapshot-count:</span> <span class="number">50000</span></span><br><span class="line">    <span class="comment"># 自动备份</span></span><br><span class="line">    <span class="comment">## rke 版本小于 0.2.x 或 rancher 版本小于 v2.2.0 时使用</span></span><br><span class="line">    <span class="attr">snapshot:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">creation:</span> <span class="string">6h</span></span><br><span class="line">    <span class="attr">retention:</span> <span class="string">24h</span></span><br><span class="line">    <span class="comment">## rke 版本大于等于 0.2.x 或 rancher 版本大于等于 v2.2.0 时使用(两段配置二选一)</span></span><br><span class="line">    <span class="attr">backup_config:</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">true</span>         <span class="comment"># 设置 true 启用 ETCD 自动备份，设置 false 禁用；</span></span><br><span class="line">      <span class="attr">interval_hours:</span> <span class="number">12</span>    <span class="comment"># 快照创建间隔时间，不加此参数，默认 5 分钟；</span></span><br><span class="line">      <span class="attr">retention:</span> <span class="number">6</span>          <span class="comment"># etcd 备份保留份数；</span></span><br></pre></td></tr></table></figure><blockquote><p>更多 etcd 配置请参考: <a href="https://www.xtplayer.cn/etcd/etcd-optimize/">etcd 优化</a></p></blockquote><h3 id="RKE-节点参数说明"><a href="#RKE-节点参数说明" class="headerlink" title="RKE 节点参数说明"></a>RKE 节点参数说明</h3><table><thead><tr><th>参数</th><th>必须</th><th>描述</th></tr></thead><tbody><tr><td><code>address</code></td><td>yes</td><td>公共域名或 IP 地址</td></tr><tr><td><code>user</code></td><td>yes</td><td>可以运行 docker 命令的用户</td></tr><tr><td><code>role</code></td><td>yes</td><td>分配给节点的 Kubernetes 角色列表</td></tr><tr><td><code>internal_address</code></td><td>no</td><td>内部集群通信的私有域名或 IP 地址</td></tr><tr><td><code>ssh_key_path</code></td><td>no</td><td>用于对节点进行身份验证的 SSH 私钥的路径（默认为~&#x2F;.ssh&#x2F;id_rsa）</td></tr></tbody></table><h3 id="镜像仓库配置"><a href="#镜像仓库配置" class="headerlink" title="镜像仓库配置"></a>镜像仓库配置</h3><p>如果需要使用自己镜像仓库的镜像或者是离线安装，那么可以在 <code>private_registries</code> 字段中配置私有镜像仓库信息。</p><table><thead><tr><th>选项</th><th>值</th><th>描述</th></tr></thead><tbody><tr><td><code>url</code></td><td>‘’</td><td>镜像仓库地址</td></tr><tr><td><code>user</code></td><td>‘’</td><td>镜像仓库用户名</td></tr><tr><td><code>password</code></td><td>‘’</td><td>镜像仓库密码</td></tr><tr><td><code>is_default</code></td><td><code>true/false</code></td><td>这个参数很重要，当配置这个参数后，rke 会自动为系统镜像<br />添加镜像仓库前缀，比如 rancher&#x2F;rancher 会变为<br />192.168.1.1&#x2F;rancher&#x2F;rancher，用于离线环境或者使用自己的私有仓库构建集群。</td></tr></tbody></table><blockquote><p>完整的配置示例，请参考<a href="https://docs.rancher.cn/docs/rke/example-yamls/_index/">完整-cluster-yml-示例</a></p></blockquote><h3 id="高级配置"><a href="#高级配置" class="headerlink" title="高级配置"></a>高级配置</h3><p>RKE 有许多配置选项可用于自定义安装以适合您的特定环境，有关选项和功能的完整列表，请查看<a href="https://docs.rancher.cn/docs/rke/config-options/_index/">RKE 文档</a> 。</p><h2 id="安装-rke、kubectl、helm-工具"><a href="#安装-rke、kubectl、helm-工具" class="headerlink" title="安装 rke、kubectl、helm 工具"></a>安装 rke、kubectl、helm 工具</h2><p>需要选择一个节点用来运行 rke、kubectl、helm 等命令，你可以叫它为 <strong>操作节点</strong>。这个节点可以是 local 集群中的任意一台，也可以是专门准备的独立的一个节点。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rke</span></span><br><span class="line">sudo curl -o /usr/local/bin/rke http://rancher-mirror.cnrancher.com/rke/v1.2.4/rke_linux-amd64</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/rke</span><br><span class="line"><span class="comment"># kubectl</span></span><br><span class="line">sudo curl -o /usr/local/bin/kubectl http://rancher-mirror.cnrancher.com/kubectl/v1.20.0/linux-amd64-v1.20.0-kubectl</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/kubectl</span><br><span class="line"><span class="comment"># helm</span></span><br><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">sudo curl -O http://rancher-mirror.cnrancher.com/helm/v3.5.0/helm-v3.5.0-linux-amd64.tar.gz;</span><br><span class="line">sudo tar -zxf helm*tar.gz;</span><br><span class="line">sudo <span class="built_in">cp</span> -rf /tmp/linux-amd64/helm /usr/local/bin/helm</span><br><span class="line">sudo <span class="built_in">chmod</span> +x /usr/local/bin/helm</span><br></pre></td></tr></table></figure><p>或者访问 <a href="http://mirror.cnrancher.com/">http://mirror.cnrancher.com/</a> 下载 rke、kubectl、helm 二进制文件，然后放在<strong>操作节点</strong>的 <code>/usr/local/bin</code> 目录下，并给与执行权限。</p><h2 id="创建-Kubernetes-集群"><a href="#创建-Kubernetes-集群" class="headerlink" title="创建 Kubernetes 集群"></a>创建 Kubernetes 集群</h2><p>运行 RKE 命令创建 Kubernetes 集群</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke up --config ./rancher-cluster.yml</span><br></pre></td></tr></table></figure><p>完成后，它应显示：<code>Finished building Kubernetes cluster successfully</code>。</p><h2 id="测试集群"><a href="#测试集群" class="headerlink" title="测试集群"></a>测试集群</h2><p>RKE 会自动创建 <code>kube_config_rancher-cluster.yml</code>。这个文件包含 kubectl 和 helm 访问 K8S 的凭据，请妥善保管。</p><blockquote><p><strong>注意:</strong> 如果您使用的文件不叫 <code>rancher-cluster.yml</code>, 那么这个 <code>kubeconfig</code> 配置文件将被命名为 <code>kube_config_&lt;FILE_NAME&gt;.yml</code>。</p></blockquote><p>通过 <code>kubectl</code> 测试您的连接，并查看您的所有节点是否处于 <code>Ready</code> 状态。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_xxx.yml get nodes</span><br><span class="line"></span><br><span class="line">NAME                          STATUS    ROLES                      AGE       VERSION</span><br><span class="line">165.227.114.63                Ready     controlplane,etcd,worker   11m       v1.10.1</span><br><span class="line">165.227.116.167               Ready     controlplane,etcd,worker   11m       v1.10.1</span><br><span class="line">165.227.127.226               Ready     controlplane,etcd,worker   11m       v1.10.1</span><br></pre></td></tr></table></figure><h2 id="检查集群-Pod-的运行状况"><a href="#检查集群-Pod-的运行状况" class="headerlink" title="检查集群 Pod 的运行状况"></a>检查集群 Pod 的运行状况</h2><ul><li>Pods 是 <code>Running</code> 或者 <code>Completed</code> 状态。</li><li><code>READY</code> 列显示所有正在运行的容器 (i.e. <code>3/3</code>)，<code>STATUS</code> 显示 POD 是 <code>Running</code>。</li><li>Pods 的 <code>STATUS</code> 是 <code>Completed</code> 为 <code>run-one Jobs</code>,这些 pods<code>READY</code> 应该为 <code>0/1</code>。</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_xxx.yml get pods -A</span><br><span class="line"></span><br><span class="line">NAMESPACE       NAME                                      READY     STATUS      RESTARTS   AGE</span><br><span class="line">ingress-nginx   nginx-ingress-controller-tnsn4            1/1       Running     0          30s</span><br><span class="line">ingress-nginx   nginx-ingress-controller-tw2ht            1/1       Running     0          30s</span><br><span class="line">ingress-nginx   nginx-ingress-controller-v874b            1/1       Running     0          30s</span><br><span class="line">kube-system     canal-jp4hz                               3/3       Running     0          30s</span><br><span class="line">kube-system     canal-z2hg8                               3/3       Running     0          30s</span><br><span class="line">kube-system     canal-z6kpw                               3/3       Running     0          30s</span><br><span class="line">kube-system     kube-dns-7588d5b5f5-sf4vh                 3/3       Running     0          30s</span><br><span class="line">kube-system     kube-dns-autoscaler-5db9bbb766-jz2k6      1/1       Running     0          30s</span><br><span class="line">kube-system     metrics-server-97bc649d5-4rl2q            1/1       Running     0          30s</span><br><span class="line">kube-system     rke-ingress-controller-deploy-job-bhzgm   0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-kubedns-addon-deploy-job-gl7t4        0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-metrics-addon-deploy-job-7ljkc        0/1       Completed   0          30s</span><br><span class="line">kube-system     rke-network-plugin-deploy-job-6pbgj       0/1       Completed   0          30s</span><br></pre></td></tr></table></figure><h2 id="Helm-安装-Rancher"><a href="#Helm-安装-Rancher" class="headerlink" title="Helm 安装 Rancher"></a>Helm 安装 Rancher</h2><h3 id="准备-Rancher-Chart"><a href="#准备-Rancher-Chart" class="headerlink" title="准备 Rancher Chart"></a>准备 Rancher Chart</h3><p>访问 <a href="http://mirror.cnrancher.com/">rancher mirror</a> 下载 Rancher Chart 压缩包，解压后应该有 rancher 文件夹。</p><h3 id="创建自签名-ssl-证书"><a href="#创建自签名-ssl-证书" class="headerlink" title="创建自签名 ssl 证书"></a>创建自签名 ssl 证书</h3><p>自签名 ssl 证书创建方法请参考 <a href="/ssl/self-signed-ssl/">自签名 ssl 证书</a>，注意不要修改生成的文件名称。</p><h3 id="创建-K8S-ssl-密文"><a href="#创建-K8S-ssl-密文" class="headerlink" title="创建 K8S ssl 密文"></a>创建 K8S ssl 密文</h3><p>使用 <code>kubectl</code> 创建 <code>tls</code> 类型的 <code>secrets</code>。</p><blockquote><p>注意：文件的名称一定要与下面的名称相同。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定配置文件</span></span><br><span class="line">kubeconfig=xxx/xxx/xx.kubeconfig.yml</span><br><span class="line"><span class="comment"># 创建命名空间</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> create namespace cattle-system</span><br><span class="line"><span class="comment"># 服务证书和私钥密文</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> \</span><br><span class="line">    -n cattle-system create secret tls tls-rancher-ingress \</span><br><span class="line">    --cert=./tls.crt \</span><br><span class="line">    --key=./tls.key</span><br><span class="line"><span class="comment"># ca 证书密文</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> \</span><br><span class="line">    -n cattle-system create secret generic tls-ca \</span><br><span class="line">    --from-file=cacerts.pem</span><br></pre></td></tr></table></figure><h3 id="安装-Rancher-Server"><a href="#安装-Rancher-Server" class="headerlink" title="安装 Rancher Server"></a>安装 Rancher Server</h3><blockquote><p>更多的 chart 参数可访问 <a href="https://docs.rancher.cn/docs/rancher2/installation_new/install-rancher-on-k8s/chart-options/_index">https://docs.rancher.cn/docs/rancher2/installation_new&#x2F;install-rancher-on-k8s&#x2F;chart-options&#x2F;_index</a></p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定配置文件</span></span><br><span class="line">kubeconfig=xxx/xxx/xx.kubeconfig.yml</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> create namespace cattle-system</span><br><span class="line">helm --kubeconfig=<span class="variable">$kubeconfig</span> install rancher ./rancher \</span><br><span class="line">    --namespace cattle-system \</span><br><span class="line">    --<span class="built_in">set</span> hostname=&lt;您自己的域名&gt; \</span><br><span class="line">    --<span class="built_in">set</span> ingress.tls.source=secret \</span><br><span class="line">    --<span class="built_in">set</span> privateCA=<span class="literal">true</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>注意:</strong> 申请证书对应的<code>域名</code>需要与 <code>hostname</code> 选项匹配，否则 <code>ingress</code> 将无法代理访问 Rancher。</p></blockquote><h2 id="可选-为-Agent-Pod-添加主机别名-x2F-etc-x2F-hosts"><a href="#可选-为-Agent-Pod-添加主机别名-x2F-etc-x2F-hosts" class="headerlink" title="(可选)为 Agent Pod 添加主机别名(&#x2F;etc&#x2F;hosts)"></a>(可选)为 Agent Pod 添加主机别名(&#x2F;etc&#x2F;hosts)</h2><p>如果你的环境中没有 DNS 服务器做域名解析，那么这个时候通过 <code>kubectl --kubeconfig=xxx.yaml -n cattle-system get pod</code> 应该是可以看到 agent pod 一直无法正常运行，查看 agent pod 日志会发现是因为连接不上 rancher server url。</p><ul><li><p>解决方法</p><p>可以通过给 <code>cattle-cluster-agent Pod</code> 和 <code>cattle-node-agent pod</code> 添加主机别名(&#x2F;etc&#x2F;hosts)，让其可以正常通过 <code>Rancher Server URL</code> 与 Rancher Server 通信<code>(前提是 IP 地址可以互通)</code>。</p><ol><li><p>cattle-cluster-agent pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> kubeconfig=xxx/xxx/xx.kubeconfig.yml</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">patch deployments cattle-cluster-agent --patch <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">    &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;template&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;hostAliases&quot;: [</span></span><br><span class="line"><span class="string">                    &#123;</span></span><br><span class="line"><span class="string">                        &quot;hostnames&quot;:</span></span><br><span class="line"><span class="string">                        [</span></span><br><span class="line"><span class="string">                            &quot;demo.cnrancher.com&quot;</span></span><br><span class="line"><span class="string">                        ],</span></span><br><span class="line"><span class="string">                            &quot;ip&quot;: &quot;192.168.1.100&quot;</span></span><br><span class="line"><span class="string">                    &#125;</span></span><br><span class="line"><span class="string">                ]</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p>cattle-node-agent pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> kubeconfig=xxx/xxx/xx.kubeconfig.yml</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system \</span><br><span class="line">patch  daemonsets cattle-node-agent --patch <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">    &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;template&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;hostAliases&quot;: [</span></span><br><span class="line"><span class="string">                    &#123;</span></span><br><span class="line"><span class="string">                        &quot;hostnames&quot;:</span></span><br><span class="line"><span class="string">                        [</span></span><br><span class="line"><span class="string">                            &quot;xxx.rancher.com&quot;</span></span><br><span class="line"><span class="string">                        ],</span></span><br><span class="line"><span class="string">                            &quot;ip&quot;: &quot;192.168.1.100&quot;</span></span><br><span class="line"><span class="string">                    &#125;</span></span><br><span class="line"><span class="string">                ]</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong> 1、替换其中的域名和 IP<br>2、别忘记 json 中的引号。</p></blockquote></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s 基础环境配置</title>
      <link href="/kubernetes/k8s-basic-environment-configuration/"/>
      <url>/kubernetes/k8s-basic-environment-configuration/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/k8s-basic-environment-configuration/" target="_blank" title="https://www.xtplayer.cn/kubernetes/k8s-basic-environment-configuration/">https://www.xtplayer.cn/kubernetes/k8s-basic-environment-configuration/</a></p><h2 id="主机基础配置"><a href="#主机基础配置" class="headerlink" title="主机基础配置"></a>主机基础配置</h2><h3 id="主机名配置"><a href="#主机名配置" class="headerlink" title="主机名配置"></a>主机名配置</h3><p>因为 K8S 或者 <code>FQDN</code> 的规定，主机名仅支持包含 <code>-</code> 或&#x2F;和 <code>.</code>(中横线和点)两种特殊符号，并且一个集群中主机名不能重复。</p><h3 id="Hosts"><a href="#Hosts" class="headerlink" title="Hosts"></a>Hosts</h3><ol><li>Linux 系统安装完成后，在 hosts(&#x2F;etc&#x2F;hosts) 中应该有 <code>localhost 指向 127.0.0.1</code>，如果没有则手动添加上。</li><li>配置每台主机的 hosts(&#x2F;etc&#x2F;hosts)，添加 <code>host_ip $hostname</code> 到 <code>/etc/hosts</code> 文件中。</li></ol><h3 id="CentOS-关闭-selinux"><a href="#CentOS-关闭-selinux" class="headerlink" title="CentOS 关闭 selinux"></a>CentOS 关闭 selinux</h3><p><code>sudo sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config</code></p><h3 id="关闭防火墙-可选-或者放行相应端口"><a href="#关闭防火墙-可选-或者放行相应端口" class="headerlink" title="关闭防火墙(可选)或者放行相应端口"></a>关闭防火墙(可选)或者放行相应端口</h3><p>   对于刚刚接触 Rancher 的用户，建议在测试环境关闭防火墙，以避免出现网络通信问题。</p><ol><li><p>关闭防火墙</p><ul><li>CentOS</li></ul><p><code>systemctl stop firewalld.service &amp;&amp; systemctl disable firewalld.service</code></p><ul><li>Ubuntu</li></ul><p><code>ufw disable</code></p></li><li><p>端口放行</p><p>端口放行请查看<a href="https://rancher.com/docs/rancher/v2.x/en/installation/requirements/ports/">端口需求</a></p></li></ol><h3 id="配置主机时间、时区、系统语言"><a href="#配置主机时间、时区、系统语言" class="headerlink" title="配置主机时间、时区、系统语言"></a>配置主机时间、时区、系统语言</h3><ol><li><p>查看时区</p><p><code>date -R</code> 或者 <code>timedatectl</code></p></li><li><p>修改时区</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ln</span> -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br></pre></td></tr></table></figure></li><li><p>修改系统语言环境</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">echo</span> <span class="string">&#x27;LANG=&quot;en_US.UTF-8&quot;&#x27;</span> &gt;&gt; /etc/profile; <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>配置主机 NTP 时间同步</p></li></ol><h3 id="配置主机-DNS-服务器"><a href="#配置主机-DNS-服务器" class="headerlink" title="配置主机 DNS 服务器"></a>配置主机 DNS 服务器</h3><p>   对于类似 Ubuntu 18 这类默认使用 <code>systemd-resolve</code> 管理 DNS 服务器的系统，建议禁用 <code>systemd-resolved</code> 服务，然后手动配置 DNS。</p><p>   因为 <code>systemd-resolve</code> 会使用 <code>127.0.x.x</code> 这样的 IP 来作为默认的 DNS 服务器地址，这个是主机的环回地址。这个地址传递到容器环境中后，容器应用无法访问这个地址。</p><p>   操作方法：</p><ol><li><p>禁用 systemd-resolved.service</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl <span class="built_in">disable</span> systemd-resolved.service</span><br><span class="line">systemctl stop systemd-resolved.service</span><br><span class="line"><span class="built_in">rm</span> -rf /etc/resolv.conf; <span class="built_in">touch</span> /etc/resolv.conf</span><br></pre></td></tr></table></figure></li><li><p>接着编辑 <code>/etc/resolv.conf</code> 添加 DNS 服务器</p></li><li><p>重启 docker 服务</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload; systemctl restart docker</span><br></pre></td></tr></table></figure></li></ol><h2 id="Kernel-调优"><a href="#Kernel-调优" class="headerlink" title="Kernel 调优"></a>Kernel 调优</h2>   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/sysctl.conf&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">net.ipv4.ip_forward=1</span></span><br><span class="line"><span class="string">net.ipv4.conf.all.forwarding=1</span></span><br><span class="line"><span class="string"># 参考 https://github.com/prometheus/node_exporter#disabled-by-default</span></span><br><span class="line"><span class="string">kernel.perf_event_paranoid=-1</span></span><br><span class="line"><span class="string">kernel.watchdog_thresh=30</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-iptables=1</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-ip6tables=1</span></span><br><span class="line"><span class="string"># 存在于 ARP 高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行，默认值：128。</span></span><br><span class="line"><span class="string">net.ipv4.neigh.default.gc_thresh1=128</span></span><br><span class="line"><span class="string"># 保存在 ARP 高速缓存中的最大记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。默认值：512。</span></span><br><span class="line"><span class="string">net.ipv4.neigh.default.gc_thresh2=6144</span></span><br><span class="line"><span class="string"># 保存在 ARP 高速缓存中的最大记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。默认值：1024。</span></span><br><span class="line"><span class="string">net.ipv4.neigh.default.gc_thresh3=8192</span></span><br><span class="line"><span class="string"># 决定检查一次相邻层记录的有效性的周期。当相邻层记录失效时，将在给它发送数据前，再解析一次。默认值：60 秒。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 系统中所有进程能够同时打开的文件句柄数量</span></span><br><span class="line"><span class="string">fs.file-max=2097152</span></span><br><span class="line"><span class="string"># 设置每个用户可以运行的 inotifywait 或 inotifywatch 命令的进程数。</span></span><br><span class="line"><span class="string">fs.inotify.max_user_instances=8192</span></span><br><span class="line"><span class="string"># inotify 用于监控文件系统事件，该文件中的值为调用 inotify_init 函数时分配给 inotify 队列的事件数目的最大值，超出这个值得事件被丢弃，但会触发 IN_Q_OVERFLOW 事件，文件系统变化越频繁，这个值就应该越大</span></span><br><span class="line"><span class="string">fs.inotify.max_queued_events=16384</span></span><br><span class="line"><span class="string"># IO 复用 epoll 监听文件句柄的数量最大值</span></span><br><span class="line"><span class="string">fs.inotify.max_user_watches=524288</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vm.swappiness=0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">net.ipv4.tcp_syncookies=1</span></span><br><span class="line"><span class="string">net.ipv4.tcp_fin_timeout=30</span></span><br><span class="line"><span class="string">net.ipv4.tcp_synack_retries=2</span></span><br><span class="line"><span class="string">net.ipv4.tcp_max_syn_backlog=8096</span></span><br><span class="line"><span class="string">net.ipv4.tcp_tw_recycle=0</span></span><br><span class="line"><span class="string">fs.aio-max-nr = 1048576</span></span><br><span class="line"><span class="string">kernel.randomize_va_space = 0</span></span><br><span class="line"><span class="string">net.core.somaxconn = 1024</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><blockquote><p>数值根据实际环境自行配置，最后执行 <code>sysctl -p</code> 保存配置。</p></blockquote><h2 id="连接数调整"><a href="#连接数调整" class="headerlink" title="连接数调整"></a>连接数调整</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/security/limits.conf &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">root soft nofile 655350</span></span><br><span class="line"><span class="string">root hard nofile 655350</span></span><br><span class="line"><span class="string">* soft nofile 655350</span></span><br><span class="line"><span class="string">* hard nofile 655350</span></span><br><span class="line"><span class="string">* soft nproc unlimited</span></span><br><span class="line"><span class="string">* hard nproc unlimited</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h2 id="模块列表"><a href="#模块列表" class="headerlink" title="模块列表"></a>模块列表</h2><p>如果要使用 ceph 存储相关功能，需保证 worker 节点加载 <code>RBD 模块</code></p><p>以下模块需要在主机上加载</p><table><thead><tr><th>模块名称</th></tr></thead><tbody><tr><td>br_netfilter</td></tr><tr><td>ip6_udp_tunnel</td></tr><tr><td>ip_set</td></tr><tr><td>ip_set_hash_ip</td></tr><tr><td>ip_set_hash_net</td></tr><tr><td>iptable_filter</td></tr><tr><td>iptable_nat</td></tr><tr><td>iptable_mangle</td></tr><tr><td>iptable_raw</td></tr><tr><td>nf_conntrack_netlink</td></tr><tr><td>nf_conntrack</td></tr><tr><td>nf_conntrack_ipv4</td></tr><tr><td>nf_defrag_ipv4</td></tr><tr><td>nf_nat</td></tr><tr><td>nf_nat_ipv4</td></tr><tr><td>nf_nat_masquerade_ipv4</td></tr><tr><td>nfnetlink</td></tr><tr><td>udp_tunnel</td></tr><tr><td>veth</td></tr><tr><td>vxlan</td></tr><tr><td>x_tables</td></tr><tr><td>xt_addrtype</td></tr><tr><td>xt_conntrack</td></tr><tr><td>xt_comment</td></tr><tr><td>xt_mark</td></tr><tr><td>xt_multiport</td></tr><tr><td>xt_nat</td></tr><tr><td>xt_recent</td></tr><tr><td>xt_set</td></tr><tr><td>xt_statistic</td></tr><tr><td>xt_tcpudp</td></tr></tbody></table><blockquote><p>提示</p></blockquote><p>模块查询: lsmod | grep &lt;模块名&gt;<br>模块加载: modprobe &lt;模块名&gt;</p><h2 id="ETCD-集群容错表"><a href="#ETCD-集群容错表" class="headerlink" title="ETCD 集群容错表"></a>ETCD 集群容错表</h2><p>建议在 ETCD 集群中使用奇数个成员，通过添加额外成员可以获得更高的失败容错。具体详情可以查阅<a href="https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size">optimal-cluster-size</a>。</p><table><thead><tr><th>集群大小</th><th>MAJORITY</th><th>失败容错</th></tr></thead><tbody><tr><td>1</td><td>1</td><td><strong>0</strong></td></tr><tr><td>2</td><td>2</td><td><strong>0</strong></td></tr><tr><td>3</td><td>2</td><td><strong>1</strong></td></tr><tr><td>4</td><td>3</td><td><strong>1</strong></td></tr><tr><td>5</td><td>3</td><td><strong>2</strong></td></tr><tr><td>6</td><td>4</td><td><strong>2</strong></td></tr><tr><td>7</td><td>4</td><td><strong>3</strong></td></tr><tr><td>8</td><td>5</td><td><strong>3</strong></td></tr><tr><td>9</td><td>5</td><td><strong>4</strong></td></tr></tbody></table><h2 id="Docker-安装与配置"><a href="#Docker-安装与配置" class="headerlink" title="Docker 安装与配置"></a>Docker 安装与配置</h2><h3 id="Docker-安装"><a href="#Docker-安装" class="headerlink" title="Docker 安装"></a>Docker 安装</h3><h4 id="修改系统源"><a href="#修改系统源" class="headerlink" title="修改系统源"></a>修改系统源</h4><ol><li><p>Ubuntu</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">sed -i <span class="string">&#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line">sed -i <span class="string">&#x27;s/security.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">apt-get update</span><br></pre></td></tr></table></figure></li><li><p>Centos 7.x</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ca-certificates ;</span><br><span class="line">update-ca-trust;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cp</span> -rf /etc/yum.repos.d /etc/yum.repos.d.backup</span><br><span class="line"></span><br><span class="line">sed -e <span class="string">&#x27;s|^mirrorlist=|#mirrorlist=|g&#x27;</span> \</span><br><span class="line">    -e <span class="string">&#x27;s|^#baseurl=http://mirror.centos.org/centos|baseurl=https://mirrors.ustc.edu.cn/centos|g&#x27;</span> \</span><br><span class="line">    -i.bak /etc/yum.repos.d/CentOS-Base.repo</span><br><span class="line"></span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure></li><li><p>centos8.x</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> -rf /etc/yum.repos.d /etc/yum.repos.d.backup</span><br><span class="line"></span><br><span class="line">sed -e <span class="string">&#x27;s|^mirrorlist=|#mirrorlist=|g&#x27;</span> \</span><br><span class="line">    -e <span class="string">&#x27;s|^#baseurl=http://mirror.centos.org/$contentdir|baseurl=https://mirrors.ustc.edu.cn/centos|g&#x27;</span> \</span><br><span class="line">    -i.bak \</span><br><span class="line">    /etc/yum.repos.d/CentOS-Linux-AppStream.repo \</span><br><span class="line">    /etc/yum.repos.d/CentOS-Linux-BaseOS.repo \</span><br><span class="line">    /etc/yum.repos.d/CentOS-Linux-Extras.repo \</span><br><span class="line">    /etc/yum.repos.d/CentOS-Linux-PowerTools.repo \</span><br><span class="line">    /etc/yum.repos.d/CentOS-Linux-Plus.repo</span><br><span class="line"></span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure></li></ol><h4 id="Docker-ce-安装"><a href="#Docker-ce-安装" class="headerlink" title="Docker-ce 安装"></a>Docker-ce 安装</h4><ol><li><p>Ubuntu</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义用户名</span></span><br><span class="line">NEW_USER=rancher</span><br><span class="line"><span class="comment"># 添加用户(可选)</span></span><br><span class="line">sudo adduser <span class="variable">$NEW_USER</span></span><br><span class="line"><span class="comment"># 为新用户设置密码</span></span><br><span class="line">sudo passwd <span class="variable">$NEW_USER</span></span><br><span class="line"><span class="comment"># 为新用户添加 sudo 权限</span></span><br><span class="line">sudo <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$NEW_USER</span> ALL=(ALL) ALL&quot;</span> &gt;&gt; /etc/sudoers</span><br><span class="line"><span class="comment"># 定义安装版本</span></span><br><span class="line"><span class="built_in">export</span> docker_version=18.09.9;</span><br><span class="line"><span class="comment"># 删除旧的 docker 组件</span></span><br><span class="line">sudo apt-get remove docker docker-engine docker.io containerd runc -y;</span><br><span class="line"><span class="comment"># 更新 apt 源</span></span><br><span class="line">sudo apt-get update;</span><br><span class="line"><span class="comment"># 对系统进行全面的更新升级，推荐升级一下（可选）</span></span><br><span class="line">sudo apt-get -y upgrade;</span><br><span class="line"><span class="comment"># 安装必要的一些系统工具</span></span><br><span class="line">sudo apt-get -y install apt-transport-https ca-certificates \</span><br><span class="line">    curl software-properties-common bash-completion  gnupg-agent;</span><br><span class="line"><span class="comment"># 安装 GPG 证书</span></span><br><span class="line">sudo curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | \</span><br><span class="line">    sudo apt-key add -;</span><br><span class="line"><span class="comment"># 添加 Docker APT 源</span></span><br><span class="line">sudo add-apt-repository <span class="string">&quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu <span class="subst">$(lsb_release -cs)</span> stable&quot;</span>;</span><br><span class="line"><span class="comment"># 更新并安装 Docker-CE</span></span><br><span class="line">sudo apt-get -y update;</span><br><span class="line">install_version=$( apt-cache madison docker-ce | grep <span class="variable">$&#123;docker_version&#125;</span> | awk <span class="string">&#x27;&#123;print $3&#125;&#x27;</span> );</span><br><span class="line"><span class="comment"># --allow-downgrades 允许降级安装</span></span><br><span class="line">sudo apt-get -y install docker-ce=<span class="variable">$&#123;install_version&#125;</span> docker-ce-cli=<span class="variable">$&#123;install_version&#125;</span> --allow-downgrades;</span><br><span class="line"><span class="comment"># 把当前用户加入 docker 组</span></span><br><span class="line">sudo usermod -aG docker <span class="variable">$NEW_USER</span>;</span><br><span class="line"><span class="comment"># 设置开机启动</span></span><br><span class="line">sudo systemctl <span class="built_in">enable</span> docker;</span><br><span class="line"><span class="comment"># 清理不需要的依赖</span></span><br><span class="line">apt-get autoremove  -y</span><br></pre></td></tr></table></figure><p><strong>Docker-engine</strong></p><p>Docker-Engine Docker 官方已经不推荐使用，请安装 Docker-CE。</p></li><li><p>Centos</p><blockquote><p><strong>注意：</strong>在安装的时候如果提示 containerd 版本过低，则需要访问 <a href="https://download.docker.com/linux/centos/7/x86_64/stable/Packages">https://download.docker.com/linux/centos/7/x86_64&#x2F;stable&#x2F;Packages</a> 下载最新版本的 containerd 到本地，然后运行 <code>yum localinstall xxx.rpm</code> 进行安装。</p></blockquote><blockquote><p>因为 CentOS 的安全限制，通过 RKE 安装 K8S 集群时候无法使用 <code>root</code> 账户。所以，建议 <code>CentOS</code> 用户使用非 <code>root</code> 用户来运 docker，不管是 <code>RKE</code> 还是 <code>custom</code> 安装 k8s，详情查看<a href="https://docs.rancher.cn/docs/rke/troubleshooting/ssh-connectivity-errors/_index/">无法为主机配置 SSH 隧道</a> 。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义用户名</span></span><br><span class="line">NEW_USER=rancher</span><br><span class="line"><span class="comment"># 添加用户(可选)</span></span><br><span class="line">sudo adduser <span class="variable">$NEW_USER</span></span><br><span class="line"><span class="comment"># 为新用户设置密码</span></span><br><span class="line">sudo passwd <span class="variable">$NEW_USER</span></span><br><span class="line"><span class="comment"># 为新用户添加 sudo 权限</span></span><br><span class="line">sudo <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$NEW_USER</span> ALL=(ALL) ALL&quot;</span> &gt;&gt; /etc/sudoers</span><br><span class="line"><span class="comment"># 卸载旧版本 Docker 软件</span></span><br><span class="line">sudo yum remove docker \</span><br><span class="line">              docker-client \</span><br><span class="line">              docker-client-latest \</span><br><span class="line">              docker-common \</span><br><span class="line">              docker-latest \</span><br><span class="line">              docker-latest-logrotate \</span><br><span class="line">              docker-logrotate \</span><br><span class="line">              docker-selinux \</span><br><span class="line">              docker-engine-selinux \</span><br><span class="line">              docker-engine \</span><br><span class="line">              container*</span><br><span class="line"><span class="comment"># 定义安装版本</span></span><br><span class="line"><span class="built_in">export</span> docker_version=18.06.3</span><br><span class="line"><span class="comment"># 对系统进行全面的更新升级，推荐升级一下（可选）</span></span><br><span class="line">sudo yum update -y;</span><br><span class="line"><span class="comment"># 安装必要的一些系统工具</span></span><br><span class="line">sudo yum install -y yum-utils device-mapper-persistent-data \</span><br><span class="line">    lvm2 bash-completion;</span><br><span class="line"><span class="comment"># Step 2: 添加软件源信息</span></span><br><span class="line">sudo yum-config-manager --add-repo \</span><br><span class="line">    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo;</span><br><span class="line"><span class="comment"># Step 3: 更新并安装 Docker-CE</span></span><br><span class="line">sudo yum makecache all;</span><br><span class="line">version=$(yum list docker-ce.x86_64 --showduplicates | <span class="built_in">sort</span> -r|grep <span class="variable">$&#123;docker_version&#125;</span>|awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>);</span><br><span class="line">sudo yum -y install --<span class="built_in">setopt</span>=obsoletes=0 docker-ce-<span class="variable">$&#123;version&#125;</span> docker-ce-selinux-<span class="variable">$&#123;version&#125;</span>;</span><br><span class="line"><span class="comment"># 如果已经安装高版本 Docker,可进行降级安装(可选)</span></span><br><span class="line">yum downgrade --<span class="built_in">setopt</span>=obsoletes=0 -y docker-ce-<span class="variable">$&#123;version&#125;</span> docker-ce-selinux-<span class="variable">$&#123;version&#125;</span>;</span><br><span class="line"><span class="comment"># 把当前用户加入 docker 组</span></span><br><span class="line">sudo usermod -aG docker <span class="variable">$NEW_USER</span>;</span><br><span class="line"><span class="comment"># 设置开机启动</span></span><br><span class="line">sudo systemctl <span class="built_in">enable</span> docker;</span><br></pre></td></tr></table></figure><p><strong>Docker-engine</strong></p><p>Docker-Engine Docker 官方已经不推荐使用，请安装 Docker-CE。</p></li></ol><h3 id="锁定-Docker-版本"><a href="#锁定-Docker-版本" class="headerlink" title="锁定 Docker 版本"></a>锁定 Docker 版本</h3><p>可能因为某些原因无意间执行了 <code>yum update</code> 或者 <code>apt-get -y upgrade;</code>导致 Docker 版本升级。为了避免此类问题发生，建议在安装好 Docker 后对 Docker 软件进行锁定，防止 Docker 意外更新。</p><h4 id="centos"><a href="#centos" class="headerlink" title="centos"></a>centos</h4><ol><li><p>安装 yum-plugin-versionlock 插件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install yum-plugin-versionlock</span><br></pre></td></tr></table></figure></li><li><p>锁定软件包</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum versionlock add docker-ce docker-ce-cli</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@izwz969o7lu6t9lh4ta6m5z ~]<span class="comment"># yum versionlock add docker-ce docker-ce-cli</span></span><br><span class="line">已加载插件：fastestmirror, versionlock</span><br><span class="line">Adding versionlock on: 0:docker-ce-17.06.2.ce-3.el7.centos</span><br><span class="line">versionlock added: 1</span><br><span class="line">[root@izwz969o7lu6t9lh4ta6m5z ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure></li><li><p>查看已锁定的软件包</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum versionlock list</span><br></pre></td></tr></table></figure></li><li><p>解锁指定的软件包</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum versionlock delete &lt;软件包名称&gt;</span><br></pre></td></tr></table></figure></li><li><p>解锁所有的软件包</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum versionlock clear</span><br></pre></td></tr></table></figure></li></ol><h4 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h4><ol><li><p>锁定软件：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-mark hold docker-ce docker-ce-cli</span><br></pre></td></tr></table></figure></li><li><p>查看已锁定的软件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-mark showhold</span><br></pre></td></tr></table></figure><p>软件包被锁定后，再次执行 <code>apt-get upgrade</code> 可以看到以下日志提示</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@alihost-03:/home/docker<span class="comment"># apt-get  upgrade</span></span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree</span><br><span class="line">Reading state information... Done</span><br><span class="line">Calculating upgrade... Done</span><br><span class="line">The following packages have been kept back:</span><br><span class="line">  docker-ce docker-ce-cli linux-generic linux-headers-generic linux-image-generic ubuntu-minimal</span><br><span class="line">0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.</span><br><span class="line">root@alihost-03:/home/docker<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>被锁定的软件不会再升级。</p></li><li><p>解除锁定：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-mark unhold docker-ce docker-ce-cli</span><br></pre></td></tr></table></figure></li></ol><h3 id="Docker-配置"><a href="#Docker-配置" class="headerlink" title="Docker 配置"></a>Docker 配置</h3><p>对于通过 systemd 来管理服务的系统(比如 CentOS7.X、Ubuntu16.X)，Docker 有两处可以配置参数: 一个是 <code>docker.service</code>  服务配置文件，一个是 Docker daemon 配置文件 daemon.json。</p><ol><li><p><code>docker.service</code></p><p>对于 CentOS 系统，<code>docker.service</code> 默认位于 <code>/usr/lib/systemd/system/docker.service</code>；对于 Ubuntu 系统，<code>docker.service</code> 默认位于 <code>/lib/systemd/system/docker.service</code></p></li><li><p><code>daemon.json</code></p><p>daemon.json 默认位于 <code>/etc/docker/daemon.json</code>，如果没有可手动创建，基于 systemd 管理的系统都是相同的路径。通过修改 <code>daemon.json</code> 来改过 Docker 配置，也是 Docker 官方推荐的方法。</p></li></ol><blockquote><p>以下说明均基于 systemd，并通过 <code>/etc/docker/daemon.json</code> 来修改配置。</p></blockquote><h4 id="配置镜像下载和上传并发数"><a href="#配置镜像下载和上传并发数" class="headerlink" title="配置镜像下载和上传并发数"></a>配置镜像下载和上传并发数</h4><p>从 Docker1.12 开始，支持自定义下载和上传镜像的并发数，默认值上传为 3 个并发，下载为 5 个并发。通过添加”max-concurrent-downloads”和”max-concurrent-uploads”参数对其修改:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;max-concurrent-downloads&quot;</span>: 3,</span><br><span class="line"><span class="string">&quot;max-concurrent-uploads&quot;</span>: 5</span><br></pre></td></tr></table></figure><h4 id="配置镜像加速地址"><a href="#配置镜像加速地址" class="headerlink" title="配置镜像加速地址"></a>配置镜像加速地址</h4><p>Rancher 从 v1.6.15 开始到 v2.x.x,Rancher 系统相关的所有镜像(包括 1.6.x 上的 K8S 镜像)都托管在 Dockerhub 仓库。Dockerhub 节点在国外，国内直接拉取镜像会有些缓慢。为了加速镜像的下载，可以给 Docker 配置国内的镜像地址。</p><p>编辑 <code>/etc/docker/daemon.json</code> 加入以下内容</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;registry-mirrors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;https://7bezldxe.mirror.aliyuncs.com/&quot;</span><span class="punctuation">,</span><span class="string">&quot;https://IP:PORT/&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><blockquote><p>可以设置多个 <code>registry-mirrors</code> 地址，以数组形式书写，地址需要添加协议头(https 或者 http)。</p></blockquote><h4 id="配置-insecure-registries私有仓库"><a href="#配置-insecure-registries私有仓库" class="headerlink" title="配置 insecure-registries私有仓库"></a>配置 <code>insecure-registries</code>私有仓库</h4><p>Docker 默认只信任 TLS 加密的仓库地址(https)，所有非 https 仓库默认无法登陆也无法拉取镜像。<code>insecure-registries</code> 字面意思为不安全的仓库，通过添加这个参数对非 https 仓库进行授信。可以设置多个 <code>insecure-registries</code> 地址，以数组形式书写，地址不能添加协议头(http)。</p><p>编辑 <code>/etc/docker/daemon.json</code> 加入以下内容:</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;insecure-registries&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;192.168.1.100&quot;</span><span class="punctuation">,</span><span class="string">&quot;IP:PORT&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h4 id="配置-Docker-存储驱动"><a href="#配置-Docker-存储驱动" class="headerlink" title="配置 Docker 存储驱动"></a>配置 Docker 存储驱动</h4><p>OverlayFS 是一个新一代的联合文件系统，类似于 AUFS，但速度更快，实现更简单。Docker 为 OverlayFS 提供了两个存储驱动程序:旧版的 <code>overlay</code>，新版的 <code>overlay2</code>(更稳定)。</p><p>先决条件:</p><ul><li><p><code>overlay2</code>: Linux 内核版本 4.0 或更高版本，或使用内核版本 3.10.0-514+的 RHEL 或 CentOS。</p></li><li><p><code>overlay</code>: 主机 Linux 内核版本 3.18+</p></li><li><p>支持的磁盘文件系统</p><ul><li>ext4(仅限 RHEL 7.1)</li><li>xfs(RHEL7.2 及更高版本)，需要启用 d_type&#x3D;true。</li></ul><blockquote><p>具体详情参考 <a href="https://docs.docker.com/storage/storagedriver/overlayfs-driver/">Docker Use the OverlayFS storage driver</a></p></blockquote></li></ul><p>编辑 <code>/etc/docker/daemon.json</code> 加入以下内容</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;storage-driver&quot;</span><span class="punctuation">:</span> <span class="string">&quot;overlay2&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;storage-opts&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;overlay2.override_kernel_check=true&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h4 id="配置日志驱动"><a href="#配置日志驱动" class="headerlink" title="配置日志驱动"></a>配置日志驱动</h4><p>容器在运行时会产生大量日志文件，很容易占满磁盘空间。通过配置日志驱动来限制文件大小与文件的数量。</p><blockquote><p>限制单个日志文件为 <code>100M</code>,最多产生 <code>3</code> 个日志文件</p></blockquote><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;log-driver&quot;</span><span class="punctuation">:</span> <span class="string">&quot;json-file&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;log-opts&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;max-size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;100m&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;max-file&quot;</span><span class="punctuation">:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h4 id="可选-修改-Docker0-默认-网络接口-IP-地址"><a href="#可选-修改-Docker0-默认-网络接口-IP-地址" class="headerlink" title="(可选)修改 Docker0 默认 网络接口 IP 地址"></a>(可选)修改 Docker0 默认 网络接口 IP 地址</h4><p>Docker 第一次运行时会自动创建名为 docker0 的网络接口，默认接口地址为 172.17.0.1&#x2F;16。在一些企业中，可能已经使用了这个网段的地址，或者规划以后会使用这个网段的地址。所以，建议在安装好 docker 服务后，第一时间修改 docker0 接口地址，避免后期出现网段冲突。</p><ul><li><p>停止 docker 运行</p><p><code>systemctl stop docker.service</code></p></li><li><p>删除已有的 docker0 接口</p><p><code>sudo ip link del docker0</code></p></li><li><p>修改 docker 配置文件</p><p>在 <code>/etc/docker/daemon.json</code> 中添加 <code>&quot;bip&quot;: &quot;192.168.100.1/24&quot;,</code></p></li><li><p>重启启动 docker</p><p><code>systemctl enable docker; systemctl daemon-reload; systemctl restart docker;</code></p></li></ul><h4 id="docker-service-配置"><a href="#docker-service-配置" class="headerlink" title="docker.service 配置"></a>docker.service 配置</h4><ol><li><p>对于 CentOS 系统，docker.service 默认位于 <code>/usr/lib/systemd/system/docker.service</code>；</p></li><li><p>对于 Ubuntu 系统，docker.service 默认位于 <code>/lib/systemd/system/docker.service</code>。</p></li></ol><p>编辑 <code>docker.service</code>，添加以下参数。</p><ul><li><p>防止 docker 服务 OOM： <code>OOMScoreAdjust=-1000</code></p></li><li><p>开启 iptables 转发链：</p><p> <code>ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT</code></p> <img src="/kubernetes/k8s-basic-environment-configuration/image-20190615165436722.ea672a70.png" class="" title="image-20190615165436722"></li></ul><h3 id="Ubuntu-Debian-系统-，docker-info-提示-WARNING-No-swap-limit-support"><a href="#Ubuntu-Debian-系统-，docker-info-提示-WARNING-No-swap-limit-support" class="headerlink" title="Ubuntu\Debian 系统 ，docker info 提示 WARNING: No swap limit support"></a>Ubuntu\Debian 系统 ，docker info 提示 WARNING: No swap limit support</h3><p>Ubuntu\Debian 系统下，默认 cgroups 未开启 swap account 功能，这样会导致设置容器内存或者 swap 资源限制不生效。可以通过以下命令解决:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统一网卡名称为 ethx</span></span><br><span class="line">sudo sed -i <span class="string">&#x27;s/en[[:alnum:]]*/eth0/g&#x27;</span> /etc/network/interfaces;</span><br><span class="line">sudo sed -i <span class="string">&#x27;s/GRUB_CMDLINE_LINUX=&quot;\(.*\)&quot;/GRUB_CMDLINE_LINUX=&quot;net.ifnames=0 cgroup_enable=memory swapaccount=1 biosdevname=0 \1&quot;/g&#x27;</span> /etc/default/grub;</span><br><span class="line">sudo update-grub;</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong> 通过以上命令可自动配置参数，如果 <code>/etc/default/grub</code> 非默认配置，需根据实际参数做调整。 <strong>提示</strong> 以上配置完成后，建议重启一次主机</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8S 弃用 docker？</title>
      <link href="/kubernetes/k8s-deprecated-docker/"/>
      <url>/kubernetes/k8s-deprecated-docker/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/k8s-deprecated-docker/" target="_blank" title="https://www.xtplayer.cn/kubernetes/k8s-deprecated-docker/">https://www.xtplayer.cn/kubernetes/k8s-deprecated-docker/</a></p><blockquote><p>本文转自 rancher 公众号</p></blockquote><p>Kubernetes 在其最新的 Changelog 中宣布，自 Kubernetes 1.20 之后将弃用 Docker 作为容器运行时。这一消息在云原生领域激起了不小的水花，在 Rancher 技术社区里许多小伙伴也对此进行了激烈的讨论。</p><p>Kubernetes 为什么选择弃用 Docker 呢？我们需要先简单了解 Dockershim。它是一个桥接服务，帮助 Kubernetes 与 Docker 进行通信，Kubelet 之前使用 dockershim 实现对 Docker 的 CRI 支持（Docker 本身目前尚未实现 CRI）。但时至今日，维护 Dockershim 已成为运维&#x2F;开发人员的沉重负担。因此 Kubernetes 社区建议大家考虑使用包含 CRI 完整实现（兼容 v1alpha1 或 v1）的可用容器运行时。从而取消了对 Docker 作为容器运行时的支持。</p><p>不过大家不必过分担心，近期从 Rancher 社区里面搜集了一些大家比较关注的问题，下面一一为大家解答：</p><h2 id="Kubernetes-Kubelet-弃用了-Docker-作为容器运行时，有代替方案吗？"><a href="#Kubernetes-Kubelet-弃用了-Docker-作为容器运行时，有代替方案吗？" class="headerlink" title="Kubernetes Kubelet 弃用了 Docker 作为容器运行时，有代替方案吗？"></a>Kubernetes Kubelet 弃用了 Docker 作为容器运行时，有代替方案吗？</h2><p>在 Kubernetes 集群中，容器运行时负责提取和运行容器镜像。Docker 只是被普遍使用的容器运行时，在 Docker 被弃用之后，我们还有两个常见的选项：<strong>containerd</strong> 和 <strong>CRI-O</strong>。</p><p>Containerd 是一个工业级标准的容器运行时，它极为简单、健壮并且具备可移植性。Containerd 可以在宿主机中管理完整的容器生命周期。这是一个 100%开源的软件，已于<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247490522&idx=1&sn=288f1fdb5dbc4cc3f338fe66f391d969&chksm=e83a9d1cdf4d140a21c2244ac6b83bfc12309b0bc73d60d32564a843552cbb20781db634208e&scene=21#wechat_redirect">去年 2 月份从 CNCF</a>毕业。</p><p>去年年初，Rancher 推出的<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247492604&idx=1&sn=897075471e091f70b192546406941827&chksm=e839653adf4eec2c6ff83465acaec9bf788f6b3c0ca2bd6f5f034857880973b145dcc394ca4e&scene=21#wechat_redirect">轻量级 Kubernetes 发行版 K3s</a>已经使用 containerd 作为默认容器运行时。</p><p><strong>containerd</strong>：<a href="https://github.com/containerd/containerd/">https://github.com/containerd/containerd/</a></p><p>CRI-O 是由 Red Hat 推出的一款容器运行时，旨在提供一种在 OCI 一致的运行时和 Kubelet 之间的集成方式。在文章后半部分我们将会进一步对比 containerd 和 CRI-O 的性能，为您在选择容器运行时的时候提供参考。</p><p>CRI-O：<a href="https://github.com/cri-o/cri-o">https://github.com/cri-o/cri-o</a></p><h2 id="我仍然可以在-Kubernetes-1-20-中使用-Docker-吗？"><a href="#我仍然可以在-Kubernetes-1-20-中使用-Docker-吗？" class="headerlink" title="我仍然可以在 Kubernetes 1.20 中使用 Docker 吗？"></a>我仍然可以在 Kubernetes 1.20 中使用 Docker 吗？</h2><p>是的，如果使用 Docker 作为运行时，在 1.20 中只会在 Kubelet 启动时打印一个警告日志。Kubernetes 最早将在 2021 年末发布 1.23 版本中将 dockershim 移除。</p><h2 id="我现有的-Docker-镜像仍然可以使用吗？"><a href="#我现有的-Docker-镜像仍然可以使用吗？" class="headerlink" title="我现有的 Docker 镜像仍然可以使用吗？"></a>我现有的 Docker 镜像仍然可以使用吗？</h2><p>仍然可以使用。Docker 生成的镜像实际上并不是特定于 Docker 的镜像，而是 OCI（Open Container Initiative）镜像。无论你使用什么工具构建镜像，任何符合 OCI 标准的镜像在 Kubernetes 看来都是一样的。containerd 和 CRI-O 都能够提取这些镜像并运行它们。所以您可以仍然使用 Docker 来构建容器镜像，并且可以继续在 containerd 和 CRI-O 上使用。</p><h2 id="我应该使用哪个-CRI-实现？"><a href="#我应该使用哪个-CRI-实现？" class="headerlink" title="我应该使用哪个 CRI 实现？"></a>我应该使用哪个 CRI 实现？</h2><p>这是一个比较复杂的问题，它取决于许多因素。如果您之前熟练使用 Docker，那么迁移到 containerd 应该是一个相对容易的选择，并且 containerd 具有更好的性能和更低的成本。当然，您也可以探索 CNCF 领域中的其他项目，来选择更适合您的环境。</p><p>来源：<a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/#which-cri-implementation-should-i-use">https://kubernetes.io/blog/2020/12/02/dockershim-faq/#which-cri-implementation-should-i-use</a></p><p>eBay 对 containerd 和 CRI-O 进行了一组性能测试，包括创建、启动、停止和删除容器，以比较它们所耗的时间。如图所示，containerd 在各个方面都表现良好，除了启动容器这项。从总用时来看，containerd 的用时比 cri-o 要短。</p><p>以下数据来自<a href="http://mp.weixin.qq.com/s?__biz=MzA3MDMyNDUzOQ==&mid=2650506128&idx=1&sn=aee08ff704c0a7be5dcf7b3dd8c0438a&chksm=8731bbc4b04632d240b98d12915573addb1c243da18d2b36f32a1f30838877a1ea32fe7cde1f&scene=21#wechat_redirect">eBay 的分享</a>：</p><img src="/kubernetes/k8s-deprecated-docker/640-20210113192444834" class="" title="图片"><ul><li>containerd 和 cri-o 的性能比较</li></ul><img src="/kubernetes/k8s-deprecated-docker/640" class="" title="图片"><ul><li>containerd 和 cri-o 的综合比较</li></ul><p>Rancher，阿里云，AWS， Google，IBM 和 Microsoft 作为初始成员（ <code>https://github.com/containerd/containerd/blob/master/ADOPTERS.md</code> ），共同建设 containerd 社区。2017 年 3 月，Docker 将 containerd 捐献给 CNCF（云原生计算基金会）。containerd 得到了快速的发展和广泛的支持。Docker 引擎已经将 containerd 作为容器生命周期管理的基础，Kubernetes 也在 2018 年 5 月，正式支持 containerd 作为容器运行时管理器。2019 年 2 月，<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247490522&idx=1&sn=288f1fdb5dbc4cc3f338fe66f391d969&chksm=e83a9d1cdf4d140a21c2244ac6b83bfc12309b0bc73d60d32564a843552cbb20781db634208e&scene=21#wechat_redirect">CNCF 宣布 containerd 毕业</a>，成为<strong>生产可用的项目</strong>，更加稳定。</p><h2 id="Rancher-对-Containerd-的支持"><a href="#Rancher-对-Containerd-的支持" class="headerlink" title="Rancher 对 Containerd 的支持"></a>Rancher 对 Containerd 的支持</h2><p>Rancher 在轻量级 Kubernetes 发行版 K3s 和 RKE2（2020 年 10 月推出）中早已将 containerd 作为默认的容器运行时。<strong>相信在 Rancher 2.x 支持 Kubernetes 1.20+ 之后会将这些宝贵经验运用到新版本的 Rancher 2.x 迭代中。</strong></p><p>其实 Kubernetes 弃用 Docker 这一决定已经酝酿很长时间了，可能对于没有密切关注这个方面的工程师来说有些措手不及。但其实无需特别担心：如果你是<strong>Kubernetes 的终端用户</strong>，这仅仅是一个后端容器运行时的更改，从使用方面来说几乎感觉不到区别；如果你是一名<strong>开发&#x2F;运维人员</strong>，你依旧可以继续使用 Docker 来构建镜像，以相同的方式将镜像推送到 Registry，并且将这些镜像部署到你的 Kubernetes 中；如果你是<strong>运行和操作集群的用户</strong>，你只需要将 Docker 切换成你需要的容器运行时即可。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s Service 之 Iptables 流转过程</title>
      <link href="/kubernetes/svc/k8s-svc-iptables-flow-process/"/>
      <url>/kubernetes/svc/k8s-svc-iptables-flow-process/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/svc/k8s-svc-iptables-flow-process/" target="_blank" title="https://www.xtplayer.cn/kubernetes/svc/k8s-svc-iptables-flow-process/">https://www.xtplayer.cn/kubernetes/svc/k8s-svc-iptables-flow-process/</a></p><h2 id="Service-介绍"><a href="#Service-介绍" class="headerlink" title="Service 介绍"></a>Service 介绍</h2><p>K8S 中 Service 通过使用 labels 直接指向 Pods，这种设计的灵活性极高，因为创建 Pods 的方式有很多，而 Service 不需要关心 Pods 通过哪种方式创建。同时也避免了 Pod 重建后 Pod IP 自动更换导致服务崩溃的问题。下文将对 Service ClusterIP 和 NodePort 这两种类型做详述。</p><ul><li><p><strong>ClusterIP</strong>：集群内部 IP，用于连接后端 Pod 实例的内部 IP，集群中 Pod 可通过 <code>http://ClusterIP:Port</code> 访问到后端 Pod 的 TargetPort 端口上，能够让集群中的 Pod 通过一个 ServiceName 或者 ClusterIP 达到可用性。</p></li><li><p><strong>NodePort</strong>：NodePort 在集群中的主机节点上为 Service 提供一个代理端口，以允许从主机网络上对 Service 进行访问。</p></li></ul><h2 id="Service-之-Iptables-流转过程"><a href="#Service-之-Iptables-流转过程" class="headerlink" title="Service 之 Iptables 流转过程"></a>Service 之 Iptables 流转过程</h2><p>在 Rancher UI 上部署工作负载，选择 Service 类型为 NodePort，为了方便用户使用，默认会新建出两种类型的 Service(NodePort+ClusterIP)，后端 POD IP 为 10.42.1.24。</p><img src="/kubernetes/svc/k8s-svc-iptables-flow-process/U9Hq95SUkO1mzbMVlRXFcodWy3joy9rkMQ.png" class="" title="img"><p>登录 POD 所在主机节点，通过 netstat 命令可以看到 kube-proxy 在主机网络上创建了 31967 监听端口，用于接收从主机网络进入的外部流量。</p><img src="/kubernetes/svc/k8s-svc-iptables-flow-process/M6novMXWfMtjC7z4-_G9avVAjlHcEeeInQ.png" class="" title="img"><h3 id="iptables-流转过程-—-gt-NodePort类型"><a href="#iptables-流转过程-—-gt-NodePort类型" class="headerlink" title="iptables 流转过程 —&gt; NodePort类型"></a>iptables 流转过程 —&gt; <strong>NodePort</strong>类型</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]<span class="comment"># iptables-save &gt; iptables-dump.txt</span></span><br><span class="line">[root@node01 ~]<span class="comment"># cat iptables-dump.txt</span></span><br><span class="line"><span class="comment"># 通过 PREROUTING 链 reject 到 KUBE-SERVICES 链</span></span><br><span class="line">-A PREROUTING -m comment --comment <span class="string">&quot;kubernetes service portals&quot;</span> -j KUBE-SERVICES</span><br><span class="line"><span class="comment"># KUBE-SERVICES 链都是 ClusterIP 的匹配规则，NodePort 只能匹配最后一条 reject 到 KUBE-NODEPORTS 链</span></span><br><span class="line">-A KUBE-SERVICES -m comment --comment <span class="string">&quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&quot;</span> -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span><br><span class="line"><span class="comment"># 第一条规则 reject 到 KUBE-MARK-MASQ 链，目的是打标签，为匹配这个标签的数据包进行 SNAT；</span></span><br><span class="line"><span class="comment"># 第二条规则 reject 到 KUBE-SVC-AZNGHPMS24SQXQFB 链</span></span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="string">&quot;default/nginx-test-nodeport:80tcp01&quot;</span> -m tcp --dport 31967 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="string">&quot;default/nginx-test-nodeport:80tcp01&quot;</span> -m tcp --dport 31967 -j KUBE-SVC-AZNGHPMS24SQXQFB</span><br><span class="line"><span class="comment"># 流转到 KUBE-MARK-MASQ 链将打上 0x4000/0x4000 的标签</span></span><br><span class="line">-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span><br><span class="line"><span class="comment"># 流转到 KUBE-SVC-AZNGHPMS24SQXQFB 将 reject 到 KUBE-SEP-BBE6YPFJIF2XXMLH</span></span><br><span class="line">-A KUBE-SVC-AZNGHPMS24SQXQFB -j KUBE-SEP-BBE6YPFJIF2XXMLH</span><br><span class="line"><span class="comment"># 第一条规则源 IP 为后端 POD 时 reject 到 KUBE-MARK-MASQ，同上述一样目的是打标签，为匹配这个标签的数据包进行 SNAT；</span></span><br><span class="line"><span class="comment"># 第二条规则 DNAT，将主机节点:31967---&gt;10.42.1.24:80</span></span><br><span class="line">-A KUBE-SEP-BBE6YPFJIF2XXMLH -s 10.42.1.24/32 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SEP-BBE6YPFJIF2XXMLH -p tcp -m tcp -j DNAT --to-destination 10.42.1.24:80</span><br><span class="line"><span class="comment"># 最后 POSTROUTING 链 reject 到 KUBE-POSTROUTING 链</span></span><br><span class="line">-A POSTROUTING -m comment --comment <span class="string">&quot;kubernetes postrouting rules&quot;</span> -j KUBE-POSTROUTING</span><br><span class="line"><span class="comment"># KUBE-POSTROUTING 链对匹配标签的数据包进行 SNAT</span></span><br><span class="line">-A KUBE-POSTROUTING -m comment --comment <span class="string">&quot;kubernetes service traffic requiring SNAT&quot;</span> -m mark --mark 0x4000/0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure><h3 id="iptables-流转过程-—-gt-ClusterIP类型"><a href="#iptables-流转过程-—-gt-ClusterIP类型" class="headerlink" title="iptables 流转过程 —&gt; ClusterIP类型"></a>iptables 流转过程 —&gt; <strong>ClusterIP</strong>类型</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过 PREROUTING 链 reject 到 KUBE-SERVICES 链</span></span><br><span class="line">-A PREROUTING -m comment --comment <span class="string">&quot;kubernetes service portals&quot;</span> -j KUBE-SERVICES</span><br><span class="line"><span class="comment"># 第一条规则是源 IP 不在 POD IP 范围段内的 reject 到 KUBE-MARK-MASQ，目的是打标签，为匹配这个标签的数据包进行 SNAT，打标签的 iptables 规则与上述一直，省略；</span></span><br><span class="line"><span class="comment"># 第二条规则是 reject 到 KUBE-SVC-FQ4KKJK52D4C6QCF 链</span></span><br><span class="line">-A KUBE-SERVICES ! -s 10.42.0.0/16 -d 10.43.250.103/32 -p tcp -m comment --comment <span class="string">&quot;default/nginx-test:80tcp01-nginx-test cluster IP&quot;</span> -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 10.43.250.103/32 -p tcp -m comment --comment <span class="string">&quot;default/nginx-test:80tcp01-nginx-test cluster IP&quot;</span> -m tcp --dport 80 -j KUBE-SVC-FQ4KKJK52D4C6QCF</span><br><span class="line"><span class="comment"># 流转到 KUBE-SVC-FQ4KKJK52D4C6QCF 链将 reject 到 KUBE-SEP-6R4HQWWR3BRXNUXG 链</span></span><br><span class="line">-A KUBE-SVC-FQ4KKJK52D4C6QCF -j KUBE-SEP-6R4HQWWR3BRXNUXG</span><br><span class="line"><span class="comment"># 第一条规则源 IP 为后端 POD 时 reject 到 KUBE-MARK-MASQ，同上述一样目的是打标签，为匹配这个标签的数据包进行 SNAT；</span></span><br><span class="line"><span class="comment"># 第二条规则 DNAT，将 ClusterIP:80---&gt;10.42.1.24:80</span></span><br><span class="line">-A KUBE-SEP-6R4HQWWR3BRXNUXG -s 10.42.1.24/32 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SEP-6R4HQWWR3BRXNUXG -p tcp -m tcp -j DNAT --to-destination 10.42.1.24:80</span><br><span class="line"><span class="comment"># 最后 POSTROUTING 链 reject 到 KUBE-POSTROUTING 链</span></span><br><span class="line">-A POSTROUTING -m comment --comment <span class="string">&quot;kubernetes postrouting rules&quot;</span> -j KUBE-POSTROUTING</span><br><span class="line"><span class="comment"># KUBE-POSTROUTING 链对匹配标签的数据包进行 SNAT</span></span><br><span class="line">-A KUBE-POSTROUTING -m comment --comment <span class="string">&quot;kubernetes service traffic requiring SNAT&quot;</span> -m mark --mark 0x4000/0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> svc </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>导入 k8s 集群更新 CA 证书后 Rancher 端的配置操作</title>
      <link href="/rancher/import-k8s-cluster-update-ca/"/>
      <url>/rancher/import-k8s-cluster-update-ca/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/import-k8s-cluster-update-ca/" target="_blank" title="https://www.xtplayer.cn/rancher/import-k8s-cluster-update-ca/">https://www.xtplayer.cn/rancher/import-k8s-cluster-update-ca/</a></p><p>在早期的 rancher 版本中，如果导入的业务 K8S 集群因为证书到期重新生成了证书（包括 kube-ca 证书），这个时候在 Rancher UI shell 终端中执行命令时会报 x509 错误，UI 的一些操作也会报 x509 相关的错误，即使重启集群所有节点也没有效果。</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>根据 rancher 的运行逻辑，在第一次导入 rancher 的时候，cluster agent 和 node agent 会上报导入集群的所有配置信息到 rancher ，比如主机 IP，主机配置资源，kube-ca 证书等。</p><p>但是有的资源是只上传一次，比如 kube-ca 证书，上传后会写入 CRD 资源中保存，后期 agent 不管是重启还是重建均不会再上传 kube-ca 证书。所以如果是更新了集群 kube-ca 证书，那需要手动去更新 CRD 资源中的旧 kube-ca 证书。</p><h2 id="解决步骤"><a href="#解决步骤" class="headerlink" title="解决步骤"></a>解决步骤</h2><h3 id="Rancher-相关操作"><a href="#Rancher-相关操作" class="headerlink" title="Rancher 相关操作"></a>Rancher 相关操作</h3><h4 id="Rancher-单容器部署备份"><a href="#Rancher-单容器部署备份" class="headerlink" title="Rancher 单容器部署备份"></a>Rancher 单容器部署备份</h4><p>  如果在 <code>docker run</code> 部署 Rancher 时，有通过 <code>-v</code> 把容器中 <code>/var/lib/rancher</code> 目录映射到主机上，那么此步骤可以跳过；如果没有映射目录，那么需要执行以下步骤进行备份：</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RANCHER_CONTAINER_ID=</span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;RANCHER_CONTAINER_ID&#125;</span> tar zcvf /tmp/rancher-data-backup-&lt;RANCHER_VERSION&gt;-&lt;DATE&gt;.tar.gz /var/lib/rancher</span><br><span class="line">docker <span class="built_in">cp</span> <span class="variable">$&#123;RANCHER_CONTAINER_ID&#125;</span>:/tmp/rancher-data-backup-&lt;RANCHER_VERSION&gt;-&lt;DATE&gt;.tar.gz .</span><br></pre></td></tr></table></figure><h4 id="Rancher-HA-部署备份"><a href="#Rancher-HA-部署备份" class="headerlink" title="Rancher HA 部署备份"></a>Rancher HA 部署备份</h4><p>  指定 rke 配置文件进行 local 集群备份：</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./rke_linux-amd64 etcd snapshot-save --name SNAPSHOT-201903xx.db --config cluster.yml</span><br></pre></td></tr></table></figure><p>  RKE 会获取每个节点的 etcd 快照，并保存到每个节点的 <code>/opt/rke/etcd-snapshots</code> 目录下。</p><h3 id="k8s-相关操作"><a href="#k8s-相关操作" class="headerlink" title="k8s 相关操作"></a>k8s 相关操作</h3><h4 id="文件准备"><a href="#文件准备" class="headerlink" title="文件准备"></a>文件准备</h4><p><code>kubeconfig</code>: 更新证书后的 kubeconfig 配置文件</p><h4 id="删除业务集群-cattle-system-命名空间中的-cattle-token-xxx"><a href="#删除业务集群-cattle-system-命名空间中的-cattle-token-xxx" class="headerlink" title="删除业务集群 cattle-system 命名空间中的 cattle-token-xxx"></a>删除业务集群 <code>cattle-system</code> 命名空间中的 <code>cattle-token-xxx</code></h4><p>通过以下命令删除 <code>cattle-token-xxx</code>，它会基于最新的 CA 证书重新生成 <code>cattle-token-xxx</code>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeconfig=xxx.yaml</span><br><span class="line">cattle_token=$( kubectl --kubeconfig=<span class="variable">$&#123;kubeconfig&#125;</span> -n cattle-system get secret | grep <span class="string">&#x27;cattle-token-&#x27;</span> | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;kubeconfig&#125;</span> -n cattle-system delete secret <span class="variable">$&#123;cattle_token&#125;</span></span><br></pre></td></tr></table></figure><h4 id="重建-Agent-Pod"><a href="#重建-Agent-Pod" class="headerlink" title="重建 Agent Pod"></a>重建 Agent Pod</h4><p>因为 Agent Pod 绑定了 <code>SA</code>，而 <strong>SA</strong> 绑定了 <code>secrets</code>，所以当删除 <code>cattle-token-xxx</code> 后需重建 Pod 以加载新的 <code>secrets</code>。</p><p>执行以下命令批量删除 Agent Pod：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeconfig=xxx.yaml</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;kubeconfig&#125;</span> -n cattle-system get pod | grep -v <span class="string">&#x27;NAME&#x27;</span> | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | xargs kubectl --kubeconfig=<span class="variable">$&#123;kubeconfig&#125;</span> -n cattle-system delete pod</span><br></pre></td></tr></table></figure><h4 id="获取-token"><a href="#获取-token" class="headerlink" title="获取 token"></a>获取 <code>token</code></h4><p>重新生成 <code>cattle-token-xxx</code> 之后，取值 <code>token</code> 并 <code>base64 -d</code> 解密后备用。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeconfig=xxx.yaml</span><br><span class="line">cattle_token=$( kubectl --kubeconfig=<span class="variable">$&#123;kubeconfig&#125;</span> -n cattle-system get secret | grep <span class="string">&#x27;cattle-token-&#x27;</span> | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;kubeconfig&#125;</span> -n cattle-system get secret <span class="variable">$&#123;cattle_token&#125;</span> -o jsonpath=&#123;.data.token&#125; | <span class="built_in">base64</span> -d</span><br></pre></td></tr></table></figure><h4 id="获取新的-CA-证书"><a href="#获取新的-CA-证书" class="headerlink" title="获取新的 CA 证书"></a>获取新的 CA 证书</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeconfig=xxx.yaml</span><br><span class="line">cattle_token=$( kubectl --kubeconfig=<span class="variable">$&#123;kubeconfig&#125;</span> -n cattle-system get secret | grep <span class="string">&#x27;cattle-token-&#x27;</span> | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;kubeconfig&#125;</span> -n cattle-system get secret <span class="variable">$&#123;cattle_token&#125;</span> -o jsonpath=&#123;.data.<span class="string">&#x27;ca\.crt&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="在-local-集群中检查业务集群的-clusters-crd-资源是否有更新"><a href="#在-local-集群中检查业务集群的-clusters-crd-资源是否有更新" class="headerlink" title="在 local 集群中检查业务集群的 clusters crd 资源是否有更新"></a>在 local 集群中检查业务集群的 clusters crd 资源是否有更新</h4><p>在重建 Agent Pod 后，cluster agent 运行起来后会自动获取新 <code>cattle-token-xxx</code> 中的 <code>ca</code> 和 <code>token</code> 并上报给 rancher，并保存在 clusters crd 资源中。</p><p>在 local 集群中执行以下命令查看对应集群的 CRD 配置 YAML 文件中 <code>serviceAccountToken</code> 和 <code>caCert</code> 是否有更新。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">local_kubeconfig=xxx.yaml</span><br><span class="line">Cluster_ID=     <span class="comment"># 浏览器地址栏查看集群 ID</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;local_kubeconfig&#125;</span> get clusters <span class="variable">$&#123;Cluster_ID&#125;</span> -o yaml</span><br></pre></td></tr></table></figure><p><code>serviceAccountToken</code> 对应上面步骤获取的 <code>token</code>，<code>caCert</code> 对应 <code>CA</code> 证书，可以通过对比工具对比一下。</p><blockquote><p>已知问题：在相对老的 rancher 版本中，非 rke 集群可能出现 <code>ca</code> 或者 <code>token</code> 未自动上报的问题，如果出现资源未更新需按照以下步骤手动修改。</p></blockquote><ul><li>备份 crd clusters 资源</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">local_kubeconfig=xxx.yaml</span><br><span class="line">Cluster_ID=     <span class="comment">#浏览器地址栏查看集群 ID</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;local_kubeconfig&#125;</span> get clusters <span class="variable">$&#123;Cluster_ID&#125;</span> -o yaml &gt; <span class="variable">$&#123;Cluster_ID&#125;</span>.yaml</span><br></pre></td></tr></table></figure><ul><li>修改 crd clusters 资源<ul><li><code>serviceAccountToken</code> 字段参数：使用上面步骤中获取的 <code>token</code> 替换；</li><li><code>caCert</code> 字段参数: 使用上面步骤获取的新 CA 证书替换；</li></ul></li></ul><h2 id="已知问题"><a href="#已知问题" class="headerlink" title="已知问题"></a>已知问题</h2><p>业务集群升级证书并按照以上步骤操作后，可以正常进入 Rancher UI，也可以正常的部署应用，但是无法查看日志和执行 web shell 终端。</p><p>这是在 Rancher 2.1.x 版本中已知问题，因为在内存中保存着一份 CRD 资源副本，Rancher 运行时使用的内存中的 CRD 资源。在更新了 CRD 资源后，Rancher 没有感知到 CRD 资源变化，会一直使用内存中的 CRD 副本。旧 CRD 副本保存着旧的 CA 证书和 token，从而导致 Agent 无法连接 K8S 集群。</p><ul><li>解决方法</li></ul><p>目前有效的方法只有重启 Rancher server pod。</p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cattle-global-data/system-library-rancher-monitoring not found</title>
      <link href="/rancher/cattle-global-data-system-library-rancher-monitoring-not-found/"/>
      <url>/rancher/cattle-global-data-system-library-rancher-monitoring-not-found/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/cattle-global-data-system-library-rancher-monitoring-not-found/" target="_blank" title="https://www.xtplayer.cn/rancher/cattle-global-data-system-library-rancher-monitoring-not-found/">https://www.xtplayer.cn/rancher/cattle-global-data-system-library-rancher-monitoring-not-found/</a></p><p>rancher2.x 环境下，可能会出现以下错误信息，尤其是在离线环境下更容易出现。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CatalogTemplate.management.cattle.io <span class="string">&quot;cattle-global-data/system-library-rancher-monitoring&quot;</span> not found。</span><br></pre></td></tr></table></figure><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><ol><li><p>rancher2.x 很多附件功能是以 <code>helm chart</code> 部署，比如 <strong>monitoring</strong>。在 <strong>全局|工具|商店设置</strong> 可以看到  <strong>system-library</strong> 这个 repo，这个 repo 中存放了 system 相关的 <code>helm chart</code>。</p><img src="/rancher/cattle-global-data-system-library-rancher-monitoring-not-found/image-20210111191633074.png" class="" title="image-20210111191633074"><img src="/rancher/cattle-global-data-system-library-rancher-monitoring-not-found/image-20210111191711800.png" class="" title="image-20210111191711800"></li><li><p>rancher 在启动时会去 <strong>system-library</strong> repo 拉取 <code>helm chart</code> 文件，如果无法拉取 chart 文件，<strong>CatalogTemplate control</strong> 就会提示：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CatalogTemplate.management.cattle.io <span class="string">&quot;cattle-global-data/system-library-rancher-monitoring&quot;</span> not found。</span><br></pre></td></tr></table></figure></li></ol><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ol><li><p>如果是离线环境，可以把 <code>https://github.com/rancher/system-charts</code> 同步到本地 git 查看，然后在 <strong>全局|工具|商店设置</strong> 中修改 <strong>system-library</strong> 中对应的 URL 地址。</p></li><li><p>从 rancher <em>v2.3.0</em> 版本开始，<strong>system-library</strong> 相关的 chart 已内置到镜像中，只需要在安装的时候设置使用内置 chart 即可。</p><ul><li>Rancher HA ： 通过 chart 安装 rancher 的时候，添加 <code>--set useBundledSystemChart=true</code>。</li><li>Rancher 单容器：Rancher 单容器安装，传递环境变量： <code>CATTLE_SYSTEM_CATALOG=bundled</code>。</li></ul></li><li><p>如果要从内置 <code>system chart</code> 切换到外部 git repo，需要添加环境变量：<code>CATTLE_SYSTEM_CATALOG=external</code>。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher 单容器安装恢复</title>
      <link href="/rancher/backup-restore/rancher-single-container-restore/"/>
      <url>/rancher/backup-restore/rancher-single-container-restore/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/backup-restore/rancher-single-container-restore/" target="_blank" title="https://www.xtplayer.cn/rancher/backup-restore/rancher-single-container-restore/">https://www.xtplayer.cn/rancher/backup-restore/rancher-single-container-restore/</a></p><h2 id="恢复准备"><a href="#恢复准备" class="headerlink" title="恢复准备"></a>恢复准备</h2><p>以下信息需要提前准备，在备份时替换相应的值。</p><table><thead><tr><th>Placeholder</th><th>Example</th><th>Description</th></tr></thead><tbody><tr><td><code>&lt;RANCHER_CONTAINER_TAG&gt;</code></td><td><code>v2.0.5</code></td><td>初始安装 Rancher 时使用的 <code>rancher/rancher</code> 镜像版本</td></tr><tr><td><code>&lt;RANCHER_CONTAINER_NAME&gt;</code></td><td><code>festive_mestorf</code></td><td>Rancher 容器名称</td></tr><tr><td><code>&lt;RANCHER_VERSION&gt;</code></td><td><code>v2.0.5</code></td><td>创建的 Rancher 数据备份对应的 Rancher 版本</td></tr><tr><td><code>&lt;DATE&gt;</code></td><td><code>9-27-18</code></td><td>备份创建时间</td></tr></tbody></table><p>在终端中输入 <code>docker ps</code> 查询 <code>&lt;RANCHER_CONTAINER_TAG&gt;</code> 和 <code>&lt;RANCHER_CONTAINER_NAME&gt;</code></p><img src="/rancher/backup-restore/rancher-single-container-restore/placeholder-ref.png" class="" title="Placeholder Reference"><h2 id="集群恢复"><a href="#集群恢复" class="headerlink" title="集群恢复"></a>集群恢复</h2><ol><li><p>停止当前运行的 Rancher 容器.可通过 <code>docker ps</code> 查看 <code>&lt;RANCHER_CONTAINER_NAME&gt;</code></p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker stop &lt;RANCHER_CONTAINER_NAME&gt;</span><br></pre></td></tr></table></figure></li><li><p>复制<a href="/rancher/backup-restore/rancher-single-container-backups/">单节点备份</a>的压缩文件(<code>rancher-data-backup-&lt;RANCHER_VERSION&gt;-&lt;DATE&gt;.tar.gz</code>)到 rancher 主机上，通过 <code>cd</code> 命令切换到压缩文件所在的目录，并执行以下命令：</p><blockquote><p><strong>警告!</strong> 此命令将从 Rancher Server 容器中删除所有数据。</p></blockquote> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run  \</span><br><span class="line">--volumes-from &lt;RANCHER_CONTAINER_NAME&gt; \</span><br><span class="line">-v <span class="variable">$PWD</span>:/backup \</span><br><span class="line">alpine \</span><br><span class="line">sh -c <span class="string">&quot;rm /var/lib/rancher/* -rf &amp;&amp; tar zxvf /backup/rancher-data-backup-&lt;RANCHER_VERSION&gt;-&lt;DATE&gt;.tar.gz&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong> 需要替换 <code>&lt;RANCHER_CONTAINER_NAME&gt;,&lt;RANCHER_VERSION&gt;,&lt;DATE&gt;</code></p></blockquote></li><li><p>重新启动 Rancher Server 容器</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker start &lt;RANCHER_CONTAINER_NAME&gt;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> backup-restore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> backup-restore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher 单容器安装备份</title>
      <link href="/rancher/backup-restore/rancher-single-container-backups/"/>
      <url>/rancher/backup-restore/rancher-single-container-backups/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/backup-restore/rancher-single-container-backups/" target="_blank" title="https://www.xtplayer.cn/rancher/backup-restore/rancher-single-container-backups/">https://www.xtplayer.cn/rancher/backup-restore/rancher-single-container-backups/</a></p><p>在完成 Rancher 的单节点安装后，或在升级 Rancher 到新版本之前，需要对 Rancher 进行数据备份。如果在 Rancher 数据损坏或者丢失，或者升级遇到问题时，可以通过最新的备份进行数据恢复。</p><h2 id="备份准备"><a href="#备份准备" class="headerlink" title="备份准备"></a>备份准备</h2><p>以下信息需要提前准备，在备份时替换相应的值。</p><table><thead><tr><th>Placeholder</th><th>Example</th><th>Description</th></tr></thead><tbody><tr><td><code>&lt;RANCHER_CONTAINER_TAG&gt;</code></td><td><code>v2.0.5</code></td><td>初始安装 Rancher 时使用的 <code>rancher/rancher</code> 镜像版本</td></tr><tr><td><code>&lt;RANCHER_CONTAINER_NAME&gt;</code></td><td><code>festive_mestorf</code></td><td>Rancher 容器名称</td></tr><tr><td><code>&lt;RANCHER_VERSION&gt;</code></td><td><code>v2.0.5</code></td><td>创建的 Rancher 数据备份对应的 Rancher 版本</td></tr><tr><td><code>&lt;DATE&gt;</code></td><td><code>9-27-18</code></td><td>备份创建时间</td></tr></tbody></table><p>在终端中输入 <code>docker ps</code> 查询 <code>&lt;RANCHER_CONTAINER_TAG&gt;</code> 和 <code>&lt;RANCHER_CONTAINER_NAME&gt;</code></p><img src="/rancher/backup-restore/rancher-single-container-backups/placeholder-ref.png" class="" title="Placeholder Reference"><h2 id="创建备份"><a href="#创建备份" class="headerlink" title="创建备份"></a>创建备份</h2><ol><li><p>浏览器访问 Rancher UI，记下浏览器左下角显示的版本号(例如:<code>v2.0.0</code>),在后续备份过程中需要这个版本号</p></li><li><p>停止当前运行 Rancher Server 的容器，替换 <code>&lt;RANCHER_CONTAINER_ID&gt;</code> 为您真实的 Rancher 容器的 ID</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker stop `&lt;RANCHER_CONTAINER_ID&gt;`</span><br></pre></td></tr></table></figure><blockquote><p><strong>提示:</strong> 您可以输入 <code>docker ps</code> 命令获取 Rancher 容器的 ID</p></blockquote></li><li><p>创建数据卷容器</p><p>备份当前 Rancher Server 容器的数据到数据卷容器中</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker create \</span><br><span class="line">--volumes-from &lt;RANCHER_CONTAINER_NAME&gt; \</span><br><span class="line">--name rancher-data-&lt;DATE&gt; \</span><br><span class="line">rancher/rancher:&lt;RANCHER_CONTAINER_TAG&gt;</span><br></pre></td></tr></table></figure></li><li><p>创建 Rancher Server 数据卷容器备份</p><p>在升级期间，新的容器需要链接到数据卷容器，并且会对数据卷容器中的数据进行<code>更新/更改</code>。因此，需要提前对数据卷容器进行备份，<code>以防升级失败时用于数据回滚</code>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run  \</span><br><span class="line">--volumes-from rancher-data-&lt;DATE&gt; \</span><br><span class="line">-v <span class="variable">$PWD</span>:/backup \</span><br><span class="line">alpine \</span><br><span class="line">tar zcvf /backup/rancher-data-backup-&lt;RANCHER_VERSION&gt;-&lt;DATE&gt;.tar.gz /var/lib/rancher</span><br></pre></td></tr></table></figure></li><li><p>备份完成后可重启 Rancher 服务容器</p></li><li><p>了解数据恢复，请点击<a href="/rancher/backup-restore/rancher-single-container-restore/">单节点数据恢复</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> backup-restore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> backup-restore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher rke 集群恢复</title>
      <link href="/rancher/backup-restore/rancher-rke-cluster-restore/"/>
      <url>/rancher/backup-restore/rancher-rke-cluster-restore/</url>
      
        <content type="html"><![CDATA[<blockquote><p>如果 rancher 是 HA 架构部署，在 HA 架构下，rancher 的数据是保存在 rke local K8S 集群中。所以在 rancher HA 架构下只需要通过 RKE 恢复 local 集群数据，即可同时恢复 rancher 的数据。</p></blockquote><blockquote><p><strong>重要提示</strong> 此方法直接使用 RKE 进行集群恢复，它适用于 RKE 创建并导入的集群或者 RKE 部署的 local 集群</p></blockquote><h2 id="恢复准备"><a href="#恢复准备" class="headerlink" title="恢复准备"></a>恢复准备</h2><blockquote><p>提示</p></blockquote><ul><li>需要在进行操作的主机上提前<a href="https://docs2.rancher.cn/rke/installation/rke-install">安装 RKE</a>、<a href="https://docs2.rancher.cn/rancher2x/install-prepare/download/rke/">RKE 下载</a>和<a href="https://docs2.rancher.cn/rancher2x/install-prepare/kubectl/">安装 kubectl</a>。</li><li>在开始还原之前，请确保已停止旧集群节点上的所有 kubernetes 服务。</li><li>建议创建三个全新节点作为集群恢复的目标节点。有关节点需求，请参阅<a href="https://docs2.rancher.cn/rancher2x/installation/helm-ha-install/">HA 安装</a>。您也可以使用现有节点，清除 Kubernetes 和 Rancher 配置，<code>这将破坏这些节点上的数据请做好备份</code>，点击了解<a href="https://docs2.rancher.cn/rancher2x/install-prepare/remove-node">节点初始化</a>。</li></ul><h2 id="准备恢复节点并复制最新快照"><a href="#准备恢复节点并复制最新快照" class="headerlink" title="准备恢复节点并复制最新快照"></a>准备恢复节点并复制最新快照</h2><p>假设集群中一个或者多个 etcd 节点发生故障，或者整个集群数据丢失，则需要进行 etcd 集群恢复。</p><p>添加恢复节点并复制最新快照:</p><ol><li><p><code>恢复节点</code>可以是全新的节点，或者是之前集群中经过初始化的某个节点；</p></li><li><p>通过远程终端登录<code>恢复节点</code>；</p></li><li><p>创建快照目录:</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /opt/rke/etcd-snapshots/</span><br></pre></td></tr></table></figure></li><li><p>复制备份的<code>最新快照</code>到 <code>/opt/rke/etcd-snapshots/</code> 目录</p><ul><li>如果使用 <code>rke 0.2 之前版本</code>做的备份，需拷贝 <code>pki.bundle.tar.gz</code> 到 <code>/opt/rke/etcd-snapshots/</code> 目录下；</li><li>如果使用 <code>rke 0.2 以及以后版本</code>做的备份，拷贝 <code>xxx..rkestate</code> 文件到 <code>rke 配置文件</code>相同目录下；</li></ul></li></ol><h2 id="设置-RKE-配置文件"><a href="#设置-RKE-配置文件" class="headerlink" title="设置 RKE 配置文件"></a>设置 RKE 配置文件</h2><p>创建原始 <code>rancher-cluster.yml</code> 文件的副本，比如：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> rancher-cluster.yml rancher-cluster-restore.yml</span><br></pre></td></tr></table></figure><p>对副本配置文件进行以下修改:</p><ul><li><p>注释 <code>service</code> 中 <code>etcd</code> 的配置；</p></li><li><p>删除或注释掉整个 <code>addons:</code>部分，Rancher 部署和设置配置已在 etcd 数据库中，恢复不再需要；</p></li><li><p>在 <code>nodes:</code>部分添加<code>恢复节点</code>，注释掉其他节点；</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">例: rancher-cluster-restore.yml</span><br><span class="line">nodes:</span><br><span class="line">- address: 52.15.238.179     <span class="comment"># `添加恢复节点`</span></span><br><span class="line">  user: ubuntu</span><br><span class="line">  role: [ etcd, controlplane, worker ]</span><br><span class="line"><span class="comment"># 注释掉其他节点；</span></span><br><span class="line"><span class="comment"># - address: 52.15.23.24</span></span><br><span class="line"><span class="comment">#   user: ubuntu</span></span><br><span class="line"><span class="comment">#   role: [ etcd, controlplane, worker ]</span></span><br><span class="line"><span class="comment"># - address: 52.15.238.133</span></span><br><span class="line"><span class="comment">#   user: ubuntu</span></span><br><span class="line"><span class="comment">#   role: [ etcd, controlplane, worker ]</span></span><br><span class="line"><span class="comment"># 注释掉 `addons:`部分</span></span><br><span class="line"><span class="comment"># addons: |-</span></span><br><span class="line"><span class="comment">#   ---</span></span><br><span class="line"><span class="comment">#   kind: Namespace</span></span><br><span class="line"><span class="comment">#   apiVersion: v1</span></span><br><span class="line"><span class="comment">#   metadata:</span></span><br><span class="line"><span class="comment">#   ---</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure></li></ul><h2 id="恢复-ETCD-数据"><a href="#恢复-ETCD-数据" class="headerlink" title="恢复 ETCD 数据"></a>恢复 ETCD 数据</h2><ol><li><p>打开 <code>shell 终端</code>，切换到 RKE 二进制文件所在的目录，并且<code>上一步</code>修改的 <code>rancher-cluster-restore.yml</code> 文件也需要放在同一路径下。</p></li><li><p>根据系统类型，选择运行以下命令还原 <code>etcd</code> 数据：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MacOS</span></span><br><span class="line">./rke etcd snapshot-restore --name &lt;snapshot&gt;.db --config rancher-cluster-restore.yml</span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line">./rke etcd snapshot-restore --name &lt;snapshot&gt;.db --config rancher-cluster-restore.yml</span><br></pre></td></tr></table></figure><blockquote><p>RKE 将在<code>恢复节点</code>上创建包含已还原数据的 <code>ETCD</code> 容器，此容器将保持运行状态，但无法完成 etcd 初始化。</p></blockquote></li></ol><h2 id="恢复集群"><a href="#恢复集群" class="headerlink" title="恢复集群"></a>恢复集群</h2><p>通过 RKE 在<code>恢复节点</code>节点上启动集群。根据系统类型，选择运行以下命令运行集群：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MacOS</span></span><br><span class="line">./rke up --config ./rancher-cluster-restore.yml</span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line">./rke up --config ./rancher-cluster-restore.yml</span><br></pre></td></tr></table></figure><h2 id="查看节点状态"><a href="#查看节点状态" class="headerlink" title="查看节点状态"></a>查看节点状态</h2><p>RKE 运行完成后会创建 <code>kubectl</code> 的配置文件 <code>kube_config_rancher-cluster-restore.yml</code>，可通过这个配置文件查询 K8S 集群节点状态：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_rancher-cluster-restore.yml  get nodes</span><br><span class="line"></span><br><span class="line">NAME            STATUS    ROLES                      AGE       VERSION</span><br><span class="line">52.15.238.179   Ready     controlplane,etcd,worker    1m       v1.10.5</span><br><span class="line">18.217.82.189   NotReady  controlplane,etcd,worker   16d       v1.10.5</span><br><span class="line">18.222.22.56    NotReady  controlplane,etcd,worker   16d       v1.10.5</span><br><span class="line">18.191.222.99   NotReady  controlplane,etcd,worker   16d       v1.10.5</span><br></pre></td></tr></table></figure><h2 id="清理旧节点"><a href="#清理旧节点" class="headerlink" title="清理旧节点"></a>清理旧节点</h2><p>通过 kubectl 从集群中删除旧节点</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_rancher-cluster-restore.yml  delete node 18.217.82.189 18.222.22.56 18.191.222.99</span><br></pre></td></tr></table></figure><h2 id="重启-恢复节点"><a href="#重启-恢复节点" class="headerlink" title="重启 恢复节点"></a>重启 <code>恢复节点</code></h2><p><code>恢复节点</code>重启后，检查 <code>Kubernetes Pods</code> 的状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=kube_config_rancher-cluster-restore.yml  get pods --all-namespaces</span><br><span class="line"></span><br><span class="line">NAMESPACE       NAME                                    READY     STATUS    RESTARTS   AGE</span><br><span class="line">cattle-system   cattle-cluster-agent-766585f6b-kj88m    0/1       Error     6          4m</span><br><span class="line">cattle-system   cattle-node-agent-wvhqm                 0/1       Error     8          8m</span><br><span class="line">cattle-system   rancher-78947c8548-jzlsr                0/1       Running   1          4m</span><br><span class="line">ingress-nginx   default-http-backend-797c5bc547-f5ztd   1/1       Running   1          4m</span><br><span class="line">ingress-nginx   nginx-ingress-controller-ljvkf          1/1       Running   1          8m</span><br><span class="line">kube-system     canal-4pf9v                             3/3       Running   3          8m</span><br><span class="line">kube-system     cert-manager-6b47fc5fc-jnrl5            1/1       Running   1          4m</span><br><span class="line">kube-system     kube-dns-7588d5b5f5-kgskt               3/3       Running   3          4m</span><br><span class="line">kube-system     kube-dns-autoscaler-5db9bbb766-s698d    1/1       Running   1          4m</span><br><span class="line">kube-system     metrics-server-97bc649d5-6w7zc          1/1       Running   1          4m</span><br><span class="line">kube-system     tiller-deploy-56c4cf647b-j4whh          1/1       Running   1          4m</span><br></pre></td></tr></table></figure><blockquote><p>直到 Rancher 服务器启动并且 DNS&#x2F;负载均衡器指向新集群，<code>cattle-cluster-agent 和 cattle-node-agent</code>pods 将处于 <code>Error 或者 CrashLoopBackOff</code> 状态。</p></blockquote><h2 id="删除残留文件"><a href="#删除残留文件" class="headerlink" title="删除残留文件"></a>删除残留文件</h2><p>集群恢复有可能会导致一些组件对应的 SA 鉴权失效，通过查看 <code>kubectl --kubeconfig=kube_config_rancher-cluster-restore.yml  get pods --all-namespaces</code> 可以看到很多 pod 一直无法正常运行，查看 pod 日志可以看到 <code>Authentication failed</code> 等信息。这个时候需要删除一些旧的认证配置然后重新生成。</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeconfig=kube_config_cluster.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 SA Token 密文</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n kube-system get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;coredns|flannel|metrics-server|horizontal|canal|calico&#x27;</span> | \</span><br><span class="line">xargs kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | grep -E <span class="string">&#x27;rancher|cattle&#x27;</span> | \</span><br><span class="line">xargs kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n ingress-nginx  get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;nginx-ingress-serviceaccount&#x27;</span> | \</span><br><span class="line">xargs kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 kube-system 命名空间的 pod</span></span><br><span class="line">kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;coredns|flannel|metrics-server|horizontal|canal|calico&#x27;</span> | \</span><br><span class="line">xargs kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 cattle-system 命名空间的 pod</span></span><br><span class="line">kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;rancher|cattle-node|cattle-cluster-agent&#x27;</span> | \</span><br><span class="line">xargs kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 ingress-nginx 命名空间的 pod</span></span><br><span class="line">kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;nginx-ingress-controller&#x27;</span> | \</span><br><span class="line">xargs kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br></pre></td></tr></table></figure><h2 id="添加其他节点"><a href="#添加其他节点" class="headerlink" title="添加其他节点"></a>添加其他节点</h2><ol><li><p>编辑 RKE 配置文件 <code>rancher-cluster-restore.yml</code>,添加或者取消其他节点的注释，<code>addons</code> 保持注释状态。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">例：rancher-cluster-restore.yml</span><br><span class="line">nodes:</span><br><span class="line">- address: 52.15.238.179     <span class="comment"># `恢复节点`</span></span><br><span class="line">  user: ubuntu</span><br><span class="line">  role: [ etcd, controlplane, worker ]</span><br><span class="line">- address: 52.15.23.24</span><br><span class="line">  user: ubuntu</span><br><span class="line">  role: [ etcd, controlplane, worker ]</span><br><span class="line">- address: 52.15.238.133</span><br><span class="line">  user: ubuntu</span><br><span class="line">  role: [ etcd, controlplane, worker ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># addons: |-</span></span><br><span class="line"><span class="comment">#   ---</span></span><br><span class="line"><span class="comment">#   kind: Namespace</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p>更新集群</p><p> 根据系统类型，选择运行以下命令更新集群：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MacOS</span></span><br><span class="line">./rke up --config ./rancher-cluster-restore.yml</span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line">./rke up --config ./rancher-cluster-restore.yml</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> backup-restore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rke </tag>
            
            <tag> rancher </tag>
            
            <tag> backup-restore </tag>
            
            <tag> rancher restore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher 自定义集群恢复</title>
      <link href="/rancher/backup-restore/rancher-custom-cluster-restore/"/>
      <url>/rancher/backup-restore/rancher-custom-cluster-restore/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/backup-restore/rancher-custom-cluster-restore/" target="_blank" title="https://www.xtplayer.cn/rancher/backup-restore/rancher-custom-cluster-restore/">https://www.xtplayer.cn/rancher/backup-restore/rancher-custom-cluster-restore/</a></p><p>根据 etcd 集群的容错机制，可以从机器重启这样的临时故障中自动恢复。对于永久性故障（比如由于硬件故障导致成员节点无法再连接到集群），它支持最多 <code>(N-1)/2</code> 个成员节点永久断开连接。 如果超过<code>(N-1)/2</code> 个成员节点断开连接，则 etcd 集群因为无法进行仲裁而无法继续正常运行，这个时候所有的 etcd 实例都将变成只读状态。</p><blockquote><p>注意: 以下两种恢复方法要求 K8S 集群没有从 Rancher UI 中删除。</p></blockquote><h2 id="在有自动备份情况下恢复集群（UI-上集群未删除）"><a href="#在有自动备份情况下恢复集群（UI-上集群未删除）" class="headerlink" title="在有自动备份情况下恢复集群（UI 上集群未删除）"></a>在有自动备份情况下恢复集群（UI 上集群未删除）</h2><blockquote><p>提示: 此方法适用于 Rancher v2.2.0 及以后版本创建的自定义 k8s 集群，并且 k8s 集群未从 Rancher UI 删除。<br>注意: 如果有在 Rancher v2.2.0 之前版本创建的 k8s 集群，在升级 Rancher 之后，您必须编辑并更新集群，以便启用自动备份功能。即使在 Rancher v2.2.0 之前创建了备份，也必须执行此步骤，因为旧的备份无法用于通过 Rancher UI 恢复 etcd 数据。</p></blockquote><h3 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h3><p>要使用自定义 k8s 集群恢复功能，需要在创建集群时开启自动备份功能。如果集群变动比较频繁，可以把备份间隔缩短一些，根据磁盘大小设置保留的副本数。</p><img src="/rancher/backup-restore/rancher-custom-cluster-restore/image-20190526181815439.c4c53761.png" class="" title="image-20190526181815439.c4c53761"><p>系统会定时在 <code>/opt/rke/etcd-snapshots</code> 目录生成备份文件，并且在<code>集群\工具\备份</code>视图下可以查看到历史备份。</p><img src="/rancher/backup-restore/rancher-custom-cluster-restore/image-20190526202210288.717adc4b.png" class="" title="image-20190526202210288.717adc4b"><h3 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h3><ol><li><p>在<strong>全局\集群</strong>视图中，定位到需要数据恢复的集群；</p></li><li><p>点击集群右侧的省略号菜单，点击从备份恢复;</p></li><li><p>选择要恢复的备份，点击保存；</p> <img src="/rancher/backup-restore/rancher-custom-cluster-restore/image-20190526204212567.5ad38b83.png" class="" title="image-20190526204212567.5ad38b83"></li></ol><h2 id="在无自动备份情况下恢复集群（UI-上集群未删除）"><a href="#在无自动备份情况下恢复集群（UI-上集群未删除）" class="headerlink" title="在无自动备份情况下恢复集群（UI 上集群未删除）"></a>在无自动备份情况下恢复集群（UI 上集群未删除）</h2><blockquote><p>提示: 此方法适用于所有 Rancher 版本创建的自定义 k8s 集群，并且 k8s 集群未从 Rancher UI 删除。</p></blockquote><p>如果集群原有两个 ETCD 节点而坏掉一个，或者原有三个 ETCD 节点坏掉两个。这个时候 ETCD 集群将自动降级，所有键值对变成只读状态，这种情况下只能进行 ETCD 集群恢复。</p><p>对于早期没有自动备份功能的 Rancher 版本创建的集群，或者没有开启自动备份的 k8s 集群，只能通过使用 <code>/var/lib/etcd</code> 目录的 etcd 数据进行恢复，如果 <code>/var/lib/etcd</code> 目录被丢失，将无法进行恢复。</p><h3 id="操作步骤-1"><a href="#操作步骤-1" class="headerlink" title="操作步骤"></a>操作步骤</h3><ol><li><p>停止异常的 ETCD 节点或者在异常 ETCD 节点上执行 docker rm -f etcd 删除 ETCD 容器，保证环境中只有一个 ETCD 实例运行；</p></li><li><p>在剩下的最后一个 etcd 节点上，运行以下命令：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --<span class="built_in">rm</span> -v /var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">    registry.cn-shenzhen.aliyuncs.com/rancher/runlike etcd</span><br></pre></td></tr></table></figure></li><li><p>运行此命令将输出 etcd 的 running 命令，例如：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --name=etcd --hostname=ubuntu2 \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_API=3&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_CACERT=/etc/kubernetes/ssl/kube-ca.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_CERT=/etc/kubernetes/ssl/kube-etcd-1-1-1-133.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_KEY=/etc/kubernetes/ssl/kube-etcd-1-1-1-133-key.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_ENDPOINT=https://0.0.0.0:2379&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCD_UNSUPPORTED_ARCH=x86_64&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span> \</span><br><span class="line">--volume=<span class="string">&quot;/var/lib/etcd:/var/lib/rancher/etcd/:z&quot;</span> \</span><br><span class="line">--volume=<span class="string">&quot;/etc/kubernetes:/etc/kubernetes:z&quot;</span> \</span><br><span class="line">--network=host \</span><br><span class="line">--restart=always \</span><br><span class="line">--label io.rancher.rke.container.name=<span class="string">&quot;etcd&quot;</span> \</span><br><span class="line">--detach=<span class="literal">true</span> rancher/coreos-etcd:v3.2.24-rancher1 /usr/local/bin/etcd \</span><br><span class="line">--peer-client-cert-auth \</span><br><span class="line">--client-cert-auth \</span><br><span class="line">--peer-cert-file=/etc/kubernetes/ssl/kube-etcd-1-1-1-133.pem \</span><br><span class="line">--initial-cluster-token=etcd-cluster-1 \</span><br><span class="line">--initial-cluster=etcd-1.1.1.128=https://1.1.1.128:2380,etcd-1.1.1.133=https://1.1.1.133:2380 \</span><br><span class="line">--peer-trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">--key-file=/etc/kubernetes/ssl/kube-etcd-1-1-1-133-key.pem \</span><br><span class="line">--data-dir=/var/lib/rancher/etcd/ \</span><br><span class="line">--advertise-client-urls=https://1.1.1.133:2379,https://1.1.1.133:4001 \</span><br><span class="line">--listen-client-urls=https://0.0.0.0:2379 \</span><br><span class="line">--trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">--peer-key-file=/etc/kubernetes/ssl/kube-etcd-1-1-1-133-key.pem \</span><br><span class="line">--heartbeat-interval=500 \</span><br><span class="line">--initial-advertise-peer-urls=https://1.1.1.133:2380 \</span><br><span class="line">--listen-peer-urls=https://0.0.0.0:2380 \</span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kube-etcd-1-1-1-133.pem \</span><br><span class="line">--election-timeout=5000 \</span><br><span class="line">--name=etcd-1.1.1.133 \</span><br><span class="line">--initial-cluster-state=new</span><br></pre></td></tr></table></figure></li><li><p>在剩下的最后一个 ETCD 节点中，停止运行的 ETCD 容器并将其重命名为 etcd-old</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker stop etcd</span><br><span class="line">docker rename etcd etcd-old</span><br></pre></td></tr></table></figure></li><li><p>在剩下的最后一个 ETCD 节点中，执行以下命令进行 ETCD 集群初始化：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义节点 IP</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 注意，如果是多 IP 主机，需要根据第二步中显示的 IP 来判断其他节点中默认使用的是什么接口的 IP，因为在 `/etc/kubernetes/ssl/` 会以 IP 为格式命名生成 ETCD SSL 证书文件。</span></span><br><span class="line"></span><br><span class="line">NODE_IP=1.1.1.128</span><br><span class="line">ETCD_IMAGES=rancher/coreos-etcd:v3.2.24-rancher1</span><br><span class="line"></span><br><span class="line">docker run --name=etcd --hostname=`hostname` \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_API=3&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_CACERT=/etc/kubernetes/ssl/kube-ca.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_CERT=/etc/kubernetes/ssl/kube-etcd-`echo <span class="variable">$NODE_IP</span>|sed &#x27;s/\./-/g&#x27;`.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_KEY=/etc/kubernetes/ssl/kube-etcd-`echo <span class="variable">$NODE_IP</span>|sed &#x27;s/\./-/g&#x27;`-key.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_ENDPOINT=https://0.0.0.0:2379&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCD_UNSUPPORTED_ARCH=x86_64&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span> \</span><br><span class="line">--volume=<span class="string">&quot;/var/lib/etcd:/var/lib/rancher/etcd/:z&quot;</span> \</span><br><span class="line">--volume=<span class="string">&quot;/etc/kubernetes:/etc/kubernetes:z&quot;</span> \</span><br><span class="line">--network=host \</span><br><span class="line">--restart=always \</span><br><span class="line">--label io.rancher.rke.container.name=<span class="string">&quot;etcd&quot;</span> \</span><br><span class="line">--detach=<span class="literal">true</span> \</span><br><span class="line"><span class="variable">$ETCD_IMAGES</span> \</span><br><span class="line">/usr/local/bin/etcd \</span><br><span class="line">--peer-client-cert-auth \</span><br><span class="line">--client-cert-auth \</span><br><span class="line">--peer-cert-file=/etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`.pem \</span><br><span class="line">--peer-key-file=/etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`-key.pem \</span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`.pem \</span><br><span class="line">--trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">--initial-cluster-token=etcd-cluster-1 \</span><br><span class="line">--peer-trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">--key-file=/etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`-key.pem \</span><br><span class="line">--data-dir=/var/lib/rancher/etcd/ \</span><br><span class="line">--advertise-client-urls=https://<span class="variable">$NODE_IP</span>:2379,https://<span class="variable">$NODE_IP</span>:4001 \</span><br><span class="line">--listen-client-urls=https://0.0.0.0:2379 \</span><br><span class="line">--listen-peer-urls=https://0.0.0.0:2380 \</span><br><span class="line">--initial-advertise-peer-urls=https://<span class="variable">$NODE_IP</span>:2380 \</span><br><span class="line">--election-timeout=5000 \</span><br><span class="line">--heartbeat-interval=500 \</span><br><span class="line">--name=etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>` \</span><br><span class="line">--initial-cluster=etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`=https://<span class="variable">$NODE_IP</span>:2380 \</span><br><span class="line">--initial-cluster-state=new  \</span><br><span class="line">--force-new-cluster</span><br></pre></td></tr></table></figure></li><li><p>在剩下的最后一个 ETCD 节点中，执行以下命令添加第一个 ETCD MEMBER 节点</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MEMBER_IP=1.1.1.133</span><br><span class="line">docker <span class="built_in">exec</span> -ti etcd etcdctl member add etcd-`<span class="built_in">echo</span> <span class="variable">$MEMBER_IP</span> | sed <span class="string">&#x27;s/\./-/g&#x27;</span>` --peer-urls=https://<span class="variable">$MEMBER_IP</span>:2380</span><br></pre></td></tr></table></figure></li><li><p>执行以上命令后将输出以下信息，请保存这些信息，在运行 member 节点时将要使用。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:~<span class="comment"># docker exec -ti etcd etcdctl member add etcd-`echo $NODE2_IP | sed &#x27;s/\./-/g&#x27;` --peer-urls=https://$NODE2_IP:2380</span></span><br><span class="line">Member 5ca934ee06d672a8 added to cluster e51c3a946e907f98</span><br><span class="line"></span><br><span class="line">ETCD_NAME=<span class="string">&quot;etcd-1-1-1-133&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;etcd-1-1-1-133=https://1.1.1.133:2380,etcd-1-1-1-128=https://1.1.1.128:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">&quot;existing&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>执行以下命令查看成员状态，正常情况新加的成员会处于未开始状态，因为新的 ETCD 实例未运行。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -ti etcd etcdctl member list</span><br><span class="line"></span><br><span class="line">root@ubuntu1:~<span class="comment"># docker exec -ti etcd etcdctl member list</span></span><br><span class="line">5ca934ee06d672a8, unstarted, , https://1.1.1.133:2380,</span><br><span class="line">a57e863dc32700cb, started, etcd-1-1-1-128, https://1.1.1.128:2380, https://1.1.1.128:2379, https://1.1.1.128:4001</span><br></pre></td></tr></table></figure></li><li><p>在第一个 member 节点上执行以下命令添加第一个 member 节点。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义节点 IP</span></span><br><span class="line"><span class="comment">## 注意，如果是多 IP 主机，需要根据第二步中显示的 IP 来判断其他节点中默认使用的是什么接口的 IP，因为在 `/etc/kubernetes/ssl/` 会以 IP 为格式命名生成 ETCD SSL 证书文件。</span></span><br><span class="line"><span class="comment"># 备份原有 ETCD 数据</span></span><br><span class="line"><span class="built_in">mv</span> /var/lib/etcd /var/lib/etcd-bak-$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M&quot;</span>)</span><br><span class="line"></span><br><span class="line">NODE_IP=1.1.1.133</span><br><span class="line">ETCD_IMAGES=rancher/coreos-etcd:v3.2.24-rancher1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下三个配置为添加成员时返回</span></span><br><span class="line"></span><br><span class="line">ETCD_NAME=<span class="string">&quot;etcd-1-1-1-133&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;etcd-1-1-1-133=https://1.1.1.133:2380,etcd-1-1-1-128=https://1.1.1.128:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">&quot;existing&quot;</span></span><br><span class="line"></span><br><span class="line">docker run --name=etcd --hostname=`hostname` \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_API=3&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_CACERT=/etc/kubernetes/ssl/kube-ca.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_CERT=/etc/kubernetes/ssl/kube-etcd-`echo <span class="variable">$NODE_IP</span>|sed &#x27;s/\./-/g&#x27;`.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_KEY=/etc/kubernetes/ssl/kube-etcd-`echo <span class="variable">$NODE_IP</span>|sed &#x27;s/\./-/g&#x27;`-key.pem&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCDCTL_ENDPOINT=https://0.0.0.0:2379&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;ETCD_UNSUPPORTED_ARCH=x86_64&quot;</span> \</span><br><span class="line">--<span class="built_in">env</span>=<span class="string">&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span> \</span><br><span class="line">--volume=<span class="string">&quot;/var/lib/etcd:/var/lib/rancher/etcd/:z&quot;</span> \</span><br><span class="line">--volume=<span class="string">&quot;/etc/kubernetes:/etc/kubernetes:z&quot;</span> \</span><br><span class="line">--network=host \</span><br><span class="line">--restart=always \</span><br><span class="line">--label io.rancher.rke.container.name=<span class="string">&quot;etcd&quot;</span> \</span><br><span class="line">--detach=<span class="literal">true</span> \</span><br><span class="line"><span class="variable">$ETCD_IMAGES</span> \</span><br><span class="line">/usr/local/bin/etcd \</span><br><span class="line">--peer-client-cert-auth \</span><br><span class="line">--client-cert-auth \</span><br><span class="line">--peer-cert-file=/etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`.pem \</span><br><span class="line">--peer-key-file=/etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`-key.pem \</span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`.pem \</span><br><span class="line">--trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">--initial-cluster-token=etcd-cluster-1 \</span><br><span class="line">--peer-trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">--key-file=/etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$NODE_IP</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`-key.pem \</span><br><span class="line">--data-dir=/var/lib/rancher/etcd/ \</span><br><span class="line">--advertise-client-urls=https://<span class="variable">$NODE_IP</span>:2379,https://<span class="variable">$NODE_IP</span>:4001 \</span><br><span class="line">--listen-client-urls=https://0.0.0.0:2379 \</span><br><span class="line">--listen-peer-urls=https://0.0.0.0:2380 \</span><br><span class="line">--initial-advertise-peer-urls=https://<span class="variable">$NODE_IP</span>:2380 \</span><br><span class="line">--election-timeout=5000 \</span><br><span class="line">--heartbeat-interval=500 \</span><br><span class="line">--name=<span class="variable">$ETCD_NAME</span> \</span><br><span class="line">--initial-cluster=<span class="variable">$ETCD_INITIAL_CLUSTER</span> \</span><br><span class="line">--initial-cluster-state=<span class="variable">$ETCD_INITIAL_CLUSTER_STATE</span></span><br><span class="line"></span><br><span class="line">再次查看集群成员列表</span><br><span class="line"></span><br><span class="line">root@ubuntu1:~<span class="comment"># docker exec -ti etcd etcdctl member list</span></span><br><span class="line">ba3bb38009530a59, started, etcd-1-1-1-128, https://1.1.1.128:2380, https://1.1.1.128:2379,https://1.1.1.128:4001</span><br><span class="line">fa12c62d5695c420, started, etcd-1-1-1-133, https://1.1.1.133:2380, https://1.1.1.133:2379,https://1.1.1.133:4001</span><br></pre></td></tr></table></figure></li><li><p>重复上一步骤添加更多 member 节点</p></li></ol><h2 id="RKE-恢复（UI-上集群已删除）"><a href="#RKE-恢复（UI-上集群已删除）" class="headerlink" title="RKE 恢复（UI 上集群已删除）"></a>RKE 恢复（UI 上集群已删除）</h2><blockquote><p>注意: 以下的恢复方法是集群已从 UI 中删除。</p></blockquote><p>在前面两种恢复方案中，前提是自定义集群没有从 UI 中删除。某些原因导致自定义集群从 UI 中删除从而导致集群配置全部丢失，对于这种场景，目前不支持通过 UI 来恢复自定义集群。但可以通过备份的数据并使用 RKE 来恢复 K8S 集群，然后再把恢复的 K8S 导入 Rancher 中管理。</p><h3 id="存在旧集群的自动备份文件"><a href="#存在旧集群的自动备份文件" class="headerlink" title="存在旧集群的自动备份文件"></a>存在旧集群的自动备份文件</h3><p>如果之前的自定义集群开启了自动备份功能，那么在&#x2F;opt&#x2F;rke&#x2F;etcd-snapshots&#x2F;下会保存 ETCD 的备份数据。这个数据是通过 ZIP 压缩的，RKE 恢复的时候需要解压，比如：unzip 2020-01-12T09:14:51Z_etcd.zip。</p><h4 id="恢复步骤"><a href="#恢复步骤" class="headerlink" title="恢复步骤"></a>恢复步骤</h4><ol><li><p>准备恢复节点，恢复节点可以是全新的节点，或者是之前集群中经过初始化的某个节点；</p></li><li><p>拷贝旧集群的备份文件到恢复节点的某个路径，比如&#x2F;home&#x2F;restore;</p></li><li><p>执行以下命令恢复备份文件到 ETCD 默认数据目录 <code>/var/lib/etcd</code>，此目录不能已存在，etcdctl 会自动创建。</p><blockquote><p>这里需要使用 etcdctl 工具，下载地址：<a href="https://github.com/etcd-io/etcd/releases">https://github.com/etcd-io/etcd/releases</a></p></blockquote> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NODE_IP=<span class="string">&#x27;&#x27;</span> (当前节点 ip)</span><br><span class="line">ETCD_NAME=$( <span class="built_in">echo</span> etcd-$( <span class="built_in">echo</span> <span class="variable">$NODE_IP</span> | sed <span class="string">&#x27;s/\./-/g&#x27;</span> ) )</span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;<span class="subst">$( echo $ETCD_NAME=https://$&#123;NODE_IP&#125;:2380 )</span>&quot;</span></span><br><span class="line"></span><br><span class="line">ETCDCTL_API=3  etcdctl snapshot restore xxxxxxxxx \</span><br><span class="line">--data-dir=<span class="string">&quot;/var/lib/etcd&quot;</span> \</span><br><span class="line">--initial-cluster=<span class="variable">$&#123;ETCD_INITIAL_CLUSTER&#125;</span> \</span><br><span class="line">--initial-advertise-peer-urls=<span class="string">&quot;https://<span class="variable">$&#123;NODE_IP&#125;</span>:2380&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>根据<a href="https://docs.rancher.cn/docs/rke/example-yamls/_index/">RKE 配置示例</a>创建集群的 cluster.yaml 配置文件；</p></li><li><p>执行 rke up –config xxxx.yaml 拉起新的集群；</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">INFO[0039] [addons] Successfully saved ConfigMap <span class="keyword">for</span> addon rke-metrics-addon to Kubernetes</span><br><span class="line">INFO[0039] [addons] Executing deploy job rke-metrics-addon</span><br><span class="line">INFO[0040] [addons] Metrics Server deployed successfully</span><br><span class="line">INFO[0040] [ingress] removing installed ingress controller</span><br><span class="line">WARN[0108] Failed to deploy addon execute job [rke-ingress-controller]: Failed to get job complete status <span class="keyword">for</span> job rke-ingress-controller-delete-job <span class="keyword">in</span> namespacekube-system</span><br><span class="line">INFO[0108] [addons] Setting up user addons</span><br><span class="line">INFO[0108] [addons] no user addons defined</span><br><span class="line">INFO[0108] Finished building Kubernetes cluster successfully</span><br></pre></td></tr></table></figure><blockquote><p>注意: 此处的 WARN 可以忽略，因为在 ETCD 数据中已经保存了 ingress 服务，而在 rke up 的时候会删除旧的 ingress 数据，比如 ingress SVC 和 ingress 对应的命名空间。删除 ingress 命名空间的时候可能会出现一直无法删除而超时的情况，最后无法获取 JOB 的状态，所以这个错误可以忽略。</p></blockquote></li><li><p>删除残留文件（可选）</p><p> 集群恢复有可能会导致一些组件对应的 SA 鉴权失效，通过查看 <code>kubectl --kubeconfig=kube_config_rancher-cluster-restore.yml get pods --all-namespaces</code> 可以看到很多 pod 一直无法正常运行，查看 pod 日志可以看到 <code>Authentication failed</code> 等信息。这个时候需要删除一些旧的认证配置然后重新生成。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeconfig=kube_config_cluster.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 SA Token 密文</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n kube-system get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;coredns|flannel|metrics-server|horizontal|canal|calico&#x27;</span> | \</span><br><span class="line">xargs kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | grep -E <span class="string">&#x27;rancher|cattle&#x27;</span> | \</span><br><span class="line">xargs kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n ingress-nginx  get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;nginx-ingress-serviceaccount&#x27;</span> | \</span><br><span class="line">xargs kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 kube-system 命名空间的 pod</span></span><br><span class="line">kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;coredns|flannel|metrics-server|horizontal|canal|calico&#x27;</span> | \</span><br><span class="line">xargs kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 cattle-system 命名空间的 pod</span></span><br><span class="line">kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;rancher|cattle-node|cattle-cluster-agent&#x27;</span> | \</span><br><span class="line">xargs kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 ingress-nginx 命名空间的 pod</span></span><br><span class="line">kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;nginx-ingress-controller&#x27;</span> | \</span><br><span class="line">xargs kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br></pre></td></tr></table></figure></li><li><p>接着在 rancher 中创建导入集群，然后把恢复的集群导入 rancher 中管理。</p><p> 重新导入的时候可能会提示 serviceaccount 或者 clusterrolebinding 已存在，那么需要通过以下命令去删除。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system clusterrole.rbac.authorization.k8s.io/proxy-clusterrole-kubeapiserver</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system clusterrolebinding.rbac.authorization.k8s.io/proxy-role-binding-kubernetes-master</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system serviceaccount/cattle</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system clusterrolebinding.rbac.authorization.k8s.io/cattle-admin-binding</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system clusterrole.rbac.authorization.k8s.io/cattle-admin</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system deployment.apps/cattle-cluster-agent</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system daemonset.apps/cattle-node-agent</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system get secrets | \</span><br><span class="line">grep cattle-credentials | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">xargs kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system secret</span><br></pre></td></tr></table></figure></li></ol><h3 id="没有旧集群的自动备份文件"><a href="#没有旧集群的自动备份文件" class="headerlink" title="没有旧集群的自动备份文件"></a>没有旧集群的自动备份文件</h3><p>对于早期的 rancher 版本，没有通过 UI 备份也没有开启自动备份。对于这种场景，只能通过&#x2F;var&#x2F;lib&#x2F;etcd 目录来重建集群，请检查&#x2F;var&#x2F;lib&#x2F;etcd 目录是否还存在。</p><h4 id="恢复步骤-1"><a href="#恢复步骤-1" class="headerlink" title="恢复步骤"></a>恢复步骤</h4><ol><li><p>准备恢复节点。恢复节点可以是全新的节点，或者是之前集群中经过初始化的某个节点；</p></li><li><p>拷贝旧集群节点中&#x2F;var&#x2F;lib&#x2F;etcd 目录到恢复节点的相同路径；</p></li><li><p>根据<a href="https://docs.rancher.cn/docs/rke/example-yamls/_index/">RKE 配置示例</a>创建集群的 cluster.yaml 配置文件；</p></li><li><p>执行 rke up –config xxxx.yaml 拉起新的集群；</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">INFO[0039] [addons] Successfully saved ConfigMap <span class="keyword">for</span> addon rke-metrics-addon to Kubernetes</span><br><span class="line">INFO[0039] [addons] Executing deploy job rke-metrics-addon</span><br><span class="line">INFO[0040] [addons] Metrics Server deployed successfully</span><br><span class="line">INFO[0040] [ingress] removing installed ingress controller</span><br><span class="line">WARN[0108] Failed to deploy addon execute job [rke-ingress-controller]: Failed to get job complete status <span class="keyword">for</span> job rke-ingress-controller-delete-job <span class="keyword">in</span>    namespacekube-system</span><br><span class="line">INFO[0108] [addons] Setting up user addons</span><br><span class="line">INFO[0108] [addons] no user addons defined</span><br><span class="line">INFO[0108] Finished building Kubernetes cluster successfully</span><br></pre></td></tr></table></figure><blockquote><p>注意: 此处的 WARN 可以忽略，因为在 ETCD 数据中已经保存了 ingress 服务，而在 rke up 的时候会删除旧的 ingress 数据，比如 ingress SVC 和 ingress 对应的命名空间。删除 ingress 命名空间的时候可能会出现一直无法删除而超时的情况，最后无法获取 JOB 的状态，所以这个错误可以忽略。</p></blockquote></li><li><p>删除残留文件（可选）</p><p> 集群恢复有可能会导致一些组件对应的 SA 鉴权失效，通过查看 <code>kubectl --kubeconfig=kube_config_rancher-cluster-restore.yml get pods --all-namespaces</code> 可以看到很多 pod 一直无法正常运行，查看 pod 日志可以看到 <code>Authentication failed</code> 等信息。这个时候需要删除一些旧的认证配置然后重新生成。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeconfig=kube_config_cluster.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 SA Token 密文</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n kube-system get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;coredns|flannel|metrics-server|horizontal|canal|calico&#x27;</span> | \</span><br><span class="line">xargs kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | grep -E <span class="string">&#x27;rancher|cattle&#x27;</span> | \</span><br><span class="line">xargs kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n ingress-nginx  get secrets | \</span><br><span class="line">grep <span class="string">&#x27;kubernetes.io/service-account-token&#x27;</span>| awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;nginx-ingress-serviceaccount&#x27;</span> | \</span><br><span class="line">xargs kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 secrets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 kube-system 命名空间的 pod</span></span><br><span class="line">kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;coredns|flannel|metrics-server|horizontal|canal|calico&#x27;</span> | \</span><br><span class="line">xargs kubectl -n kube-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 cattle-system 命名空间的 pod</span></span><br><span class="line">kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;rancher|cattle-node|cattle-cluster-agent&#x27;</span> | \</span><br><span class="line">xargs kubectl -n cattle-system --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重建 ingress-nginx 命名空间的 pod</span></span><br><span class="line">kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> get pod | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">grep -E <span class="string">&#x27;nginx-ingress-controller&#x27;</span> | \</span><br><span class="line">xargs kubectl -n ingress-nginx --kubeconfig=<span class="variable">$kubeconfig</span> delete --force --grace-period=0 pod</span><br></pre></td></tr></table></figure></li><li><p>接着在 rancher 中创建导入集群，然后把恢复的集群导入 rancher 中管理。</p><p> 重新导入的时候可能会提示 serviceaccount 或者 clusterrolebinding 已存在，那么需要通过以下命令去删除。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system clusterrole.rbac.authorization.k8s.io/proxy-clusterrole-kubeapiserver</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system clusterrolebinding.rbac.authorization.k8s.io/proxy-role-binding-kubernetes-master</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system serviceaccount/cattle</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system clusterrolebinding.rbac.authorization.k8s.io/cattle-admin-binding</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system clusterrole.rbac.authorization.k8s.io/cattle-admin</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system deployment.apps/cattle-cluster-agent</span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system daemonset.apps/cattle-node-agent</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$kubeconfig</span> -n cattle-system get secrets | \</span><br><span class="line">grep cattle-credentials | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | \</span><br><span class="line">xargs kubectl --kubeconfig=<span class="variable">$kubeconfig</span> delete -n cattle-system secret</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> backup-restore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rke </tag>
            
            <tag> rancher </tag>
            
            <tag> backup-restore </tag>
            
            <tag> rancher restore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher rke 集群备份</title>
      <link href="/rancher/backup-restore/rancher-rke-cluster-backups/"/>
      <url>/rancher/backup-restore/rancher-rke-cluster-backups/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/backup-restore/rancher-rke-cluster-backups/" target="_blank" title="https://www.xtplayer.cn/rancher/backup-restore/rancher-rke-cluster-backups/">https://www.xtplayer.cn/rancher/backup-restore/rancher-rke-cluster-backups/</a></p><blockquote><p>如果 rancher 是 HA 架构部署，在 HA 架构下，rancher 的数据是保存在 rke local K8S 集群中。所以在 rancher HA 架构下只需要通过 RKE 备份 local 集群数据，即可同时备份 rancher 的数据。</p></blockquote><blockquote><p><strong>重要提示</strong> 此方法直接使用 RKE 进行集群备份，它适用于 RKE 创建并导入的业务集群或者 RKE 部署的 local 集群</p></blockquote><p>本节介绍在 Rancher HA 下如何备份数据。</p><ul><li><p>Rancher Kubernetes Engine v0.1.7 或更高版本</p><p>RKE v0.1.7 以及更高版本才支持 <code>etcd</code> 快照功能</p></li><li><p><code>rancher-cluster.yml</code></p><p>需要使用到安装 Rancher 的 RKE 配置文件 <code>rancher-cluster.yml</code>，将此文件需放在与 RKE 二进制文件同级目录中</p></li></ul><h2 id="创建-ETCD-数据快照"><a href="#创建-ETCD-数据快照" class="headerlink" title="创建 ETCD 数据快照"></a>创建 ETCD 数据快照</h2><p>有两种方案创建 <code>etcd</code> 快照: 定时自动创建快照和或手动创建快照，每种方式对应特定的场景。</p><ul><li><p>方案 A: 定时自动创建快照</p><p>在 Rancher HA 安装后，我们建议配置 RKE 以定时(默认 5 分钟)自动创建快照，以便始终拥有可用的安全恢复点。</p></li><li><p>方案 B: 手动创建快照</p><p>我们建议在升级或恢复其他快照等事件之前创建一次性快照。</p></li></ul><h3 id="方案-A-定时自动创建快照"><a href="#方案-A-定时自动创建快照" class="headerlink" title="方案 A: 定时自动创建快照"></a>方案 A: 定时自动创建快照</h3><p>对于通过 RKE 高可用安装的 Rancher，我们建议开启定时自动创建快照，以便始终拥有安全的恢复点。</p><p>定时自动创建快照服务是 RKE 附带的服务，默认没有开启。可以通过在 <code>rancher-cluster.yml</code> 中添加配置来启用 etcd-snapshot(定时自动创建快照)服务。</p><p><strong>启用定时自动创建快照:</strong></p><ol><li><p>编辑 <code>rancher-cluster.yml</code> 配置文件；</p></li><li><p>在 <code>rancher-cluster.yml</code> 配置文件中添加以下代码:</p><p><em>rke 版本小于 0.2.x 或 rancher 版本小于 v2.2.0 时使用</em></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">services:</span><br><span class="line">  etcd:</span><br><span class="line">    snapshot: <span class="literal">true</span>          <span class="comment"># 设置 true 启用 ETCD 自动备份，设置 false 禁用；</span></span><br><span class="line">    creation: 6h0s          <span class="comment"># 快照创建间隔时间，不加此参数，默认 5 分钟；</span></span><br><span class="line">    retention: 24h          <span class="comment"># 快照有效期，此时间后快照将被删除；</span></span><br></pre></td></tr></table></figure><p><em>rke 版本大于等于 0.2.x 或 rancher 版本大于等于 v2.2.0 时使用</em></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">services:</span><br><span class="line">  etcd:</span><br><span class="line">    backup_config:</span><br><span class="line">      enabled: <span class="literal">true</span>         <span class="comment"># 设置 true 启用 ETCD 自动备份，设置 false 禁用；</span></span><br><span class="line">      interval_hours: 12    <span class="comment"># 快照创建间隔时间，不加此参数，默认 5 分钟；</span></span><br><span class="line">      retention: 6          <span class="comment"># etcd 备份保留份数；</span></span><br><span class="line">      <span class="comment"># S3 配置选项</span></span><br><span class="line">      s3backupconfig:</span><br><span class="line">        access_key: <span class="string">&quot;myaccesskey&quot;</span></span><br><span class="line">        secret_key:  <span class="string">&quot;myaccesssecret&quot;</span></span><br><span class="line">        bucket_name: <span class="string">&quot;my-backup-bucket&quot;</span></span><br><span class="line">        folder: <span class="string">&quot;folder-name&quot;</span> <span class="comment"># 此参数 v2.3.0 之后可用</span></span><br><span class="line">        endpoint: <span class="string">&quot;s3.eu-west-1.amazonaws.com&quot;</span></span><br><span class="line">        region: <span class="string">&quot;eu-west-1&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>根据实际需求修改以上参数；</p></li><li><p>保存并关闭 <code>rancher-cluster.yml</code>；</p></li><li><p>打开<strong>Terminal</strong>并切换路径到 RKE 二进制文件所在目录.确保 <code>rancher-cluster.yml</code> 也在这个路径下；</p></li><li><p>运行以下命令:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MacOS</span></span><br><span class="line">./rke_darwin-amd64 up --config rancher-cluster.yml</span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line">./rke_linux-amd64 up --config rancher-cluster.yml</span><br></pre></td></tr></table></figure></li></ol><blockquote><p><strong>结果:</strong> RKE 会在每个 etcd 节点上定时获取快照，并将快照将保存到每个 etcd 节点的:<code>/opt/rke/etcd-snapshots/</code> 目录下</p></blockquote><h3 id="方案-B-手动创建快照"><a href="#方案-B-手动创建快照" class="headerlink" title="方案 B: 手动创建快照"></a>方案 B: 手动创建快照</h3><blockquote><p><strong>警告</strong> 1、在 rke v0.2.0 以前的版本，RKE 将备份证书和配置文件到 <code>pki.bundle.tar.gz</code> 文件中，并保存在 <code>/opt/rke/etcd-snapshots</code> 目录中。通过 v0.2.0 之前的版本恢复系统时，需要快照和 pki 文件。<br>2、从 rke v0.2.0 开始，因为架构调整不再需要 <code>pki.bundle.tar.gz</code> 文件，当 rke 创建集群后，会在配置文件当前目录下生成 <code>xxxx.rkestate</code> 文件，文件中保存了集群的配置信息和各组件使用的证书信息。</p></blockquote><p><strong>手动创建快照:</strong></p><ol><li><p>打开<strong>Terminal</strong>并切换路径到 RKE 二进制文件所在目录.确保 <code>rancher-cluster.yml</code> 也在该路径下</p></li><li><p>输入以下命令:</p><blockquote><p>注意:替换 <code>&lt;SNAPSHOT.db&gt;</code> 为您设置的快照名称，例如:&lt;SNAPSHOT.db&gt;</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MacOS</span></span><br><span class="line">./rke etcd snapshot-save --name &lt;SNAPSHOT.db&gt; --config rancher-cluster.yml</span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line">./rke etcd snapshot-save --name &lt;SNAPSHOT.db&gt; --config rancher-cluster.yml</span><br></pre></td></tr></table></figure></li></ol><blockquote><p><strong>结果:</strong> RKE 会获取每个 <code>etcd</code> 节点的快照，并保存在每个 etcd 节点的 <code>/opt/rke/etcd-snapshots</code> 目录下；</p></blockquote><h2 id="备份快照到安全位置"><a href="#备份快照到安全位置" class="headerlink" title="备份快照到安全位置"></a>备份快照到安全位置</h2><p>在创建快照后，应该把它保存到安全的地方，以便在集群遇到灾难情况时快照不受影响，这个位置应该是持久的。</p><p>复制 <code>/opt/rke/etcd-snapshots</code> 目录下所有文件到安全位置。</p><ul><li>在 rke v0.2.0 以前的版本，备份 <code>/opt/rke/etcd-snapshots</code> 目录中的快照文件和 <code>pki.bundle.tar.gz</code> 文件，以及 rke 配置文件到安全位置，通过 v0.2.0 之前的版本恢复系统时，需要这些文件。</li><li>在 rke v0.2.0 以及以后的版本，备份 <code>/opt/rke/etcd-snapshots</code> 目录中的快照文件和 rke 配置文件，以及配置文件当前目录下的 <code>xxxx.rkestate</code> 文件，通过 v0.2.0 之后版本恢复系统时，需要这些文件。</li></ul>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> backup-restore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rke </tag>
            
            <tag> rancher </tag>
            
            <tag> backup-restore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher 自定义集群备份</title>
      <link href="/rancher/backup-restore/rancher-custom-cluster-backups/"/>
      <url>/rancher/backup-restore/rancher-custom-cluster-backups/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/backup-restore/rancher-custom-cluster-backups/" target="_blank" title="https://www.xtplayer.cn/rancher/backup-restore/rancher-custom-cluster-backups/">https://www.xtplayer.cn/rancher/backup-restore/rancher-custom-cluster-backups/</a></p><p><em>自 v2.2.0 起可用，仅支持自定义集群</em></p><p>在 Rancher UI 中，可以很容易对<code>自定义集群</code>做 etcd 备份和恢复。备份 etcd 数据并将其保存到 etcd 节点的 <code>/opt/rke/etcd-snapshots/</code> 目录下或上传到 S3 存储。</p><p>提示: Rancher 建议为所有生产集群配置 <code>etcd</code> 自动备份。</p><blockquote><p><strong>注意：</strong> 如果您有通过 <code>Rancher v2.2.0</code> 之前版本创建的自定义 Kubernetes 集群，则在升级 Rancher 之后，必须编辑该集群并保存它，以便启用<code>自动备份</code>功能。即使您已经使用 Rancher v2.2.0 之前版本创建了备份，也必须执行此步骤。因为早期的自动备份无法用于<a href="/rancher/backup-restore/rancher-custom-cluster-restore/">通过 UI 备份和还原 etcd</a>。</p></blockquote><h2 id="自动备份"><a href="#自动备份" class="headerlink" title="自动备份"></a>自动备份</h2><p>默认情况下，Rancher 启动的 Kubernetes 集群将启动自动备份（默认保存在本地磁盘 <code>/opt/rke/etcd-snapshots/</code> 目录下）。为了防止本地磁盘故障，建议使用<a href="https://rancher.com/docs/rancher/v2.x/en/cluster-admin/backing-up-etcd/#s3-backup-target">S3</a>或将 <code>/opt/rke/etcd-snapshots/</code> 目录备份到其他磁盘或者其他远程存储服务。在集群配置或编辑集群时，在<strong>集群选项</strong>的<code>高级集群选项</code>部分找到<code>自动备份</code>的配置。</p><p>在<strong>高级集群选项</strong>部分中，有几个选项可用于配置：</p><table><thead><tr><th align="left">选项</th><th align="left">描述</th><th align="left">默认值</th></tr></thead><tbody><tr><td align="left">etcd 备份存储</td><td align="left">选择要保存 etcd 备份的位置。<code>local 或在 S3</code></td><td align="left"><code>local</code></td></tr><tr><td align="left">Etcd 备份轮换</td><td align="left"><code>启用/禁用</code></td><td align="left"><code>启用</code></td></tr><tr><td align="left">备份周期</td><td align="left">etcd 重复备份之间的时间间隔（小时）</td><td align="left"><code>12</code></td></tr><tr><td align="left">备份副本</td><td align="left">要保留的备份副本数</td><td align="left"><code>6</code></td></tr></tbody></table><h3 id="S3-参数"><a href="#S3-参数" class="headerlink" title="S3 参数"></a>S3 参数</h3><table><thead><tr><th align="left">选项</th><th align="left">描述</th><th align="left">需要</th></tr></thead><tbody><tr><td align="left">S3 Bucket Name</td><td align="left">存储备份 Bucket 名称</td><td align="left">*</td></tr><tr><td align="left">S3 Region</td><td align="left">备用存储 Bucket 的 S3 区域</td><td align="left"></td></tr><tr><td align="left">S3 Region Endpoint</td><td align="left">S3Bucket 访问端点</td><td align="left">*</td></tr><tr><td align="left">S3 Folder</td><td align="left">文件备份路径</td><td align="left"></td></tr><tr><td align="left">S3 Access Key</td><td align="left"></td><td align="left">*</td></tr><tr><td align="left">S3 Secret Key</td><td align="left"></td><td align="left">*</td></tr><tr><td align="left">自定义 CA 证书</td><td align="left"><em>从 v2.2.5 可用</em>用于访问私有 S3 后端的自定义证书</td><td align="left"></td></tr></tbody></table><h2 id="手动备份"><a href="#手动备份" class="headerlink" title="手动备份"></a>手动备份</h2><p>除了自动备份外，Rancher UI 还支持手动临时创建备份。在对集群做修改变动前（例如，升级集群 Kubernetes 版本）需要备份集群数据，以防止升级失败后用于数据回滚或者防止数据损坏或丢失。</p><ol><li>在<strong>全局 》 集群</strong>视图中，切换到要手动备份的集群。</li><li>点击右侧的<code>省略号</code>菜单，然后点击<code>备份</code></li></ol><h2 id="安全时间戳"><a href="#安全时间戳" class="headerlink" title="安全时间戳"></a>安全时间戳</h2><p><em>自 v2.3.0 起可用</em></p><p>从 v2.2.6 版本开始，备份文件已打上时间戳。但是在某些存储后端中，时间戳格式可能不兼容。比如下图中的时间戳通过冒号<code>:</code>来分割，而有些后端存储不支持文件名中有特殊字符。从 Rancher v2.3.0 开始，添加了 <code>safe_timestamp</code> 参数，该参数以支持兼容的文件名。当此参数设置 <code>true</code> 为时，将替换快照文件名时间戳中的所有特殊字符。</p><blockquote><p><strong>注意：</strong> 此选项在 UI 中不直接可用，仅通过 <code>Edit as Yaml</code> 修改。</p></blockquote><h2 id="查看可用备份"><a href="#查看可用备份" class="headerlink" title="查看可用备份"></a>查看可用备份</h2><ol><li>在<strong>全局</strong>视图中，切换到要查看备份的集群。</li><li>从顶部菜单栏中选择<strong>工具&gt;备份</strong>，以查看已保存的备份列表。这些备份包括创建的时间戳。</li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> backup-restore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> backup-restore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher 离线安装镜像同步</title>
      <link href="/rancher/install/rancher-install-offline-images/"/>
      <url>/rancher/install/rancher-install-offline-images/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/rancher-install-offline-images/" target="_blank" title="https://www.xtplayer.cn/rancher/install/rancher-install-offline-images/">https://www.xtplayer.cn/rancher/install/rancher-install-offline-images/</a></p><h2 id="准备文件"><a href="#准备文件" class="headerlink" title="准备文件"></a>准备文件</h2><ol><li><p>使用可以访问 Internet 的主机，访问我们的版本 <a href="https://github.com/rancher/rancher/releases">发布页面</a>，找到需要安装的 <code>Rancher 2.x.x</code> 版本。不要下载 <code>rc</code> 或者 <code>Pre-release</code> 版本，因为它们不适用于稳定的生产环境。</p></li><li><p>从发行版本的 <strong>Assets</strong> 部分，下载 <code>rancher-images.txt</code>，此文件包含安装 Rancher 所需的所有镜像的列表。</p></li><li><p>通过 RKE 生成镜像清单</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke config --system-images -all &gt;&gt; ./rancher-images.txt</span><br></pre></td></tr></table></figure></li><li><p>对镜像列表进行排序和去重，以去除重复的镜像。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sort</span> -u rancher-images.txt -o rancher-images.txt</span><br></pre></td></tr></table></figure></li><li><p>复制以下脚本保存为 <code>rancher-save-images.sh</code>，与 <code>rancher-images.txt</code> 放在同一目录层级；</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 定义日志</span></span><br><span class="line">workdir=`<span class="built_in">pwd</span>`</span><br><span class="line">log_file=<span class="variable">$&#123;workdir&#125;</span>/sync_images_$(<span class="built_in">date</span> +<span class="string">&quot;%Y-%m-%d&quot;</span>).<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">logger</span></span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">log</span>=<span class="variable">$1</span></span><br><span class="line">    cur_time=<span class="string">&#x27;[&#x27;</span>$(<span class="built_in">date</span> +<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)<span class="string">&#x27;]&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$&#123;cur_time&#125;</span> <span class="variable">$&#123;log&#125;</span> | <span class="built_in">tee</span> -a <span class="variable">$&#123;log_file&#125;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">list=<span class="string">&quot;rancher-images.txt&quot;</span></span><br><span class="line"><span class="comment">#images=&quot;rancher-images.tar.gz&quot;</span></span><br><span class="line"></span><br><span class="line">POSITIONAL=()</span><br><span class="line"><span class="keyword">while</span> [[ <span class="variable">$#</span> -gt 0 ]];</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    key=<span class="string">&quot;<span class="variable">$1</span>&quot;</span></span><br><span class="line">    <span class="keyword">case</span> <span class="variable">$key</span> <span class="keyword">in</span></span><br><span class="line">        -i|--images)</span><br><span class="line">        images=<span class="string">&quot;<span class="variable">$2</span>&quot;</span></span><br><span class="line">        <span class="built_in">shift</span> <span class="comment"># past argument</span></span><br><span class="line">        <span class="built_in">shift</span> <span class="comment"># past value</span></span><br><span class="line">        ;;</span><br><span class="line">        -l|--image-list)</span><br><span class="line">        list=<span class="string">&quot;<span class="variable">$2</span>&quot;</span></span><br><span class="line">        <span class="built_in">shift</span> <span class="comment"># past argument</span></span><br><span class="line">        <span class="built_in">shift</span> <span class="comment"># past value</span></span><br><span class="line">        ;;</span><br><span class="line">        -h|--<span class="built_in">help</span>)</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;true&quot;</span></span><br><span class="line">        <span class="built_in">shift</span></span><br><span class="line">    ;;</span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">usage</span></span> () &#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;USAGE: <span class="variable">$0</span> [--image-list rancher-images.txt] [--images rancher-images.tar.gz]&quot;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;  [-l|--images-list path] text file with list of images. 1 per line.&quot;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;  [-l|--images path] tar.gz generated by docker save.&quot;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;  [-h|--help] Usage message&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$help</span> ]]; <span class="keyword">then</span></span><br><span class="line">    usage</span><br><span class="line">    <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(<span class="built_in">cat</span> <span class="variable">$&#123;list&#125;</span>);</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">     docker pull <span class="variable">$&#123;i&#125;</span> &gt;&gt; /dev/null 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line">     <span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">         logger <span class="string">&quot;image <span class="variable">$&#123;i&#125;</span> download failed.&quot;</span></span><br><span class="line">     <span class="keyword">else</span></span><br><span class="line">         logger <span class="string">&quot;image <span class="variable">$&#123;i&#125;</span> download complete.&quot;</span></span><br><span class="line">     <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"> logger <span class="string">&quot;save all images to rancher-images.tgz.&quot;</span></span><br><span class="line"> docker save $(<span class="built_in">cat</span> <span class="variable">$&#123;list&#125;</span>) | gzip &gt; rancher-images.tgz</span><br></pre></td></tr></table></figure></li><li><p>复制以下脚本保存为 <code>rancher-push-images.sh</code>，与 <code>rancher-images.txt</code> 放在同一目录层级；</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 镜像上传说明</span></span><br><span class="line"><span class="comment"># 需要先在镜像仓库中创建 rancher 项目</span></span><br><span class="line"><span class="comment"># 根据实际情况更改以下私有仓库地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义日志</span></span><br><span class="line">workdir=`<span class="built_in">pwd</span>`</span><br><span class="line">log_file=<span class="variable">$&#123;workdir&#125;</span>/sync_images_$(<span class="built_in">date</span> +<span class="string">&quot;%Y-%m-%d&quot;</span>).<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">logger</span></span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">log</span>=<span class="variable">$1</span></span><br><span class="line">    cur_time=<span class="string">&#x27;[&#x27;</span>$(<span class="built_in">date</span> +<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)<span class="string">&#x27;]&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$&#123;cur_time&#125;</span> <span class="variable">$&#123;log&#125;</span> | <span class="built_in">tee</span> -a <span class="variable">$&#123;log_file&#125;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">images_hub</span></span>() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">read</span> -p <span class="string">&quot;输入镜像仓库地址(不加 http/https): &quot;</span> registry</span><br><span class="line">        <span class="built_in">read</span> -p <span class="string">&quot;输入镜像仓库用户名: &quot;</span> registry_user</span><br><span class="line">        <span class="built_in">read</span> -p <span class="string">&quot;输入镜像仓库用户密码: &quot;</span> registry_password</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;您设置的仓库地址为: <span class="variable">$&#123;registry&#125;</span>,用户名: <span class="variable">$&#123;registry_user&#125;</span>,密码: xxx&quot;</span></span><br><span class="line">        <span class="built_in">read</span> -p <span class="string">&quot;是否确认(Y/N): &quot;</span> confirm</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> [ <span class="variable">$confirm</span> != Y ] &amp;&amp; [ <span class="variable">$confirm</span> != y ] &amp;&amp; [ <span class="variable">$confirm</span> == <span class="string">&#x27;&#x27;</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;输入不能为空，重新输入&quot;</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">images_hub</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;镜像仓库 <span class="subst">$(docker login -u $&#123;registry_user&#125; -p $&#123;registry_password&#125; $&#123;registry&#125;)</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#images=$(docker images -a | grep -v TAG | awk &#x27;&#123;print $1 &quot;:&quot; $2&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">images=$( <span class="built_in">cat</span> rancher-images.txt )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义全局项目，如果想把镜像全部同步到一个仓库，则指定一个全局项目名称；</span></span><br><span class="line">global_namespace=<span class="variable">$1</span>   <span class="comment"># rancher</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">docker_push</span></span>() &#123;</span><br><span class="line">    <span class="keyword">for</span> imgs <span class="keyword">in</span> $( <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;images&#125;</span>&quot;</span> );</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="keyword">if</span> [[ -n <span class="string">&quot;<span class="variable">$global_namespace</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line"></span><br><span class="line">            n=$(<span class="built_in">echo</span> <span class="variable">$&#123;imgs&#125;</span> | awk -F<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123;print NF-1&#125;&#x27;</span>)</span><br><span class="line">            <span class="comment"># 如果镜像名中没有/，那么此镜像一定是 library 仓库的镜像；</span></span><br><span class="line">            <span class="keyword">if</span> [ <span class="variable">$&#123;n&#125;</span> -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">                img_tag=<span class="variable">$&#123;imgs&#125;</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">#重命名镜像</span></span><br><span class="line">                docker tag <span class="variable">$&#123;imgs&#125;</span> <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;global_namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">                <span class="comment">#删除原始镜像</span></span><br><span class="line">                <span class="comment">#docker rmi $&#123;imgs&#125;</span></span><br><span class="line">                <span class="comment">#上传镜像</span></span><br><span class="line">                docker push <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;global_namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果镜像名中有一个/，那么/左侧为项目名，右侧为镜像名和 tag</span></span><br><span class="line">            <span class="keyword">elif</span> [ <span class="variable">$&#123;n&#125;</span> -eq 1 ]; <span class="keyword">then</span></span><br><span class="line">                img_tag=$(<span class="built_in">echo</span> <span class="variable">$&#123;imgs&#125;</span> | awk -F<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#重命名镜像</span></span><br><span class="line">                docker tag <span class="variable">$&#123;imgs&#125;</span> <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;global_namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">                <span class="comment">#删除旧镜像</span></span><br><span class="line">                <span class="comment">#docker rmi $&#123;imgs&#125;</span></span><br><span class="line">                <span class="comment">#上传镜像</span></span><br><span class="line">                docker push <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;global_namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果镜像名中有两个/，</span></span><br><span class="line">            <span class="keyword">elif</span> [ <span class="variable">$&#123;n&#125;</span> -eq 2 ]; <span class="keyword">then</span></span><br><span class="line">                img_tag=$(<span class="built_in">echo</span> <span class="variable">$&#123;imgs&#125;</span> | awk -F<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123;print $3&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#重命名镜像</span></span><br><span class="line">                docker tag <span class="variable">$&#123;imgs&#125;</span> <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;global_namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">                <span class="comment">#删除旧镜像</span></span><br><span class="line">                <span class="comment">#docker rmi $&#123;imgs&#125;</span></span><br><span class="line">                <span class="comment">#上传镜像</span></span><br><span class="line">                docker push <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;global_namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="comment">#标准镜像为四层结构，即：仓库地址/项目名/镜像名:tag,如不符合此标准，即为非有效镜像。</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">&quot;No available images&quot;</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            n=$(<span class="built_in">echo</span> <span class="variable">$&#123;imgs&#125;</span> | awk -F<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123;print NF-1&#125;&#x27;</span>)</span><br><span class="line">            <span class="comment"># 如果镜像名中没有/，那么此镜像一定是 library 仓库的镜像；</span></span><br><span class="line">            <span class="keyword">if</span> [ <span class="variable">$&#123;n&#125;</span> -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">                img_tag=<span class="variable">$&#123;imgs&#125;</span></span><br><span class="line">                namespace_1=library</span><br><span class="line">                namespace_2=rancher</span><br><span class="line"></span><br><span class="line">                <span class="comment">#重命名镜像</span></span><br><span class="line">                docker tag <span class="variable">$&#123;imgs&#125;</span> <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;namespace_1&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">                docker tag <span class="variable">$&#123;imgs&#125;</span> <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;namespace_2&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">                <span class="comment">#删除原始镜像</span></span><br><span class="line">                <span class="comment">#docker rmi $&#123;imgs&#125;</span></span><br><span class="line">                <span class="comment">#上传镜像</span></span><br><span class="line">                docker push <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;namespace_1&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">                docker push <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;namespace_2&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果镜像名中有一个/，那么/左侧为项目名，右侧为镜像名和 tag</span></span><br><span class="line">            <span class="keyword">elif</span> [ <span class="variable">$&#123;n&#125;</span> -eq 1 ]; <span class="keyword">then</span></span><br><span class="line">                img_tag=$(<span class="built_in">echo</span> <span class="variable">$&#123;imgs&#125;</span> | awk -F<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br><span class="line">                namespace=$(<span class="built_in">echo</span> <span class="variable">$&#123;imgs&#125;</span> | awk -F<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#重命名镜像</span></span><br><span class="line">                docker tag <span class="variable">$&#123;imgs&#125;</span> <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">                <span class="comment">#删除旧镜像</span></span><br><span class="line">                <span class="comment">#docker rmi $&#123;imgs&#125;</span></span><br><span class="line">                <span class="comment">#上传镜像</span></span><br><span class="line">                docker push <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果镜像名中有两个/，</span></span><br><span class="line">            <span class="keyword">elif</span> [ <span class="variable">$&#123;n&#125;</span> -eq 2 ]; <span class="keyword">then</span></span><br><span class="line">                img_tag=$(<span class="built_in">echo</span> <span class="variable">$&#123;imgs&#125;</span> | awk -F<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123;print $3&#125;&#x27;</span>)</span><br><span class="line">                namespace=$(<span class="built_in">echo</span> <span class="variable">$&#123;imgs&#125;</span> | awk -F<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#重命名镜像</span></span><br><span class="line">                docker tag <span class="variable">$&#123;imgs&#125;</span> <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">                <span class="comment">#删除旧镜像</span></span><br><span class="line">                <span class="comment">#docker rmi $&#123;imgs&#125;</span></span><br><span class="line">                <span class="comment">#上传镜像</span></span><br><span class="line">                docker push <span class="variable">$&#123;registry&#125;</span>/<span class="variable">$&#123;namespace&#125;</span>/<span class="variable">$&#123;img_tag&#125;</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="comment">#标准镜像为四层结构，即：仓库地址/项目名/镜像名:tag,如不符合此标准，即为非有效镜像。</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">&quot;No available images&quot;</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">docker_push</span><br></pre></td></tr></table></figure></li></ol><h2 id="同步镜像"><a href="#同步镜像" class="headerlink" title="同步镜像"></a>同步镜像</h2><ol><li><p>在可用访问 Internet 的主机中，使用 <code>rancher-save-images.sh</code> 和 <code>rancher-images.txt</code> 创建所有所需镜像的压缩包。</p><blockquote><p><strong>注意:</strong> 镜像同步需要接近 20GB 的空磁盘空间。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x ./rancher-save-images.sh &amp;&amp; \</span><br><span class="line">./rancher-save-images.sh --image-list ./rancher-images.txt</span><br></pre></td></tr></table></figure><p><strong>结果:</strong> 在当前目录生成所有镜像的压缩文件 <code>rancher-images.tgz</code>。</p></li><li><p>拷贝所有文件到内网环境中的 Linux 主机上，然后执行以下命令去加载压缩包</p><blockquote><p><strong>注意</strong> 如果上一步下载镜像的主机可以连接内网的镜像仓库，那么此步骤可以跳过，直接在 上一步 的主机上执行下一个步骤。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gunzip -c rancher-images.tgz | docker load</span><br></pre></td></tr></table></figure></li><li><p>使用 <code>rancher-push-images.sh</code> 把导入的 docker 镜像自动 tag 重命名，然后上传到私有镜像仓库</p><blockquote><p><strong>注意</strong> 镜像列表中存在多个 repo 的镜像，<code>rancher-push-images.sh</code> 脚本默认把镜像推送到与原镜像相同的 repo 中。有的镜像仓库服务不支持自动创建 repo（比如 <code>Harbor</code>），需要提前去镜像仓库服务手动创建好对应的 repo，然后再执行推送。如果想把镜像列表中的所有镜像推送到某个指定的 repo（比如：统一推送到 rancher 下），可以传递参数给 <code>rancher-push-images.sh</code> 脚本。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x ./rancher-push-images.sh &amp;&amp; \</span><br><span class="line">./rancher-push-images.sh</span><br><span class="line"><span class="comment"># 如果要把镜像统一推送到指定的 repo，比如：rancher</span></span><br><span class="line"><span class="built_in">chmod</span> +x ./rancher-push-images.sh &amp;&amp; \</span><br><span class="line">./rancher-push-images.sh rancher</span><br></pre></td></tr></table></figure><blockquote><p>执行脚本后会要求输入<code>镜像仓库地址</code>和<code>用户名、用户密码</code></p></blockquote></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher k8s 对接 ceph 存储</title>
      <link href="/kubernetes/storage/k8s-storage-ceph/"/>
      <url>/kubernetes/storage/k8s-storage-ceph/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/storage/k8s-storage-ceph/" target="_blank" title="https://www.xtplayer.cn/kubernetes/storage/k8s-storage-ceph/">https://www.xtplayer.cn/kubernetes/storage/k8s-storage-ceph/</a></p><blockquote><ol><li>本文编写的前提是已有正常工作的 ceph 存储服务，并且 Rancher 集群能正常访问 ceph 存储服务，另外这里我们对接的是 Rancher 持久化存储的存储类。\</li></ol></blockquote><ol><li>随着 UI 翻译的更新，可能有些参数名称与实际名称不相同。</li></ol><h2 id="配置-ceph-secret"><a href="#配置-ceph-secret" class="headerlink" title="配置 ceph secret"></a>配置 ceph secret</h2><p>Rancher 连接 ceph 集群需要 <code>ceph secret</code>，在 ceph 服务器中执行以下命令生成 ceph secret:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph auth get-key client.admin |<span class="built_in">base64</span></span><br></pre></td></tr></table></figure><img src="/kubernetes/storage/k8s-storage-ceph/clip_image002.png" class="" title="img"><h2 id="创建-secret-对象"><a href="#创建-secret-对象" class="headerlink" title="创建 secret 对象"></a>创建 secret 对象</h2><p>将 <code>key</code> 替换为实际 ceph 的 <code>secret</code>，然后 import yaml 到 rancher 集群。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line"><span class="built_in">type</span>: <span class="string">&quot;kubernetes.io/rbd&quot;</span></span><br><span class="line">data:</span><br><span class="line">  key: QVFEMDJ1VmE0L1pxSEJBQUtTUnFwS3JFVjErRjFNM1kwQ2lyWmc9PQ==</span><br></pre></td></tr></table></figure><img src="/kubernetes/storage/k8s-storage-ceph/clip_image003.png" class="" title="img"><h2 id="UI-配置存储类"><a href="#UI-配置存储类" class="headerlink" title="UI 配置存储类"></a>UI 配置存储类</h2><ol><li><p>进入<code>集群</code>视图，在<code>存储</code>菜单下选择<code>存储类</code></p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image004.png" class="" title="img"></li><li><p>设置存储名称，并选择 <code>ceph-rbd</code> 类</p> <img src="/kubernetes/storage/k8s-storage-ceph/image-20181111221714979.png" class="" title="image-20181111221714979"> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image005.png" class="" title="img"></li><li><p>配置 ceph-rbd 参数，填写对应的 ceph-monitor 地址和管理员 ID(),还有 secret-name</p><ul><li>监控：ceph-monitor 地址</li><li>管理员 ID：ceph-monitor 登录账户</li><li>管理密文命名空间：管理密文导入的命名空间，根据实际导入的命名空间填写</li></ul> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image006.png" class="" title="img"></li><li><p>点击页面最下方的保存</p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image007.png" class="" title="img"></li></ol><h2 id="创建应用并挂载数据卷"><a href="#创建应用并挂载数据卷" class="headerlink" title="创建应用并挂载数据卷"></a>创建应用并挂载数据卷</h2><h3 id="方法一：手动创建卷再挂载"><a href="#方法一：手动创建卷再挂载" class="headerlink" title="方法一：手动创建卷再挂载"></a>方法一：手动创建卷再挂载</h3><ol><li><p>切换到项目视图，依次点击<code>工作负载</code>&#x2F;<code> 数据卷</code>&#x2F;<code> 添加卷</code></p> <img src="/kubernetes/storage/k8s-storage-ceph/image-20181111224750914.png" class="" title="image-20181111224750914"></li><li><p>填写卷配置信息，比如选择对应的存储类和卷的大小</p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image008.png" class="" title="img"></li><li><p>点击页面下方的<code>创建</code></p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image009.png" class="" title="img"></li><li><p>创建工作负载，选择对应的存储卷和挂载的目录</p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image010.png" class="" title="img"> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image011.png" class="" title="img"> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image012.png" class="" title="img"></li><li><p>通过 <code>web 终端</code>登录 Pod 查看挂载情况</p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image013.png" class="" title="img"></li><li><p>登录 ceph server 查看</p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image014.png" class="" title="img"></li></ol><h3 id="方法二：创建应用的时候同时创建卷"><a href="#方法二：创建应用的时候同时创建卷" class="headerlink" title="方法二：创建应用的时候同时创建卷"></a>方法二：创建应用的时候同时创建卷</h3><ol><li><p>创建工作负载，配置数据卷选择<code>添加新的持久化卷(声明)</code></p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image015.png" class="" title="img"></li><li><p>配置相应参数，比如添加<code>卷声明名称</code>，选择对应的<code>存储类</code>和<code>容量大小</code></p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image016.png" class="" title="img"></li><li><p>配置容器的挂载路径</p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image017.png" class="" title="img"></li><li><p>启动工作负载并登录容器查看卷挂载</p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image018.png" class="" title="img"></li><li><p>登录 ceph server 查看</p> <img src="/kubernetes/storage/k8s-storage-ceph/clip_image014.png" class="" title="img"></li></ol><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><img src="/kubernetes/storage/k8s-storage-ceph/image-20191105174349181.png" class="" title="image-20191105174349181"><p>如果出现以上问题，请检查 worker 节点上是否有加载 <code>RBD</code> 模块。执行 <code>lsmod | grep rbd</code> 查看是否有信息返回，如果没有则执行 <code>modprobe rbd</code> 进行模块加载。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> storage </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> storage </tag>
            
            <tag> ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s 中的 pvc pv sc</title>
      <link href="/kubernetes/storage/k8s-pvc-pv-sc/"/>
      <url>/kubernetes/storage/k8s-pvc-pv-sc/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/storage/k8s-pvc-pv-sc/" target="_blank" title="https://www.xtplayer.cn/kubernetes/storage/k8s-pvc-pv-sc/">https://www.xtplayer.cn/kubernetes/storage/k8s-pvc-pv-sc/</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>Persistent Volume Claims( PVC) ：应用 Pod 从集群请求存储资源的对象，可以理解为应用 Pod 可以访问和使用多少存储空间的凭证。 通过 PVC，可以指定存储服务器类型(SC)，可以指定需要多大的存储空间(PV)，可以配置卷的访问模式(单主机读写、多主机只读、多主机读写)。</li><li>Persistent Volume(PV)：在 Storage 上划分的一块用于存储数据的空间。</li><li>Storage Class(SC)：对接后端存储服务器(Storage)的驱动(插件)，配置 Storage Class 对象时，需要提供对接存储的相关信息，比如存储地址、认证用户名和密码等。</li><li>Storage：真实存储数据的服务器，包含服务器地址和认证等信息。</li></ul><img src="/kubernetes/storage/k8s-pvc-pv-sc/image-20190501174646105.png" class="" title="image-20190501174646105">]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> storage </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher2 keycloak 认证</title>
      <link href="/rancher/authentication/rancher2-keycloak-authentication/"/>
      <url>/rancher/authentication/rancher2-keycloak-authentication/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/authentication/rancher2-keycloak-authentication/" target="_blank" title="https://www.xtplayer.cn/rancher/authentication/rancher2-keycloak-authentication/">https://www.xtplayer.cn/rancher/authentication/rancher2-keycloak-authentication/</a></p><p>版本支持: <code>Rancher v2.1.0+</code></p><blockquote><p><strong>注意</strong> 在开始之前，请熟悉 <a href="/rancher/authentication/rancher2-authentication/">外部身份验证配置和主要用户</a> 的概念。</p></blockquote><h2 id="Keycloak-配置"><a href="#Keycloak-配置" class="headerlink" title="Keycloak 配置"></a>Keycloak 配置</h2><ol><li><p>新建一个 Clients</p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image002.15a3d56a.png" class="" title="A screenshot of a cell phone Description automatically generated"><p><strong>参数配置：</strong></p><ul><li>Client ID：https&#x2F;&#x2F;<rancher_server_url>&#x2F;v1-saml&#x2F;keycloak&#x2F;saml&#x2F;metadata</li><li>Client Protocol：saml</li></ul></li><li><p>配置 clients 基本配置</p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image003.f1eb0c1b.png" class="" title="A screenshot of a cell phone Description automatically generated"><p><strong>参数配置：</strong></p><ul><li>Client ID：<code>https//&lt;rancher_server_url&gt;/v1-saml/keycloak/saml/metadata</code></li><li>Name：clientName</li><li>Enabled：on</li><li>Sign Documents：on</li><li>Sign Assertions：on</li><li>Valid Redirect URIs：<code>https://&lt;rancher_server_url&gt;/v1-saml/keycloak/saml/acs</code></li><li>除了三个开关为打开状态，其余全部关闭</li></ul></li><li><p>配置 client mappers</p><p>在 Mappers 菜单中点击创建，然后设置名称、选择类型、设置 <code>SAML Attribute Name 和 Property</code>。注意 SAML Attribute Name 和 Property 很关键，建议与 mappers 名称保持相同，其他参数保持默认。</p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image004.def0d456.png" class="" title="img"><p>分别创建 <code>username uid name roles</code>，最终效果如下：</p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image005.030d8710.png" class="" title="img"></li><li><p>获取 xml</p><p>访问 url 拿到 client xml 并进行改造，url 地址：<code>http://&lt;keycloak_url&gt;/auth/realms/master/protocol/saml/descriptor</code></p></li><li><p>改造 xml：</p><p>修改 <code>EntitiesDescriptor</code> 为 <code>EntityDescriptor</code>,删除 <code>EntityDescriptor</code></p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image006.af7c6d59.png" class="" title="A screenshot of a cell phone Description automatically generated"><p>改造后的 xml：</p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image007.ab0b891d.png" class="" title="A screenshot of a cell phone Description automatically generated"></li></ol><h2 id="Rancher-UI-配置"><a href="#Rancher-UI-配置" class="headerlink" title="Rancher UI 配置"></a>Rancher UI 配置</h2><ol><li><p>Openssl 生成自签名证书命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout myservice.key -out myservice.cert</span><br></pre></td></tr></table></figure></li><li><p>认证账号配置对应属性关系：</p><ul><li>Display Name Field ≫ <strong>name</strong></li></ul><p>说明：该参数传递领域名称</p><ul><li>User Name Field ≫ <strong>username</strong></li></ul><p>说明：该参数传递用户名</p><ul><li>UID Field ≫ <strong>username</strong></li></ul><p>说明：该参数传递 UID</p><ul><li>Groups Field ≫ <strong>roles</strong></li></ul><p>说明：改参数传递用户组</p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image008.b8073e62.png" class="" title="A screenshot of a social media post Description automatically generated"></li></ol><p>如果以上配置都正确，保存后跳转 keycloak 登陆界面，登陆即可保存配置</p><h2 id="对接后使用说明"><a href="#对接后使用说明" class="headerlink" title="对接后使用说明"></a>对接后使用说明</h2><ol><li><p>keycloak 中创建用户后并不会直接同步到 Rancher 中，需要登录一次 rancher 才会同步到 rancher 中的用户列表；</p></li><li><p>新用户第一次登陆赋予的 rancher 角色是 rancher 中定义的新用户默认角色；</p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image009.9a6d946f.png" class="" title="A screenshot of a cell phone Description automatically generated"></li><li><p>登陆后同步到 rancher 用户列表中，可以使用管理账户在 rancher 中管理该用户的角色权限</p><img src="/rancher/authentication/rancher2-keycloak-authentication/clip_image010.2e71e878.png" class="" title="A screenshot of a cell phone Description automatically generated"></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> authentication </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher2.x 对接外部认证系统</title>
      <link href="/rancher/authentication/rancher2-authentication/"/>
      <url>/rancher/authentication/rancher2-authentication/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/authentication/rancher2-authentication/" target="_blank" title="https://www.xtplayer.cn/rancher/authentication/rancher2-authentication/">https://www.xtplayer.cn/rancher/authentication/rancher2-authentication/</a></p><p>Rancher 为 Kubernetes 增强的一个关键功能是集中用户身份验证，此功能允许您的用户使用一组凭据对所有 Kubernetes 集群进行身份验证。</p><p>此集中式用户身份验证是使用 Rancher 身份验证代理完成的，该代理与 Rancher 一起安装。此代理会对您的用户进行身份验证，并使用服务帐户将其请求转发给您的 Kubernetes 集群。</p><h2 id="外部认证与本地认证"><a href="#外部认证与本地认证" class="headerlink" title="外部认证与本地认证"></a>外部认证与本地认证</h2><p>Rancher 提供本地身份验证，也可以与外部身份验证服务集成：</p><table><thead><tr><th>Auth Service</th><th>可用版本</th></tr></thead><tbody><tr><td><a href="/rancher/authentication/rancher2-ad-authentication/">Microsoft Active Directory</a></td><td>v2.0.0</td></tr><tr><td><a href="/rancher/authentication/rancher2-github-authentication/">GitHub</a></td><td>v2.0.0</td></tr><tr><td><a href="/rancher/authentication/rancher2-azure-ad-authentication/">Microsoft Azure AD</a></td><td>v2.0.3</td></tr><tr><td>FreeIPA</td><td>v2.0.5</td></tr><tr><td><a href="/rancher/authentication/rancher2-openldap-authentication/">OpenLDAP</a></td><td>v2.0.5</td></tr><tr><td>Microsoft AD FS</td><td>v2.0.7</td></tr><tr><td>PingIdentity</td><td>v2.0.7</td></tr><tr><td><a href="/rancher/authentication/rancher2-keycloak-authentication/">Keycloak</a></td><td>v2.1.0</td></tr><tr><td><a href="/rancher/authentication/rancher2-okta-authentication/">Okta</a></td><td>v2.2.0</td></tr></tbody></table><p>在大多数情况下，您应该使用外部身份验证服务，因为外部验证服务可以统一的进行用户管理。如果没有外部身份验证服务，您也可以通过本地身份验证来管理 Rancher 用户。</p><h2 id="外部身份验证配置和主要用户"><a href="#外部身份验证配置和主要用户" class="headerlink" title="外部身份验证配置和主要用户"></a>外部身份验证配置和主要用户</h2><p>外部身份验证的配置需要：</p><ul><li><p>一个具有管理员角色的本地管理员账号，例如：<code>local_admin</code></p></li><li><p>一个可以使用外部身份验证服务进行身份验证的外部服务账号，例如：<code>demo</code></p><p>外部身份验证的配置会影响 Rancher 中本地用户的管理方式。请按照以下列表更好地了解这些效果。</p></li></ul><ol><li><p>以<code>本地管理员</code>登录 Rancher，对外部身份验证服务进行完整配置。</p><img src="/rancher/authentication/rancher2-authentication/sign-in.png" class="" title="Sign In"></li><li><p>Rancher 将外部服务账号与本地管理员账号联系在一起。这两个账号共享本地管理员用户的用户 ID。</p><img src="/rancher/authentication/rancher2-authentication/principal-ID.png" class="" title="Principal ID Sharing"></li><li><p>完成配置后，Rancher 会自动注销本地管理员账号。</p><img src="/rancher/authentication/rancher2-authentication/sign-out-local.png" class="" title="Sign Out Local Principal"></li><li><p>然后，Rancher 将自动以外部服务用户登录。</p><img src="/rancher/authentication/rancher2-authentication/sign-in-external.png" class="" title="Sign In External Principal"></li><li><p>由于外部服务账号和本地管理员账号共享一个 ID，因此在<code>用户</code>页面上不会显示外部服务账号的唯一标识。</p><img src="/rancher/authentication/rancher2-authentication/users-page.png" class="" title="Sign In External Principal"></li><li><p>外部服务账号和本地管理员账号共享相同的访问权限。</p></li></ol><blockquote><p><strong>重要说明</strong> 不管启用哪种外部身份验证服务，Rancher 内置的<code>超级管理员 admin</code> 将一直启用。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> authentication </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher2 AD 域认证</title>
      <link href="/rancher/authentication/rancher2-ad-authentication/"/>
      <url>/rancher/authentication/rancher2-ad-authentication/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/authentication/rancher2-ad-authentication/" target="_blank" title="https://www.xtplayer.cn/rancher/authentication/rancher2-ad-authentication/">https://www.xtplayer.cn/rancher/authentication/rancher2-ad-authentication/</a></p><p>如果您的组织使用 Microsoft Active Directory 作为统一用户管理系统，则 Rancher 可以集成 Active Directory 服务以进行统一身份验证。Rancher 根据 Active Directory 管理的用户和组来控制对集群和项目的访问，同时允许最终用户在登录 Rancher UI 时使用其 AD 凭据进行身份验证。</p><p>Rancher 使用 LDAP 与 Active Directory 服务通信。因此，Active Directory 的身份验证流程与 <a href="https://docs2.rancher.cn/rancher2x/user-manual/global/security/openldap">OpenLDAP</a>身份验证集成方法相同。</p><blockquote><p><strong>注意</strong> 在开始之前，请熟悉 <a href="/rancher/authentication/rancher2-authentication/">外部身份验证配置和主要用户</a> 的概念。</p></blockquote><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><p>您需要通过 AD 管理员创建或获取新的 AD 用户，以用作 Rancher 的<code>服务帐户</code>。此用户必须具有足够的权限才能执行 LDAP 搜索并读取 AD 域下的用户和组的属性。</p><p>通常应该使用域用户帐户(非管理员)来实现此目的，因为默认情况下此用户对域中的大多数对象具有只读权限。</p><p>但请注意，在某些锁定的 Active Directory 配置中，此默认行为可能不适用。在这种情况下，您需要确保服务帐户用户至少具有在基本 OU(封闭用户和组)上授予的<code>读取和列出内容</code>权限，或者全局授予域。</p><blockquote><p><strong>使用 TLS？</strong> 如果 AD 服务器使用的证书是自签名的，或者不是来自公认的证书颁发机构，请确保手头有 PEM 格式的 CA 证书(与所有中间证书连接)。您必须在配置期间设置此证书，以便 Rancher 能够验证证书。</p></blockquote><h2 id="选择-AD-认证"><a href="#选择-AD-认证" class="headerlink" title="选择 AD 认证"></a>选择 AD 认证</h2><ol><li>使用本地 admin 帐户登录 Rancher UI 。</li><li>从<code>全局</code>视图中，导航到<code>安全&gt;认证</code>。</li><li>选择 <code>Active Directory</code>，配置 AD 认证参数。</li></ol><h2 id="配置-AD-服务器"><a href="#配置-AD-服务器" class="headerlink" title="配置 AD 服务器"></a>配置 AD 服务器</h2><p>在标题为<code>配置 Active Directory 服务器</code>的部分中，填写特定于 Active Directory 服务的配置信息。有关每个参数所需值的详细信息，请参阅下表。</p><blockquote><p><strong>注意</strong> 如果您不确定要在 用户&#x2F;组  搜索库字段中输入什么值， 请查看 [使用 ldapsearch 识别 Search-Base 和架构](&#x2F;rancher&#x2F;authentication&#x2F;rancher2-ad-authentication&#x2F;#使用 ldapsearch 识别 Search-Base 和架构)。</p></blockquote><p><strong>表 1：AD 服务器参数</strong></p><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>主机名</td><td>指定 AD 服务器的域名或 IP 地址</td></tr><tr><td>端口</td><td>指定 Active Directory 服务器侦听的端口。未加密的 LDAP 通常使用标准端口 <code>389</code>，而 LDAPS 使用端口 <code>636</code>。</td></tr><tr><td>TLS</td><td>选中此框以启用 LDAP over SSL&#x2F;TLS(通常称为 LDAPS)。</td></tr><tr><td>服务器连接超时</td><td>Rancher 无法访问 AD 服务器后等待的持续时间。</td></tr><tr><td>服务帐户用户名</td><td>输入对您的登录域具有只读访问权限的 AD 帐户的用户名(请参阅<a href="https://docs2.rancher.cn/rancher2x/user-manual/global/security/authentication/ad.html#%E4%B8%80%E3%80%81%E5%85%88%E5%86%B3%E6%9D%A1%E4%BB%B6">先决条件</a>)。用户名可以用 NetBIOS 格式输入(例如: <code>DOMAIN\serviceaccount</code>)或 UPN 格式(例如: <code>serviceaccount@domain.com</code>)。</td></tr><tr><td>服务帐户密码</td><td>服务帐户的密码。</td></tr><tr><td>默认登录域</td><td>使用 AD 域的 NetBIOS 名称配置此字段时，在没有域(例如 <code>jdoe</code>)时输入的用户名将在绑定到 AD 服务器时自动转换为斜线的 NetBIOS 登录(例如 <code>LOGIN_DOMAIN\jdoe</code>)。如果您的用户使用 UPN(例如 <code>jdoe@acme.com</code>)作为用户名进行身份验证，则此字段<code>必须</code>为空。</td></tr><tr><td>User Search Base</td><td>目录树中节点的专有名称，从该节点开始搜索用户对象。所有用户必须是此基本 DN 的后代。例如：<code>ou=people，dc=acme，dc=com</code>。</td></tr><tr><td>Group Search Base</td><td>如果您的组位于与 <code>User Search Base</code> 配置的节点不同的节点下，则需要在此处提供可分辨名称。否则将其留空。例如：<code>ou=groups，dc=acme，dc=com</code>。</td></tr></tbody></table><h2 id="自定义架构（可选"><a href="#自定义架构（可选" class="headerlink" title="自定义架构（可选)"></a>自定义架构（可选)</h2><blockquote><p><strong>注意</strong> 如果您的 AD 服务器为标准配置，那可以跳过此步骤</p></blockquote><p>在标题为<code>自定义架构</code>部分中，您必须为 Rancher 提供与目录中使用的模式相对应的<code>用户和组</code>属性的正确配置。</p><p>Rancher 使用 LDAP 查询来搜索和检索有关 Active Directory 中的用户和组的信息，本节中配置的属性映射用于构建搜索过滤器并解析组成员身份。因此，提供的设置反映 AD 域的实际情况至关重要。</p><blockquote><p><strong>注意</strong> 如果您不熟悉 Active Directory 域中使用的架构，请参阅使用 ldapsearch 识别搜索库和架构以确定正确的配置值。</p></blockquote><h3 id="用户架构"><a href="#用户架构" class="headerlink" title="用户架构"></a>用户架构</h3><p>下表详细介绍了用户架构部分配置的参数。</p><p><strong>表 2：用户架构配置参数</strong></p><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>对象类型</td><td>域中用户对象的类型名称。</td></tr><tr><td>用户名属性</td><td>其值作为显示名称。</td></tr><tr><td>登录属性</td><td>该值与用户在登录 Rancher 时输入的凭据的用户名部分相匹配的属性。如果您的用户使用其 UPN(例如“ <a href="mailto:&#106;&#100;&#x6f;&#x65;&#64;&#x61;&#99;&#109;&#x65;&#46;&#x63;&#x6f;&#109;">&#106;&#100;&#x6f;&#x65;&#64;&#x61;&#99;&#109;&#x65;&#46;&#x63;&#x6f;&#109;</a>”)作为用户名进行身份验证，则通常必须将此字段设置为 <code>userPrincipalName</code>。否则，对于旧的 NetBIOS 样式的登录名称(例如“jdoe”)通常是这样 <code>sAMAccountName</code>。</td></tr><tr><td>用户成员属性</td><td>包含用户所属组的属性。</td></tr><tr><td>搜索属性</td><td>当用户输入文本以在 UI 中添加用户或组时，Rancher 将查询 AD 服务器并尝试按此设置中提供的属性匹配用户。可以通过使用管道(“|”)符号分隔多个属性来指定它们。要匹配 UPN 用户名(例如 <a href="mailto:&#106;&#100;&#111;&#x65;&#x40;&#97;&#x63;&#x6d;&#101;&#x2e;&#99;&#111;&#109;">&#106;&#100;&#111;&#x65;&#x40;&#97;&#x63;&#x6d;&#101;&#x2e;&#99;&#111;&#109;</a> )，通常应将此字段的值设置为 <code>userPrincipalName</code>。</td></tr><tr><td>用户启用的属性</td><td>包含表示用户帐户标志的按位枚举的整数值的属性。Rancher 使用它来确定是否禁用了用户帐户。您通常应将此设置保留为 AD 标准 <code>userAccountControl</code>。</td></tr><tr><td>禁用状态位掩码</td><td>这是 <code>User Enabled Attribute</code> 指定已禁用的用户帐户的值。您通常应将此设置保留为 Microsoft Active Directory 架构中指定的默认值“2”(请参阅<a href="https://docs.microsoft.com/en-us/windows/desktop/adschema/a-useraccountcontrol#remarks">此处</a>)。</td></tr></tbody></table><h3 id="组架构"><a href="#组架构" class="headerlink" title="组架构"></a>组架构</h3><p>下表详细说明了组架构配置的参数。</p><p><strong>表 3: 组架构配置参数</strong></p><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>对象类型</td><td>域中组对象的类型名称。</td></tr><tr><td>名称属性</td><td>其值作为显示名称。</td></tr><tr><td>组成员用户属性</td><td>格式与中的组成员匹配的<strong>用户属性</strong>的名称 <code>Group Member Mapping Attribute</code>。</td></tr><tr><td>组成员映射属性</td><td>包含组成员的组属性的名称。</td></tr><tr><td>搜索属性</td><td>在将群组添加到集群或项目时用于构建搜索过滤器的属性。请参阅用户架构的说明 <code>Search Attribute</code>。</td></tr><tr><td>组 DN 属性</td><td>group 属性的名称，其格式与描述用户成员身份的 user 属性中的值匹配。见 <code>User Member Attribute</code>。</td></tr><tr><td>嵌套组成员资格</td><td>此设置定义 Rancher 是否应解析嵌套组成员资格。仅在您的组织使用这些嵌套成员资格时使用(即您拥有包含其他组作为成员的组)。</td></tr></tbody></table><h3 id="使用-ldapsearch-识别-Search-Base-和架构"><a href="#使用-ldapsearch-识别-Search-Base-和架构" class="headerlink" title="使用 ldapsearch 识别 Search Base 和架构"></a>使用 ldapsearch 识别 Search Base 和架构</h3><p>为了成功配置 AD 身份验证，您必须提供与 AD 服务器的层次结构和架构相关的正确配置。</p><p>该<a href="http://manpages.ubuntu.com/manpages/artful/man1/ldapsearch.1.html"><code>ldapsearch</code></a>工具可以帮助您查询 AD 服务器用户和组对象的模式。</p><p>处于演示的目的，我们假设:</p><ul><li>Active Directory 服务器的主机名为 <code>ad.acme.com</code>。</li><li>服务器正在侦听端口上的未加密连接 <code>389</code>。</li><li>Active Directory 域是 <code>acme</code></li><li>拥有一个有效的 AD 帐户，其中包含用户名 <code>jdoe</code> 和密码 <code>secret</code></li></ul><h4 id="识别-Search-Base"><a href="#识别-Search-Base" class="headerlink" title="识别 Search Base"></a>识别 Search Base</h4><p>首先，我们将使用 <code>ldapsearch</code> 来识别用户和组的父节点的专有名称(DN)：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ldapsearch -x -D <span class="string">&quot;acme\jdoe&quot;</span> -w <span class="string">&quot;secret&quot;</span> -p 389 \</span><br><span class="line">-h ad.acme.com -b <span class="string">&quot;dc=acme,dc=com&quot;</span> -s sub <span class="string">&quot;sAMAccountName=jdoe&quot;</span></span><br></pre></td></tr></table></figure><p>此命令执行 LDAP 搜索，<code>search base</code> 设置为根域(<code>-b &quot;dc=acme,dc=com&quot;</code>)，过滤器以用户(<code>sAMAccountNam=jdoe</code>)为目标，返回所述用户的属性：</p><img src="/rancher/authentication/rancher2-ad-authentication/ldapsearch-user.png" class="" title="LDAP User"><p>由于在这种情况下用户的 DN 是 <code>CN=John Doe,CN=Users,DC=acme,DC=com[5]</code>，我们应该使用父节点 DN 配置用户 Search Base<code>CN=Users,DC=acme,DC=com</code>。</p><p>类似地，基于 <code>memberOf</code> 属性[4]中引用的组的 DN，组 Search Base 的正确值应该是该值的父节点，即:<code>OU=Groups,DC=acme,DC=com</code>。</p><h4 id="识别用户架构"><a href="#识别用户架构" class="headerlink" title="识别用户架构"></a>识别用户架构</h4><p>上述 <code>ldapsearch</code> 查询的输出结果还可以确定用户架构的配置：</p><ul><li><code>Object Class</code>: <strong>person</strong> [1]</li><li><code>Username Attribute</code>: <strong>name</strong> [2]</li><li><code>Login Attribute</code>: <strong>sAMAccountName</strong> [3]</li><li><code>User Member Attribute</code>: <strong>memberOf</strong> [4]</li></ul><blockquote><p><strong>注意</strong> 如果组织中的 AD 用户使用他们的 UPN(例如 <code>jdoe@acme.com</code> )而不是短登录名进行身份验证，那么我们必须将 <code>Login Attribute</code> 设置为 <code>userPrincipalName</code>。<br>还可以 <code>Search Attribute</code> 参数设置为 <code>sAMAccountName | name</code>。这样，通过输入用户名或全名，可以通过 Rancher UI 将用户添加到集群&#x2F;项目中。</p></blockquote><h4 id="识别组架构"><a href="#识别组架构" class="headerlink" title="识别组架构"></a>识别组架构</h4><p>接下来，我们查询与此用户关联的一个组，在这种情况下 <code>CN=examplegroup,OU=Groups,DC=acme,DC=com</code>：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ldapsearch -x -D <span class="string">&quot;acme\jdoe&quot;</span> -w <span class="string">&quot;secret&quot;</span> -p 389 \</span><br><span class="line">-h ad.acme.com -b <span class="string">&quot;ou=groups,dc=acme,dc=com&quot;</span> \</span><br><span class="line">-s sub <span class="string">&quot;CN=examplegroup&quot;</span></span><br></pre></td></tr></table></figure><p>以上命令将显示组对象的属性：</p><img src="/rancher/authentication/rancher2-ad-authentication/ldapsearch-group.png" class="" title="LDAP Group"><ul><li><code>Object Class</code>: <strong>group</strong> [1]</li><li><code>Name Attribute</code>: <strong>name</strong> [2]</li><li><code>Group Member Mapping Attribute</code>: <strong>member</strong> [3]</li><li><code>Search Attribute</code>: <strong>sAMAccountName</strong> [4]</li></ul><p>查看成员属性值，我们可以看到它包含引用用户的 DN。这对应用户对象中的 <code>distinguishedName</code> 属性。因此必须将 <code>Group Member User Attribute</code> 参数的值设置为此属性值。以同样的方式，我们可以观察到用户对象中 <code>memberOf</code> 属性中的值对应于组的 <code>distinguishedName [5]</code>。因此，我们需要将 <code>Group DN Attribute</code> 参数的值设置为此属性值。</p><h2 id="测试认证"><a href="#测试认证" class="headerlink" title="测试认证"></a>测试认证</h2><p>完成配置后，继续测试与 AD 服务器的连接。如果测试成功，将隐式启用使用已配置的 Active Directory 进行身份验证。</p><blockquote><p><strong>注意</strong> 与在此步骤中输入的凭据相关的 AD 用户将映射到本地主体帐户并在 Rancher 中分配管理员权限。因此，您应该有意识地决定使用哪个 AD 帐户执行此步骤。</p></blockquote><ol><li>输入应映射到本地主帐户的 AD 帐户的<strong>用户名</strong>和<strong>密码</strong>。</li><li>单击<strong>使用 Active Directory</strong>进行<strong>身份验证</strong>以完成设置。</li></ol><blockquote><p><strong>注意</strong> 如果 LDAP 服务中断，您可以使用本地帐户和密码登录。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> authentication </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>POD 中容器异常重启原因定位</title>
      <link href="/kubernetes/pod-container-restart-reason-check/"/>
      <url>/kubernetes/pod-container-restart-reason-check/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/pod-container-restart-reason-check/" target="_blank" title="https://www.xtplayer.cn/kubernetes/pod-container-restart-reason-check/">https://www.xtplayer.cn/kubernetes/pod-container-restart-reason-check/</a></p><p>在 K8S 的使用过程中，POD 中容器重启应该是很容易遇到，如下图可以明确的看到 POD 中容器重启的次数。</p><img src="/kubernetes/pod-container-restart-reason-check/image-20210107181721577.png" class="" title="image-20210107181721577">当遇到容器异常重启，应该进一步的找到容器重启的原因，避免下次再出现相同的问题。下面我将分享定位容器异常重启的几个步骤。<h2 id="检查-POD-事件"><a href="#检查-POD-事件" class="headerlink" title="检查 POD 事件"></a>检查 POD 事件</h2><ol><li><p>对于 POD 层面的原因造成的容器重启，比如健康检查等，一般在 POD 事件中会有相应的事件信息。这个时候点击 POD 名称进入 POD 详情页面，然后点击<strong>事件</strong>则可以看到具体的 POD 事件。</p><img src="/kubernetes/pod-container-restart-reason-check/image-20210107182521132.png" class="" title="image-20210107182521132"></li><li><p>POD 事件非持久性的，默认只保留 1 个小时。所以如果等的时间过长，事件中则没有事件显示。</p><img src="/kubernetes/pod-container-restart-reason-check/image-20210107182852453.png" class="" title="image-20210107182852453"></li></ol><h2 id="检查容器退出原因（reason）和状态码"><a href="#检查容器退出原因（reason）和状态码" class="headerlink" title="检查容器退出原因（reason）和状态码"></a>检查容器退出原因（reason）和状态码</h2><p>有时候如果容器做了资源限制，比如内存限制了 1G。当容器使用的内存资源超过 1G 时，就会触发 Linux 系统的 OOM(Out Of Memory Killer)机制。当触发 OOM 后，容器进程将会被 KILL 掉，因此容器会停止。</p><p>但是在 K8S 集群中，K8S 的机制会保证 POD 中的容器一直可用，因此 K8S 会自动重新创建一个新的容器，旧的容器会一直处于停止状态。</p><ul><li><p>企业版</p><p>在企业版的 rancher 中，鼠标放在状态栏上，可以查看到容器重启的原因。</p><img src="/kubernetes/pod-container-restart-reason-check/image-20210107184252976.png" class="" title="image-20210107184252976"></li><li><p>开源版</p><p>开源版只能通过查看 POD YAML 来查看退出原因（reason）和状态码，在 <code>status.containerStatuses.lastState</code> 字段下可以看到具体的退出原因（reason）和状态码。</p><img src="/kubernetes/pod-container-restart-reason-check/image-20210107191057461.png" class="" title="image-20210107191057461"><img src="/kubernetes/pod-container-restart-reason-check/image-20210107191352901.png" class="" title="image-20210107191352901"></li><li><p>常见退出状态码</p><ul><li><strong>退出代码 0</strong>：一般为容器正常退出</li><li><strong>退出代码 1</strong>：由于容器中 pid 为 1 的进程错误而失败</li><li><strong>退出代码 137</strong>：由于容器收到 <strong>SIGKILL</strong> 信号而失败（手动执行或“oom-killer” [OUT-OF-MEMORY]）</li><li><strong>退出代码 139</strong>：由于容器收到 <strong>SIGSEGV</strong> 信号而失败</li><li><strong>退出代码 143</strong>：由于容器收到 <strong>SIGTERM</strong> 信号而失败</li></ul></li></ul><h2 id="查看退出容器日志"><a href="#查看退出容器日志" class="headerlink" title="查看退出容器日志"></a>查看退出容器日志</h2><p>前面已说到，K8S 的机制会保证 POD 中的容器一直可用，因此 K8S 会自动重新创建一个新的容器，旧的容器会一直处于停止状态。因此，如果是容器中进程异常退出导致的容器重启，那么就需要通过旧容器的日志来具体定位问题。</p><ol><li><p>点击 POD 的查看日志</p> <img src="/kubernetes/pod-container-restart-reason-check/image-20210107191910194.png" class="" title="image-20210107191910194"></li><li><p>在弹出窗口左下角可以看到 <strong>以前的容器</strong> 复选框</p> <img src="/kubernetes/pod-container-restart-reason-check/image-20210107192012283.png" class="" title="image-20210107192012283"></li><li><p>勾选 <strong>以前的容器</strong> 复选框后，可以看到日志内容会发生变化。点击 <strong>回到底部</strong> 则可以看到容器退出前的错误日志</p> <img src="/kubernetes/pod-container-restart-reason-check/image-20210107192359600.png" class="" title="image-20210107192359600"></li></ol>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>博客同步至 OSCHINA</title>
      <link href="/oschina/blog-sync-to-oschina/"/>
      <url>/oschina/blog-sync-to-oschina/</url>
      
        <content type="html"><![CDATA[<p>我的博客即将同步至 OSCHINA 社区，这是我的 OSCHINA ID：洪晓露，邀请大家一同入驻：<a href="https://www.oschina.net/sharing-plan/apply">https://www.oschina.net/sharing-plan/apply</a></p>]]></content>
      
      
      <categories>
          
          <category> oschina </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OSCHINA </tag>
            
            <tag> oschina </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher nginx ingress 实现灰度发布</title>
      <link href="/rancher/rancher-nginx-ingress-canary/"/>
      <url>/rancher/rancher-nginx-ingress-canary/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/rancher-nginx-ingress-canary/" target="_blank" title="https://www.xtplayer.cn/rancher/rancher-nginx-ingress-canary/">https://www.xtplayer.cn/rancher/rancher-nginx-ingress-canary/</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Nginx-ingress 是一个以 Nginx 为核心组件的 K8S 负载均衡工具，支持通过配置 Ingress 规则的 Annotations 来实现不同场景下的灰度发布和测试。</p><p>Ingress Annotations 支持以下 4 种 Canary 规则：</p><ul><li><p><code>nginx.ingress.kubernetes.io/canary-by-header</code>：基于 Request Header 的流量切分，适用于灰度发布以及 A&#x2F;B 测试。当 Request Header 设置为 always 时，请求将会被一直发送到 Canary 版本；当 Request Header 设置为 never 时，请求不会被发送到 Canary 入口；对于任何其他 Header 值，将忽略 Header，并通过优先级将请求与其他金丝雀规则进行优先级的比较。</p></li><li><p><code>nginx.ingress.kubernetes.io/canary-by-header-value</code>：要匹配的 Request Header 的值，用于通知 Ingress 将请求路由到 Canary Ingress 中指定的服务。当 Request Header 设置为此值时，它将被路由到 Canary 入口。该规则允许用户自定义 Request Header 的值，必须与上一个 annotation (即：canary-by-header）一起使用。</p></li><li><p><code>nginx.ingress.kubernetes.io/canary-weight</code>：基于服务权重的流量切分，适用于蓝绿部署，权重范围 0 - 100 按百分比将请求路由到 Canary Ingress 中指定的服务。权重为 0 意味着该金丝雀规则不会向 Canary 入口的服务发送任何请求。权重为 100 意味着所有请求都将被发送到 Canary 入口。</p></li><li><p><code>nginx.ingress.kubernetes.io/canary-by-cookie：</code>基于 Cookie 的流量切分，适用于灰度发布与 A&#x2F;B 测试。用于通知 Ingress 将请求路由到 Canary Ingress 中指定的服务的 cookie。当 cookie 值设置为 always 时，它将被路由到 Canary 入口；当 cookie 值设置为 never 时，请求不会被发送到 Canary 入口；对于任何其他值，将忽略 cookie 并将请求与其他金丝雀规则进行优先级的比较。</p></li></ul><p><strong>注意：</strong>金丝雀规则按优先顺序进行如下排序: <code>canary-by-header - &gt; canary-by-cookie - &gt; canary-weight</code></p><p>我们可以把以上的四个 Annotation 规则可以总体划分为以下两类：</p><ul><li><p>基于权重的 Canary 规则</p><img src="/rancher/rancher-nginx-ingress-canary/7s52AX_nxD_cIVEcI3L63BjmNtBbHXrOOA.png" class="" title="enter description here"></li><li><p>基于用户请求的 Canary 规则</p></li></ul><img src="/rancher/rancher-nginx-ingress-canary/SHx_-HINxY0bsp1ocAxSwdhjwGp19fJV-A.png" class="" title="enter description here"><p><strong>注意：</strong> Nginx-ingress 在 0.21.0 版本中引入的 Canary 功能，因此要确保 ingress 版本大于等于 0.21.0。</p><h2 id="测试准备"><a href="#测试准备" class="headerlink" title="测试准备"></a>测试准备</h2><ul><li><p>通过 RancherUI 创建 deployment，分别为 <strong>nginx-v1</strong>和<strong>nginx-v2</strong>。</p>  <img src="/rancher/rancher-nginx-ingress-canary/gFJIMh0ub6fdDFD8MXLp6UchIC3GSyYAGQ.jpeg" class="" title="depoyment"></li><li><p>创建 ingress 规则</p>  <img src="/rancher/rancher-nginx-ingress-canary/qHVLo_RzDBS0Z6Xa42f-2WyWogM4mT1tlw.jpeg" class="" title="ingress"></li><li><p>接着进入容器中，修改 nginx 默认页面 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html，页面修改后访问效果分别为：</p><ol><li><img src="/rancher/rancher-nginx-ingress-canary/9nUiFJfQrCc5apJWvKqnmsmtHeGQjEtalA.jpeg" class="" title="img"></li><li><img src="/rancher/rancher-nginx-ingress-canary/FbYXp_xTyDTKJwiNtHEjbIkNB_h5cpCNWw.png" class="" title="img"></li></ol></li></ul><h2 id="测试步骤"><a href="#测试步骤" class="headerlink" title="测试步骤"></a>测试步骤</h2><h3 id="测试场景-1：基于-header-实现蓝绿或者-A-x2F-B-test"><a href="#测试场景-1：基于-header-实现蓝绿或者-A-x2F-B-test" class="headerlink" title="测试场景 1：基于 header 实现蓝绿或者 A&#x2F;B test"></a>测试场景 1：基于 header 实现蓝绿或者 A&#x2F;B test</h3><ol><li><p>我们基于 V2 进行 ingress 的配置，添加 ingress 的 annotation：</p> <img src="/rancher/rancher-nginx-ingress-canary/oD65leVvcZTMyD_yvrkWn76OwlO37N76KA.jpeg" class="" title="v2 版本 ingress 配置"></li><li><p>我们在客户端执行命令（带 Rancher：Best 的请求头）：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -H <span class="string">&quot;Rancher: Best&quot;</span> http://es01.zyl.com</span><br></pre></td></tr></table></figure><p> 可以看到当我们带请求头去访问 es01.zyl.com 这个页面时，流量会被自动转发到 V2 的页面里，当我们不带请求头直接访问就会是 V1 版本的页面</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl  http://es01.zyl.com</span><br></pre></td></tr></table></figure> <img src="/rancher/rancher-nginx-ingress-canary/eVVQBAC4ul9xXRY88rARItAB7FLAm4efVg.png" class="" title="v1 版本"></li></ol><h3 id="测试场景-2：基于流量实现灰度或者蓝绿"><a href="#测试场景-2：基于流量实现灰度或者蓝绿" class="headerlink" title="测试场景 2：基于流量实现灰度或者蓝绿"></a>测试场景 2：基于流量实现灰度或者蓝绿</h3><ol><li><p>v2 版本 ingress annotation 配置如下：</p> <img src="/rancher/rancher-nginx-ingress-canary/iW_99OrulWgCVAY3r27_aiOfVk2vFklEUA.jpeg" class="" title="基于流量 ingress 设置"><p> 此例中我配置的权重是 50%流量会进入 V2 版本，因此我们在浏览器里不断刷新访问后会发现 V1 和 V2 两个页面，总体概率是 50%到 V2，50%到 V1，可以直接调整权重的值实现灰度发布。</p> <img src="/rancher/rancher-nginx-ingress-canary/4XUL0H8ZI9c4RPdk4-2w-WJ2MKZT4FgXYw.jpeg" class="" title="v1 版本"></li></ol><h3 id="两种场景的-yaml-文件截图"><a href="#两种场景的-yaml-文件截图" class="headerlink" title="两种场景的 yaml 文件截图"></a>两种场景的 yaml 文件截图</h3><ul><li><p>基于 Header</p>  <img src="/rancher/rancher-nginx-ingress-canary/46PCvcRRBr2O9mh0VEU6VbjiJlrpC6sDwg.jpeg" class="" title="基于 header"></li><li><p>基于流量权重</p>  <img src="/rancher/rancher-nginx-ingress-canary/p0kC32QjjCbRnGCtXnIpc7GuWn-U15CVMA.jpeg" class="" title="基于流量权重"></li></ul>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> 灰度发布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8S 自动部署工具</title>
      <link href="/kubernetes/automatic-k8s-deployment-tool/"/>
      <url>/kubernetes/automatic-k8s-deployment-tool/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/automatic-k8s-deployment-tool/" target="_blank" title="https://www.xtplayer.cn/kubernetes/automatic-k8s-deployment-tool/">https://www.xtplayer.cn/kubernetes/automatic-k8s-deployment-tool/</a></p><p>你或者你的团队是否正在寻找下一代架构？或者你已经拥有了这样的架构，但是仍然在探索自动化和管理的最佳方式。在本文中，我们将借助 env0 的能力来部署 Rancher 环境。</p><ul><li><p>Rancher：开源 Kubernetes 管理平台</p><p>对于采用容器的团队来说，Rancher 是一个完整的软件堆栈。它解决了多 Kubernetes 集群管理的运维和安全挑战，同时为 DevOps 团队提供了运行容器化工作负载的集成工具。在本次 demo 中我们将使用 Rancher，您可以访问以下链接查看如何快速运行 Rancher：<a href="https://github.com/rancher/quickstart">https://github.com/rancher/quickstart</a></p></li><li><p>env0：基础架构自动化平台</p><p>env0 是一个基础架构自动化平台，它可以将 Terraform 及其他框架下的 deployment 集中到一个协作区，以方便你的整个团队使用。你可以通过下方链接登录并免费使用该产品：<a href="https://app.env0.com/login">https://app.env0.com/login</a></p></li><li><p>Rancher+env0 的神奇效果</p><p>自动化部署和摧毁环境是加速开发的关键要素。即使你只是将静态环境用于生产和开发这样的事情，有时你也需要重新部署以升级或者进行其他任务。例如，你可能正在重定位 deployment 到另一个区域或可用性区域，或者在动态环境中其他更大的用处，如每个拉取请求的环境。</p><p>将 Kubernetes 用于你的应用基础架构可以使这种开发变得更加容易，因为你已经准备好了你的应用配置部署文件，一旦新的集群启动和运行，就可以重新部署。当你的应用重新部署到新的基础架构并不是一项巨大的工程时，你就可以自由地在你认为合适的时候重新创建那个基础架构。而我们将使用 env0 来自动化该基础架构的拆除和重新部署，使其更加简单。访问以下链接免费注册：<a href="https://app.env0.com/login">https://app.env0.com/login</a></p></li></ul><h2 id="设置步骤"><a href="#设置步骤" class="headerlink" title="设置步骤"></a>设置步骤</h2><p>我们首先需要一些代码。请记住我在前文给你的 repo（ <a href="https://github.com/rancher/quickstart">https://github.com/rancher/quickstart</a> ）。在本例中，由于 AWS 中的身份和访问管理（IAM）设置，我们将使用该文件夹中的 AWS 文件夹，并进行微调。我们的团队将 AWS 中的 12 小时编程密钥用于我们的 lab account。此账户还需要“AWS Session Token”凭证。因此，标准的密钥对并不适用于我们。我在此使用的代码是经过修改的，具体内容可以访问以下链接：</p><p><a href="https://github.com/vtimd/quickstart/tree/master/aws">https://github.com/vtimd/quickstart/tree/master/aws</a></p><p>我已经将这些 AWS 凭证作为<strong>全局变量</strong>放在 env0 中，以便我在之后的其他项目中能够随时使用它们。如果你为每个项目或环境分配不同的凭证集，你可以简单地将它们设置为<strong>项目级</strong>或<strong>环境级</strong>变量。</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356484" class="" title="图片"><h3 id="全局变量"><a href="#全局变量" class="headerlink" title="全局变量"></a>全局变量</h3><p>设置完成后，我们想要配置在项目中使用的模板。这是一个特别简单的过程——起名，然后把它指向我们要使用的 Github repo 就可以了。或者，如果你使用其他版本控制系统（VCS）提供程序，只需提供 URL 和 git token 进行 auth 就可以了。要设置这个模板，点击<strong>Organization Templates</strong>部分下方的<strong>Create New Template</strong>即可。</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356546" class="" title="图片"><h3 id="模板设置"><a href="#模板设置" class="headerlink" title="模板设置"></a>模板设置</h3><p>如果我们访问项目，我们将看到<strong>Create New Project</strong>按钮。项目创建也极为简单，仅需命名和描述即可。完成这些之后，我们就进入<strong>项目模板</strong>中，对新创建的项目进行操作。</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356398" class="" title="图片"><p>我们将模板分配到项目之后，可以开始部署我们的 Rancher 环境。</p><h2 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h2><p>现在我们到了一个有意思的部分。现在我们已经准备好部署 Rancher 环境。在 env0 平台中这是一个简单的过程。我们将启动我们刚刚创建的新项目，然后访问项目环境。在这里我们可以看到当前所有的环境并且可以启动一个新的环境。在这一页中，我们点击<strong>Create New Environment</strong>。</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356446" class="" title="图片"><p>我们完成操作之后，我们将看到项目模板的页面以及可用的所有模板。由于我们只启用了 Rancher-Quickstart 模板，所以我们在这里应该看到的只有这个模板。我们继续点击<strong>Run Now</strong>。</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356469" class="" title="图片"><p>在创建新环境的页面上，我们将填写环境名称、Workspace 名称（可选，因为如果你没有指定名称，env0 会自动生成一个）以及 Revision。我选择了在每次推送到 git 分支时选择重新部署。这基本上实现了环境的持续部署（CD），因为它将在每次推送时重新运行 env0 中的部署。</p><p>同时，我在 Destroy Environment 部分设置了一个自定义的 Time-To-Live（TTL）。由于仅仅为了本次 demo，我不希望它永远保留在这里。这是一个很好的功能，可以用于管理短期的 demo、沙箱或开发实例，如果你遗忘了这些已经启动的实例或环境，可能需要为此付费。</p><p>此前我们已经设置了变量部分，并且从组织层到项目层再到环境层这些都是继承的。如果产生了冲突，那么优先采用适用范围最小的变量。所以如果你想覆盖任何组织或项目变量，你可以在这里为这个特定环境注入它们。剩下要做的就是点击<strong>Run</strong>。</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356493" class="" title="图片"><p>就这样，我们要开始部署了！</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356509" class="" title="图片"><ul><li>Deployment 1</li></ul><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356519" class="" title="图片"><ul><li><p>Deployment 2</p><p>env0 会在部署完成后马上给你发送邮件。</p></li></ul><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356519" class="" title="图片"><ul><li>Active 环境信息</li></ul><p>当部署完成返回环境时，检查<strong>Resources</strong> 标签页。这里，我们将找到在 Terraform 文件中所有的指定输出以及由 deployment 创建的 Resources。在这里我们将获取<strong>rancher_server_url</strong>以访问我们的新环境。</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356544" class="" title="图片"><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>在这里，当我们点击 URL 时，它会被复制到剪贴板。我们可以把它放到另一个标签页里，就可以看到 Rancher 登录页面啦。</p><img src="/kubernetes/automatic-k8s-deployment-tool/640-20210106183356556" class="" title="图片"><h2 id="总-结"><a href="#总-结" class="headerlink" title="总 结"></a>总 结</h2><p>大功告成！我们已经介绍了什么是 Rancher 和 env0，为什么它们搭配使用很有效，以及如何设置 env0 以自动化部署 Rancher 与 Terraform 的 Kubernetes 环境。如果你遵循了本教程的步骤，那么现在你应该拥有一个超棒的 Kubernetes 工作环境以部署你的应用程序。所以，打开你的 YAML manifest，好好享受吧!</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher16 通过 webhook 自动升级服务</title>
      <link href="/rancher/rancher16-webhook-auto-update/"/>
      <url>/rancher/rancher16-webhook-auto-update/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/rancher16-webhook-auto-update/" target="_blank" title="https://www.xtplayer.cn/rancher/rancher16-webhook-auto-update/">https://www.xtplayer.cn/rancher/rancher16-webhook-auto-update/</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>前面的文章我们有讲述了如何通过 Rancher-webhook 实现 Service&#x2F;Host 的弹性伸缩。今天我们再来演示一下如何通过 Rancher-webhook 对接三方的 CI 系统，实现微服务镜像的自动构建与服务的自动升级。</p><p>PS: CI 即持续集成，包括但不限于自动编译、发布和测试、自动构建，我们这里说的 CI 系统仅限于自动构建这一步。 前面已经对 webhook 做了介绍，这里不再讲解。<br>本文主要基于阿里云的容器镜像服务，整个流程大致如下图所示:</p><img src="/rancher/rancher16-webhook-auto-update/1527424471499000.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424471499000.png"><h2 id="基础准备"><a href="#基础准备" class="headerlink" title="基础准备"></a>基础准备</h2><ol><li>安装支持的 docker (<a href="http://rancher.com/docs/rancher/v1.6/en/hosts/#supported-docker-versions">http://rancher.com/docs/rancher/v1.6/en/hosts/#supported-docker-versions</a>);</li><li>安装 Rancher v1.6.11 (<a href="https://hub.docker.com/u/rancher">https://hub.docker.com/u/rancher</a>);</li><li>因为是对接云端 CI，所以 Rancher server 需要能被公网访问；</li><li>在 GitHub 创建一个 test 仓库并上传一个 Dkoerfile 文件，文件中只写 FROM busybox 一行代码；</li><li>去 <code>http://requestbin.net/</code> 创建一个 RequestBin，用于接收并查看阿里云 webhook 消息的内容；</li></ol><h2 id="测试服务准备"><a href="#测试服务准备" class="headerlink" title="测试服务准备"></a>测试服务准备</h2><ol><li><p>登录 Rancher WEB ，进入 应用\用户视图，新建名为 app 的测试应用栈；</p></li><li><p>给应用打上 test&#x3D;true 的标签，其他参数保存默认；</p> <img src="/rancher/rancher16-webhook-auto-update/1527424471489800.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424471489800.png"> <img src="/rancher/rancher16-webhook-auto-update/1527424471973045.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424471973045.png"><p> PS：这个地方的标签，是为后面通过 webhook 升级时候调取服务用，需要保证标签的唯一性，不然相同标签的服务都会被升级 测试服务已经创建好，接下来创建一条 webhook 升级策略。四、添加 webhook 接收器</p></li><li><p>通过 API 进入 webhook，点击添加接收器，配置接收器：</p> <img src="/rancher/rancher16-webhook-auto-update/1527424471650964.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424471650964.png"> <img src="/rancher/rancher16-webhook-auto-update/1527424471986742.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424471986742.png"> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">名称:根据喜好填写；</span><br><span class="line">类型:选择升级服务；</span><br><span class="line">镜像仓库 Webhook 参数格式：目前可以选择阿里云或者 docker hub</span><br><span class="line">镜像标签:这个标签对应仓库中构建镜像的标签</span><br><span class="line">服务选择器标签:这里填写创建服务时填写的标签；</span><br><span class="line">其他参数保持默认</span><br></pre></td></tr></table></figure></li><li><p>接收器创创建好后，可以点击右侧的触发地址把地址复制到其他地方备用。</p> <img src="/rancher/rancher16-webhook-auto-update/1527424471112979.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424471112979.png"></li></ol><h2 id="创建阿里云测试镜像仓库"><a href="#创建阿里云测试镜像仓库" class="headerlink" title="创建阿里云测试镜像仓库"></a>创建阿里云测试镜像仓库</h2><ol><li><p>通过 dev.aliyun.com 登录阿里云容器服务，进入控制台，点击右上角创建镜像仓库</p> <img src="/rancher/rancher16-webhook-auto-update/1527424472109055.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424472109055.png"> <img src="/rancher/rancher16-webhook-auto-update/1527424472398412.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424472398412.png"><p> ps:选择与你服务器所在的区域，镜像可以走内网下载。 找到刚刚创建的仓库，点击管理</p> <img src="/rancher/rancher16-webhook-auto-update/1527424472425306.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424472425306.png"></li><li><p>添加一条 webhook，webhook URL 为 rancher-webhook 中复制的地址。</p> <img src="/rancher/rancher16-webhook-auto-update/1527424472478305.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424472478305.png"></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>修改并提交 test 代码仓库中的 Dockerfile 文件 在阿里云容器服务中查看构建进度</p> <img src="/rancher/rancher16-webhook-auto-update/1527424472617034.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424472617034.png"> <img src="/rancher/rancher16-webhook-auto-update/1527424472840600.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424472840600.png"> <img src="/rancher/rancher16-webhook-auto-update/1527424472963414.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424472963414.png"> <img src="/rancher/rancher16-webhook-auto-update/1527424473384612.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424473384612.png"></li></ol><h2 id="查看阿里云-webhook-的信息内容"><a href="#查看阿里云-webhook-的信息内容" class="headerlink" title="查看阿里云 webhook 的信息内容"></a>查看阿里云 webhook 的信息内容</h2><ol><li><p>复制 <code>http://requestbin.net/</code> 中创建的 RequestBin 地址</p> <img src="/rancher/rancher16-webhook-auto-update/1527424473974408.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424473974408.png"></li><li><p>再在阿里云云服务中添加一条 webhook</p> <img src="/rancher/rancher16-webhook-auto-update/1527424473499533.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424473499533.png"></li><li><p>再次修改并提交 test 代码仓库中的 Dockerfile 文件，并进入 <code>http://requestb.net</code> 页面查看：</p> <img src="/rancher/rancher16-webhook-auto-update/1527424473406928.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424473406928.png"></li><li><p>再对 RAW BODY 格式化：</p> <img src="/rancher/rancher16-webhook-auto-update/1527424473804069.png" class="" title="https:&#x2F;&#x2F;forums.cnrancher.com&#x2F;uploads&#x2F;image&#x2F;yuancheng&#x2F;20180527&#x2F;1527424473804069.png"></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8S 节点初始化</title>
      <link href="/rancher/node-init/"/>
      <url>/rancher/node-init/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/node-init/" target="_blank" title="https://www.xtplayer.cn/rancher/node-init/">https://www.xtplayer.cn/rancher/node-init/</a></p><h2 id="自动清理节点"><a href="#自动清理节点" class="headerlink" title="自动清理节点"></a>自动清理节点</h2><p>将节点添加到集群时后，会创建容器、虚拟网络接口等资源和证书、配置文件。从集群中正常删除节点时(如果处于 Active 状态)，将自动清除这些资源，并且只需重新启动节点即可。当节点无法访问且无法使用自动清理，或者异常导致节点脱离集群后，如果需要再次将节点加入集群，那么需要手动进行节点初始化操作。</p><h2 id="手动清理节点"><a href="#手动清理节点" class="headerlink" title="手动清理节点"></a>手动清理节点</h2><blockquote><p><strong>警告:</strong> 以下操作将删除节点中的数据，在执行命令之前，请确保已进行数据备份。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">KUBE_SVC=<span class="string">&#x27;</span></span><br><span class="line"><span class="string">kubelet</span></span><br><span class="line"><span class="string">kube-scheduler</span></span><br><span class="line"><span class="string">kube-proxy</span></span><br><span class="line"><span class="string">kube-controller-manager</span></span><br><span class="line"><span class="string">kube-apiserver</span></span><br><span class="line"><span class="string">&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> kube_svc <span class="keyword">in</span> <span class="variable">$&#123;KUBE_SVC&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="comment"># 停止服务</span></span><br><span class="line">  <span class="keyword">if</span> [[ `systemctl is-active <span class="variable">$&#123;kube_svc&#125;</span>` == <span class="string">&#x27;active&#x27;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    systemctl stop <span class="variable">$&#123;kube_svc&#125;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">  <span class="comment"># 禁止服务开机启动</span></span><br><span class="line">  <span class="keyword">if</span> [[ `systemctl is-enabled <span class="variable">$&#123;kube_svc&#125;</span>` == <span class="string">&#x27;enabled&#x27;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    systemctl <span class="built_in">disable</span> <span class="variable">$&#123;kube_svc&#125;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止所有容器</span></span><br><span class="line">docker stop $(docker ps -aq)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除所有容器</span></span><br><span class="line">docker <span class="built_in">rm</span> -f $(docker ps -qa)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除所有容器卷</span></span><br><span class="line">docker volume <span class="built_in">rm</span> $(docker volume <span class="built_in">ls</span> -q)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载 mount 目录</span></span><br><span class="line"><span class="keyword">for</span> mount <span class="keyword">in</span> $(mount | grep tmpfs | grep <span class="string">&#x27;/var/lib/kubelet&#x27;</span> | awk <span class="string">&#x27;&#123; print $3 &#125;&#x27;</span>) /var/lib/kubelet /var/lib/rancher;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  umount <span class="variable">$mount</span>;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 备份目录</span></span><br><span class="line"><span class="built_in">mv</span> /etc/kubernetes /etc/kubernetes-bak-$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M&quot;</span>)</span><br><span class="line"><span class="built_in">mv</span> /var/lib/etcd /var/lib/etcd-bak-$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M&quot;</span>)</span><br><span class="line"><span class="built_in">mv</span> /var/lib/rancher /var/lib/rancher-bak-$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M&quot;</span>)</span><br><span class="line"><span class="built_in">mv</span> /opt/rke /opt/rke-bak-$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除残留路径</span></span><br><span class="line"><span class="built_in">rm</span> -rf /etc/ceph \</span><br><span class="line">    /etc/cni \</span><br><span class="line">    /opt/cni \</span><br><span class="line">    /run/secrets/kubernetes.io \</span><br><span class="line">    /run/calico \</span><br><span class="line">    /run/flannel \</span><br><span class="line">    /var/lib/calico \</span><br><span class="line">    /var/lib/cni \</span><br><span class="line">    /var/lib/kubelet \</span><br><span class="line">    /var/log/containers \</span><br><span class="line">    /var/log/kube-audit \</span><br><span class="line">    /var/log/pods \</span><br><span class="line">    /var/run/calico</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理网络接口</span></span><br><span class="line">no_del_net_inter=<span class="string">&#x27;</span></span><br><span class="line"><span class="string">lo</span></span><br><span class="line"><span class="string">docker0</span></span><br><span class="line"><span class="string">eth</span></span><br><span class="line"><span class="string">ens</span></span><br><span class="line"><span class="string">bond</span></span><br><span class="line"><span class="string">&#x27;</span></span><br><span class="line"></span><br><span class="line">network_interface=`<span class="built_in">ls</span> /sys/class/net`</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> net_inter <span class="keyword">in</span> <span class="variable">$&#123;network_interface&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> ! <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;no_del_net_inter&#125;</span>&quot;</span> | grep -qE <span class="variable">$&#123;net_inter:0:3&#125;</span>; <span class="keyword">then</span></span><br><span class="line">    ip <span class="built_in">link</span> delete <span class="variable">$net_inter</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理残留进程</span></span><br><span class="line">port_list=<span class="string">&#x27;</span></span><br><span class="line"><span class="string">80</span></span><br><span class="line"><span class="string">443</span></span><br><span class="line"><span class="string">6443</span></span><br><span class="line"><span class="string">2376</span></span><br><span class="line"><span class="string">2379</span></span><br><span class="line"><span class="string">2380</span></span><br><span class="line"><span class="string">8472</span></span><br><span class="line"><span class="string">9099</span></span><br><span class="line"><span class="string">10250</span></span><br><span class="line"><span class="string">10254</span></span><br><span class="line"><span class="string">&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> port <span class="keyword">in</span> <span class="variable">$&#123;port_list&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  pid=`netstat -atlnup | grep -w <span class="variable">$&#123;port&#125;</span> | grep -v - | awk <span class="string">&#x27;&#123;print $7&#125;&#x27;</span> | awk -F <span class="string">&#x27;/&#x27;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | <span class="built_in">sort</span> -rnk2 | <span class="built_in">uniq</span>`</span><br><span class="line">  <span class="keyword">if</span> [[ -n <span class="variable">$&#123;pid&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">kill</span> -15 <span class="variable">$&#123;pid&#125;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">kube_pid=`ps -ef | grep -v grep | grep kube | awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>`</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -n <span class="variable">$&#123;kube_pid&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">kill</span> -15 <span class="variable">$&#123;kube_pid&#125;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理 Iptables 表</span></span><br><span class="line"><span class="comment">## 注意：如果节点 Iptables 有特殊配置，以下命令请谨慎操作</span></span><br><span class="line">sudo iptables --flush</span><br><span class="line">sudo iptables --flush --table nat</span><br><span class="line">sudo iptables --flush --table filter</span><br><span class="line">sudo iptables --table nat --delete-chain</span><br><span class="line">sudo iptables --table filter --delete-chain</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> 节点初始化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rancher k8s 使用 iscsi 存储</title>
      <link href="/rancher/rancher-k8s-use-iscsi-storage/"/>
      <url>/rancher/rancher-k8s-use-iscsi-storage/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/rancher-k8s-use-iscsi-storage/" target="_blank" title="https://www.xtplayer.cn/rancher/rancher-k8s-use-iscsi-storage/">https://www.xtplayer.cn/rancher/rancher-k8s-use-iscsi-storage/</a></p><h2 id="iSCSI-命名约定"><a href="#iSCSI-命名约定" class="headerlink" title="iSCSI 命名约定"></a>iSCSI 命名约定</h2><p>iSCSI 使用一种特殊、唯一的名称来标识 iSCSI 节点（目标或启动器）。此名称类似于与光纤通道设备相关联的全球名称 (WWN)，可作为一种通用的节点识别方式使用。iSCSI 名称通过两种不同方式格式化。最常见的是 IQN 格式。有关 iSCSI 命名要求和字符串配置文件的更多详细信息，请参见 IETF 网站上的 RFC 3721 和 RFC 3722。</p><h3 id="iSCSI-限定名-IQN-格式"><a href="#iSCSI-限定名-IQN-格式" class="headerlink" title="iSCSI 限定名 (IQN) 格式"></a>iSCSI 限定名 (IQN) 格式</h3><p><strong>IQN</strong> 格式采用 <strong>iqn.yyyy-mm.naming-authority:unique name</strong> 的形式，其中：</p><ul><li><p><strong>yyyy-mm</strong> 是命名机构成立的年份和月份。</p></li><li><p>naming-authority 通常是命名机构的 Internet 域名的反向语法。例如，iscsi.vmware.com 命名机构的 iSCSI 限定名形式可能是 iqn.1998-01.com.vmware.iscsi。此名称表示 vmware.com 域名于 1998 年 1 月注册，iscsi 是一个由 vmware.com 维护的子域。</p></li><li><p><strong>unique name</strong> 是希望使用的任何名称，如主机的名称。命名机构必须确保在冒号后面分配的任何名称都是唯一的，</p><p><strong>例如：</strong></p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">iqn.1998-01.com.vmware.iscsi:name1</span><br><span class="line"></span><br><span class="line">iqn.1998-01.com.vmware.iscsi:name2</span><br><span class="line"></span><br><span class="line">iqn.1998-01.com.vmware.iscsi:name999</span><br></pre></td></tr></table></figure><h3 id="企业唯一标识符-EUI-格式"><a href="#企业唯一标识符-EUI-格式" class="headerlink" title="企业唯一标识符 (EUI) 格式"></a>企业唯一标识符 (EUI) 格式</h3><p>EUI 格式采用 eui.16 hex digits 的形式。例如: eui.0123456789ABCDEF。</p><p>16 位十六进制数字是 IEEE EUI（扩展唯一标识符）格式的 64 位数的文本表示形式。前 24 位是 IEEE 向特定公司注册的公司 ID。后 40 位由持有该公司 ID 的实体分配，并且必须是唯一的。</p><h2 id="更新-Kubernetes-集群-可选"><a href="#更新-Kubernetes-集群-可选" class="headerlink" title="更新 Kubernetes 集群(可选)"></a>更新 Kubernetes 集群(可选)</h2><p>Rancher 安装的 Kubernetes 集群，利用 iSCSI 启动器工具在 iSCSI 卷上存储数据，该工具嵌入在 kubelet 的 rancher&#x2F;hyperkubeDocker 镜像中。在某些情况下，安装在 <code>kubelet</code> 容器中的启动器和 iscsi 服务器版本可能不匹配，从而导致连接失败。</p><p>如果遇到此问题，可以将集群中每个节点上安装启动器工具，然后将启动器工具映射到 <code>kubelet</code> 容器来解决版本问题。</p><table><thead><tr><th align="left">平台</th><th align="left">包裹名字</th><th align="left">安装命令</th></tr></thead><tbody><tr><td align="left">Ubuntu&#x2F;Debian</td><td align="left"><code>open-iscsi</code></td><td align="left"><code>sudo apt install open-iscsi</code></td></tr><tr><td align="left">RHEL</td><td align="left"><code>iscsi-initiator-utils</code></td><td align="left"><code>yum install iscsi-initiator-utils -y</code></td></tr></tbody></table><p>在节点上安装启动器工具后，编辑集群 YAML，编辑 kubelet 配置挂载主机 SCSI 二进制文件和配置目录，如下面的示例所示。</p><blockquote><p>在更新 Kubernetes YAML 之前，请确保在集群节点上安装了 <code>open-iscsi</code>（deb）或 <code>iscsi-initiator-utils</code>（yum）软件包。如果在 Kubernetes YAML 中创建绑定挂载 <em>之前</em> 未安装此软件包，Docker 将自动在每个节点上创建目录和文件。</p></blockquote><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">kubelet:</span></span><br><span class="line">    <span class="attr">extra_binds:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;/etc/iscsi:/etc/iscsi&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;/sbin/iscsiadm:/sbin/iscsiadm&quot;</span></span><br></pre></td></tr></table></figure><h2 id="创建-ISCSI-服务器"><a href="#创建-ISCSI-服务器" class="headerlink" title="创建 ISCSI 服务器"></a>创建 ISCSI 服务器</h2><h3 id="操作系统以为-centos7-6-为例"><a href="#操作系统以为-centos7-6-为例" class="headerlink" title="操作系统以为 centos7.6 为例"></a>操作系统以为 centos7.6 为例</h3><table><thead><tr><th align="left">主机名</th><th align="center">配置</th><th align="center">IP 地址</th><th align="center">角色</th><th align="center">操作系统用户</th><th>操作系统</th></tr></thead><tbody><tr><td align="left">iscsi-target-server</td><td align="center">2C4G</td><td align="center">1.1.1.47</td><td align="center">iSCSI Target</td><td align="center">root</td><td>centos</td></tr><tr><td align="left">iscsi-target-agent</td><td align="center">-</td><td align="center">1.1.1.46</td><td align="center">iSCSI Initiator</td><td align="center">root</td><td>centos</td></tr></tbody></table><blockquote><p>注意：以上配置仅用于本次文档演示</p></blockquote><h3 id="iSCSI-Target-服务器搭建"><a href="#iSCSI-Target-服务器搭建" class="headerlink" title="iSCSI Target 服务器搭建"></a>iSCSI Target 服务器搭建</h3><ol><li><p>基础环境配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 禁止 selinux</span></span><br><span class="line">setenforce 0 <span class="comment"># 临时禁止，重启后恢复</span></span><br><span class="line">sudo sed -i <span class="string">&#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27;</span> /etc/selinux/config <span class="comment"># 永久禁止，需要重启服务器</span></span><br><span class="line"><span class="comment"># 关闭防火墙</span></span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld.service \</span><br><span class="line">&amp;&amp; systemctl stop firewalld.service</span><br><span class="line"><span class="comment"># 安装 ntp 时间同步服务器</span></span><br><span class="line">yum install ntp -y \</span><br><span class="line">&amp;&amp; systemctl <span class="built_in">enable</span> ntpd.service \</span><br><span class="line">&amp;&amp; systemctl start ntpd.service</span><br></pre></td></tr></table></figure></li><li><p>安装 iSCSI target 管理工具</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install targetcli.noarch -y \</span><br><span class="line">&amp;&amp; systemctl <span class="built_in">enable</span> target.service \</span><br><span class="line">&amp;&amp; systemctl start target.service</span><br></pre></td></tr></table></figure></li><li><p>创建一个用于存放磁盘文件的目录，比如 <code>/iscsi_disks</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /iscsi_disks</span><br></pre></td></tr></table></figure></li><li><p>进入 <strong>target</strong> 管理命令行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">targetcli</span><br></pre></td></tr></table></figure></li><li><p>创建磁盘镜像</p><p>iscsi 支持多种存储对象，可以通过 <code>ls /backstores/</code> 查看，这里演示选择文件存储 <code>fileio</code>，如果用于生产建议选择<code>块设备 block</code>。</p><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502105823857.png" class="" title="image-20190502105823857"><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建磁盘镜像</span></span><br><span class="line"><span class="built_in">cd</span> /backstores/fileio</span><br><span class="line">create disk01 /iscsi_disks/disk01.img 10G</span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502110142240.png" class="" title="image-20190502110142240"></li><li><p>创建 <code>target</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /iscsi</span><br><span class="line">create iqn.2019-05.com.example:servers1</span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502110728898.png" class="" title="image-20190502110728898"></li><li><p>创建 <strong>LUN</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /iscsi/iqn.2019-05.com.example:servers1/tpg1/luns</span><br><span class="line">create /backstores/fileio/disk01</span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502110938378.png" class="" title="image-20190502110938378"></li><li><p>(可选)配置 <strong>ACL</strong>，限制哪些客户端能连接</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /iscsi/iqn.2019-05.com.example:servers1/tpg1/acls</span><br><span class="line">create iqn.2019-05.com.example:agent1</span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502111622579.png" class="" title="image-20190502111622579"></li><li><p>配置通信身份验证</p><ul><li><p>单向登录认证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /iscsi/iqn.2019-05.com.example:servers1/tpg1/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启 tpg1 级别的登录认证</span></span><br><span class="line"><span class="built_in">set</span> attribute authentication=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用 read/write 模式</span></span><br><span class="line"><span class="built_in">set</span> attribute demo_mode_write_protect=0</span><br><span class="line"><span class="built_in">set</span> attribute generate_node_acls=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 tpg1 级别的登录认证</span></span><br><span class="line"><span class="built_in">set</span> attribute authentication=1</span><br><span class="line"><span class="built_in">set</span> auth userid=agent_login_user1 <span class="comment"># 传入账号，也就是客户端访问服务端认证使用的账号</span></span><br><span class="line"><span class="built_in">set</span> auth password=agent_login_passwd1 <span class="comment">#传入账号对应的密码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 acls 级别的登录认证</span></span><br><span class="line"><span class="built_in">cd</span> /iscsi/iqn.2019-05.com.example:servers1/tpg1/acls/iqn.2019-05.com.example:agent1</span><br><span class="line"><span class="built_in">set</span> auth userid=agent_login_user1 <span class="comment"># 传入账号，也就是客户端访问服务端认证使用的账号</span></span><br><span class="line"><span class="built_in">set</span> auth password=agent_login_passwd1 <span class="comment">#传入账号对应的密码</span></span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502113817782.png" class="" title="image-20190502113817782"><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502113836685.png" class="" title="image-20190502113836685"></li><li><p>双向登录认证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /iscsi/iqn.2019-05.com.example:servers1/tpg1/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启 tpg1 级别的登录认证</span></span><br><span class="line"><span class="built_in">set</span> attribute authentication=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用 read/write 模式</span></span><br><span class="line"><span class="built_in">set</span> attribute demo_mode_write_protect=0</span><br><span class="line"><span class="built_in">set</span> attribute generate_node_acls=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 tpg1 级别的登录认证</span></span><br><span class="line"><span class="built_in">set</span> attribute authentication=1</span><br><span class="line"><span class="built_in">set</span> auth userid=agent_login_user1 <span class="comment"># 传入账号，客户端登录服务端用于校验的账号</span></span><br><span class="line"><span class="built_in">set</span> auth password=agent_login_passwd1 <span class="comment"># 传入账号对应的密码</span></span><br><span class="line"><span class="built_in">set</span> auth mutual_userid=server_check_user1 <span class="comment"># 传出账号，服务器用于验证客户端的账号</span></span><br><span class="line"><span class="built_in">set</span> auth mutual_password=server_check_passwd1 <span class="comment"># 传出账号对应的密码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 acls 级别的登录认证</span></span><br><span class="line"><span class="built_in">cd</span> /iscsi/iqn.2019-05.com.example:servers1/tpg1/acls/iqn.2019-05.com.example:agent1</span><br><span class="line"><span class="built_in">set</span> auth userid=agent_login_user1 <span class="comment"># 传入账号，客户端登录服务端用于校验的账号</span></span><br><span class="line"><span class="built_in">set</span> auth password=agent_login_passwd1 <span class="comment"># 传入账号对应的密码</span></span><br><span class="line"><span class="built_in">set</span> auth mutual_userid=server_check_user1 <span class="comment"># 传出账号，服务器用于验证客户端的账号</span></span><br><span class="line"><span class="built_in">set</span> auth mutual_password=server_check_passwd1 <span class="comment"># 传出账号对应的密码</span></span><br></pre></td></tr></table></figure></li></ul><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502114449356.png" class="" title="image-20190502114449356"><ul><li><p>单向发现认证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /iscsi</span><br><span class="line"><span class="built_in">set</span> discovery_auth <span class="built_in">enable</span>=1</span><br><span class="line"><span class="built_in">set</span> discovery_auth userid=agent_discovery_user2 <span class="comment"># 传入账号，客户端发现服务端用于校验的账号</span></span><br><span class="line"><span class="built_in">set</span> discovery_auth password=agent_discovery_passwd2 <span class="comment"># 传入账号对应的密码</span></span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502114944156.png" class="" title="image-20190502114944156"></li><li><p>双向发现认证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /iscsi</span><br><span class="line"><span class="built_in">set</span> discovery_auth <span class="built_in">enable</span>=1</span><br><span class="line"><span class="built_in">set</span> discovery_auth userid=agent_discovery_user2 <span class="comment"># 传入账号，客户端发现服务端用于校验的账号</span></span><br><span class="line"><span class="built_in">set</span> discovery_auth password=agent_discovery_passwd2 <span class="comment"># 传入账号对应的密码</span></span><br><span class="line"><span class="built_in">set</span> discovery_auth mutual_userid=server_check_user2 <span class="comment"># 传出账号，服务器用于验证客户端的账号</span></span><br><span class="line"><span class="built_in">set</span> discovery_auth mutual_password=server_check_passwd2 <span class="comment"># 传出账号对应的密码</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>执行 <code>exit</code> 退出编辑并自动保存</p><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502115342809.png" class="" title="image-20190502115342809"></li></ol><h3 id="客户端校验"><a href="#客户端校验" class="headerlink" title="客户端校验"></a>客户端校验</h3><ol><li><p>基础环境配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 禁止 selinux</span></span><br><span class="line">setenforce 0 <span class="comment"># 临时禁止，重启后恢复</span></span><br><span class="line"><span class="comment"># 关闭防火墙</span></span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld.service \</span><br><span class="line">&amp;&amp; systemctl stop firewalld.service</span><br><span class="line"><span class="comment"># 安装 ntp 时间同步服务器</span></span><br><span class="line">yum install ntp -y \</span><br><span class="line">&amp;&amp; systemctl <span class="built_in">enable</span> ntpd.service \</span><br><span class="line">&amp;&amp; systemctl start ntpd.service</span><br></pre></td></tr></table></figure></li><li><p>安装 <strong>iscsi-initiator-utils</strong> 工具</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install iscsi-initiator-utils -y</span><br></pre></td></tr></table></figure></li><li><p>配置客户端 <strong>iqn</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/iscsi/initiatorname.iscsi</span><br><span class="line"><span class="comment"># 修改为之前 ACL 中创建的客户端 iqn</span></span><br><span class="line">InitiatorName=iqn.2019-05.com.example:agent1</span><br></pre></td></tr></table></figure></li><li><p>配置客户端认证参数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编辑配置文件</span></span><br><span class="line">vi /etc/iscsi/iscsid.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># *************</span></span><br><span class="line"><span class="comment"># CHAP Settings</span></span><br><span class="line"><span class="comment"># *************</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启登录认证方法</span></span><br><span class="line">node.session.auth.authmethod = CHAP</span><br><span class="line"><span class="comment"># 登录认证</span></span><br><span class="line"><span class="comment">## 对应服务端的传入账号，客户端用此账号校验服务器</span></span><br><span class="line">node.session.auth.username = agent_login_user1</span><br><span class="line">node.session.auth.password = agent_login_passwd1</span><br><span class="line"><span class="comment"># 双向登录认证</span></span><br><span class="line"><span class="comment">## 对应服务端的传出账号，服务器用此账号校验客户端</span></span><br><span class="line">node.session.auth.username_in = server_check_user1</span><br><span class="line">node.session.auth.password_in = server_check_passwd1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启发现认证方法</span></span><br><span class="line">discovery.sendtargets.auth.authmethod = CHAP</span><br><span class="line"><span class="comment"># 单向发现认证</span></span><br><span class="line"><span class="comment"># 对应服务端的传入账号，客户端用此账号发现服务器</span></span><br><span class="line">discovery.sendtargets.auth.username = agent_discovery_user2</span><br><span class="line">discovery.sendtargets.auth.password = agent_discovery_passwd2</span><br><span class="line"><span class="comment"># 双向发现认证</span></span><br><span class="line"><span class="comment"># 对应服务端的传出账号，服务器用此账号校验客户端</span></span><br><span class="line">discovery.sendtargets.auth.username_in = server_check_user2</span><br><span class="line">discovery.sendtargets.auth.password_in = server_check_passwd2</span><br><span class="line"></span><br><span class="line"><span class="comment"># ********</span></span><br><span class="line"><span class="comment"># Timeouts</span></span><br><span class="line"><span class="comment"># ********</span></span><br></pre></td></tr></table></figure></li><li><p>发现测试</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">iscsiadm -m discovery -t sendtargets -p 1.1.1.47</span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502121812570.png" class="" title="image-20190502121812570"></li><li><p>登录测试</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">iscsiadm -m node -T iqn.2019-05.com.example:servers1 -p 1.1.1.47 -l</span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502123011786.png" class="" title="image-20190502123011786"></li></ol><h2 id="配置-K8S-持久卷-PV"><a href="#配置-K8S-持久卷-PV" class="headerlink" title="配置 K8S 持久卷(PV)"></a>配置 K8S 持久卷(PV)</h2><h3 id="创建密文"><a href="#创建密文" class="headerlink" title="创建密文"></a>创建密文</h3><ol><li><p>K8S 中需要以为密文方式传入客户端的认证账号，并且需要 base64 加密</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">&quot;kubernetes.io/iscsi-chap&quot;</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">discovery.sendtargets.auth.password:</span> <span class="string">YWdlbnRfZGlzY292ZXJ5X3Bhc3N3ZDIK</span></span><br><span class="line">  <span class="attr">discovery.sendtargets.auth.username_in:</span> <span class="string">c2VydmVyX2NoZWNrX3VzZXIyCg==</span></span><br><span class="line">  <span class="attr">discovery.sendtargets.auth.password_in:</span> <span class="string">c2VydmVyX2NoZWNrX3Bhc3N3ZDIK</span></span><br><span class="line">  <span class="attr">node.session.auth.password:</span> <span class="string">YWdlbnRfbG9naW5fcGFzc3dkMQo=</span></span><br><span class="line">  <span class="attr">node.session.auth.username_in:</span> <span class="string">c2VydmVyX2NoZWNrX3VzZXIxCg==</span></span><br><span class="line">  <span class="attr">node.session.auth.password_in:</span> <span class="string">c2VydmVyX2NoZWNrX3Bhc3N3ZDEK</span></span><br></pre></td></tr></table></figure><blockquote><p>参考地址：<a href="https://github.com/kubernetes/examples/tree/master/volumes/iscsi">https://github.com/kubernetes/examples/tree/master/volumes/iscsi</a></p></blockquote></li><li><p>登录到 rancher ui，切换到任意项目(这里以 default 命名空间为例)\工作负载页面下，点击右上角的<code>导入 YAML</code> 按钮。</p><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502130448080.png" class="" title="image-20190502130448080"></li><li><p>复制粘贴 yaml 内容，其他参数默认，最后点击 <code>导入</code>。</p><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502130555666.png" class="" title="image-20190502130555666"></li></ol><h3 id="创建-PV"><a href="#创建-PV" class="headerlink" title="创建 PV"></a>创建 PV</h3><ol><li>登录 rancher ui，切换到<code>指定集群\存储\持久卷</code>，点击右上角的添加卷(pv);</li></ol><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502125012918.png" class="" title="image-20190502125012918"><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502130753062.png" class="" title="image-20190502130753062"><h3 id="创建测试应用"><a href="#创建测试应用" class="headerlink" title="创建测试应用"></a>创建测试应用</h3><ol><li><p>切换到项目视图下，点击创建工作负载；</p></li><li><p>配置应用的基本参数，</p><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502131153699.png" class="" title="image-20190502131153699"></li><li><p>配置容器卷，选择添加新的持久卷</p><ul><li><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502131243542.png" class="" title="image-20190502131243542"></li><li><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502125241993.png" class="" title="image-20190502125241993"></li><li><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502131341819.png" class="" title="image-20190502131341819"></li></ul></li><li><p>配置完成后点击启动。</p></li><li><p>应用运行正常后，通过 web shell 查看磁盘挂载情况</p><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502133334778.png" class="" title="image-20190502133334778"></li><li><p>检查测试</p><ul><li>查看 target 生成的镜像文件大小</li></ul><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502132810453.png" class="" title="image-20190502132810453"><ul><li>进入容器&#x2F;demo 路径下，通过 <code>dd</code> 命令写入一个文件</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=/demo/demo count=2 bs=1024M</span><br></pre></td></tr></table></figure><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502132955690.png" class="" title="image-20190502132955690"><img src="/rancher/rancher-k8s-use-iscsi-storage/image-20190502133046703.png" class="" title="image-20190502133046703"></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> storage </tag>
            
            <tag> rancher </tag>
            
            <tag> iscsi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于 eBPF</title>
      <link href="/linux/about-ebpf/"/>
      <url>/linux/about-ebpf/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文：<a href="https://docs.projectcalico.org/about/about-ebpf">https://docs.projectcalico.org/about/about-ebpf</a></p></blockquote><p>eBPF 是一种 Linux 内核功能，允许将快速而安全的微型程序加载到内核中以自定义其操作。</p><p>通过本文档，您将了解：</p><ul><li>eBPF 的一般背景。</li><li>eBPF 的各种用途。</li><li>Calico 如何在 eBPF 数据平面中使用 eBPF。</li></ul><h3 id="什么是-eBPF？"><a href="#什么是-eBPF？" class="headerlink" title="什么是 eBPF？"></a>什么是 eBPF？</h3><p>eBPF 是嵌入在 Linux 内核中的 “虚拟机”。它允许将微型程序加载到内核中，并附加到钩子上，而钩子会在发生某些事件时触发。这样可以自定义内核的行为（有时是很严格的）。尽管每种钩子的 eBPF 虚拟机都相同，但是钩子的功能有很大差异。虽然将程序加载到内核中可能很危险，但内核通过非常严格的静态验证器运行所有程序。验证程序会将程序沙盒化，以确保它只能访问允许的内存部分，并确保它必须快速终止。</p><h3 id="为什么称为-eBPF？"><a href="#为什么称为-eBPF？" class="headerlink" title="为什么称为 eBPF？"></a>为什么称为 eBPF？</h3><p>eBPF 是 “扩展 Berkeley 包过滤器” 的缩写。Berkeley 包过滤器是一种更早，更专业的虚拟机，专门用于过滤数据包。诸如<strong>tcpdump</strong>此类使用此“经典” BPF VM 来选择应发送到用户空间进行分析的数据包的工具。eBPF 是 BPF 的一个扩展版本，适用于内核内部的通用用途。尽管名称不为人知，但 eBPF 的用途不仅仅限于数据包过滤。</p><h3 id="eBPF-可以做什么？"><a href="#eBPF-可以做什么？" class="headerlink" title="eBPF 可以做什么？"></a>eBPF 可以做什么？</h3><h4 id="eBPF-类型"><a href="#eBPF-类型" class="headerlink" title="eBPF 类型"></a>eBPF 类型</h4><p>内核中可以将 eBPF 程序附加到几类钩子。eBPF 程序的功能在很大程度上取决于它所连接的钩子：</p><ul><li><strong>Tracing</strong> 程序可以附加到内核中很大一部分功能上。跟踪程序对于收集统计信息和内核的深入调试很有用。 <em>大多数</em>跟踪钩子仅允许对该函数正在处理的数据进行只读访问，但是有些跟踪钩子允许修改数据。Calico 团队使用跟踪程序来帮助在开发过程中调试 Calico。例如，找出内核为何意外丢弃数据包的原因。</li><li><strong>Traffic Control</strong> (<code>tc</code>) 程序可以在入口和出口附加到给定的网络设备。内核为每个数据包执行一次程序。由于钩子是用于数据包处理的，因此内核允许程序修改或扩展数据包、删除数据包，将其标记为排队或将数据包重定向到另一个接口。Calico 的 eBPF 数据平面基于这种钩子。我们使用 tc 程序对 Kubernetes 服务进行负载平衡，实施网络策略，并为已建立连接的流量创建快速路径。</li><li><strong>XDP</strong>或“ eXpress 数据路径”实际上是 eBPF 钩子的名称。每个网络设备都有一个 XDP 入口钩子，在内核为数据包分配套接字缓冲区之前，它会为每个传入数据包触发一次。XDP 可以为诸如 DoS 保护（在 Calico 的标准 Linux 数据平面中支持）和入口负载平衡（在 facebook 的 Katran 中使用）之类的用例提供出色的性能。XDP 的缺点是，它需要网络设备驱动程序支持才能获得良好的性能，并且与 Pod 网络的互操作性不是很好。</li><li>几种类型的<strong>套接字</strong>程序可以在套接字上进行各种操作。例如，允许 eBPF 程序更改新创建的套接字的目标 IP，或强制套接字绑定到“正确的”源 IP 地址。Calico 使用此类程序进行 Kubernetes Services 的连接时负载平衡。这减少了开销，因为在数据包处理路径上没有<a href="https://docs.projectcalico.org/about/about-networking#NAT">DNAT</a>。</li><li>有各种与安全性相关的钩子，允许以各种方式管理程序行为。例如，<strong>seccomp</strong>钩子允许以细粒度的方式管理系统调用。</li><li>而且……当您准备好此功能时，可能还会有更多的问题。eBPF 正在内核中进行大量开发。</li></ul><p>内核通过“辅助功能”公开每个钩子的功能。例如，该 <code>tc</code> 钩子具有帮助程序功能以调整数据包的大小，但是该助手在跟踪钩子中不可用。使用 eBPF 的挑战之一是不同的内核版本支持不同的帮助程序，而缺少帮助程序可能导致无法实现特定功能。</p><h4 id="BPF-映射"><a href="#BPF-映射" class="headerlink" title="BPF 映射"></a>BPF 映射</h4><p>附加到 eBPF 钩子的程序可以访问 BPF maps。BPF 映射有两个主要用途：</p><ul><li>它们允许 BPF 程序存储和检索长期存在的数据。</li><li>它们允许 BPF 程序和用户空间程序之间的通信。BPF 程序可以读取由用户空间写入的数据，反之亦然。</li></ul><p>BPF 映射有很多类型，包括一些允许在程序之间跳转的特殊类型，还有一些充当队列和堆栈而不是严格用作<strong>键&#x2F;值</strong>映射。Calico 使用映射来跟踪活动连接，并使用策略和服务 NAT 信息配置 BPF 程序。由于映射访问可能相对昂贵，因此 Calico 的目标是仅对已建立的流中的每个数据包执行单个映射查找。</p><p>可以使用 <code>bpftool</code> 内核随附的命令行工具检查 bpf 映射的内容。</p><h3 id="Calico-的-eBPF-数据平面"><a href="#Calico-的-eBPF-数据平面" class="headerlink" title="Calico 的 eBPF 数据平面"></a>Calico 的 eBPF 数据平面</h3><p>Calico 的 eBPF 数据平面是标准 Linux 数据平面（基于 iptables）的替代方案。虽然标准数据平面通过与 kube-proxy 以及您自己的 iptables 规则互通来关注兼容性，但是 eBPF 数据平面着重于性能，延迟和改善用户体验，而这些功能是标准数据平面所无法实现的。作为其一部分，eBPF 数据平面用 eBPF 实现代替了 kube-proxy。主要的“用户体验”功能是在流量到达 NodePort 时保留来自群集外部流量的源 IP。这使您的服务端日志和网络策略在该路径上更加有用。</p><h4 id="功能比较"><a href="#功能比较" class="headerlink" title="功能比较"></a>功能比较</h4><p>尽管 eBPF 数据平面具有一些标准 Linux 数据平面所缺少的功能，但反之亦然：</p><table><thead><tr><th align="left">功能</th><th align="left">标准 Linux 数据平面</th><th align="left">eBPF 数据平面</th></tr></thead><tbody><tr><td align="left">吞吐量</td><td align="left">专为 10GBit +设计</td><td align="left">专为 40GBit +设计</td></tr><tr><td align="left">第一个封包延迟</td><td align="left">低（kube-proxy 服务延迟是主要的因素）</td><td align="left">更低</td></tr><tr><td align="left">后续数据包延迟</td><td align="left">低</td><td align="left">更低</td></tr><tr><td align="left">保留群集中的源 IP</td><td align="left">是</td><td align="left">是</td></tr><tr><td align="left">保留外部源 IP</td><td align="left">只有 <code>externalTrafficPolicy: Local</code></td><td align="left">是</td></tr><tr><td align="left">Direct Server Return</td><td align="left">不支持</td><td align="left">支持（需要兼容的基础网络）</td></tr><tr><td align="left">连接跟踪</td><td align="left">Linux 内核的 conntrack 表（大小可以调整）</td><td align="left">BPF 地图（固定大小）</td></tr><tr><td align="left">策略规则</td><td align="left">映射到 iptables 规则</td><td align="left">映射到 BPF 指令</td></tr><tr><td align="left">策略选择器</td><td align="left">映射到 IP 集</td><td align="left">映射到 BPF 映射</td></tr><tr><td align="left">Kubernetes 服务</td><td align="left">kube-proxy iptables 或 IPVS 模式</td><td align="left">BPF program and maps</td></tr><tr><td align="left">IPIP</td><td align="left">支持</td><td align="left">支持（由于内核限制，没有性能优势）</td></tr><tr><td align="left">VXLAN</td><td align="left">支持</td><td align="left">支持</td></tr><tr><td align="left">Wireguard</td><td align="left">支持</td><td align="left">支持</td></tr><tr><td align="left">Other routing</td><td align="left">支持</td><td align="left">支持</td></tr><tr><td align="left">支持第三方 CNI 插件</td><td align="left">是（仅兼容插件）</td><td align="left">是（仅兼容插件）</td></tr><tr><td align="left">与其他 iptables 规则兼容</td><td align="left">是（可以在其他规则之上或之下编写规则）</td><td align="left">部分 iptables 绕过工作负载流量</td></tr><tr><td align="left">XDP DoS 保护</td><td align="left">支持</td><td align="left">不支持（尚未）</td></tr><tr><td align="left">IPv6</td><td align="left">支持</td><td align="left">不支持（尚未）</td></tr><tr><td align="left">主机端点策略</td><td align="left">支持</td><td align="left">不支持（尚未）</td></tr><tr><td align="left">企业版</td><td align="left">可用</td><td align="left">不支持（尚未）</td></tr></tbody></table><h4 id="架构概述"><a href="#架构概述" class="headerlink" title="架构概述"></a>架构概述</h4><p>Calico 的 eBPF 数据面附加 eBPF 程序到 tc 钩子上的每个 Calico 接口以及您的数据和隧道接口。这允许 Calico 尽早发现工作负载包，并通过绕过 iptables 和内核通常会进行的其他包处理的快速路径来处理它们。</p><img src="/linux/about-ebpf/bpf-pod-to-pod.svg" class="" title="该图显示了 Pod 到 Pod 网络的数据包路径； BPF 程序已附加到客户端窗格的 veth 界面； 它在 BPF 映射中执行 conntrack 查找，并绕过 iptables 将数据包直接转发到第二个 pod"><p>The logic to implement load balancing and packet parsing is pre-compiled ahead of time and relies on a set of BPF maps to store the NAT frontend and backend information. One map stores the metadata of the service, allowing for <code>externalTrafficPolicy</code> and “sticky” services to be honoured. A second map stores the IPs of the backing pods.</p><p>在 eBPF 模式下，Calico 使用 BPF MAP 存储策略选择器匹配的 IP 集，从而将您的策略转换为优化的 eBPF 字节码。</p><img src="/linux/about-ebpf/bpf-policy.svg" class="" title="BPF 程序的详细信息，其中显示了将数据包发送到单独的（生成的）策略程序"><p>为了提高服务的性能，Calico 还通过挂接到 BPF 套接字钩子来实现连接到负载平衡。当程序试图连接到 Kubernetes 服务时，Calico 会尝试拦截连接，并将套接字配置为直接连接后端 pod 的 IP。这从服务连接中删除了所有的 NAT 开销。</p><img src="/linux/about-ebpf/bpf-connect-time.svg" class="" title="该图显示了连接到套接字连接调用的 BPF 程序； 它在连接时进行 NAT">]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> eBPF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gogs 安装，升级</title>
      <link href="/git/gogs-install-update/"/>
      <url>/git/gogs-install-update/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/git/gogs-install-update/" target="_blank" title="https://www.xtplayer.cn/git/gogs-install-update/">https://www.xtplayer.cn/git/gogs-install-update/</a></p><p>Gogs 是一款极易搭建的轻量级自助 Git 服务。</p><p>Gogs 的目标是打造一个最简单、最快速和最轻松的方式搭建自助 Git 服务。使用 Go 语言开发使得 Gogs 能够通过独立的二进制分发，并且支持 Go 语言支持的 所有平台，包括 Linux、Mac OS X、Windows 以及 ARM 平台。</p><h2 id="功能特性"><a href="#功能特性" class="headerlink" title="功能特性"></a>功能特性</h2><ul><li>支持活动时间线</li><li>支持 SSH 以及 HTTP&#x2F;HTTPS 协议</li><li>支持 SMTP、LDAP 和反向代理的用户认证</li><li>支持反向代理子路径</li><li>支持用户、组织和仓库管理系统</li><li>支持添加和删除仓库协作者</li><li>支持仓库和组织级别 Web 钩子（包括 Slack 集成）</li><li>支持仓库 Git 钩子和部署密钥</li><li>支持仓库工单（Issue）、合并请求（Pull Request）、Wiki 以及保护分支</li><li>支持迁移和镜像仓库以及它的 Wiki</li><li>支持在线编辑仓库文件和 Wiki</li><li>支持自定义源的 Gravatar 和 Federated Avatar</li><li>支持 Jupyter Notebook</li><li>支持邮件服务</li><li>支持后台管理面板</li><li>支持 MySQL、PostgreSQL、SQLite3、MSSQL 和 <a href="https://github.com/pingcap/tidb">TiDB</a>（实验性支持） 数据库</li><li>支持多语言本地化（22 种语言）</li></ul><h2 id="系统要求"><a href="#系统要求" class="headerlink" title="系统要求"></a>系统要求</h2><ul><li>最低的系统硬件要求为一个廉价的树莓派</li><li>如果用于团队项目，建议使用 2 核 CPU 及 1GB 内存</li></ul><h2 id="浏览器支持"><a href="#浏览器支持" class="headerlink" title="浏览器支持"></a>浏览器支持</h2><ul><li>请根据 <a href="https://github.com/Semantic-Org/Semantic-UI#browser-support">Semantic UI</a> 查看具体支持的浏览器版本。</li><li>官方支持的最小 UI 尺寸为 <strong>1024*768</strong>，UI 不一定会在更小尺寸的设备上被破坏，但我们无法保证且不会修复。</li></ul><h2 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h2><p>这里我们以 docker 容器方式单节点安装，只需要一条命令即可。在 <a href="https://github.com/gogs/gogs/tree/main/docker">https://github.com/gogs/gogs/tree/main/docker</a> 此处可以查询镜像构建文件以及数据存储路径等参数。默认数据保存在 <code>/data</code> 目录下，所以在只需要在启动 docker 容器的时候通过卷映射的方式把数据映射到主机上即可。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --restart always --name=gogs -tid -p 10022:22 -p 10080:3000 -v /var/gogs:/data gogs/gogs:0.12.3</span><br></pre></td></tr></table></figure><blockquote><p>git clone 代码的时候，可以通过 ssh 连接或者通过 http 连接。一般常用的是 http 连接方式，如果需要通过 ssh 连接，则这里需要把 22 端口映射出去。</p></blockquote><p>接着访问 <code>node ip + 端口</code> 进入配置界面。</p><h3 id="首次运行安装"><a href="#首次运行安装" class="headerlink" title="首次运行安装"></a>首次运行安装</h3><p>Gogs 支持多种数据库，一般单机模式选择轻量级的 SQLite3 数据库，数据库文件路径建议保持默认。</p><img src="/git/gogs-install-update/image-20201224214202330.png" class="" title="image-20201224214202330"><h3 id="应用基本设置"><a href="#应用基本设置" class="headerlink" title="应用基本设置"></a>应用基本设置</h3><ul><li><p><strong>应用名称</strong>: 可以随意设置</p></li><li><p><strong>仓库根目录</strong>：保持默认</p></li><li><p><strong>运行系统用户</strong>：保持默认</p></li><li><p><strong>域名</strong>：</p><p>Gogs 运行节点 IP 或者 域名或者 VIP</p></li><li><p><strong>SSH 端口号</strong></p><p>这个端口需要设置为 <code>docker -p</code> 映射 22 到主机上的端口</p></li><li><p><strong>HTTP 端口号</strong></p><p>这个端口需要设置为 <code>docker -p</code> 映射 3000 到主机上的端口</p></li><li><p><strong>应用 URL</strong></p><p><strong>域名</strong> + <strong>HTTP 端口号</strong></p></li><li><p><strong>日志路径</strong>：保持默认</p></li></ul><img src="/git/gogs-install-update/image-20201224220724547.png" class="" title="image-20201224220724547"><h3 id="可选设置"><a href="#可选设置" class="headerlink" title="可选设置"></a>可选设置</h3><ul><li><p>邮件服务设置</p><p>如果 Gogs 节点可以访问互联网，或者有内部的邮件服务器，那么可以配置邮件服务，可以用来忘记密码的时候恢复密码。</p></li><li><p>服务器和其它服务设置</p><ul><li>如果是离线环境，建议启用离线模式</li><li>Gravatar 头像国内拉取非常缓慢，建议禁用 Gravatar 服务</li></ul></li><li><p>管理员帐号设置</p><p>可以在安装的时候创建一个管理员帐号，账号名称不能为 <code>admin</code>。或者在安装好之后手动注册，注册的第一个用户将自动获得管理员权限。</p></li></ul><img src="/git/gogs-install-update/image-20201224215551279.png" class="" title="image-20201224215551279"><p>最后点击立即安装。</p><h2 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h2><p>升级方法很简单，只需要 <code>docker pull</code> 新的镜像，然后用相同的命令重新运行容器</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull gogs/gogs:0.12.x</span><br><span class="line">docker stop gogs</span><br><span class="line">docker rename gogs gogs-old</span><br><span class="line">docker run --restart always --name=gogs -tid -p 10022:22 -p 10080:3000 -v /data/gogs:/data gogs/gogs:0.12.x</span><br></pre></td></tr></table></figure><p>在升级完成后，确认没有问题后执行以下命令删除旧的容器：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">rm</span> -f gogs-old</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Api Server 和 ETCD 健康状态检查</title>
      <link href="/kubernetes/api-server-and-etcd-health-state-check/"/>
      <url>/kubernetes/api-server-and-etcd-health-state-check/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/api-server-and-etcd-health-state-check/" target="_blank" title="https://www.xtplayer.cn/kubernetes/api-server-and-etcd-health-state-check/">https://www.xtplayer.cn/kubernetes/api-server-and-etcd-health-state-check/</a></p><h2 id="kube-apiserver-健康状态检查"><a href="#kube-apiserver-健康状态检查" class="headerlink" title="kube-apiserver 健康状态检查"></a>kube-apiserver 健康状态检查</h2><p>rancher 自定义集群或者 rke 集群，kube-scheduler 和 kube-controller-manager 以及 kube-apiserver 同时运行在一个主机上，并且都是以 host 网络默认运行。 当 kube-scheduler 和 kube-controller-manager 去连接 127.0.0.1:6443 时，其实就是在连接 kube-apiserver。有时如果提示 kube-scheduler 和 kube-controller-manager 连接 127.0.0.1:6443 失败，那么需要检查一下看是否是 kube-apiserver 本身运行异常，或者因为网络问题连接不上 kube-apiserver。</p><p>因为 rancher 自定义集群或者 rke 集群中，所有组件都开启了 ssl 认证，并且 kube-apiserver 和 etcd 均开启了双向认证。所以如果要去直接访问，则需要传递对应的客户端证书。通过在 master 主机上执行以下命令，正常状态下将返回 <code>OK</code> 两个字母。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl --cacert /etc/kubernetes/ssl/kube-ca.pem --cert /etc/kubernetes/ssl/kube-node.pem --key /etc/kubernetes/ssl/kube-node-key.pem https://127.0.0.1:6443/healthz</span><br></pre></td></tr></table></figure><h2 id="ETCD-健康状态检查"><a href="#ETCD-健康状态检查" class="headerlink" title="ETCD 健康状态检查"></a>ETCD 健康状态检查</h2><p>登录一台 ETCD 节点，然后通过 NODE_IP 指定 ETCD 节点 IP，再运行以下命令去检查 ETCD 服务状态。可以更换 IP 去检查多个服务状态。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NODE_IP=192.168.1.x</span><br><span class="line">curl --cacert /etc/kubernetes/ssl/kube-ca.pem --cert /etc/kubernetes/ssl/kube-node.pem --key /etc/kubernetes/ssl/kube-node-key.pem https://<span class="variable">$&#123;NODE_IP&#125;</span>:2379/health</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k3s containerd 配置 mirror 和 insecure</title>
      <link href="/containerd/containerd-mirror-config/"/>
      <url>/containerd/containerd-mirror-config/</url>
      
        <content type="html"><![CDATA[<p>containerd 使用了类似 k8s 中 <strong>svc</strong> 与 <strong>endpoint</strong> 的概念。svc 可以理解为访问 url，这个 url 会解析到对应的 endpoint 上。也可以理解 <strong>mirror</strong> 配置就是一个反向代理，它把客户端的请求代理到 endpoint 配置的后端镜像仓库。mirror 名称可以随意填写，但是必须符合 <code>IP 或域名</code> 的定义规则。并且可以配置多个 endpoint，默认解析到第一个 endpoint，如果第一个 endpoint 没有返回数据，则自动切换到第二个 endpoint，以此类推。</p><p>比如以下配置示例：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mirrors:</span><br><span class="line">  <span class="string">&quot;*&quot;</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">&quot;http://192.168.50.119&quot;</span></span><br><span class="line">  <span class="string">&quot;192.168.50.119&quot;</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">&quot;http://192.168.50.119&quot;</span></span><br><span class="line">  <span class="string">&quot;reg.test.com&quot;</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">&quot;http://192.168.50.119&quot;</span></span><br><span class="line">  <span class="string">&quot;docker.io&quot;</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">&quot;https://7bezldxe.mirror.aliyuncs.com&quot;</span></span><br><span class="line">      - <span class="string">&quot;https://registry-1.docker.io&quot;</span></span><br></pre></td></tr></table></figure><p>可以通过 <code>crictl pull 192.168.50.119/library/alpine</code> 和 <code>crictl pull reg.test.com/library/alpine</code> 获取到镜像，但镜像都是从同一个后端仓库获取。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment"># systemctl restart k3s.service</span></span><br><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment"># crictl pull 192.168.50.119/library/alpine</span></span><br><span class="line">Image is up to <span class="built_in">date</span> <span class="keyword">for</span> sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e</span><br><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment"># crictl pull reg.test.com/library/alpine</span></span><br><span class="line">Image is up to <span class="built_in">date</span> <span class="keyword">for</span> sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e</span><br><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment">#</span></span><br></pre></td></tr></table></figure><blockquote><p>注意: <strong>mirror</strong> 名称也可以设置为 <code>*</code> , 表示适配任意的仓库名称来获取到镜像，比如：</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment"># crictl pull 1.1.1.2/library/alpine</span></span><br><span class="line">Image is up to <span class="built_in">date</span> <span class="keyword">for</span> sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e</span><br><span class="line"></span><br><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment"># crictl pull 1.1.1.3/library/alpine</span></span><br><span class="line">Image is up to <span class="built_in">date</span> <span class="keyword">for</span> sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e</span><br><span class="line"></span><br><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment"># crictl pull x.x.x.x/library/alpine</span></span><br><span class="line">Image is up to <span class="built_in">date</span> <span class="keyword">for</span> sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e</span><br><span class="line"></span><br><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment"># crictl pull x.x.x.3x/library/alpine</span></span><br><span class="line">Image is up to <span class="built_in">date</span> <span class="keyword">for</span> sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e</span><br><span class="line">root@rancher-server:/etc/rancher/k3s<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>这样配置会出现很多镜像名称，不方便管理，不建议这样配置。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">test.1/tttttt/alpine                                lates               a24bb4013296f       2.8MB</span><br><span class="line">1.1.1.1/tttttt/alpine                               lates               a24bb4013296f       2.8MB</span><br><span class="line">1.1.1.1/library/alpine                              latest              a24bb4013296f       2.8MB</span><br><span class="line">1.1.1.3/library/alpine                              latest              a24bb4013296f       2.8MB</span><br><span class="line">192.168.50.1/tttttt/alpine                          lates               a24bb4013296f       2.8MB</span><br><span class="line">192.168.50.119/library/alpine                       latest              a24bb4013296f       2.8MB</span><br><span class="line">test.com/tttttt/alpine                              lates               a24bb4013296f       2.8MB</span><br><span class="line">x.x.x.x/library/alpine                              latest              a24bb4013296f       2.8MB</span><br><span class="line">docker.io/tttttt/alpine                             lates               a24bb4013296f       2.8MB</span><br><span class="line">192.168.50.110/tttttt/alpine                        lates               a24bb4013296f       2.8MB</span><br><span class="line">192.168.50.119/tttttt/alpine                        lates               a24bb4013296f       2.8MB</span><br><span class="line">x.x.x.3x/library/alpine                             latest              a24bb4013296f       2.8MB</span><br><span class="line">reg.test.com/library/alpine                         latest              a24bb4013296f       2.8MB</span><br><span class="line">1.1.1.2/library/alpine                              latest              a24bb4013296f       2.8MB</span><br></pre></td></tr></table></figure><h2 id="非安全（http）私有仓库配置"><a href="#非安全（http）私有仓库配置" class="headerlink" title="非安全（http）私有仓库配置"></a>非安全（http）私有仓库配置</h2><p>配置非安全（http）私有仓库，只需要在 endpoint 中指定 http 地址的后端仓库即可。</p><ul><li>以 <code>http://192.168.50.119</code> 仓库为例</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">mirrors:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    endpoint:</span></span><br><span class="line"><span class="string">      - &quot;http://192.168.50.119&quot;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl restart k3s</span><br></pre></td></tr></table></figure><h2 id="安全（https）私有仓库配置"><a href="#安全（https）私有仓库配置" class="headerlink" title="安全（https）私有仓库配置"></a>安全（https）私有仓库配置</h2><ul><li>使用授信 ssl 证书</li></ul><p>与非安全（http）私有仓库配置类似，只需要配置 endpoint 中指定 https 地址的后端仓库即可。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">mirrors:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    endpoint:</span></span><br><span class="line"><span class="string">      - &quot;https://192.168.50.119&quot;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl restart k3s</span><br></pre></td></tr></table></figure><ul><li>使用自签 ssl 证书</li></ul><p>如果后端仓库使用的是自签名的 ssl 证书，那么需要配置 CA 证书 用于 ssl 证书的校验。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">mirrors:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    endpoint:</span></span><br><span class="line"><span class="string">      - &quot;https://192.168.50.119&quot;</span></span><br><span class="line"><span class="string">configs:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    tls:</span></span><br><span class="line"><span class="string">      ca_file:   # path to the ca file used in the registry</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl restart k3s</span><br></pre></td></tr></table></figure><ul><li>ssl 双向认证</li></ul><p>如果镜像仓库配置了双向认证，这个时候 containerd 作为客户端，那么需要为 containerd 配置 ssl 证书用于镜像仓库对 containerd 做认证。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">mirrors:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    endpoint:</span></span><br><span class="line"><span class="string">      - &quot;https://192.168.50.119&quot;</span></span><br><span class="line"><span class="string">configs:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    tls:</span></span><br><span class="line"><span class="string">      cert_file: # path to the cert file used in the registry</span></span><br><span class="line"><span class="string">      key_file:  # path to the key file used in the registry</span></span><br><span class="line"><span class="string">      ca_file:   # path to the ca file used in the registry</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl restart k3s</span><br></pre></td></tr></table></figure><table><thead><tr><th align="left">DIRECTIVE</th><th align="left">DESCRIPTION</th></tr></thead><tbody><tr><td align="left"><code>cert_file</code></td><td align="left">The client certificate path that will be used to authenticate with the registry</td></tr><tr><td align="left"><code>key_file</code></td><td align="left">The client key path that will be used to authenticate with the registry</td></tr><tr><td align="left"><code>ca_file</code></td><td align="left">Defines the CA certificate path to be used to verify the registry’s server cert file</td></tr></tbody></table><h2 id="仓库授权认证"><a href="#仓库授权认证" class="headerlink" title="仓库授权认证"></a>仓库授权认证</h2><p>对于仓库中的私有项目，需要用户名和密码认证授权才能获取镜像，可以通过添加 configs 来配置用户名和密码。配置仓库认证时，<code>mirror</code> 需要与 <code>configs</code> 匹配。比如，如果配置了一个 <code>mirrors</code> 为 <code>192.168.50.119</code>，那么在 <code>configs</code> 中也需要配置一个 <code>192.168.50.119</code>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">mirrors:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    endpoint:</span></span><br><span class="line"><span class="string">      - &quot;https://192.168.50.119&quot;</span></span><br><span class="line"><span class="string">configs:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    auth:</span></span><br><span class="line"><span class="string">      username: xxxxxx # this is the registry username</span></span><br><span class="line"><span class="string">      password: xxxxxx # this is the registry password</span></span><br><span class="line"><span class="string">    tls:</span></span><br><span class="line"><span class="string">      cert_file: # path to the cert file used in the registry</span></span><br><span class="line"><span class="string">      key_file:  # path to the key file used in the registry</span></span><br><span class="line"><span class="string">      ca_file:   # path to the ca file used in the registry</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl restart k3s</span><br></pre></td></tr></table></figure><h2 id="加速器配置"><a href="#加速器配置" class="headerlink" title="加速器配置"></a>加速器配置</h2><p>containerd 与 docker 都有默认仓库，并且都为 <code>docker.io</code>。如果配置中未指定 mirror 为 <code>docker.io</code>，重启 containerd 后会自动加载 <code>docker.io</code> 配置。与 docker 不同的是，containerd 可以修改 <code>docker.io</code> 对应的 endpoint（ 默认为 <a href="https://registry-1.docker.io/">https://registry-1.docker.io</a> ），而 docker 无法修改。</p><p>docker 中可以通过 <code>registry-mirrors</code> 设置镜像加速地址。如果 pull 的镜像不带仓库地址（<code>项目名+镜像名:tag</code>），则会从默认镜像仓库去拉取镜像。如果配置了镜像加速地址，会先访问镜像加速仓库，如果没有返回数据，再访问默认吧镜像仓库。</p><p>containerd 目前没有直接配置镜像加速的功能，因为 containerd 中可以修改 <code>docker.io</code> 对应的 endpoint，所以可以通过修改 endpoint 来实现镜像加速下载。因为 endpoint 是轮训访问，所以可以给 <code>docker.io</code>  配置多个仓库地址来实现 <code>加速地址+默认仓库地址</code>。如下配置示例：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">mirrors:</span></span><br><span class="line"><span class="string">  &quot;192.168.50.119&quot;:</span></span><br><span class="line"><span class="string">    endpoint:</span></span><br><span class="line"><span class="string">      - &quot;http://192.168.50.119&quot;</span></span><br><span class="line"><span class="string">  &quot;docker.io&quot;:</span></span><br><span class="line"><span class="string">    endpoint:</span></span><br><span class="line"><span class="string">      - &quot;https://7bezldxe.mirror.aliyuncs.com&quot;</span></span><br><span class="line"><span class="string">      - &quot;https://registry-1.docker.io&quot;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl restart k3s</span><br></pre></td></tr></table></figure><h2 id="完整配置示例"><a href="#完整配置示例" class="headerlink" title="完整配置示例"></a>完整配置示例</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mirrors:</span><br><span class="line">  <span class="string">&quot;192.168.50.119&quot;</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">&quot;http://192.168.50.119&quot;</span></span><br><span class="line">  <span class="string">&quot;docker.io&quot;</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">&quot;https://7bezldxe.mirror.aliyuncs.com&quot;</span></span><br><span class="line">      - <span class="string">&quot;https://registry-1.docker.io&quot;</span></span><br><span class="line">configs:</span><br><span class="line">  <span class="string">&quot;192.168.50.119&quot;</span>:</span><br><span class="line">    auth:</span><br><span class="line">      username: <span class="string">&#x27;&#x27;</span> <span class="comment"># this is the registry username</span></span><br><span class="line">      password: <span class="string">&#x27;&#x27;</span> <span class="comment"># this is the registry password</span></span><br><span class="line">    tls:</span><br><span class="line">      cert_file: <span class="string">&#x27;&#x27;</span> <span class="comment"># path to the cert file used in the registry</span></span><br><span class="line">      key_file: <span class="string">&#x27;&#x27;</span> <span class="comment"># path to the key file used in the registry</span></span><br><span class="line">      ca_file: <span class="string">&#x27;&#x27;</span> <span class="comment"># path to the ca file used in the registry</span></span><br><span class="line">  <span class="string">&quot;docker.io&quot;</span>:</span><br><span class="line">    auth:</span><br><span class="line">      username: <span class="string">&#x27;&#x27;</span> <span class="comment"># this is the registry username</span></span><br><span class="line">      password: <span class="string">&#x27;&#x27;</span> <span class="comment"># this is the registry password</span></span><br><span class="line">    tls:</span><br><span class="line">      cert_file: <span class="string">&#x27;&#x27;</span> <span class="comment"># path to the cert file used in the registry</span></span><br><span class="line">      key_file: <span class="string">&#x27;&#x27;</span> <span class="comment"># path to the key file used in the registry</span></span><br><span class="line">      ca_file: <span class="string">&#x27;&#x27;</span> <span class="comment"># path to the ca file used in the registry</span></span><br></pre></td></tr></table></figure><blockquote><p>如果 <code>docker.io</code> 的 endpoint 对应了带有私有项目的镜像仓库，那么这里需要为 <code>docker.io</code> 添加 auth 配置。</p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://rancher.com/docs/k3s/latest/en/installation/private-registry/">k3s-private-registry</a></li><li><a href="https://github.com/containerd/cri/blob/master/docs/registry.md">cri-containerd Configure Registry Credentials</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> containerd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> containerd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>golang pprof etcd 性能分析</title>
      <link href="/etcd/etcd-pprof-performance-analysis/"/>
      <url>/etcd/etcd-pprof-performance-analysis/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/etcd/etcd-pprof-performance-analysis/" target="_blank" title="https://www.xtplayer.cn/etcd/etcd-pprof-performance-analysis/">https://www.xtplayer.cn/etcd/etcd-pprof-performance-analysis/</a></p><h2 id="pprof-是什么？"><a href="#pprof-是什么？" class="headerlink" title="pprof 是什么？"></a>pprof 是什么？</h2><p>pprof 是用于可视化的性能分析工具，可以捕捉到多维度的运行状态的数据。</p><p>pprof 以 profile.proto 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）。</p><p>profile.proto 是一个 Protocol Buffer v3 的描述文件，它描述了一组 callstack 和 symbolization 信息， 作用是表示统计分析的一组采样的调用栈，是很常见的 stacktrace 配置文件格式。</p><h3 id="支持什么模式"><a href="#支持什么模式" class="headerlink" title="支持什么模式"></a>支持什么模式</h3><ul><li>Report generation：报告生成</li><li>Interactive terminal use：交互式终端使用</li><li>Web interface：Web 界面</li></ul><h3 id="可以做什么"><a href="#可以做什么" class="headerlink" title="可以做什么"></a>可以做什么</h3><ul><li>CPU Profiling：CPU 分析，按照一定的频率采集所监听的应用程序 CPU（含寄存器）的使用情况，可确定应用程序在主动消耗 CPU 周期时花费时间的位置</li><li>Memory Profiling：内存分析，在应用程序进行堆分配时记录堆栈跟踪，用于监视当前和历史内存使用情况，以及检查内存泄漏</li><li>Block Profiling：阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置</li><li>Mutex Profiling：互斥锁分析，报告互斥锁的竞争情况</li></ul><h2 id="GO-pprof-tool-安装"><a href="#GO-pprof-tool-安装" class="headerlink" title="GO pprof tool 安装"></a>GO pprof tool 安装</h2><p>为了方便，我们制作了 tools 工具包镜像 <code>registry.cn-shenzhen.aliyuncs.com/rancher/tools</code>，此镜像内置了 go pprof tool，以及常用的系统维护，网络维护工具。</p><blockquote><p>go pprof tool 安装参考：<a href="https://github.com/google/pprof#building-pprof">https://github.com/google/pprof#building-pprof</a></p></blockquote><blockquote><p><a href="https://github.com/xiaoluhong/kubernetes-issues-solution/blob/master/tools/Dockerfile">https://github.com/xiaoluhong/kubernetes-issues-solution/blob/master/tools/Dockerfile</a></p></blockquote><h2 id="etcd-启用-pprof"><a href="#etcd-启用-pprof" class="headerlink" title="etcd 启用 pprof"></a>etcd 启用 pprof</h2><p>接下来以 etcd 性能分析为例。</p><p>对于 rancher 自定义集群或者 rke 集群，默认没有开启 <strong>pprof</strong> 分析功能。可在集群配置或者 rke 配置文件中添加 <code>extra_env ETCD_ENABLE_PPROF=true</code> 或 <code>extra_args enable-pprof: true</code>，两个参数二选一。其他类型的 K8S 集群，可以在 etcd 的启动参数中添加 <code>ETCD_ENABLE_PPROF=true</code> 或 <code>--enable-pprof=true</code>，也是两个参数二选一。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">    <span class="attr">etcd:</span></span><br><span class="line">      <span class="attr">extra_args:</span></span><br><span class="line">        <span class="attr">max-request-bytes:</span> <span class="number">10485760</span></span><br><span class="line">        <span class="attr">snapshot-count:</span> <span class="number">50000</span></span><br><span class="line">        <span class="attr">log-level:</span> <span class="string">info</span> <span class="comment"># supports debug, info, warn, error, panic, or fatal.</span></span><br><span class="line">        <span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line">       <span class="comment"># enable-pprof: true</span></span><br><span class="line">      <span class="attr">backup_config:</span></span><br><span class="line">        <span class="attr">interval_hours:</span> <span class="number">12</span></span><br><span class="line">        <span class="attr">retention:</span> <span class="number">6</span></span><br><span class="line">      <span class="attr">extra_env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">ETCD_ENABLE_PPROF=true</span></span><br></pre></td></tr></table></figure><h2 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h2><p>对于 rancher 自定义集群或者 rke 集群，默认开启了 etcd 双向认证，那么在进行 pprof 查看数据的时候，需要提供客户端 ssl 证书。可以直接通过挂载卷（-v &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;:ro）的方式把主机证书目录挂载到 tools 容器中。</p><h3 id="获取应用当前的内存情况"><a href="#获取应用当前的内存情况" class="headerlink" title="获取应用当前的内存情况"></a>获取应用当前的内存情况</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NODE_IP=<span class="string">&#x27;192.168.1.224&#x27;</span></span><br><span class="line"></span><br><span class="line">docker run --<span class="built_in">rm</span> -ti -p 10086:10086 -e NODE_IP=<span class="variable">$&#123;NODE_IP&#125;</span> \</span><br><span class="line">  -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/:ro \</span><br><span class="line">  registry.cn-shenzhen.aliyuncs.com/rancher/tools \</span><br><span class="line">  pprof \</span><br><span class="line">  -tls_ca /etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">  -tls_cert /etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`.pem \</span><br><span class="line">  -tls_key /etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`-key.pem \</span><br><span class="line">  -http=0.0.0.0:10086 https://`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>`:2379/debug/pprof/heap</span><br></pre></td></tr></table></figure><img src="/etcd/etcd-pprof-performance-analysis/image-20201221133812040.png" class="" title="image-20201221133812040"><h3 id="采集应用-60s-内的-cpu-使用情况"><a href="#采集应用-60s-内的-cpu-使用情况" class="headerlink" title="采集应用 60s 内的 cpu 使用情况"></a>采集应用 60s 内的 cpu 使用情况</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NODE_IP=<span class="string">&#x27;192.168.1.224&#x27;</span></span><br><span class="line"></span><br><span class="line">docker run --<span class="built_in">rm</span> -ti -p 10086:10086 -e NODE_IP=<span class="variable">$&#123;NODE_IP&#125;</span> \</span><br><span class="line">  -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/:ro \</span><br><span class="line">  registry.cn-shenzhen.aliyuncs.com/rancher/tools \</span><br><span class="line">  pprof \</span><br><span class="line">  -tls_ca /etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">  -tls_cert /etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`.pem \</span><br><span class="line">  -tls_key /etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`-key.pem \</span><br><span class="line">  -http=0.0.0.0:10086 https://`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>`:2379/debug/pprof/profile</span><br></pre></td></tr></table></figure><h3 id="采集当前-goroutine-情况"><a href="#采集当前-goroutine-情况" class="headerlink" title="采集当前 goroutine 情况"></a>采集当前 goroutine 情况</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NODE_IP=<span class="string">&#x27;192.168.1.224&#x27;</span></span><br><span class="line"></span><br><span class="line">docker run --<span class="built_in">rm</span> -ti -p 10086:10086 -e NODE_IP=<span class="variable">$&#123;NODE_IP&#125;</span> \</span><br><span class="line">  -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/:ro \</span><br><span class="line">  registry.cn-shenzhen.aliyuncs.com/rancher/tools \</span><br><span class="line">  pprof \</span><br><span class="line">  -tls_ca /etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">  -tls_cert /etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`.pem \</span><br><span class="line">  -tls_key /etc/kubernetes/ssl/kube-etcd-`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>|sed <span class="string">&#x27;s/\./-/g&#x27;</span>`-key.pem \</span><br><span class="line">  -http=0.0.0.0:10086 https://`<span class="built_in">echo</span> <span class="variable">$&#123;NODE_IP&#125;</span>`:2379/debug/pprof/goroutine</span><br></pre></td></tr></table></figure><h2 id="数据查看"><a href="#数据查看" class="headerlink" title="数据查看"></a>数据查看</h2><p>通过访问 <code>主机 IP:10086</code> 查看 pprof Web 界面，点击 <strong>VIEW</strong> 查看不通的视图。</p><img src="/etcd/etcd-pprof-performance-analysis/image-20201221140007039.png" class="" title="image-20201221140007039"><img src="/etcd/etcd-pprof-performance-analysis/image-20201221134007282.png" class="" title="image-20201221134007282">]]></content>
      
      
      <categories>
          
          <category> etcd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> etcd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>复用 Released 状态的 pv</title>
      <link href="/kubernetes/reuse-released-pv/"/>
      <url>/kubernetes/reuse-released-pv/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/reuse-released-pv/" target="_blank" title="https://www.xtplayer.cn/kubernetes/reuse-released-pv/">https://www.xtplayer.cn/kubernetes/reuse-released-pv/</a></p><h2 id="PV-回收策略"><a href="#PV-回收策略" class="headerlink" title="PV 回收策略"></a>PV 回收策略</h2><p>当用户不再使用其存储卷时，他们可以从 API 中将 PVC 对象删除，从而允许 该资源被回收再利用。PersistentVolume 对象的回收策略告诉集群，当其被 从申领中释放时如何处理该数据卷。 目前，数据卷可以被 Retained（保留）、Recycled（回收，<code>Recycle</code> 已被废弃）或 Deleted（删除）。</p><h3 id="保留（Retain）"><a href="#保留（Retain）" class="headerlink" title="保留（Retain）"></a>保留（Retain）</h3><p>回收策略 <code>Retain</code> 使得用户可以手动回收资源。当 PersistentVolumeClaim 对象 被删除时，PersistentVolume 卷仍然存在，对应的数据卷被视为”已释放（released）”。 由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。 管理员可以通过下面的步骤来手动回收该卷：</p><ol><li>删除 PersistentVolume 对象。与之相关的、位于外部基础设施中的存储资产 （例如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）在 PV 删除之后仍然存在。</li><li>根据情况，手动清除所关联的存储资产上的数据。</li><li>手动删除所关联的存储资产；如果你希望重用该存储资产，可以基于存储资产的 定义创建新的 PersistentVolume 卷对象。</li></ol><h3 id="删除（Delete）"><a href="#删除（Delete）" class="headerlink" title="删除（Delete）"></a>删除（Delete）</h3><p>对于支持 <code>Delete</code> 回收策略的卷插件，删除动作会将 PersistentVolume 对象从 Kubernetes 中移除，同时也会从外部基础设施（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）中移除所关联的存储资产。 动态供应的卷会继承<a href="https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/#reclaim-policy">其 StorageClass 中设置的回收策略</a>，该策略默认 为 <code>Delete</code>。 管理员需要根据用户的期望来配置 StorageClass；否则 PV 卷被创建之后必须要被 编辑或者修补。参阅<a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/change-pv-reclaim-policy/">更改 PV 卷的回收策略</a>.</p><blockquote><p>参考：<a href="https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/#reclaiming">https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/#reclaiming</a></p></blockquote><h2 id="准备测试-pv-和-pvc"><a href="#准备测试-pv-和-pvc" class="headerlink" title="准备测试 pv 和 pvc"></a>准备测试 pv 和 pvc</h2><ol><li><p>点击 <strong>集群|存储|持久卷</strong>，点击右侧添加 pv：</p> <img src="/kubernetes/reuse-released-pv/image-20201208191644698.png" class="" title="image-20201208191644698"></li><li><p>进入任意项目，点击 PVC，然后点击右侧 <strong>添加 pvc</strong> ：</p> <img src="/kubernetes/reuse-released-pv/image-20201208191446393.png" class="" title="image-20201208191446393"> <img src="/kubernetes/reuse-released-pv/image-20201208191718549.png" class="" title="image-20201208191718549"> <img src="/kubernetes/reuse-released-pv/image-20201208191812212.png" class="" title="image-20201208191812212"></li></ol><h2 id="通过命令查看-pv-和-pvc-yaml-配置"><a href="#通过命令查看-pv-和-pvc-yaml-配置" class="headerlink" title="通过命令查看 pv 和 pvc yaml 配置"></a>通过命令查看 pv 和 pvc yaml 配置</h2><ul><li><p>pv</p>  <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">hxl@rancher:~$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pv</span> <span class="string">test-path-pv</span> <span class="string">-oyaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">field.cattle.io/creatorId:</span> <span class="string">user-wf5x7</span></span><br><span class="line">    <span class="attr">pv.kubernetes.io/bound-by-controller:</span> <span class="string">&quot;yes&quot;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">&quot;2020-12-08T11:17:06Z&quot;</span></span><br><span class="line">  <span class="attr">finalizers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kubernetes.io/pv-protection</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">cattle.io/creator:</span> <span class="string">norman</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-path-pv</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">&quot;24187470&quot;</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/persistentvolumes/test-path-pv</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">99f719ba-3314-48f3-9473-11e0b19a8806</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line">  <span class="attr">claimRef:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test-path-pvc</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">resourceVersion:</span> <span class="string">&quot;24187465&quot;</span></span><br><span class="line">    <span class="attr">uid:</span> <span class="string">88a75ff5-232a-44d5-8bbd-933fb325b80f</span></span><br><span class="line">  <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/tmp/test-path-pv</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">phase:</span> <span class="string">Bound</span></span><br></pre></td></tr></table></figure></li><li><p>pvc</p>  <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">hxl@rancher:~$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pvc</span> <span class="string">test-path-pvc</span> <span class="string">-oyaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">field.cattle.io/creatorId:</span> <span class="string">user-wf5x7</span></span><br><span class="line">    <span class="attr">pv.kubernetes.io/bind-completed:</span> <span class="string">&quot;yes&quot;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">&quot;2020-12-08T11:17:57Z&quot;</span></span><br><span class="line">  <span class="attr">finalizers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kubernetes.io/pvc-protection</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">cattle.io/creator:</span> <span class="string">norman</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-path-pvc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">&quot;24187481&quot;</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/default/persistentvolumeclaims/test-path-pvc</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">88a75ff5-232a-44d5-8bbd-933fb325b80f</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">volumeName:</span> <span class="string">test-path-pv</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line">  <span class="attr">phase:</span> <span class="string">Bound</span></span><br></pre></td></tr></table></figure></li></ul><p>在 pv 的 <strong>spec.claimRef</strong> 可以看到对应 pvc 的具体信息，重要的有 name、namespace、uid。</p><h2 id="测试删除-pvc"><a href="#测试删除-pvc" class="headerlink" title="测试删除 pvc"></a>测试删除 pvc</h2><img src="/kubernetes/reuse-released-pv/image-20201208192929197.png" class="" title="image-20201208192929197"><p>因为 pv 的 <strong>persistentVolumeReclaimPolicy</strong> 属性为 <strong>Retain</strong>，在删除 pvc 后 pv，会自动保留，所以 pv 状态变为 <strong>Released</strong>。</p><img src="/kubernetes/reuse-released-pv/image-20201208193132639.png" class="" title="image-20201208193132639"><h2 id="编辑-pv"><a href="#编辑-pv" class="headerlink" title="编辑 pv"></a>编辑 pv</h2><p>为了 pv 可以重新被挂载，需要执行 <strong>kubectl edit pv test-path-pv</strong> 命令，清理 <strong>spec.claimRef</strong> 中的内容，以使 pv 成为可用状态。</p><img src="/kubernetes/reuse-released-pv/image-20201208193355828.png" class="" title="image-20201208193355828"><img src="/kubernetes/reuse-released-pv/image-20201208193605364.png" class="" title="image-20201208193605364"><p>也可以执行以下命令去快速清除 pv 的 claimRef:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl patch pv &lt;pv-name&gt; -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;claimRef&quot;: null&#125;&#125;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="重新创建-pvc"><a href="#重新创建-pvc" class="headerlink" title="重新创建 pvc"></a>重新创建 pvc</h2><p>进入任意项目，点击 PVC，然后点击右侧 <strong>添加 pvc</strong></p><img src="/kubernetes/reuse-released-pv/image-20201208193745013.png" class="" title="image-20201208193745013"><img src="/kubernetes/reuse-released-pv/image-20201208193825152.png" class="" title="image-20201208193825152">]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Released pv </tag>
            
            <tag> storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher 不显示全局功能的解决方案</title>
      <link href="/rancher/rancher-not-display-global-monitoring/"/>
      <url>/rancher/rancher-not-display-global-monitoring/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/rancher-not-display-global-monitoring/" target="_blank" title="https://www.xtplayer.cn/rancher/rancher-not-display-global-monitoring/">https://www.xtplayer.cn/rancher/rancher-not-display-global-monitoring/</a></p><p>有时候在 rancher ui 全局界面无法查看一些功能，比如全局监控，或者企业版无法在<strong>镜像仓库</strong>菜单中查看 harbor 配置等。这是一个历史遗留的 BUG，访问 <code>server_url/v3/users?me=true</code> 时可以看到多个用户具有 <code>me: true</code> 字段。</p><img src="/rancher/rancher-not-display-global-monitoring/image-20201201171735689.png" class="" title="image-20201201171735689"><img src="/rancher/rancher-not-display-global-monitoring/image-20201201171745975.png" class="" title="image-20201201171745975"><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>在 local 集群或者 rancher server 容器中执行以下脚本：</p><blockquote><p><strong>注意</strong>：操作前先对 local 集群或者 rancher server 容器做数据备份。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> `kubectl get users.management.cattle.io|awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>|grep -v NAME`;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">   <span class="keyword">if</span> [ <span class="string">&quot;<span class="subst">$( kubectl  get user $&#123;user&#125; -oyaml|grep -w &#x27;me: true&#x27; )</span>&quot;</span> != <span class="string">&#x27;&#x27;</span> ]; <span class="keyword">then</span></span><br><span class="line">        kubectl patch <span class="built_in">users</span> <span class="variable">$&#123;user&#125;</span> -p <span class="string">&#x27;&#123;&quot;me&quot;: false&#125;&#x27;</span> --<span class="built_in">type</span>=<span class="string">&#x27;merge&#x27;</span>;</span><br><span class="line">   <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强制删除 Terminating 状态的 namespace</title>
      <link href="/kubernetes/forces-delete-terminated-namespace/"/>
      <url>/kubernetes/forces-delete-terminated-namespace/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/forces-delete-terminated-namespace/" target="_blank" title="https://www.xtplayer.cn/kubernetes/forces-delete-terminated-namespace/">https://www.xtplayer.cn/kubernetes/forces-delete-terminated-namespace/</a></p><p>有时候删除命名空间后，发现命名空间一直处于 Terminating 状态。通过执行 <code>kubectl delete namespace $&#123;namespace&#125; --force --grace-period=0</code> 强制删除命令依然无法删除。</p><h2 id="rancher-自定义-k8s-集群或者导入-rancher-管理的-k8s-集群"><a href="#rancher-自定义-k8s-集群或者导入-rancher-管理的-k8s-集群" class="headerlink" title="rancher 自定义 k8s 集群或者导入 rancher 管理的 k8s 集群"></a>rancher 自定义 k8s 集群或者导入 rancher 管理的 k8s 集群</h2><ol><li><p>登录 rancher ui，切换到对应集群首页，点击 <strong>执行 kubectl 命令行</strong>。</p> <img src="/kubernetes/forces-delete-terminated-namespace/image-20201201134337440.png" class="" title="image-20201201134337440"></li><li><p>接着执行以下命令：</p></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 指定需要删除的命名空间</span></span><br><span class="line">NAMESPACE= <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下为固定格式，不需要修改</span></span><br><span class="line">RANCHER_SERVER_URL=$( kubectl config view -o json|jq -r .clusters[0].cluster.server )</span><br><span class="line">CLUSTER_TOKEN=$( kubectl config view -o json|jq -r .<span class="built_in">users</span>[0].user.token )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取删除 finalizers 后的命名空间 json 配置</span></span><br><span class="line">kubectl get ns <span class="variable">$&#123;NAMESPACE&#125;</span> -ojson| \</span><br><span class="line">  jq <span class="string">&#x27;del(.spec.finalizers[])&#x27;</span>| \</span><br><span class="line">  jq <span class="string">&#x27;del(.metadata.finalizers)&#x27;</span> &gt; <span class="variable">$&#123;NAMESPACE&#125;</span>.json</span><br><span class="line"></span><br><span class="line">curl -k \</span><br><span class="line">-H <span class="string">&quot;Content-Type: application/json&quot;</span> \</span><br><span class="line">-H <span class="string">&quot;Authorization: Bearer <span class="variable">$&#123;CLUSTER_TOKEN&#125;</span>&quot;</span> \</span><br><span class="line">-X PUT \</span><br><span class="line">--data-binary @<span class="variable">$&#123;NAMESPACE&#125;</span>.json \</span><br><span class="line"><span class="variable">$&#123;RANCHER_SERVER_URL&#125;</span>/api/v1/namespaces/<span class="variable">$&#123;NAMESPACE&#125;</span>/finalize</span><br></pre></td></tr></table></figure><h2 id="非-rancher-管理的-k8s-集群"><a href="#非-rancher-管理的-k8s-集群" class="headerlink" title="非 rancher 管理的 k8s 集群"></a>非 rancher 管理的 k8s 集群</h2><p>主机上需要安装 jq 和 curl 工具，<a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NAMESPACE=<span class="string">&#x27;&#x27;</span> <span class="comment"># 指定需要删除的命名空间</span></span><br><span class="line">KUBE_CONFIG=<span class="string">&#x27;&#x27;</span> <span class="comment"># 指定 K8S 配置文件</span></span><br><span class="line"></span><br><span class="line">K8S_API_URL=$( kubectl --kubeconfig=<span class="variable">$&#123;KUBE_CONFIG&#125;</span> config view --raw -o json|jq -r <span class="string">&#x27;.clusters[0].cluster.server&#x27;</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：如果 config 中证书是以文件保存，此处命令</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;KUBE_CONFIG&#125;</span> config view --raw -o json| \</span><br><span class="line">  jq -r <span class="string">&#x27;.users[0].user.&quot;client-certificate-data&quot;&#x27;</span>| \</span><br><span class="line">  <span class="built_in">tr</span> -d <span class="string">&#x27;&quot;&#x27;</span>|<span class="built_in">base64</span> --decode &gt; /tmp/client_cert.pem</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;KUBE_CONFIG&#125;</span> config view --raw -o json| \</span><br><span class="line">  jq -r <span class="string">&#x27;.users[0].user.&quot;client-key-data&quot;&#x27;</span>| \</span><br><span class="line">  <span class="built_in">tr</span> -d <span class="string">&#x27;&quot;&#x27;</span>|<span class="built_in">base64</span> --decode &gt; /tmp/client_key.pem</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;KUBE_CONFIG&#125;</span> config view --raw -o json| \</span><br><span class="line">  jq -r <span class="string">&#x27;.clusters[0].cluster.&quot;certificate-authority-data&quot;&#x27;</span>| \</span><br><span class="line">  <span class="built_in">tr</span> -d <span class="string">&#x27;&quot;&#x27;</span>|<span class="built_in">base64</span> --decode &gt; /tmp/client_ca.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取删除 finalizers 后的命名空间 json 配置</span></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;KUBE_CONFIG&#125;</span> get ns <span class="variable">$&#123;NAMESPACE&#125;</span> -ojson| \</span><br><span class="line">  jq <span class="string">&#x27;del(.spec.finalizers[])&#x27;</span>| \</span><br><span class="line">  jq <span class="string">&#x27;del(.metadata.finalizers)&#x27;</span> &gt; <span class="variable">$&#123;NAMESPACE&#125;</span>.json</span><br><span class="line"></span><br><span class="line">curl -k \</span><br><span class="line">--cert /tmp/client_cert.pem \</span><br><span class="line">--key /tmp/client_key.pem \</span><br><span class="line">--cacert /tmp/client_ca.pem \</span><br><span class="line">-H <span class="string">&quot;Content-Type: application/json&quot;</span> \</span><br><span class="line">-X PUT \</span><br><span class="line">--data-binary @<span class="variable">$&#123;NAMESPACE&#125;</span>.json \</span><br><span class="line"><span class="variable">$&#123;K8S_API_URL&#125;</span>/api/v1/namespaces/<span class="variable">$&#123;NAMESPACE&#125;</span>/finalize</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> terminated </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher 自带监控收集导入 K8S 集群的 ETCD 数据</title>
      <link href="/rancher/monitors/rancher-monitors-collection-external-etcd-data/"/>
      <url>/rancher/monitors/rancher-monitors-collection-external-etcd-data/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/monitors/rancher-monitors-collection-external-etcd-data/" target="_blank" title="https://www.xtplayer.cn/rancher/monitors/rancher-monitors-collection-external-etcd-data/">https://www.xtplayer.cn/rancher/monitors/rancher-monitors-collection-external-etcd-data/</a></p><p>目前，rancher 自带监控暂时只支持收集 rancher 自定义集群的 ETCD 监控数据。对于 rke 创建并导入或者其他工具创建并导入的 k8s 集群，因为架构的差异，暂时不能直接支持监控 ETCD 监控数据。</p><p>rancher 内置监控也是通过 prometheus-operator 部署，只需要自定义一些 serviceMonitor 配置即可实现其他类型集群 ETCD 监控数据收集。</p><h2 id="方案介绍"><a href="#方案介绍" class="headerlink" title="方案介绍"></a>方案介绍</h2><p>serviceMonitor 是通过对 service 获取数据的一种方式。prometheus-operator 可以通过 serviceMonitor 自动识别带有某些 label 的 service，并从这些 service 获取数据。serviceMonitor 也是由 prometheus-operator 自动发现。</p><p>对应一些启用 ssl 认证的服务，需要提供 ssl 证书给 prometheus 用以进行认证 。首先创建一个 secret 用来存放 ssl 证书，再把证书挂载到 prometheus 容器内。接下来创建 ServiceMonitor 对象，用于 Prometheus 添加监控项，再为 ServiceMonitor 对象关联 metrics 数据接口对应的 Service 对象，确保通过 Service 对象可以正确获取到 metrics 数据。</p><h2 id="创建-Secrets-挂载证书"><a href="#创建-Secrets-挂载证书" class="headerlink" title="创建 Secrets 挂载证书"></a>创建 Secrets 挂载证书</h2><p>在 ETCD 节点中，通过以下命令去创建 ETCD 证书密文：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create secret generic exporter-etcd-cert -n cattle-prometheus \</span><br><span class="line">  --from-file=exporter-etcd-ca.pem=&lt; etcd ca 文件路径 &gt; \</span><br><span class="line">  --from-file=exporter-etcd-key.pem=&lt; etcd key 文件路径 &gt; \</span><br><span class="line">  --from-file=exporter-etcd-cert.pem=&lt; etcd 证书文件路径 &gt;</span><br></pre></td></tr></table></figure><h2 id="添加-Secrets-到-Prometheus-容器"><a href="#添加-Secrets-到-Prometheus-容器" class="headerlink" title="添加 Secrets 到 Prometheus 容器"></a>添加 Secrets 到 Prometheus 容器</h2><p>依次进入 <strong>集群|工具|监控</strong> ，在监控配置页面点击右下角的 <strong>显示高级选项</strong>。添加 <strong>添加应答</strong>，添加以下应答：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">prometheus.secrets[0]=exporter-etcd-cert</span><br><span class="line"><span class="comment"># 如果有多个密文，则依次叠加</span></span><br><span class="line">prometheus.secrets[1]=xxxx</span><br></pre></td></tr></table></figure><h2 id="创建-etcd-server-svc，把外部-etcd-服务引入-k8s-集群"><a href="#创建-etcd-server-svc，把外部-etcd-服务引入-k8s-集群" class="headerlink" title="创建 etcd-server svc，把外部 etcd 服务引入 k8s 集群"></a>创建 etcd-server svc，把外部 etcd 服务引入 k8s 集群</h2><ol><li><p>访问 <strong>system 项目|服务发现</strong>，统计 <strong>添加 DNS 记录</strong>。</p> <img src="/rancher/monitors/rancher-monitors-collection-external-etcd-data/image-20201130184840193.png" class="" title="image-20201130184840193"></li><li><p>选择 <strong>外部 IP 地址</strong>，并在右侧的 <strong>目标 IP 地址</strong> 填写 etcd 节点 ip，命名空间选择 <code>cattle-prometheus</code></p><img src="/rancher/monitors/rancher-monitors-collection-external-etcd-data/image-20210225191727314.png" class="" title="image-20210225191727314"></li><li><p>点击右侧 <strong>显示高级选项</strong>，</p><ol><li><p>类型选择 <strong>headless service</strong></p></li><li><p>添加端口映射</p><img src="/rancher/monitors/rancher-monitors-collection-external-etcd-data/image-20210223123606233.png" class="" title="image-20210223123606233"></li></ol></li><li><p>在底部 <strong>标签与注释</strong> 中添加以下两个标签，最后点击创建。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jobLabel=exporter-etcd-server</span><br><span class="line">app=exporter-etcd-server</span><br></pre></td></tr></table></figure></li></ol><h2 id="创建-etcd-ServiceMonitor-对象"><a href="#创建-etcd-ServiceMonitor-对象" class="headerlink" title="创建 etcd ServiceMonitor 对象"></a>创建 etcd ServiceMonitor 对象</h2><p>保存以下内容为：<code>prometheus-serviceMonitorEtcd.yaml</code>，然后执行 <code>kubectl apply -f prometheus-serviceMonitorEtcd.yaml</code></p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceMonitor</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">exporter-etcd-server-monitoring</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">cattle-prometheus</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">exporter-etcd-server</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">jobLabel:</span> <span class="string">external-etcd-server</span> <span class="comment"># exporter-etcd-server svc 的标签 jobLabel=exporter-etcd-server</span></span><br><span class="line">  <span class="attr">endpoints:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="string">https-2379</span> <span class="comment"># 此处的设置需要与 etcd-server svc 中端口映射的 name 对应</span></span><br><span class="line">    <span class="attr">scheme:</span> <span class="string">https</span></span><br><span class="line">    <span class="attr">interval:</span> <span class="string">30s</span></span><br><span class="line">    <span class="attr">tlsConfig:</span> <span class="comment"># 此处的证书路径与密文挂载到 prometheus 容器中的路径一致</span></span><br><span class="line">      <span class="attr">caFile:</span> <span class="string">/etc/prometheus/secrets/exporter-etcd-cert/exporter-etcd-ca.pem</span></span><br><span class="line">      <span class="attr">certFile:</span> <span class="string">/etc/prometheus/secrets/exporter-etcd-cert/exporter-etcd-cert.pem</span></span><br><span class="line">      <span class="attr">keyFile:</span> <span class="string">/etc/prometheus/secrets/exporter-etcd-cert/exporter-etcd-key.pem</span></span><br><span class="line">      <span class="attr">insecureSkipVerify:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">namespaceSelector:</span></span><br><span class="line">    <span class="attr">matchNames:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">cattle-prometheus</span> <span class="comment"># exporter-etcd-server svc 所在的命名空间</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">exporter-etcd-server</span> <span class="comment"># exporter-etcd-server svc 的标签</span></span><br></pre></td></tr></table></figure><h2 id="验证-prometheus-状态"><a href="#验证-prometheus-状态" class="headerlink" title="验证 prometheus 状态"></a>验证 prometheus 状态</h2><ol><li><p>点击 <strong>system 项目|应用商店</strong> 的访问入口，进入 prometheus 管理界面</p> <img src="/rancher/monitors/rancher-monitors-collection-external-etcd-data/image-20201130190651558.png" class="" title="image-20201130190651558"></li><li><p>点击 <strong>Status|Targets 和 Status|Service Discovery</strong> 查看状态</p> <img src="/rancher/monitors/rancher-monitors-collection-external-etcd-data/image-20201130190804103.png" class="" title="image-20201130190804103"> <img src="/rancher/monitors/rancher-monitors-collection-external-etcd-data/image-20201130191002037.png" class="" title="image-20201130191002037"> <img src="/rancher/monitors/rancher-monitors-collection-external-etcd-data/image-20201130191033691.png" class="" title="image-20201130191033691"></li></ol><h2 id="验证-Grafana-状态"><a href="#验证-Grafana-状态" class="headerlink" title="验证 Grafana 状态"></a>验证 Grafana 状态</h2><ul><li><p>与 prometheus 相同， 点击 <strong>system 项目|应用商店</strong> 的访问入口，进入 Grafana 管理界面，点击左上角的 <strong>Home</strong></p></li><li><p>接着点击 ETCD。</p></li><li><p>如果结果如下图显示，则说明数据可以正常显示，配置成功。</p></li></ul><img src="/rancher/monitors/rancher-monitors-collection-external-etcd-data/image-20201130191245446.png" class="" title="image-20201130191245446">]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> monitors </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher monitors </tag>
            
            <tag> monitors </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vagrant 快速搭建开发测试环境</title>
      <link href="/vagrant/vagrant-quick-start/"/>
      <url>/vagrant/vagrant-quick-start/</url>
      
        <content type="html"><![CDATA[<h2 id="Vagrant-简介"><a href="#Vagrant-简介" class="headerlink" title="Vagrant 简介"></a>Vagrant 简介</h2><p>Vagrant 是用于在单个工作流程中构建和管理虚拟机环境的工具，它具有高度的自动化和易于使用的工作流程，大大缩短了开发环境的部署时间，提高了效率。</p><p>Vagrant 与 Docker 有点类似，Docker 可以使用 Docker  images 来运行不同的容器，而 Vagrant 也有与 Docker images 相似的 Vagrant Box，Vagrant Box 也可以理解为是一个镜像。可以使用一个 <code>base box</code> 运行虚拟机，然后在虚拟机中安装某些软件，然后将这个虚拟机导出为新的 Box。为了保证环境的统一性，你可以将此 Box 分发给其他同事使用。</p><h2 id="Vargant-的优点"><a href="#Vargant-的优点" class="headerlink" title="Vargant 的优点"></a>Vargant 的优点</h2><ol><li><p>环境配置统一</p><p> 一次配置打包，统一分发给团队成员，避免诸如 编码问题、缺少模块、配置文件不同 带来的问题。</p></li><li><p>环境快速搭建与销毁</p><p> 通过统一的 <code>Vagrantfile</code> 配置文件，需要时可以快速的搭建，不需要时可以快速销毁，减少搭建环境的时间成本，减少资源的空闲占用提高资源使用率。</p></li><li><p>批量部署多种不同的环境</p><p> 通过 <code>Vagrantfile</code> 配置文件可以定义不通的环境，然后一次性快速搭建。</p></li></ol><h2 id="Vagrant-安装"><a href="#Vagrant-安装" class="headerlink" title="Vagrant 安装"></a>Vagrant 安装</h2><p>Vagrant 支持多种平台，比如有 Windows 版、MacOS 版，也有 linux 版本，访问 <a href="http://downloads.vagrantup.com/">http://downloads.vagrantup.com</a> 根据使用平台进行下载并安装。</p><h2 id="VirtualBox-安装"><a href="#VirtualBox-安装" class="headerlink" title="VirtualBox 安装"></a>VirtualBox 安装</h2><p>Vagrant 它只是一个虚拟机的管理工具，它本身并不支持虚拟化功能。Vagrant 需要一个外部的虚拟化管理工具来实现虚拟机的创建，并通过虚拟化管理工具的 API 来实现虚拟机的管理。开源社区中目前用的最多的应该是 <a href="https://www.vagrantup.com/docs/providers/virtualbox">VirtualBox</a> ，它也支持 <a href="https://www.vagrantup.com/docs/providers/vmware/installation">VMware</a> ,不过目前 VMware 是收费的。</p><p>VirtualBox 下载请访问 <code>https://www.virtualbox.org/wiki/Downloads</code> 。</p><h2 id="Vagrant-Box"><a href="#Vagrant-Box" class="headerlink" title="Vagrant Box"></a>Vagrant Box</h2><p>上面两个步骤完成后 Vagrant 环境就安装完成了，接下来需要下载 Box 镜像来启动虚拟机。Vagrant 提供了与 Dockerhub 类似的镜像仓库 <a href="https://app.vagrantup.com/boxes/search">https://app.vagrantup.com/boxes/search</a> ，里面托管了大量 linux 官方或者三方的 Box 镜像。</p><p>以 <code>ubuntu/trusty64</code> 为例:</p><img src="/vagrant/vagrant-quick-start/image-20201024210307310.png" class="" title="image-20201024210307310"><p>执行 <code>vagrant box add ubuntu/trusty64</code> 将会自动从 vagrant 镜像仓库拉取镜像，默认是 <strong>latest</strong> 版本，如果想添加指定版本的镜像，可以使用 <a href="https://www.vagrantup.com/docs/cli/box#box-version-value">–box-version</a> 参数。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hxl@rancher:~/vagrant$ vagrant box add ubuntu/trusty64</span><br><span class="line">==&gt; box: Loading metadata <span class="keyword">for</span> box <span class="string">&#x27;ubuntu/trusty64&#x27;</span></span><br><span class="line">    box: URL: https://vagrantcloud.com/ubuntu/trusty64</span><br><span class="line">==&gt; box: Adding box <span class="string">&#x27;ubuntu/trusty64&#x27;</span> (v20190514.0.0) <span class="keyword">for</span> provider: virtualbox</span><br><span class="line">    box: Downloading: https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/20190514.0.0/providers/virtualbox.box</span><br><span class="line">Download redirected to host: cloud-images.ubuntu.com</span><br><span class="line">Progress: 0% (Rate: 142k/s, Estimated time remaining: 1:41:47)</span><br></pre></td></tr></table></figure><blockquote><p>命令行参数说明：<a href="https://www.vagrantup.com/docs/cli/box">https://www.vagrantup.com/docs/cli/box</a></p></blockquote><h3 id="Vagrant-Box-Mirror"><a href="#Vagrant-Box-Mirror" class="headerlink" title="Vagrant Box Mirror"></a>Vagrant Box Mirror</h3><p>vagrant 镜像仓库是在国外，下载速度很慢。国内的一些镜像源平台同步了相应的镜像，比如以下源：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">https://mirrors.ustc.edu.cn/centos-cloud/centos/7/vagrant/x86_64/images/</span><br><span class="line">https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/bionic/current/</span><br></pre></td></tr></table></figure><p>访问以上源地址，选择并复制 Box 具体地址。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/bionic/current/bionic-server-cloudimg-amd64-vagrant.box</span><br></pre></td></tr></table></figure><p>执行以下命令把 Box 下载到本地，title 可以任意设置，代表这个 Box 在本地的名称，url 为在线 Box 的地址。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vagrant box add &#123;title&#125; &#123;url&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整示例</span></span><br><span class="line">vagrant box add ubuntu-1804-base https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/bionic/current/bionic-server-cloudimg-amd64-vagrant.box</span><br></pre></td></tr></table></figure><p>上面命令执行完成后，运行 <code>vagrant box list</code> 可以看到名为 ubuntu&#x2F;bionic-base 的 Box 名称。</p><h2 id="Vagrantfile-配置"><a href="#Vagrantfile-配置" class="headerlink" title="Vagrantfile 配置"></a>Vagrantfile 配置</h2><p>Vagrant 需要一个名为 <code>Vagrantfile</code> 的配置文件来启动虚拟机，Vagrantfile 中定义了虚拟机的所有配置参数，具体的配置参数可以访问 <a href="https://www.vagrantup.com/docs/vagrantfile">https://www.vagrantup.com/docs/vagrantfile</a> 。</p><p>以下是一个 <code>Vagrantfile</code> 配置示例：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Vagrant.configure(<span class="string">&quot;2&quot;</span>) <span class="keyword">do</span> |config|</span><br><span class="line"></span><br><span class="line">  config.vm.box_check_update = <span class="literal">false</span></span><br><span class="line">  config.vm.graceful_halt_timeout = 60</span><br><span class="line">  config.ssh.forward_agent = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># centos 7.8</span></span><br><span class="line">  config.vm.define <span class="string">&quot;test-centos7-8&quot;</span> <span class="keyword">do</span> |node|</span><br><span class="line">    node.vm.box = <span class="string">&quot;centos7-8-base&quot;</span></span><br><span class="line">    node.vm.synced_folder <span class="string">&quot;.&quot;</span>, <span class="string">&quot;/vagrant&quot;</span>, <span class="built_in">type</span>: <span class="string">&quot;virtualbox&quot;</span></span><br><span class="line">    node.ssh.username = <span class="string">&#x27;root&#x27;</span></span><br><span class="line">    node.vm.provider <span class="string">&quot;virtualbox&quot;</span> <span class="keyword">do</span> |v|</span><br><span class="line">      v.gui = <span class="literal">false</span></span><br><span class="line">      v.memory = 1024</span><br><span class="line">      v.cpus = 2</span><br><span class="line">      v.name = <span class="string">&quot;test-centos7-8&quot;</span></span><br><span class="line">    end</span><br><span class="line">    <span class="comment">#私有网</span></span><br><span class="line">    node.vm.network <span class="string">&quot;private_network&quot;</span>, hostname: <span class="literal">true</span></span><br><span class="line">    <span class="comment">#安装 ansible 的脚本，放在 vagrantfile 同目录下</span></span><br><span class="line">    node.vm.hostname = <span class="string">&quot;test-centos7-8&quot;</span></span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  <span class="comment"># ubuntu 18.04</span></span><br><span class="line">  config.vm.define <span class="string">&quot;test-ubuntu18-04&quot;</span> <span class="keyword">do</span> |node|</span><br><span class="line">    node.vm.box = <span class="string">&quot;ubuntu-1804-base&quot;</span></span><br><span class="line">    node.vm.provider <span class="string">&quot;virtualbox&quot;</span> <span class="keyword">do</span> |node|</span><br><span class="line">      node.memory = 4096</span><br><span class="line">      node.cpus = 2</span><br><span class="line">      node.name = <span class="string">&quot;test-ubuntu18-04&quot;</span></span><br><span class="line">    end</span><br><span class="line">    node.vm.network <span class="string">&quot;private_network&quot;</span>, hostname: <span class="literal">true</span></span><br><span class="line">    node.vm.hostname = <span class="string">&quot;test-ubuntu18-04&quot;</span></span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h2 id="Vagrant-运行虚拟机"><a href="#Vagrant-运行虚拟机" class="headerlink" title="Vagrant 运行虚拟机"></a>Vagrant 运行虚拟机</h2><p>在 <code>Vagrantfile</code> 配置文件准备好之后，在 shell 终端中切换到  <code>Vagrantfile</code>  所在的目录，执行 <code>vagrant up</code> 即可创建虚拟机。</p><h2 id="vagrant-ssh-延迟处理"><a href="#vagrant-ssh-延迟处理" class="headerlink" title="vagrant ssh 延迟处理"></a>vagrant ssh 延迟处理</h2><p>时常我们需要通过 vagrant ssh 访问虚拟机终端，但是目前执行 vagrant ssh 登录虚拟机终端会有 2 到 3 秒的延迟，但是直接通过 ssh 登录虚拟机却没有延迟。</p><p>目前这个问题 vagrant 官方并没有优化方案，为了避免这个几秒延迟，我绕过 <code>vagrant ssh</code> 去登录虚拟机终端，通过 ssh 别名的方式登录。 如果你与我一样无法忍受这几秒的延迟，可以参考一下我以下的方法。</p><ol><li><p>在 <code>Vagrantfile</code>  所在的目录，执行 <code>vagrant ssh-config</code> 打印每个虚拟机的 ssh 配置文件，执行命令需要先运行虚拟机。命令运行后应该会获取到类似以下的配置文件：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hxl@rancher:~/vagrant$ vagrant ssh-config</span><br><span class="line"></span><br><span class="line">Host test-centos7-8</span><br><span class="line">  HostName 127.0.0.1</span><br><span class="line">  User root</span><br><span class="line">  Port 2222</span><br><span class="line">  UserKnownHostsFile /dev/null</span><br><span class="line">  StrictHostKeyChecking no</span><br><span class="line">  PasswordAuthentication no</span><br><span class="line">  IdentityFile /Users/hxl/.vagrant.d/insecure_private_key</span><br><span class="line">  IdentitiesOnly <span class="built_in">yes</span></span><br><span class="line">  LogLevel FATAL</span><br><span class="line">  ForwardAgent <span class="built_in">yes</span></span><br><span class="line"></span><br><span class="line">Host test-ubuntu18-04</span><br><span class="line">  HostName 127.0.0.1</span><br><span class="line">  User root</span><br><span class="line">  Port 2200</span><br><span class="line">  UserKnownHostsFile /dev/null</span><br><span class="line">  StrictHostKeyChecking no</span><br><span class="line">  PasswordAuthentication no</span><br><span class="line">  IdentityFile /Users/hxl/.vagrant.d/insecure_private_key</span><br><span class="line">  IdentitiesOnly <span class="built_in">yes</span></span><br><span class="line">  LogLevel FATAL</span><br><span class="line">  ForwardAgent <span class="built_in">yes</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>把上一步的配置文件保存到 <code>~/.ssh/config</code> 中。</p></li><li><p>在 <code>~/.bashrc</code> 中添加以下命令，如果没有则手动创建。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">alias</span> vagrantssh=<span class="string">&#x27;ssh&#x27;</span></span><br></pre></td></tr></table></figure><p> 这样配置之后，执行 <code>vagrantssh test-ubuntu18-04</code> 即可快速登录虚拟机终端。但是如果虚拟机比较多则可能无法记住全部的主机名，那接下来需要为 vagrantssh 添加自动补全的功能，通过 TAB 自动补全虚拟机名称。</p></li><li><p>创建 <code>~/vagrant-bash-completion-ext.sh</code> 文件，并保存以下命令到 <code>~/vagrant-bash-completion-ext.sh</code> 文件。</p><blockquote><p><strong>注意：</strong> 脚本第二行 export pwd 之后需要修改为 <code>Vagrantfile</code> 文件所在目录的绝对路径，本示例是把 <code>Vagrantfile</code> 保存在 ~&#x2F;vagrant 目录中。</p></blockquote> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">export</span> <span class="built_in">pwd</span>=$( <span class="built_in">echo</span> ~/vagrant )</span><br><span class="line"><span class="function"><span class="title">__pwdln</span></span>() &#123;</span><br><span class="line">   pwdmod=<span class="string">&quot;<span class="variable">$&#123;pwd&#125;</span>/&quot;</span></span><br><span class="line">   itr=0</span><br><span class="line">   until [[ -z <span class="string">&quot;<span class="variable">$pwdmod</span>&quot;</span> ]];<span class="keyword">do</span></span><br><span class="line">      itr=$((<span class="variable">$itr</span>+<span class="number">1</span>))</span><br><span class="line">      pwdmod=<span class="string">&quot;<span class="variable">$&#123;pwdmod#*/&#125;</span>&quot;</span></span><br><span class="line">   <span class="keyword">done</span></span><br><span class="line">   <span class="built_in">echo</span> -n $((<span class="variable">$itr</span>-<span class="number">1</span>))</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="title">__vagrantinvestigate</span></span>() &#123;</span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;pwd&#125;</span>/.vagrant&quot;</span> -o -d <span class="string">&quot;<span class="variable">$&#123;pwd&#125;</span>/.vagrant&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;pwd&#125;</span>/.vagrant&quot;</span></span><br><span class="line">      <span class="built_in">return</span> 0</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">      pwdmod2=<span class="string">&quot;<span class="variable">$&#123;pwd&#125;</span>&quot;</span></span><br><span class="line">      <span class="keyword">for</span> (( i=2; i&lt;=$(__pwdln); i++ ));<span class="keyword">do</span></span><br><span class="line">         pwdmod2=<span class="string">&quot;<span class="variable">$&#123;pwdmod2%/*&#125;</span>&quot;</span></span><br><span class="line">         <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;pwdmod2&#125;</span>/.vagrant&quot;</span> -o -d <span class="string">&quot;<span class="variable">$&#123;pwdmod2&#125;</span>/.vagrant&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;pwdmod2&#125;</span>/.vagrant&quot;</span></span><br><span class="line">            <span class="built_in">return</span> 0</span><br><span class="line">         <span class="keyword">fi</span></span><br><span class="line">      <span class="keyword">done</span></span><br><span class="line">   <span class="keyword">fi</span></span><br><span class="line">   <span class="built_in">return</span> 1</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="title">_hostlist</span></span>() &#123;</span><br><span class="line">    cur=<span class="string">&quot;<span class="variable">$&#123;COMP_WORDS[COMP_CWORD]&#125;</span>&quot;</span></span><br><span class="line">    prev=<span class="string">&quot;<span class="variable">$&#123;COMP_WORDS[COMP_CWORD-1]&#125;</span>&quot;</span></span><br><span class="line">    vagrant_state_file=$(__vagrantinvestigate) || <span class="built_in">return</span> 1</span><br><span class="line">    <span class="built_in">local</span> vm_list=$(find <span class="string">&quot;<span class="variable">$&#123;vagrant_state_file&#125;</span>/machines&quot;</span> -mindepth 1 -maxdepth 1 -<span class="built_in">type</span> d -<span class="built_in">exec</span> <span class="built_in">basename</span> &#123;&#125; \;)</span><br><span class="line">    COMPREPLY=($(compgen -W <span class="string">&quot;<span class="variable">$&#123;vm_list&#125;</span>&quot;</span> -- <span class="variable">$&#123;cur&#125;</span>))</span><br><span class="line">    <span class="built_in">return</span> 0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>再在 <code>~/.bashrc</code> 文件中添加以下命令，注意命令中有个点。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">complete -F _hostlist vagrantssh</span><br><span class="line">. ~/vagrant-bash-completion-ext.sh</span><br></pre></td></tr></table></figure></li><li><p>以上配置完成后，退出 ssh 终端然后重启打开终端，执行 vagrantssh 即可自动补全 虚拟机名称。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> vagrant </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vagrant </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher Calico BGP 对接 F5</title>
      <link href="/f5/f5-calico-bgp-ingress/"/>
      <url>/f5/f5-calico-bgp-ingress/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/f5/f5-calico-bgp-ingress/" target="_blank" title="https://www.xtplayer.cn/f5/f5-calico-bgp-ingress/">https://www.xtplayer.cn/f5/f5-calico-bgp-ingress/</a></p><p>本文档基于 <code>BIGIP-16.0.0.1-0.0.3.ALL-virtual-edition</code> 版本编写，启用 F5 的 BGP 功能需要高级路由模块证书。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929230947925.png" class="" title="image-20200929230947925"><p>如果仅用于测试， <a href="https://www.f5.com/zh_cn/trials/big-ip-virtual-edition">https://www.f5.com/zh_cn/trials/big-ip-virtual-edition</a> 此处可以申请试用版的序列号。</p><h2 id="Rancher-Calico-BGP-配置"><a href="#Rancher-Calico-BGP-配置" class="headerlink" title="Rancher Calico BGP 配置"></a>Rancher Calico BGP 配置</h2><h3 id="K8S-集群部署"><a href="#K8S-集群部署" class="headerlink" title="K8S 集群部署"></a>K8S 集群部署</h3><p>目前 Rancher UI 部署 Calico 网络驱动暂不支持自定义高级配置，对于某些高级功能，需要通过手动修改网络驱动工作负载配置来实现。但是，在下一次升级 K8S 集群的时候可能会导致网络驱动自定义配置丢失。</p><p>如果对网络组件有特殊配置，建议在通过 Rancher UI 创建集群的时候，编辑集群 <strong>YAML</strong> 文件或者 RKE 配置文件中设置网络驱动为 <code>none</code></p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">network:</span></span><br><span class="line">   <span class="attr">plugin:</span> <span class="string">none</span></span><br></pre></td></tr></table></figure><img src="/f5/f5-calico-bgp-ingress/image-20200929214159368.png" class="" title="image-20200929214159368"><p>以此来关闭集群原有网络驱动部署功能，在集群创建好之后通过手动部署网络驱动，部署 <strong>YAML</strong> 文件见附件。</p><p>自定义集群，因为没有部署网络驱动会有以下的错误提示，这属于正常现象。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929215141618.png" class="" title="image-20200929215141618"><h3 id="安装-kubectl-和-calicoctl-工具"><a href="#安装-kubectl-和-calicoctl-工具" class="headerlink" title="安装 kubectl 和 calicoctl 工具"></a>安装 kubectl 和 calicoctl 工具</h3><ol><li><p>kubectl 工具可以在 <a href="http://mirror.cnrancher.com/">http://mirror.cnrancher.com/</a> 进行下载，然后把 <strong>kubectl</strong> 拷贝到 <code>/usr/local/bin/kubectl</code>，并给与执行权限： <code>chmod +x /usr/local/bin/kubectl</code>；</p></li><li><p>访问 <a href="https://github.com/projectcalico/calicoctl/releases">https://github.com/projectcalico/calicoctl/releases</a> 去下载 <strong>calicoctl</strong>，把 calicoctl 拷贝到 <code>/usr/local/bin/calicoctl</code>，并给执行权限：<code>chmod +x /usr/local/bin/calicoctl</code>；</p></li><li><p>创建 kubectl 配置目录：<code>mkdir -p ~/.kube/</code></p></li><li><p>配置 calicoctl</p><p>执行以下命令，然后重新登录终端。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span>&gt;&gt;~/.profile&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">export DATASTORE_TYPE=kubernetes</span></span><br><span class="line"><span class="string">export KUBECONFIG=~/.kube/config</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>拷贝 kubeconfig 文件</p><p>当集群安装完成后，在集群页面点击 <code>Kubeconfig File</code> 并拷贝文件，然后将文件保存在 <code>~/.kube/config</code> 文件中。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929215410843.png" class="" title="image-20200929215410843"></li><li><p>kubeconfig 配置好之后，执行 <code>kubectl get no</code> 和 <code>calicoctl get no</code> 分别可以看到以下内容：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@vagrant2:~<span class="comment"># kubectl  get no</span></span><br><span class="line">NAME       STATUS     ROLES                      AGE     VERSION</span><br><span class="line">vagrant1   NotReady   controlplane,etcd,worker   10m     v1.18.8</span><br><span class="line">vagrant2   NotReady   worker                     8m27s   v1.18.8</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@vagrant2:~<span class="comment"># calicoctl get no</span></span><br><span class="line">NAME</span><br><span class="line">vagrant1</span><br><span class="line">vagrant2</span><br></pre></td></tr></table></figure><blockquote><p>因为没有网络驱动，所以状态为 NotReady。</p><p>以上步骤建议在所有节点均执行配置。</p></blockquote></li></ol><h3 id="网络驱动部署"><a href="#网络驱动部署" class="headerlink" title="网络驱动部署"></a>网络驱动部署</h3><ol><li><p>拷贝附件的 <code>calicotemplate.yml</code> 和  <code>BGPConfiguration.yml</code> 到配置好 <code>kubectl 和 calicoctl</code> 的任意一台主机上。</p></li><li><p>然后执行 <code>kubectl apply -f calicotemplate.yml</code> 和 <code>calicoctl create -f BGPConfiguration.yml</code>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@vagrant2:~<span class="comment"># kubectl apply -f calicotemplate.yml</span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/calico-node created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/calico-node created</span><br><span class="line">configmap/calico-config created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created</span><br><span class="line">daemonset.apps/calico-node created</span><br><span class="line">serviceaccount/calico-kube-controllers created</span><br><span class="line">serviceaccount/calico-node created</span><br><span class="line">deployment.apps/calico-kube-controllers created</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@vagrant2:~<span class="comment"># calicoctl create -f BGPConfiguration.yml</span></span><br><span class="line">Successfully created 2 resource(s)</span><br><span class="line">root@vagrant2:~<span class="comment">#</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong>：<code>calicotemplate.yml</code> 中的 <code>CALICO_IPV4POOL_CIDR</code> 和 <code>BGPConfiguration.yml</code> 中的 <code>asNumber</code> 需要根据实际做调整。</p></blockquote></li><li><p>回到集群首页，可以看到集群已运行正常，节点也运行正常。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929221332992.png" class="" title="image-20200929221332992"><img src="/f5/f5-calico-bgp-ingress/image-20200929221341770.png" class="" title="image-20200929221341770"></li></ol><h3 id="Calico-网络验证"><a href="#Calico-网络验证" class="headerlink" title="Calico 网络验证"></a>Calico 网络验证</h3><ol><li><p>在节点上执行 <code>route -n</code> 查看路由表，可以看到目标网段的网关均为对应节点的节点 IP，数据转发接口为主机的 <strong>eth0</strong>。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929221844878.png" class="" title="image-20200929221844878"><img src="/f5/f5-calico-bgp-ingress/image-20200929221915114.png" class="" title="image-20200929221915114"></li><li><p>在节点 1 找一个 Pod IP ，然后在节点 2 去 ping，验证是否可以 ping 通。</p></li><li><p>通过 calicoctl 查看 bgpPeer 和 bgpConfiguration 配置。</p></li><li><p>在每个节点上执行 <code>calicoctl node status</code> 检查节点状态。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@vagrant1:~<span class="comment">#  calicoctl node status</span></span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+----------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line">|  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |              INFO              |</span><br><span class="line">+----------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line">| 192.168.50.132 | node-to-node mesh | up    | 14:02:48 | Established                    |</span><br><span class="line">| 192.168.50.176 | global            | start | 14:28:14 | Active Socket: Host is         |</span><br><span class="line">|                |                   |       |          | unreachable                    |</span><br><span class="line">+----------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line"></span><br><span class="line">IPv6 BGP status</span><br><span class="line">No IPv6 peers found.</span><br><span class="line"></span><br><span class="line">root@vagrant1:~<span class="comment">#</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@vagrant2:~<span class="comment"># calicoctl node status</span></span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+----------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line">|  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |              INFO              |</span><br><span class="line">+----------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line">| 192.168.50.120 | node-to-node mesh | up    | 14:02:44 | Established                    |</span><br><span class="line">| 192.168.50.176 | global            | start | 14:27:51 | Connect Socket: Host is        |</span><br><span class="line">|                |                   |       |          | unreachable                    |</span><br><span class="line">+----------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line"></span><br><span class="line">IPv6 BGP status</span><br><span class="line">No IPv6 peers found.</span><br><span class="line"></span><br><span class="line">root@vagrant2:~<span class="comment">#</span></span><br></pre></td></tr></table></figure><blockquote><p>192.168.50.176 是通过 BGPConfiguration.yml 添加的 BGPPeer，现在未配置 F5 BGP，所以无法连接。</p></blockquote></li></ol><h2 id="F5-配置"><a href="#F5-配置" class="headerlink" title="F5 配置"></a>F5 配置</h2><h3 id="创建-vlan-虚拟接口"><a href="#创建-vlan-虚拟接口" class="headerlink" title="创建 vlan 虚拟接口"></a>创建 vlan 虚拟接口</h3><p>为了与 K8S 集群通信，这里需要创建一个虚拟接口。</p><ol><li><p>依次访问 <code>network|vlans</code>，点击 <code>vlans list</code> 旁边的加号（➕）。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929122519775.png" class="" title="image-20200929122519775"></li><li><p>配置接口参数</p><ul><li><p>Name：接口名称，随意设置；</p></li><li><p>Description： 可选；</p></li><li><p>Tag： 如果在 <code>Interface|Tagging</code> 中选择了 <code>Tagged</code>， 那么这里设置的 <code>tag</code> 将会附加到选择的接口上，默认 4094 （可选）；</p></li><li><p>Interfaces：在 <code>Interface</code> 中选择一个网卡接口，然后在 <code>Tagging</code> 中选择是否打上 <code>vlan tag</code>，最后点击 <code>Add</code>；</p></li><li><p>其他参数可保持默认，最后点击最下边的完成（finished）。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929131429554.png" class="" title="image-20200929131429554"></li></ul></li></ol><h3 id="创建-Self-IPs"><a href="#创建-Self-IPs" class="headerlink" title="创建 Self IPs"></a>创建 Self IPs</h3><p>上一步中创建了虚拟 vlan 接口，接下来给接口添加一个用于与 K8S 集群通信的 ip。</p><ol><li><p>依次访问 <code>network|Self IPs</code>，点击旁边的加号（➕）；</p></li><li><p>配置参数：</p><ul><li><p>Name：名称，随意设置；</p></li><li><p>IP Address：根据实际情况设置；</p></li><li><p>Netmask：根据实际情况设置；</p></li><li><p>VLAN&#x2F;Tunnel：选择上一步骤中创建的 vlan 虚拟接口；</p></li><li><p>Port Lockdown：选择允许所有；</p></li><li><p>其他保持默认，最后点击最下边的完成（finished）。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929140043704.png" class="" title="image-20200929140043704"></li></ul></li></ol><blockquote><p><strong>注意：</strong>因为 K8S 容器环境的特殊性，<code>Self IPs</code> 建议使用与 K8S 集群节点 IP 相同网段的 IP。如果 <code>Self IPs</code> 与 K8S 集群节点 IP 不是相同网段，则需要处理中间环节的网络路由问题。</p></blockquote><h3 id="配置-Route-Domains"><a href="#配置-Route-Domains" class="headerlink" title="配置 Route Domains"></a>配置 Route Domains</h3><ol><li><p>依次访问 <code>network|Route Domains</code>；</p><img src="/f5/f5-calico-bgp-ingress/image-20200929140447916.png" class="" title="image-20200929140447916"></li><li><p>点击  <code>name</code> 为 0 的条目；</p></li><li><p>在 <code>Dynamic Routing Protocols</code> 中，点击 <code>BGP</code>，然后点击 <strong>向左箭头号</strong>；</p><img src="/f5/f5-calico-bgp-ingress/image-20200929140728638.png" class="" title="image-20200929140728638"></li><li><p>其他参数保持默认，最后点击 <code>update</code>。</p></li></ol><h3 id="F5-启用-BGP"><a href="#F5-启用-BGP" class="headerlink" title="F5 启用 BGP"></a>F5 启用 BGP</h3><ol><li><p>ssh 登录 F5 设备；</p></li><li><p>运行以下命令启用 BGP；</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 访问 IMI Shell</span></span><br><span class="line">imish</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换到启用模式</span></span><br><span class="line"><span class="built_in">enable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入配置模式</span></span><br><span class="line">config terminal</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置路由 bgp，编号为 64512</span></span><br><span class="line">router bgp 64512</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 BGP 对等组</span></span><br><span class="line">neighbor calico-k8s peer-group</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将对等组指定为 BGP 邻居</span></span><br><span class="line">neighbor calico-k8s remote-as 64512</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加所有的对等节点，包括所有的 K8S 节点</span></span><br><span class="line">neighbor 192.168.50.120 peer-group calico-k8s</span><br><span class="line">neighbor 192.168.50.132 peer-group calico-k8s</span><br><span class="line">redistribute kernel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存配置</span></span><br><span class="line">write</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出</span></span><br><span class="line">end</span><br></pre></td></tr></table></figure></li></ol><h2 id="整体状态检查与测试"><a href="#整体状态检查与测试" class="headerlink" title="整体状态检查与测试"></a>整体状态检查与测试</h2><h3 id="F5-状态检查"><a href="#F5-状态检查" class="headerlink" title="F5 状态检查"></a>F5 状态检查</h3><ol><li><p>访问 IMI Shell</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">imish</span><br></pre></td></tr></table></figure></li><li><p>查看 BGP 配置及连接状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">show ip bgp neighbors</span><br></pre></td></tr></table></figure><img src="/f5/f5-calico-bgp-ingress/image-20200929151327826.png" class="" title="image-20200929151327826"><img src="/f5/f5-calico-bgp-ingress/image-20200929151403935.png" class="" title="image-20200929151403935"><blockquote><p><strong>注意</strong>：如果是先配置 F5，因为还未配置 calico，<code>remote router ID</code> 会为 <code>0.0.0.0</code>，<code>BGP state</code> 为 <code>Active</code> 状态。</p></blockquote></li><li><p>查看 F5 路由表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">f5-test.local[0]&gt;show ip route</span><br><span class="line"></span><br><span class="line">Codes: K - kernel, C - connected, S - static, R - RIP, B - BGP</span><br><span class="line">       O - OSPF, IA - OSPF inter area</span><br><span class="line">       N1 - OSPF NSSA external <span class="built_in">type</span> 1, N2 - OSPF NSSA external <span class="built_in">type</span> 2</span><br><span class="line">       E1 - OSPF external <span class="built_in">type</span> 1, E2 - OSPF external <span class="built_in">type</span> 2</span><br><span class="line">       i - IS-IS, L1 - IS-IS level-1, L2 - IS-IS level-2, ia - IS-IS inter area</span><br><span class="line">       * - candidate default</span><br><span class="line"></span><br><span class="line">B       10.42.105.192/26 [200/0] via 192.168.50.132, rancher-k8s-bgp, 00:08:22</span><br><span class="line">B       10.42.192.0/26 [200/0] via 192.168.50.120, rancher-k8s-bgp, 00:08:22</span><br><span class="line">C       127.0.0.1/32 is directly connected, lo</span><br><span class="line">C       127.1.1.254/32 is directly connected, tmm</span><br><span class="line">C       192.168.50.0/24 is directly connected, rancher-k8s-bgp</span><br><span class="line"></span><br><span class="line">Gateway of last resort is not <span class="built_in">set</span></span><br><span class="line">f5-test.local[0]&gt;</span><br></pre></td></tr></table></figure></li></ol><h3 id="calico-节点状态检查"><a href="#calico-节点状态检查" class="headerlink" title="calico 节点状态检查"></a>calico 节点状态检查</h3><ul><li><p>在 K8S 节点上，再次执行 <code>calicoctl node status</code> 查看节点状态。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@vagrant2:~<span class="comment"># calicoctl node status</span></span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line">|  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |    INFO     |</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line">| 192.168.50.120 | node-to-node mesh | up    | 14:02:51 | Established |</span><br><span class="line">| 192.168.50.176 | global            | up    | 14:43:10 | Established |</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line"></span><br><span class="line">IPv6 BGP status</span><br><span class="line">No IPv6 peers found.</span><br><span class="line"></span><br><span class="line">root@vagrant2:~<span class="comment">#</span></span><br></pre></td></tr></table></figure> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@vagrant1:~<span class="comment"># calicoctl node status</span></span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line">|  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |    INFO     |</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line">| 192.168.50.132 | node-to-node mesh | up    | 14:02:53 | Established |</span><br><span class="line">| 192.168.50.176 | global            | up    | 14:43:10 | Established |</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line"></span><br><span class="line">IPv6 BGP status</span><br><span class="line">No IPv6 peers found.</span><br><span class="line"></span><br><span class="line">root@vagrant1:~<span class="comment">#</span></span><br></pre></td></tr></table></figure></li><li><p>可以看到 PEER 之间均为连接状态。</p></li></ul><h3 id="连通性测试"><a href="#连通性测试" class="headerlink" title="连通性测试"></a>连通性测试</h3><ol><li><p>在 K8S 集群中创建一个 Pod 或者选择一个已有的 Pod，获取 Pod IP。</p><img src="/f5/f5-calico-bgp-ingress/image-20200929225936293.png" class="" title="image-20200929225936293"></li><li><p>在 F5 上去做 ping 测试。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@f5-test:Active:Standalone] config <span class="comment"># ping 10.42.105.194 -c 6</span></span><br><span class="line">PING 10.42.105.194 (10.42.105.194) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.42.105.194: icmp_seq=1 ttl=63 time=3.16 ms</span><br><span class="line">64 bytes from 10.42.105.194: icmp_seq=2 ttl=63 time=2.53 ms</span><br><span class="line">64 bytes from 10.42.105.194: icmp_seq=3 ttl=63 time=2.99 ms</span><br><span class="line">64 bytes from 10.42.105.194: icmp_seq=4 ttl=63 time=3.01 ms</span><br><span class="line">64 bytes from 10.42.105.194: icmp_seq=5 ttl=63 time=3.49 ms</span><br><span class="line">64 bytes from 10.42.105.194: icmp_seq=6 ttl=63 time=2.44 ms</span><br><span class="line"></span><br><span class="line">--- 10.42.105.194 ping statistics ---</span><br><span class="line">6 packets transmitted, 6 received, 0% packet loss, time 5002ms</span><br><span class="line">rtt min/avg/max/mdev = 2.443/2.941/3.496/0.365 ms</span><br><span class="line">[root@f5-test:Active:Standalone] config <span class="comment">#</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><ol><li><a href="/download/BGPConfiguration.yml">calicotemplate.yml</a></li><li><a href="/download/BGPConfiguration.yml">BGPConfiguration.yml</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> f5 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BGP </tag>
            
            <tag> calico </tag>
            
            <tag> F5 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Etcd 原理阐释及部署设置的最佳实践</title>
      <link href="/etcd/etcd-best-practices/"/>
      <url>/etcd/etcd-best-practices/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/etcd/etcd-best-practices/" target="_blank" title="https://www.xtplayer.cn/etcd/etcd-best-practices/">https://www.xtplayer.cn/etcd/etcd-best-practices/</a></p><h2 id="介-绍"><a href="#介-绍" class="headerlink" title="介 绍"></a>介 绍</h2><p>Etcd 是一个开源的分布式键值存储，它由 CoreOS 团队开发，现在由 Cloud Native Computing Foundation 负责管理。这个词的发音是“et-cee-dee”，表示在多台机器上分发 Unix 系统的“&#x2F;etc”目录，其中包含了大量的全局配置文件。它是许多分布式系统的主干，为跨服务器集群存储数据提供可靠的方式。它适用于各种操作系统，包括 Linux、BSD 和 OS X。</p><p>Etcd 具有下面这些属性：</p><ul><li>完全复制：集群中的每个节点都可以使用完整的存档</li><li>高可用性：Etcd 可用于避免硬件的单点故障或网络问题</li><li>一致性：每次读取都会返回跨多主机的最新写入</li><li>简单：包括一个定义良好、面向用户的 API（gRPC）</li><li>安全：实现了带有可选的客户端证书身份验证的自动化 TLS</li><li>快速：每秒 10000 次写入的基准速度</li><li>可靠：使用 Raft 算法实现了存储的合理分布</li></ul><h3 id="Etcd-的工作原理"><a href="#Etcd-的工作原理" class="headerlink" title="Etcd 的工作原理"></a>Etcd 的工作原理</h3><p>在理解 Etcd 的工作机制之前，我们先定义三个关键概念：leaders、elections 以及 terms。在一个基于 Raft 的系统中，集群使用 election 为给定的 term 选择 leader。</p><p>Leader 处理所有需要集群一致协商的客户端请求。不需要一致协商的请求（如读取）可以由任何集群成员处理。Leader 负责接受新的更改，将信息复制到 follower 节点，并在 follower 验证接受后提交更改。每个集群在任何给定的时间内只能有一个 leader。</p><p>如果 leader 挂了或者不再响应了，那么其他节点将在预定的时间超时之后开启一个新的 term 来创建新 election。每个节点维护一个随机的 election 计时器，该计时器表示节点在调用新的 election 以及选择自己作为候选之前需要等待的时间。</p><p>如果节点在超时发生之前没有收到 leader 的消息，则该节点将通过启动新的 term、将自己标记为候选，并要求其他节点投票来开始新的 election。每个节点投票给请求其投票的第一个候选。如果候选从集群中的大多数节点处获得了选票，那么它就成为了新的 leader。但是，如果存在多个候选且获得了相同数量的选票，那么现有的 election term 将在没有 leader 的情况下结束，而新的 term 将以新的随机选举计时器开始。</p><p>如上所述，任何更改都必须连接到 leader 节点。Etcd 没有立即接受和提交更改，而是使用 Raft 算法确保大多数节点都同意更改。Leader 将提议的新值发送到集群中的每个节点。然后，节点发送一条消息确认收到了新值。如果大多数节点确认接收，那么 leader 提交新值，并向每个节点发送将该值提交到日志的消息。这意味着每次更改都需要得到集群节点的仲裁才能提交。</p><h3 id="Kubernetes-中的-Etcd"><a href="#Kubernetes-中的-Etcd" class="headerlink" title="Kubernetes 中的 Etcd"></a>Kubernetes 中的 Etcd</h3><p>自从 2014 年成为 Kubernetes 的一部分以来，Etcd 社区呈现指数级的增长。CoreOS、谷歌、Redhat、IBM、思科、华为等等均是 Etcd 的贡献成员。其中 AWS、谷歌云平台和 Azure 等大型云提供商成功在生产环境中使用了 Etcd。</p><p>Etcd 在 Kubernetes 中的工作是为分布式系统安全存储关键数据。它最著名的是 Kubernetes 的主数据存储，用于存储配置数据、状态和元数据。由于 Kubernetes 通常运行在几台机器的集群上，因此它是一个分布式系统，需要 Etcd 这样的分布式数据存储。</p><p>Etcd 使得跨集群存储数据和监控更改变得更加容易，它允许来自 Kubernetes 集群的任何节点读取和写入数据。Kubernetes 使用 Etcd 的 watch 功能来监控系统实际（actual）状态或期望（desired）状态的变化。如果这两个状态不同，Kubernetes 会做出一些改变来调和这两个状态。kubectl 命令的每次读取都从 Etcd 存储的数据中检索，所做的任何更改（kubectl apply）都会在 Etcd 中创建或更新条目，每次崩溃都会触发 etcd 中值的修改。</p><h3 id="部署以及硬件建议"><a href="#部署以及硬件建议" class="headerlink" title="部署以及硬件建议"></a>部署以及硬件建议</h3><p>出于测试或开发目的，Etcd 可以在笔记本电脑或轻量云上运行。然而，在生产环境中运行 Etcd 集群时，我们应该考虑 Etcd 官方文档提供的指导。它为良好稳定的生产部署提供了一个良好的起点。需要留意的是：</p><ul><li>Etcd 会将数据写入磁盘，因此强烈推荐使用 SSD</li><li>始终使用奇数个集群数量，因为需要通过仲裁来更新集群的状态</li><li>出于性能考虑，集群通常不超过 7 个节点</li></ul><p>让我们回顾一下在 Kubernetes 中部署 Etcd 集群所需的步骤。之后，我们将演示一些基本的 CLI 命令以及 API 调用。我们将结合 Kubernetes 的概念（如 StatefulSets 和 PersistentVolume）进行部署。</p><h2 id="预先准备"><a href="#预先准备" class="headerlink" title="预先准备"></a>预先准备</h2><p>在继续 demo 之前，我们需要准备：</p><ul><li>一个谷歌云平台的账号：免费的 tier 应该足够了。你也可以选择大多数其他云提供商，只需进行少量修改即可。</li><li>一个运行 Rancher 的服务器</li></ul><h2 id="启动-Rancher-实例"><a href="#启动-Rancher-实例" class="headerlink" title="启动 Rancher 实例"></a>启动 Rancher 实例</h2><p>在你控制的服务器上启动 Rancher 实例。这里有一个非常简单直观的入门指南：<a href="https://rancher.com/quick-start/">https://rancher.com/quick-start/</a></p><h2 id="使用-Rancher-部署-GKE-集群"><a href="#使用-Rancher-部署-GKE-集群" class="headerlink" title="使用 Rancher 部署 GKE 集群"></a>使用 Rancher 部署 GKE 集群</h2><p>参照本指南使用 Rancher 在 GCP 账户中设置和配置 Kubernetes 集群：</p><p><a href="https://rancher.com/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/gke/">https://rancher.com/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/gke/</a></p><p>在运行 Rancher 实例的同一服务器上安装 Google Cloud SDK 以及 kubelet 命令。按照上面提供的链接安装 SDK，并通过 Rancher UI 安装 kubelet。</p><p>使用 gcloud init 和 gcloud auth login，确保 gcloud 命令能够访问你的 GCP 账户。</p><p>集群部署后，输入下面的命令检查基本的 kubectl 功能：</p><img src="/etcd/etcd-best-practices/640-20200921190224638.png" class="" title="img"><p>在部署 Etcd 集群（通过 kubectl 或在 Rancher 的 UI 中导入 YAML 文件）之前，我们需要配置一些项。在 GCE 中，默认的持久化磁盘是 pd-standard。我们将为 Etcd 部署配置 pd-ssd。这不是强制性的，不过根据 Etcd 的建议，SSD 是非常好的选择。查看此链接可以了解其他云提供商的存储类：</p><p><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a></p><p>让我们检查一下 GCE 提供的可用存储类。正如预期的那样，我们看到了一个默认的结果，叫做 standard：</p><img src="/etcd/etcd-best-practices/640-20200921190224628.png" class="" title="img"><p>应用下面这个 YAML 文件，更新 zone 的值来匹配你的首选项，这样我们就可以使用 SSD 存储了：</p><img src="/etcd/etcd-best-practices/640-20200921190224571.png" class="" title="img"><p>我们再一次检查，可以看到，除了默认 standard 类之外，ssd 也可以使用了：</p><img src="/etcd/etcd-best-practices/640-20200921190224605.png" class="" title="img"><p>现在我们可以继续部署 Etcd 集群了。我们将创建一个带有 3 个副本的 StatefulSet，每个副本都有一个 ssd storageClass 的专用卷。我们还需要部署两个服务，一个用于内部集群通信，一个用于通过 API 从外部访问集群。</p><p>在搭建集群时，我们需要将一些参数传递给 Etcd 二进制文件再到数据存储中。Listen-client-urls 和 listen-peer-urls 选项指定 Etcd 服务器用于接受传入连接的本地地址。指定 0.0.0.0 作为 IP 地址意味着 Etcd 将监听所有可用接口上的连接。Advertise-client-urls 和 initial-advertise-peer-urls 参数指定了在 Etcd 客户端或者其他 Etcd 成员联系 etcd 服务器时应该使用的地址。</p><p>下面的 YAML 文件定义了我们的两个服务以及 Etcd StatefulSe 图：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># etcd-sts.yaml---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">etcd-client</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">etcd-client</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">2379</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">2379</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">etcd</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">etcd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">2379</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">client</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">2380</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">peer</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">etcd</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">etcd</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">etcd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">serviceName:</span> <span class="string">etcd</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">etcd</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">etcd</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">etcd</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">quay.io/coreos/etcd:latest</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">2379</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">client</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">2380</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">peer</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/run/etcd</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">|</span>            <span class="string">PEERS=&quot;etcd-0=http://etcd-0.etcd:2380,etcd-1=http://etcd-1.etcd:2380,etcd-2=http://etcd-2.etcd:2380&quot;</span></span><br><span class="line">            <span class="string">exec</span> <span class="string">etcd</span> <span class="string">--name</span> <span class="string">$&#123;HOSTNAME&#125;</span> <span class="string">\</span></span><br><span class="line">              <span class="string">--listen-peer-urls</span> <span class="string">http://0.0.0.0:2380</span> <span class="string">\</span></span><br><span class="line">              <span class="string">--listen-client-urls</span> <span class="string">http://0.0.0.0:2379</span> <span class="string">\</span></span><br><span class="line">              <span class="string">--advertise-client-urls</span> <span class="string">http://$&#123;HOSTNAME&#125;.etcd:2379</span> <span class="string">\</span></span><br><span class="line">              <span class="string">--initial-advertise-peer-urls</span> <span class="string">http://$&#123;HOSTNAME&#125;:2380</span> <span class="string">\</span></span><br><span class="line">              <span class="string">--initial-cluster-token</span> <span class="string">etcd-cluster-1</span> <span class="string">\</span></span><br><span class="line">              <span class="string">--initial-cluster</span> <span class="string">$&#123;PEERS&#125;</span> <span class="string">\</span></span><br><span class="line">              <span class="string">--initial-cluster-state</span> <span class="string">new</span> <span class="string">\</span></span><br><span class="line">              <span class="string">--data-dir</span> <span class="string">/var/run/etcd/default.etcd</span></span><br><span class="line">  <span class="attr">volumeClaimTemplates:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">storageClassName:</span> <span class="string">ssd</span></span><br><span class="line">      <span class="attr">accessModes:</span> [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure><p>输入下列命令应用 YAML：</p>  <img src="/etcd/etcd-best-practices/640-20200921190224735.png" class="" title="img"><p>在应用 YAML 文件后，我们可以在 Rancher 提供的不同选项卡中定义资源：</p>  <img src="/etcd/etcd-best-practices/640-20200921190224692.jpeg" class="" title="img">  <img src="/etcd/etcd-best-practices/640-20200921190224638.jpeg" class="" title="img">  <img src="/etcd/etcd-best-practices/640-20200921190224655.jpeg" class="" title="img"><h2 id="与-Etcd-交互"><a href="#与-Etcd-交互" class="headerlink" title="与 Etcd 交互"></a>与 Etcd 交互</h2><p>与 Etcd 交互的方式主要有两种：使用 etcdctl 命令或者直接通过 RESTful API。我们将简要介绍这两种方法，不过你还可以通过访问这里和这里的完整文档找到更加深入的信息和示例。</p><p>Etcdctl 是一个和 Etcd 服务器交互的命令行接口。它可以用于执行各种操作，如设置、更新或者删除键、验证集群健康情况、添加或删除 Etcd 节点以及生成数据库快照。默认情况下，etcdctl 使用 v2 API 与 Etcd 服务器通信来获得向后兼容性。如果希望 etcdctl 使用 v3 API 和 Etcd 通信，则必须通过 ETCDCTL_API 环境变量将版本设置为 3。</p><p>对于 API，发送到 Etcd 服务器的每一个请求都是一个 gRPC 远程过程调用。这个 gRPC 网关提供一个 RESTful 代理，能够将 HTTP&#x2F;JSON 请求转换为 gRPC 消息。</p><p>让我们来找到 API 调用所需的外部 IP：</p><img src="/etcd/etcd-best-practices/640-20200921190224689.png" class="" title="img"><p>我们应该还能找到 3 个 pods 的名称，这样我们就可以使用 etcdctl 命令：</p><img src="/etcd/etcd-best-practices/640-20200921190224679.png" class="" title="img"><p>我们检查 Etcd 版本。为此我们可以使用 API 或 CLI（v2 和 v3）.根据你选择的方法， 输出的结果将略有不同。</p><p>使用此命令可直接与 API 联系：</p><img src="/etcd/etcd-best-practices/640-20200921190224678.png" class="" title="img"><p>检查 API 版本为 v2 的 etcdctl 客户端，输入：</p><img src="/etcd/etcd-best-practices/640-20200921190224733.png" class="" title="img"><p>检查 API 版本为 v3 的 etcdctl 客户端，则输入：</p><img src="/etcd/etcd-best-practices/640-20200921190224766.png" class="" title="img"><p>接下来，列出集群成员，就像我们上面做的那样：</p><img src="/etcd/etcd-best-practices/640-20200921190224717.png" class="" title="img">  <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;members&quot;</span><span class="string">:</span>[&#123;<span class="string">&quot;id&quot;</span><span class="string">:&quot;2e80f96756a54ca9&quot;</span>,<span class="string">&quot;name&quot;</span><span class="string">:&quot;etcd-0&quot;</span>,<span class="string">&quot;peerURLs&quot;</span><span class="string">:</span>[<span class="string">&quot;http://etcd-0.etcd:2380&quot;</span>],<span class="string">&quot;clientURLs&quot;</span><span class="string">:</span>[<span class="string">&quot;http://etcd-0.etcd:2379&quot;</span>]&#125;,&#123;<span class="string">&quot;id&quot;</span><span class="string">:&quot;7fd61f3f79d97779&quot;</span>,<span class="string">&quot;name&quot;</span><span class="string">:&quot;etcd-1&quot;</span>,<span class="string">&quot;peerURLs&quot;</span><span class="string">:</span>[<span class="string">&quot;http:// etcd-1.etcd:2380&quot;</span>],<span class="string">&quot;clientURLs&quot;</span><span class="string">:</span>[<span class="string">&quot;http://etcd-1.etcd:2379&quot;</span>]&#125;,&#123;<span class="string">&quot;id&quot;</span><span class="string">:&quot;b429c86e3cd4e077&quot;</span>,<span class="string">&quot;name&quot;</span><span class="string">:&quot;etcd-2&quot;</span>,<span class="string">&quot;peerURLs&quot;</span><span class="string">:</span>[<span class="string">&quot;http://etcd-2.etcd:2380&quot;</span>],<span class="string">&quot;clientURLs&quot;</span><span class="string">:</span>[<span class="string">&quot;http://etcd-2.etcd:2379&quot;</span>]&#125;]&#125;</span><br></pre></td></tr></table></figure><ul><li>V2 版本的 etcdctl：</li></ul><img src="/etcd/etcd-best-practices/640-20200921190224772.png" class="" title="img"><ul><li>V3 版本的 etcdctl：</li></ul><img src="/etcd/etcd-best-practices/640-20200921190224773.png" class="" title="img"><h2 id="在-Etcd-中设置和检索值"><a href="#在-Etcd-中设置和检索值" class="headerlink" title="在 Etcd 中设置和检索值"></a>在 Etcd 中设置和检索值</h2><p>下面我们将介绍的最后一个示例是在 Etcd 集群中全部 3 个 pods 上创建一个键并检查其值。然后我们会杀掉 leader，在我们的场景中是 etcd-0，然后来看看新的 leader 是如何选出来的。最后，在集群恢复之后，我们将在所有成员上验证之前创建的键的值。我们会看到，没有数据丢失的情况发生，集群只是换了一个 leader 而已。</p><p>我们可以通过输入下面的命令来验证集群最初是健康的：</p><img src="/etcd/etcd-best-practices/640-20200921190224765.png" class="" title="img"><p>接下来，验证当前 leader。最后一个字段表明 etcd-0 是我们集群中的 leader：</p><img src="/etcd/etcd-best-practices/640-20200921190224774.png" class="" title="img"><p>使用该 API，我们将创建一个名为 message 的键并给它分配一个值，请记住在下面的命令中把 IP 地址替换为你在集群中通过下面命令获取到的地址：</p><img src="/etcd/etcd-best-practices/640-20200921190224787.png" class="" title="img"><p>无论查询哪个成员，键都具有相同的值。这帮助我们验证值是否已经复制到其他节点并提交到日志。</p><img src="/etcd/etcd-best-practices/640-20200921190224805-0686144.png" class="" title="img"><h2 id="演示高可用性和恢复"><a href="#演示高可用性和恢复" class="headerlink" title="演示高可用性和恢复"></a>演示高可用性和恢复</h2><p>接下来，我们可以杀掉 Etcd 集群 leader。这样我们可以看到新的 leader 是如何选出的，以及集群如何从 degraded 状态中恢复过来。删除与上面发现的 Etcd leader 相关的 pod：</p><img src="/etcd/etcd-best-practices/640-20200921190224805.png" class="" title="img"><p>下面我们检查一下集群的健康情况：</p><img src="/etcd/etcd-best-practices/640-20200921190224841.png" class="" title="img"><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">failed to check the health of member 2e80f96756a54ca9 on http://etcd-0.etcd:2379: Get http://etcd-0.etcd:2379/health: dial tcp: lookup etcd-0.etcd on 10.15.240.10:53:</span> <span class="literal">no</span> <span class="string">such</span> <span class="string">host</span></span><br><span class="line"><span class="attr">member 2e80f96756a54ca9 is unreachable:</span> [<span class="string">http://etcd-0.etcd:2379</span>] <span class="string">are</span> <span class="string">all</span> <span class="string">unreachable</span></span><br><span class="line"><span class="attr">member 7fd61f3f79d97779 is healthy:</span> <span class="string">got</span> <span class="string">healthy</span> <span class="string">result</span> <span class="string">from</span> <span class="string">http://etcd-1.etcd:2379</span></span><br><span class="line"><span class="attr">member b429c86e3cd4e077 is healthy:</span> <span class="string">got</span> <span class="string">healthy</span> <span class="string">result</span> <span class="string">from</span> <span class="string">http://etcd-2.etcd:2379cluster</span> <span class="string">is</span> <span class="string">degraded</span></span><br><span class="line"><span class="string">command</span> <span class="string">terminated</span> <span class="string">with</span> <span class="string">exit</span> <span class="string">code</span> <span class="number">5</span></span><br></pre></td></tr></table></figure><p>上面的信息表明，由于失去了 leader 节点，集群出于 degrade 状态。</p><p>一旦 Kubernetes 通过启动新实例来响应删除的 pod，Etcd 集群应该就恢复过来了：</p><img src="/etcd/etcd-best-practices/640-20200921190224862.png" class="" title="img"><p>输入下面指令，我们可以看到新的 leader 已经选出来了：</p><img src="/etcd/etcd-best-practices/640-20200921190224857.png" class="" title="img"><p>在我们的例子中，etcd-1 节点被选为 leader</p><p>如果我们再一次检查 message 键的值，会发现没有出现数据的损失：</p><img src="/etcd/etcd-best-practices/640-20200921190224833.png" class="" title="img"><h2 id="结-论"><a href="#结-论" class="headerlink" title="结 论"></a>结 论</h2><p>Etcd 是一种非常强大、高可用以及可靠的分布式键值存储，专门为特定用例设计。常见的例子包括存储数据哭连接细节、缓存设置、特性标记等等。它被设计成顺序一致的，因此在整个集群中每个事件都是以相同的顺序存储。</p><p>我们了解了如何在 Rancher 的帮助下用 Kubernetes 建立并运行 etcd 集群。之后，我们能够使用一些基本的 Etcd 命令进行操作。为了更好的了解这个项目，键是如何组织的，如何为键设置 TTLs，或者如何备份所有数据，参考官方的 Etcd repo 会是个不错的选择：</p><p><a href="https://github.com/etcd-io/etcd/tree/master/Documentation">https://github.com/etcd-io/etcd/tree/master/Documentation</a></p>]]></content>
      
      
      <categories>
          
          <category> etcd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> etcd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速使用 Prometheus 监控 Etcd</title>
      <link href="/etcd/monitor-etcd-quickly-using-prometheus/"/>
      <url>/etcd/monitor-etcd-quickly-using-prometheus/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/etcd/monitor-etcd-quickly-using-prometheus/" target="_blank" title="https://www.xtplayer.cn/etcd/monitor-etcd-quickly-using-prometheus/">https://www.xtplayer.cn/etcd/monitor-etcd-quickly-using-prometheus/</a></p><p>本教程是<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247490456&idx=1&sn=6805596d29e299fae5c670efc126beb0&chksm=e83a9d5edf4d14480a1532ac5d89bbef8eaa76ba597563f09b4c998bc1e15b46ee53b171b08e&scene=21#wechat_redirect">「Etcd 超全解：原理阐释及部署设置的最佳实践」</a>的后续文章。在本文中，我们将安装一个 Etcd 集群并使用 Prometheus 和 Grafana 配置监控，以上这些操作我们都通过 Rancher 进行。</p><p>我们将看到在不需要依赖的情况下充分利用 Rancher 的应用商店实现这一目标是如此容易。在本文中，我们将<strong>不需要</strong>：</p><ul><li>为运行 kubectl 专门配置交互框，并指向 Kubernetes 集群</li><li>kubectl 的使用知识，因为我们可以使用 Rancher UI 完成这一切</li><li>安装&#x2F;配置 Helm binary</li></ul><p>Demo 前期准备</p><p><strong>你将需要：</strong></p><ul><li>一个 Google 云平台的账号（免费的即可）。任意其他云也可以。</li><li>Rancher v2.4.7（撰写本文时的最新版本）</li><li>运行在 GKE（版本为 1.16.3-gke.1）上的 Kubernetes 集群（在 EKS 或 AKS 上运行也可以）</li></ul><h2 id="启动一个-Rancher-实例"><a href="#启动一个-Rancher-实例" class="headerlink" title="启动一个 Rancher 实例"></a>启动一个 Rancher 实例</h2><p>首先，启动你的 Rancher 实例。你可以访问以下链接查看快速启动指南：</p><p><a href="https://www.rancher.cn/quick-start/">https://www.rancher.cn/quick-start/</a></p><h2 id="使用-Rancher-部署一个-GKE-集群"><a href="#使用-Rancher-部署一个-GKE-集群" class="headerlink" title="使用 Rancher 部署一个 GKE 集群"></a>使用 Rancher 部署一个 GKE 集群</h2><p>使用 Rancher 来设置和配置一个 Kubernetes 集群，你可以查看相关文档：</p><p><a href="https://docs.rancher.cn/docs/rancher2/cluster-provisioning/production/_index/">https://docs.rancher.cn/docs/rancher2/cluster-provisioning/production/_index/</a></p><h2 id="部署-etcd、Prometheus-和-Grafana"><a href="#部署-etcd、Prometheus-和-Grafana" class="headerlink" title="部署 etcd、Prometheus 和 Grafana"></a>部署 etcd、Prometheus 和 Grafana</h2><p>我们可以利用 Rancher 的应用商店来安装所有的软件。应用商店是 Helm chart 的集合，它可以让用户能够轻松地重复部署这些应用程序。</p><p>当我们的集群启动并运行后，让我们选择为其创建的 Default 项目，在 Apps 选项卡中，点击【Launch】。</p><ol><li><p>我们要安装的第一个应用是 etcd-operator。保留它预先填充的所有默认值，并确保你也启用了 etcd 集群的创建（为了 demo 的简单性，我们取消选择 etcd Backup Operator 和 etcd Restore Operator）。</p></li><li><p>Operator 的作用是观察、分析和行动。它使用 Kubernetes API 来观察当前集群的状态。如果运行状态与所需状态之间有任何差异，它就会发现并修复它们。</p><p>例如，假设我们正在运行一个有三个成员的 etcd 集群。如果发生了一些事情，其中一个成员倒下了，Operator 会观察到这一点。它根据所需的状态做一个差异，然后根据差异恢复丢失的成员。于是，我们在没有人为干预的情况下拥有了一个健康的集群。</p></li></ol><img src="/etcd/monitor-etcd-quickly-using-prometheus/640.png" class="" title="img"><p>要安装 Prometheus 和 Grafana，请激活 Rancher 中集成的集群监控支持。从 【全局】视图中，选择你要配置的集群，并选择【工具】→【监控】以启用它。为了允许对 Grafana 的更改持久化，请确保为 Grafana 和 Prometheus 启用持久化存储。如果你没有设置任何持久化存储，可以了解一下<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247493937&idx=1&sn=b63c19b151cc696bf21105a582e53c49&chksm=e8396ff7df4ee6e1f66eca13c5c75539add6cc94b7562b4b981cb9a5b9e27886c55570fc11a8&scene=21#wechat_redirect">Longhorn</a>，这是 Kubernetes 的云端分布式块存储。</p><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136087.png" class="" title="img"><p>当一切都在安装时，你可以探索一些选项卡。检查工作负载（Pods、Deployments、DaemonSet）或创建的服务的进度。</p><p>让我们连接到一个 etcd Pod，以便使用一些基本的 etcdctl 命令（更多细节可以查阅<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247490456&idx=1&sn=6805596d29e299fae5c670efc126beb0&chksm=e83a9d5edf4d14480a1532ac5d89bbef8eaa76ba597563f09b4c998bc1e15b46ee53b171b08e&scene=21#wechat_redirect">之前的文章</a>）。选择一个 Pod，点击它的垂直省略号（3 个竖点）菜单按钮，然后选择<strong>Execute Shell</strong>。</p><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136045.png" class="" title="img"><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136095.png" class="" title="img"><h3 id="配置-Prometheus-以及-Grafana"><a href="#配置-Prometheus-以及-Grafana" class="headerlink" title="配置 Prometheus 以及 Grafana"></a>配置 Prometheus 以及 Grafana</h3><p>监控 etcd 集群的最佳和最简单的方法之一是使用 Prometheus 和 Grafana。让我们登录到 Grafana——在集群概览中点击任意 Grafana 图标即可登录。</p><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136046.png" class="" title="img"><p>Grafana 已经预先配置了 Prometheus 作为数据源，包含几个可视化集群状态的 dashboard。</p><p>登录 Grafana 为 etcd 添加一个仪表盘。默认的用户名和密码都是 “admin”（第一次登录时，会提示你更改）。然后用 id3070 导入默认的 etcd 仪表盘模板。点击加载，然后剩下的步骤就是选择 Prometheus 数据源。</p><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136117.png" class="" title="img"><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136116.png" class="" title="img"><p>我们已经成功导入了 dashboard，我们可以看到各种图表，但是没有数据显示。为什么呢？我们已经运行了 Prometheus，并且 Grafana 也与之集成。可问题是我们没有告诉 Prometheus 去收集与我们 etcd 集群相关的目标。</p><p>让我们回到 Rancher 中去解决这个问题。进入系统项目，点击【资源】标签下的导入 YAML。然后将以下资源导入到 cattle-prometheus 命名空间中：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceMonitor</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">source:</span> <span class="string">rancher-monitoring</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">etcd</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">cattle-prometheus</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">endpoints:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="string">client</span></span><br><span class="line">  <span class="attr">namespaceSelector:</span></span><br><span class="line">    <span class="attr">matchNames:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">etcd-operator</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">etcd</span></span><br></pre></td></tr></table></figure><p>我们如何验证我们的新配置是有效的，并且确保 Prometheus 正在执行它的工作呢？我们需要对其进行检查，请点击系统项目的 Apps 选项卡，并点击 cluster-monitoring 应用程序中的第二个&#x2F;index&#x2F;.html 链接。</p><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136129.png" class="" title="img"><p>这将打开 Prometheus web UI 界面。在界面中，进入 Graphs 并手动执行一些查询，如果有数据显示，那么就说明我们的设置已经完成。</p><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136195.png" class="" title="img"><p>我们需要做的最后一件事是检查 Grafana 并且查看我们有相关的数据图表。</p><img src="/etcd/monitor-etcd-quickly-using-prometheus/640-20200921185136195-0685496.png" class="" title="img"><h3 id="卸载应用程序和集群"><a href="#卸载应用程序和集群" class="headerlink" title="卸载应用程序和集群"></a>卸载应用程序和集群</h3><p>要清理我们在本文中使用的资源，我们只需要在全局层级中，选择我们的集群并点击【Delete】。</p><p>通过这样做，除了为 Prometheus 创建的持久化存储外，所有的东西都将被删除。我们需要从我们的云提供商控制台来处理这个问题。</p><p>当然，我们可以只从 Rancher 中执行清理，但步骤略有不同。</p><ul><li>disable monitoring：在全局层级，导航到集群，选择工具→监控并点击【Disable】按钮。</li><li>移除持久化存储：进入 “系统项目”→”资源”→”工作负载”→”卷”；选择你的卷并单击 “删除”。</li><li>删除集群：在全局层级选择集群并删除它。</li></ul><h2 id="总-结"><a href="#总-结" class="headerlink" title="总 结"></a>总 结</h2><p>在这个 demo 中，我们看到了如何使用 Rancher 安装 Etcd（使用 etcd-operator），Prometheus 和 Grafana。所有的集成都是开箱即用的：我们只需要添加一些东西就可以完成所有的配置。Rancher 还提供了所有所需的可视性，在必要的情况下，可以方便地进行故障排除。</p>]]></content>
      
      
      <categories>
          
          <category> etcd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> etcd </tag>
            
            <tag> Prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher k8s 资源管理</title>
      <link href="/kubernetes/rancher-k8s-resource-management/"/>
      <url>/kubernetes/rancher-k8s-resource-management/</url>
      
        <content type="html"><![CDATA[<h2 id="cgroup-简介"><a href="#cgroup-简介" class="headerlink" title="cgroup 简介"></a>cgroup 简介</h2><p>控制群组 (<em>control group</em>)(简称<em>cgroup</em>) 是 Linux kernel 的一项功能。从使用的角度看，cgroup 是一个目录树结构，目录中可以创建多层子目录，这些目录称为<code>**cgroup 目录**</code>。在一些场景中为了体现层级关系，还会称为<code>**cgroup 子目录**</code>。</p><p>通过 cgroup 可对 CPU 时间片、系统内存、磁盘 IO、网络带宽等资源进行精细化控制，以便硬件资源可以在应用程序和用户间智能分配，从而增加整体效率。</p><p>通过将 cgroup 层级与 <strong>systemd</strong> 单位树绑定，可以把资源管理设置从进程级别转换至应用程序级别。因此，可以使用<strong>systemctl</strong>指令或通过修改 <strong>systemd</strong> 服务配置文件来管理系统资源。更多关于 systemd 相关配置请查阅附件文档。</p><h3 id="Linux-Kernel-的-cgroup-资源管控器"><a href="#Linux-Kernel-的-cgroup-资源管控器" class="headerlink" title="Linux Kernel 的 cgroup 资源管控器"></a>Linux Kernel 的 cgroup 资源管控器</h3><p>cgroup 资源管控器也称为 cgroup 子系统，代表一种单一资源：如 CPU 时间片或者内存。</p><p>Linux kernel 提供一系列资源管控器，由 <strong>systemd</strong> 自动挂载。如需了解目前已挂载的资源管控器列表，可通过查看文件: <strong>&#x2F;proc&#x2F;cgroups</strong>，或使用 <strong>lssubsys</strong> 工具查看。</p><p>在 <code>centos7+、Redhat7+、Ubuntu16+</code> 等 以 systemd 作为进程初始化工具的系统中，默认 cgroup 由 <strong>systemd</strong> 自动挂载到 <code>/sys/fs/cgroup</code> 目录下。在 <code>/sys/fs/cgroup</code> 目录下将自动创建以下 cgroup 子系统：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@alihost01:/sys/fs/cgroup<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 15 root root 380 Jun 21 14:45 ./</span><br><span class="line">drwxr-xr-x 11 root root   0 Jul  1 18:07 ../</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 blkio/</span><br><span class="line">lrwxrwxrwx  1 root root  11 Jun 21 14:45 cpu -&gt; cpu,cpuacct/</span><br><span class="line">lrwxrwxrwx  1 root root  11 Jun 21 14:45 cpuacct -&gt; cpu,cpuacct/</span><br><span class="line">drwxr-xr-x  2 root root  40 Jun 21 14:45 cpuacct,cpu/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 cpu,cpuacct/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 cpuset/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 devices/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 freezer/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 hugetlb/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jun 21 18:14 memory/</span><br><span class="line">lrwxrwxrwx  1 root root  16 Jun 21 14:45 net_cls -&gt; net_cls,net_prio/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 net_cls,net_prio/</span><br><span class="line">lrwxrwxrwx  1 root root  16 Jun 21 14:45 net_prio -&gt; net_cls,net_prio/</span><br><span class="line">drwxr-xr-x  2 root root  40 Jun 21 14:45 net_prio,net_cls/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 perf_event/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 pids/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 systemd/</span><br></pre></td></tr></table></figure><p>目前 Linux 支持下面 12 种常用的 cgroup 子系统：</p><ul><li><a href="https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt">cpu</a> (since Linux 2.6.24; CONFIG_cgroup_SCHED)<br>用来限制 cgroup 的 CPU 使用率。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpuacct.txt">cpuacct</a> (since Linux 2.6.24; CONFIG_cgroup_CPUACCT)<br>统计 cgroup 的 CPU 的使用率。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt">cpuset</a> (since Linux 2.6.24; CONFIG_CPUSETS)<br>绑定 cgroup 到指定 CPUs 和 NUMA 节点。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt">memory</a> (since Linux 2.6.25; CONFIG_MEMCG)<br>统计和限制 cgroup 的内存的使用率，包括 process memory, kernel memory, 和 swap。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt">devices</a> (since Linux 2.6.26; CONFIG_cgroup_DEVICE)<br>限制 cgroup 创建(mknod)和访问设备的权限。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/freezer-subsystem.txt">freezer</a> (since Linux 2.6.28; CONFIG_cgroup_FREEZER)<br>suspend 和 restore 一个 cgroup 中的所有进程。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/net_cls.txt">net_cls</a> (since Linux 2.6.29; CONFIG_cgroup_NET_CLASSID)<br>将一个 cgroup 中进程创建的所有网络包加上一个 classid 标记，用于<a href="http://man7.org/linux/man-pages/man8/tc.8.html">tc</a>和 iptables。 只对发出去的网络包生效，对收到的网络包不起作用。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/blkio-controller.txt">blkio</a> (since Linux 2.6.33; CONFIG_BLK_cgroup)<br>限制 cgroup 访问块设备的 IO 速度。</li><li><a href="https://www.kernel.org/doc/Documentation/perf-record.txt">perf_event</a> (since Linux 2.6.39; CONFIG_cgroup_PERF)<br>对 cgroup 进行性能监控</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/net_prio.txt">net_prio</a> (since Linux 3.3; CONFIG_cgroup_NET_PRIO)<br>针对每个网络接口设置 cgroup 的访问优先级。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/hugetlb.txt">hugetlb</a> (since Linux 3.5; CONFIG_cgroup_HUGETLB)<br>限制 cgroup 的 huge pages 的使用量。</li><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/pids.txt">pids</a> (since Linux 4.3; CONFIG_cgroup_PIDS)<br>限制一个 cgroup 及其子 cgroup 中的总进程数。</li></ul><blockquote><p><strong>注意：</strong> <code>/sys/fs/cgroup/systemd</code> 目录非 cgroup 子系统，是 systemd 维护的自己使用的的层级结构。</p></blockquote><h3 id="cgroup-层级结构"><a href="#cgroup-层级结构" class="headerlink" title="cgroup 层级结构"></a>cgroup 层级结构</h3><p>以 memory 子系统为例，其他子系统类似。</p><p>进入 <code>/sys/fs/cgroup/memory</code> 目录，可以看到以下内容：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">dr-xr-xr-x  5 root root   0 Jul  2 09:01 ./</span><br><span class="line">drwxr-xr-x 13 root root 340 Jul  2 08:59 ../</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.clone_children</span><br><span class="line">--w--w--w-  1 root root   0 Jul  2 09:01 cgroup.event_control</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.procs</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 cgroup.sane_behavior</span><br><span class="line">drwxr-xr-x  2 root root   0 Jul  2 08:59 init.scope/</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.failcnt</span><br><span class="line">--w-------  1 root root   0 Jul  2 09:01 memory.force_empty</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.failcnt</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.kmem.slabinfo</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.tcp.failcnt</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.tcp.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.tcp.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.kmem.tcp.usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.kmem.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.max_usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.memsw.failcnt</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.memsw.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.memsw.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.memsw.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.move_charge_at_immigrate</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.numa_stat</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.oom_control</span><br><span class="line">----------  1 root root   0 Jul  2 09:01 memory.pressure_level</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.soft_limit_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.stat</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.swappiness</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 08:59 memory.use_hierarchy</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 notify_on_release</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 release_agent</span><br><span class="line">drwxr-xr-x 64 root root   0 Jul  2 08:59 system.slice/</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 tasks</span><br><span class="line">drwxr-xr-x  3 root root   0 Jul  2 08:59 user.slice/</span><br></pre></td></tr></table></figure><ul><li><p>根 cgroup</p><p>虽然 <code>/sys/fs/cgroup/memory</code> 属于 <strong>cgroup 子系统</strong>，但它也是当前子系统的 <strong>根 cgroup</strong>，所以在 <strong>&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory</strong> 中可以看到相应的配置文件，并且根 cgroup 不支持资源限制。</p></li><li><p>子 cgroup</p><p>在 <strong>&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory</strong> 目录（<strong>根 cgroup</strong>）中看到的文件夹，比如 <strong>system.slice</strong>，叫做 <strong>子 cgroup</strong>。</p><p>进入 system.slice 目录可以发现，子 cgroup 与根 cgroup 拥有相同的配置文件。如果要限制内存最大使用量，可通过配置子 cgroup 的 <code>memory.limit_in_bytes</code> 进行限制，默认为 <code>-1</code> 不做限制。子 cgroup 中还可以创建子 cgroup，达到资源的更细化控制。</p></li><li><p>创建子 cgroup</p><p>可以在 <code>/sys/fs/cgroup/memory</code> 目录中，通过 <strong>mkdir</strong> 来创建文件夹，从而创建子 cgroup，子 cgroup 中配置文件将会自动生成。<code>通过 mkdir 创建的子 cgroup 是临时的，重启主机后子 cgroup 将会丢失</code>。</p></li><li><p>实践</p><p>不建议对顶级 cgroup 做资源限制，这样会导致其他子 cgroup 资源限制受影响。建议根据应用类型创建不同的子 cgroup，把应用绑定在不同的子 cgroup 中。</p></li><li><p>配置文件说明</p><ul><li><p>cgroup.clone_children<br>这个文件只对 cpuset 子系统有影响，当该文件的内容为 1 时，新创建的 cgroup 将会继承父 cgroup 的配置，即从父 cgroup 里面拷贝配置文件来初始化新 cgroup，可以参考<a href="https://lkml.org/lkml/2010/7/29/368">这里</a></p></li><li><p>cgroup.procs<br>当前 cgroup 中的所有进程 PID，可以手动把进程 PID 添加到当前 cgroup.procs 中，以实现进程与 cgroup 绑定。 系统不保证进程 PID 是顺序排列的，且进程 PID 有可能重复</p></li><li><p>cgroup.sane_behavior<br>具体功能不详，可以参考<a href="https://lkml.org/lkml/2014/7/2/684">这里</a>。</p></li><li><p>notify_on_release<br>该文件的内容为 1 时，当 cgroup 退出时（不再包含任何进程和子 cgroup），将调用 release_agent 里面配置的命令。新 cgroup 被创建时将默认继承父 cgroup 的这项配置。</p></li><li><p>release_agent<br>里面包含了 cgroup 退出时将会执行的命令，系统调用该命令时会将相应 cgroup 的相对路径当作参数传进去。 注意：这个文件只会存在于 root cgroup 下面，其他 cgroup 里面不会有这个文件。</p></li><li><p>tasks<br>当前 cgroup 中的所有线程 ID，当 PID 被添加到当前的 cgroup.procs 时，会自动把对应的线程添加到当前 tasks 中。系统不保证线程 ID 是顺序排列的。</p><p><strong>更多文件说明可以查看附件文档的附录 A。</strong></p></li></ul></li></ul><h2 id="原生-Docker-容器资源限制"><a href="#原生-Docker-容器资源限制" class="headerlink" title="原生 Docker 容器资源限制"></a>原生 Docker 容器资源限制</h2><blockquote><p><strong>注意：</strong> 以下内容均以 memory 子系统为例</p></blockquote><ol><li><p>通过执行 <code>docker run -tid --memory 1G alpine</code> 命令运行一个容器；</p></li><li><p>docker 在创建容器时，会在每个 cgroup 子系统的根 cgroup 目录下自动创建 <code>docker</code> cgroup 目录。比如：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">dr-xr-xr-x  6 root root   0 Jul  2 09:08 ./</span><br><span class="line">drwxr-xr-x 13 root root 340 Jul  2 08:59 ../</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.clone_children</span><br><span class="line">--w--w--w-  1 root root   0 Jul  2 09:01 cgroup.event_control</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.procs</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 cgroup.sane_behavior</span><br><span class="line">drwxr-xr-x  3 root root   0 Jul  2 09:08 docker/</span><br><span class="line">drwxr-xr-x  2 root root   0 Jul  2 09:01 init.scope/</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.failcnt</span><br><span class="line">--w-------  1 root root   0 Jul  2 09:01 memory.force_empty</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.failcnt</span><br></pre></td></tr></table></figure></li><li><p>容器的组成</p><p> 从宿主机角度看，一个运行的完整容器是以进程形式存在。运行一个容器后通过 <code>ps -ef</code> 可以查看进程相互依赖关系：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root       1311   1230  0 09:01 pts/0    00:00:00 -bash</span><br><span class="line">root       1406      1  0 09:08 ?        00:00:06 /usr/bin/dockerd -H fd://</span><br><span class="line">root       1414   1406  0 09:08 ?        00:00:12 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root       2727   1414  0 09:30 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/    47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9 -addr</span><br><span class="line">root       2762   2727  0 09:30 pts/0    00:00:00 /bin/sh</span><br><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker<span class="comment">#</span></span><br></pre></td></tr></table></figure><p> 2762 为容器中的进程，其父进程 2727 为 docker-containerd-shim 进程，docker-containerd-shim 由 docker-containerd 管理，其父进程为 1414（docker-containerd）。</p></li><li><p>容器与 cgroup 的绑定关系</p><p> 通过执行 <code>cd /sys/fs/cgroup/memory; systemd-cgls</code> 可以查看到主机上所有应用进程与 cgroup 的关系。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">├─memory</span><br><span class="line">│ ├─docker</span><br><span class="line">│ │ └─47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9</span><br><span class="line">│ │   └─2762 /bin/sh</span><br><span class="line">│ ├─system.slice</span><br><span class="line">│ │ ├─mdadm.service</span><br><span class="line">│ │ ├─rsyslog.service</span><br><span class="line">│ │ │ └─920 /usr/sbin/rsyslogd -n</span><br><span class="line">│ │ ├─docker.service</span><br><span class="line">│ │ │ ├─1406 /usr/bin/dockerd -H fd://</span><br><span class="line">│ │ │ ├─1414 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">│ │ │ └─2727 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9 -address /var/run/docker/containerd/docker</span><br><span class="line">│ │ ├─lxcfs.service</span><br><span class="line">│ │ │ └─892 /usr/bin/lxcfs /var/lib/lxcfs/</span><br><span class="line">│ │ └─acpid.service</span><br><span class="line">│ │   └─905 /usr/sbin/acpid</span><br><span class="line">│ └─user.slice</span><br></pre></td></tr></table></figure><p> 通过 <code>systemd-cgls</code> 可以发现，docker-containerd-shim 被绑定在 <strong>system.slice cgroup</strong> 中，可以理解为它是属于 dcoker 系统级的进程，而容器中应用进程则绑定到 <strong>docker cgroup</strong> 中。</p></li><li><p>进入 <code>docker</code> cgroup 目录，可以看到以容器 ID 为名创建的 cgroup 目录和其他配置文件；</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 3 root root 0 Jul  2 09:30 ./</span><br><span class="line">dr-xr-xr-x 6 root root 0 Jul  2 09:08 ../</span><br><span class="line">drwxr-xr-x 2 root root 0 Jul  2 09:40 47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9/</span><br><span class="line">-rw-r--r-- 1 root root 0 Jul  2 09:11 cgroup.clone_children</span><br><span class="line">--w--w--w- 1 root root 0 Jul  2 09:11 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root root 0 Jul  2 09:11 cgroup.procs</span><br><span class="line">-rw-r--r-- 1 root root 0 Jul  2 09:11 memory.failcnt</span><br></pre></td></tr></table></figure></li><li><p>在 <code>docker</code> cgroup 目录中，执行 <code>cat memory.limit_in_bytes</code></p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker<span class="comment"># cat memory.limit_in_bytes</span></span><br><span class="line">9223372036854771712</span><br><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker<span class="comment">#</span></span><br></pre></td></tr></table></figure><p> 可以看到结果是一个很大的值，表示不受限制。</p></li><li><p>查看内存限制值</p><p> 进入 <strong>容器 ID</strong> 命名的 cgroup 目录，执行 <code>cat memory.limit_in_bytes</code></p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9<span class="comment"># cat memory.limit_in_bytes</span></span><br><span class="line">1073741824</span><br><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9<span class="comment">#</span></span><br></pre></td></tr></table></figure><p> 因为在启动容器的时候有添加内存限制参数 <code>--memory 1G</code>，所以这里的值正好是 1G。</p></li><li><p>验证当前 cgroup 绑定的进程 PID</p><ul><li><p>验证应用进程 PID</p><p>在当前子 cgroup 目录下，执行 <code>cat cgroup.procs</code></p></li></ul> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9<span class="comment"># cat cgroup.procs</span></span><br><span class="line">2762</span><br><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9<span class="comment">#</span></span><br></pre></td></tr></table></figure><p> 得到的进程 PID 正好是容器中应用进程的 PID，从而验证了第 4 步 <strong>容器与 cgroup 的绑定关系</strong> 中返回的结果。</p><ul><li><p>验证 docker-containerd-shim PID</p><p>多运行几个容器，然后通过 <code>ps -ef</code> 确定 docker-containerd-shim 的 PID 号。</p><p>接着执行 <code>cat /sys/fs/cgroup/memory/system.slice/docker.service/cgroup.procs</code> 查看 PID 号。</p><p>可以发现 docker-containerd-shim 进程 PID 全部被绑定到 docker.service 子 cgroup 中。</p></li></ul></li><li><p><strong>总结</strong></p><ul><li><p>在使用 docker 创建容器时，会自动在每个 <strong>cgroup 子系统</strong> 中创建 <code>docker</code> cgroup 目录，<code>docker</code> cgroup 目录默认不做资源限制。然后会以容器 ID 为名称，在 <code>docker</code> cgroup 目录下创建 <strong>子 cgroup 目录</strong>，假设 <code>docker run</code> 的时候添加了内存限制参数（–memory ），那么会 <strong>自动修改以容器 ID 命名的 cgroup 目录</strong> 下的 <code>memory.limit_in_bytes</code> 文件，这样就实现了对单个容器最大内存使用的限制。</p></li><li><p>因为是以容器 ID 为名称创建的子 cgroup 目录，所以所有的子 cgroup 不会冲突，并且对一个子 cgroup 做资源限制，不会影响其他子 cgroup。<strong>每个容器中的所有进程将会绑定在以当前容器 ID 命名的子 cgroup 组中，容器 docker-containerd-shim 进程 PID 将会统一绑定在 <code>/sys/fs/cgroup/memory/system.slice/docker.service</code></strong> cgroup 组中。</p></li><li><p>根据上面的逻辑，如果想控制所有容器进程内存使用量不超过预期值，那么只需要配置 <strong>docker</strong> cgroup 资源使用量即可。</p></li></ul> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /sys/fs/cgroup/memory/docker</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;10G&#x27;</span> &gt; memory.limit_in_bytes</span><br></pre></td></tr></table></figure></li></ol><h2 id="Kubernets-Pod-资源限制"><a href="#Kubernets-Pod-资源限制" class="headerlink" title="Kubernets Pod 资源限制"></a>Kubernets Pod 资源限制</h2><ol><li><p>kubelet 在创建 Pod 时，如果没有通过参数 <code>--cgroup-root</code>（参数使用后续讲解）指定顶级 cgroup 组，那么会自动在 cgroup 子系统根 cgroup 中创建 <code>kubepods</code> cgroup 目录。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  2 16:29 ./</span><br><span class="line">drwxr-xr-x 15 root root 380 Jul  2 16:29 ../</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.clone_children</span><br><span class="line">--w--w--w-  1 root root   0 Jul  2 09:01 cgroup.event_control</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.procs</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 cgroup.sane_behavior</span><br><span class="line">drwxr-xr-x  8 root root   0 Jul  2 16:29 docker/</span><br><span class="line">drwxr-xr-x  2 root root   0 Jul  2 09:01 init.scope/</span><br><span class="line">drwxr-xr-x  4 root root   0 Jul  2 16:30 kubepods/</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.failcnt</span><br><span class="line">--w-------  1 root root   0 Jul  2 09:01 memory.force_empty</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.failcnt</span><br></pre></td></tr></table></figure></li><li><p>与原生 Docker 容器相似，通过执行 <code>cd /sys/fs/cgroup/memory; systemd-cgls</code> 查询 Pod 与 cgroup 绑定关系.</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory<span class="comment"># cd /sys/fs/cgroup/memory; systemd-cgls</span></span><br><span class="line">Working directory /sys/fs/cgroup/memory:</span><br><span class="line">├─docker</span><br><span class="line">│ ├─8f2e3c82eb801f692c889b9d6b84b1a7c245c3d782798ef60e1e23f58e5ed130</span><br><span class="line">│ │ └─5188 /usr/local/bin/etcd --peer-client-cert-auth --client-cert-auth --advertise-client-urls=https://1.1.1.128:2379,https://1.1.1.128:4001 --listen-client-urls=https://0.0.0.0:2379     --trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem --</span><br><span class="line">│ ├─38241e69344a759d5e1c54dc2f74c5d2323d123f896f03fdb80fb971d060ce17</span><br><span class="line">│ │ └─6291 kubelet --serialize-image-pulls=<span class="literal">false</span> --registry-qps=0 --allow-privileged=<span class="literal">true</span> --authentication-token-webhook=<span class="literal">true</span> --read-only-port=0 --cluster-domain=cluster.local     --kube-reserved=cpu=0.25,memory=2000Mi --cni-conf-dir=/etc/cni</span><br><span class="line">│ ├─0ada8f8d47dc91a9e07e4e533bab36f83eabd3d427c89b65be91e7302fbc30a9</span><br><span class="line">│ │ └─kube-proxy</span><br><span class="line">│ │   └─6818 kube-proxy --hostname-override=1.1.1.128 --kubeconfig=/etc/kubernetes/ssl/kubecfg-kube-proxy.yaml --v=2 --healthz-bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16</span><br><span class="line">├─system.slice</span><br><span class="line">│ ├─mdadm.service</span><br><span class="line">│ │ └─975 /sbin/mdadm --monitor --pid-file /run/mdadm/monitor.pid --daemonise --scan --syslog</span><br><span class="line">│ ├─rsyslog.service</span><br><span class="line">│ │ └─920 /usr/sbin/rsyslogd -n</span><br><span class="line">│ ├─docker.service</span><br><span class="line">│ │ ├─3465 /usr/bin/dockerd -H fd://</span><br><span class="line">│ │ ├─3474 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">│ │ ├─5170 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/8f2e3c82eb801f692c889b9d6b84b1a7c245c3d782798ef60e1e23f58e5ed130     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─5414 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/bb8280628aca338887460df574cb947f6045bc31dd8bbf107722aa7534f2c07d     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─5699 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/3d71cb60793053da0456db1882b87290877f477a6b2f4dd58244ed9c53b4a30a     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─5989 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/d58f02925fac52add81d765e2c34ea54ed59ddfc2cacc545e029a790e5344d76     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─6274 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/38241e69344a759d5e1c54dc2f74c5d2323d123f896f03fdb80fb971d060ce17     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─6800 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/0ada8f8d47dc91a9e07e4e533bab36f83eabd3d427c89b65be91e7302fbc30a9     -address /var/run/docker/containerd/docker-c</span><br><span class="line">└─kubepods</span><br><span class="line">  ├─burstable</span><br><span class="line">  │ ├─pod87a169c0-9ca3-11e9-a530-000c29fe6663</span><br><span class="line">  │ │ ├─57dcc6eb699c8a3ffbbe79ff5500a165dd200b7be6faec8f4298c4bea11a00db</span><br><span class="line">  │ │ │ └─8133 /pause</span><br><span class="line">  │ │ ├─082df6b4c1e5bc1778755061a0d9d3fac044c51720bdadd6b1218acb749ce7d0</span><br><span class="line">  │ │ │ └─8459 /kube-dns --domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2</span><br><span class="line">  │ │ ├─2d398ff9b51acf2abca20490c77650236b1b7960d7f8bb3f565d22b53aa45d40</span><br><span class="line">  │ │ │ ├─8555 /dnsmasq-nanny -v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=<span class="literal">true</span> -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1<span class="comment">#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/i</span></span><br><span class="line">  │ │ │ └─8643 /usr/sbin/dnsmasq -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1<span class="comment">#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053</span></span><br><span class="line">  │ │ └─c97d22a0f7d7ae03139f9409ae4e9f81d2a9731c3c2622fdebe1474109d6e7ed</span><br><span class="line">  │ │   └─8620 /sidecar --v=2 --logtostderr --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A</span><br><span class="line">  └─besteffort</span><br><span class="line">    └─pod8b3e50ed-9ca3-11e9-a530-000c29fe6663</span><br><span class="line">      ├─6ee56058fb4fd0a4a4aaa184af867eb51b053f26587e8e5614512f23c518fd3c</span><br><span class="line">      │ └─8942 /metrics-server --kubelet-insecure-tls --kubelet-preferred-address-types=InternalIP --logtostderr</span><br><span class="line">      └─5586cf42f50a251536af28e58e540b4192780063e1f6eacb7c9efbe0e0ca9856</span><br><span class="line">        └─8740 /pause</span><br></pre></td></tr></table></figure></li><li><p>可以确定，Pod 相关的进程会全部绑定到 <strong>kubepods cgroup</strong> 组中。</p></li><li><p>kubepods cgroup 又分为两个子 cgroup：<strong>burstable 和 besteffort。</strong></p></li></ol><p>  根据 Pod 配置的资源限制参数的不同，将自动将 Pod 中的进程绑定到不同的子 cgroup 中（在 QoS 服务质量管理部分将说明 Pod 绑定子 cgroup 的逻辑）。</p><p>  下文中说到的 K8S 集群资源预留，就是通过限制 <strong>docker cgroup</strong> 和 <strong>kubepods cgroup</strong> 的资源来达到整体的资源平衡。</p><h2 id="K8S-集群资源预留"><a href="#K8S-集群资源预留" class="headerlink" title="K8S 集群资源预留"></a>K8S 集群资源预留</h2><p>为了保证节点可以正常稳定运行，需要对节点资源进行合理的功能性划分与限制。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">      node-capacity (节点总资源)</span><br><span class="line">--------------------------------------------</span><br><span class="line">|     kube-reserved (kube 组件预留资源)       |</span><br><span class="line">|-----------------------------------------|</span><br><span class="line">|     system-reserved (系统服务预留资源）     |</span><br><span class="line">|-----------------------------------------|</span><br><span class="line">|     eviction-threshold (驱逐阈值)         |</span><br><span class="line">|-----------------------------------------|</span><br><span class="line">|      allocatable (Pod 可分配)              |</span><br><span class="line">--------------------------------------------</span><br></pre></td></tr></table></figure><p>根据以上表格，可以大致把节点总的资源划分为四小块。</p><ul><li><p><strong>Node-Capacity</strong></p><p>节点总的资源。</p></li><li><p><strong>Kube-Reserved</strong></p><p>给 k8s 系统组件预留的资源（包括 kubelet、kube-apiserver、kube-scheduler 等）。</p></li><li><p><strong>System-Reserved</strong></p><p>给 Linux 系统进程（kernel、sshd、Dockerd 等）预留的资源。</p></li><li><p><strong>Eviction-Threshold</strong></p><p>硬驱逐阈值，当节点可用内存值低于此值时，kubelet 会进行 Pod 的驱逐。</p></li><li><p><strong>Allocatable</strong></p><p>  真正可供节点上 Pod 使用的资源总容量，<code>kube-scheduler</code> 调度 Pod 时参考此值（kubectl describe node 可查看），节点上所有 Pods 的资源请求值（request）不超过 Allocatable。</p><p>  可通过一个公式计算可供 Pod 使用资源总量：</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Allocatable] = [Node-Capacity] - [Kube-Reserved] - [System-Reserved] - [Eviction-Threshold]</span><br></pre></td></tr></table></figure><p>  从公式可以看出，如果不设置 <code>kube-reserved、system-reserved、Hard-Eviction-Threshold</code>，节点上可以让 Pod 使用的资源总量等于节点的总资源量。如果不做资源划分与限制，Pod 与宿主机系统进程以及 k8s 系统组件争抢资源，导致主机资源耗尽出现异常，例如常见的 Docker 运行卡顿、ssh 无法连接、K8S 节点未就绪 (NotReady)。</p></li></ul><h3 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h3><p>kubelet 的启动参数中涉及资源预留的主要有以下几个：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--cgroups-per-qos</span><br><span class="line">--cgroup-driver</span><br><span class="line">--cgroup-root</span><br><span class="line">--enforce-node-allocatable</span><br><span class="line">--kube-reserved</span><br><span class="line">--kube-reserved-cgroup</span><br><span class="line">--system-reserved</span><br><span class="line">--system-reserved-cgroup</span><br><span class="line">--eviction-hard</span><br><span class="line">--eviction-soft</span><br><span class="line">--eviction-soft-grace-period</span><br><span class="line">--eviction-max-pod-grace-period</span><br><span class="line">--eviction-pressure-transition-period</span><br></pre></td></tr></table></figure><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li><p><code>--cgroups-per-qos</code></p><p>默认开启（true）</p><p>开启这个参数后，所有 Pod 的 cgroup 都将挂载到 kubelet 管理的 cgroup 目录下。要想启用节点资源限制，必须开启此参数。</p></li><li><p><code>--cgroup-driver</code></p><p>指定 kubelet 使用的 cgroup driver，默认 <code>cgroupfs</code>，可以选择 <code>systemd</code>，这个值需要与 Docker Runtime 所使用的 cgroup Driver 保持一致。rke1 创建的集群，因为是以容器运行的 kubelet 无法调用 <code>systemd</code> ，所以这个值一定要为 <code>cgroupfs</code>。</p></li><li><p><code>--cgroup-root</code></p><p>指定 Pod 使用的顶级 cgroup，默认为空，即把 Pod cgroup 挂载到根 cgroup 下，建议默认为空。这个 cgroup 组就是前面说到的 <strong>kubepods</strong> 组，默认 kubelet 会自动创建。如果不想使用默认的 cgroup，则需要先手动创建 cgroup，不然 kubelet 无法启动。</p></li><li><p><code>--kube-reserved</code></p><p>为 kube 系统组件预留的资源值，这个值只是用于调度计算，并不是实际限制。</p><p>示例配置：<code>--kube-reserved=cpu=1,memory=1Gi,ephemeral-storage=10Gi</code>。</p></li><li><p><code>--kube-reserved-cgroup</code></p><p>用于 kube 系统组件资源限制的 cgroup 组，如果要对 kube 系统组件做资源限制则需要配置这个 cgroup 组。rke 集群环境中，K8S 系统核心组件均以原生 docker 容器运行，那么其绑定的 cgroup 组为 <code>docker</code>。所以，如果是 rancher 创建的集群或者 RKE 创建的集群，这个参数需要配置为 <code>/docker</code></p><blockquote><p>注意，这里指定的 cgroup 及其子系统需要预先创建好，kubelet 不会自动创建。如果配置为 <code>/docker</code>，docker 已经自动创建 <code>docker cgroup</code>，则不需要再手动创建。</p></blockquote></li><li><p><code>--system-reserved</code></p><p>为宿主机系统组件预留的资源值，这个值只是用于调度计算，并不是实际限制。</p><p>示例配置：<code>--system-reserved=cpu=1,memory=1Gi,ephemeral-storage=10Gi</code>。</p></li><li><p><code>--system-reserved-cgroup</code></p><p>用于宿主机系统组件资源限制的 cgroup 组，如果要对宿主机系统组件做资源限制则需要配置这个 cgroup 组。建议不配置这个参数，它会使用默认的 cgroup 组。</p><blockquote><p>注意，这里指定的 cgroup 及其子系统需要预先创建好，kubelet 不会自动创建。</p></blockquote></li><li><p><code>--enforce-node-allocatable</code></p><p>这个参数可以理解为资源限制的开关。</p><p>前面说到的 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 仅用于调度计算，当配置了这两个参数后，也就告诉 <strong>调度器</strong> kube 系统组件和宿主机系统服务已经预留了一部分资源，<strong>调度器</strong> 会根据 <code>Allocatable</code> 计算公式计算出可供 Pod 调度的实际资源值。</p><p>但在未做 资源限制 的情况下，Pod 实际使用的资源是可以超过 Pod 可调度的资源值。如果要保证 Pod 实际使用不会超过 <code>Allocatable</code> 计算的实际可调度的资源，则需要通过 <code>--enforce-node-allocatable</code> 开启资源限制功能。</p><p><code>--enforce-node-allocatable</code> 支持三种类型进程的资源限制：<code>pods</code>，<code>kube-reserved</code>，<code>system-reserve</code>。</p><p>这三种类型可以同时选择或者只选择其中一种或者多种。资源限制 功能通过宿主机的 cgroup 来实现，不管选择哪一种类型，都需要指定对应的 cgroup 组，并且 cgroup 组需要预先创建好，kubelet 不会自动创建，如果配置的 cgroup 组不存在，则 kubelet 启动会报错。</p><p>假设设置 <code>--enforce-node-allocatable=pod,kube-reserved,system-reserved</code>，参数配置好之后 kubelet 将会把 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 配置的预留值写入 <code>--kube-reserved-cgroup</code> 和 <code>--system-reserved-cgroup</code> 对应 cgroup 组的 <code>memory.limit_in_bytes</code> 文件中。Pod 进程对应的 cgroup 组默认为 <code>kubepods</code>，执行 <code>cat /sys/fs/cgroup/memory/kubepods/memory.limit_in_bytes</code> 可以发现限制的值正好为 <code>node-capacity - kube-reserved - system-reserved</code>。</p><p>根据以上配置就把整个节点资源精确划分为三部分来使用。但是实际应用中发现，如果设置 <code>kube-reserved</code> 和 <code>system-reserve</code> 的值较小，当集群负载上去之后因为资源被限制导致 K8S 基础组件运行出现异常。如果 <code>kube-reserved</code> 和 <code>system-reserve</code> 设置的值较大，相应的 Pod 能使用的资源又会较少，而 K8S 系统组件也不会一直使用那么多资源，从而造成不必要的资源浪费。</p><p>在实际应用中，建议只限制 Pod 的资源，即配置 <code>--enforce-node-allocatable=pod</code>。这样就不会把 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 配置的预留值写入 <code>--kube-reserved-cgroup</code> 和 <code>--system-reserved-cgroup</code> 对应 cgroup 组的 <code>memory.limit_in_bytes</code> 文件中。</p></li><li><p><code>--eviction-soft</code></p><p>软驱逐阈值。</p><p>为了避免资源压力导致系统不稳定，当 <code>节点总资源 - kube 系统组件预留 - 宿主机系统服务预留 - Pod 实际使用资源</code> 小于软驱逐阈值的时候，kubelet 触发驱逐 Pod 的信号。</p><ul><li><p><code>--eviction-soft-grace-period</code></p><p>超过软驱逐阈值时并不会立即执行驱逐，它会等待 <code>--eviction-soft-grace-period</code> 配置的时间。在这段时间内，kubelet 每 <code>10s</code> 会重新获取监控数据，如果最后一次获取的数据仍然触发了驱逐阈值，最后才会执行 Pod 驱逐。</p></li><li><p><code>--eviction-max-pod-grace-period</code></p><p>强制驱逐 Pod 宽限期。</p><p>驱逐 Pod 时会先发送 <code>SIGTERM</code> 信号给 Pod，然后 Pod 再发送 SIGTERM 信号给容器并等待容器停止运行，默认等待 <code>30s</code>。如果在这段时间内容器没有退出，则 kubelet 会发送 <code>SIGKILL</code> 信号强制删除 Pod，通过 <code>--eviction-max-pod-grace-period</code> 可以指定 Pod 终止的宽限时间。我们也可以通过 <code>pod.Spec.TerminationGracePeriodSeconds</code> 配置 Pod 终止的宽限时间，Rancher 部署的应用默认为 <code>30S</code>。如果配置了 <code>pod.Spec.TerminationGracePeriodSeconds</code> 和 <code>--eviction-max-pod-grace-period</code>，将会取两者最小值作为 Pod 最终终止时间。</p></li></ul></li><li><p><code>--eviction-hard</code></p><p>硬驱逐阈值。</p><p>硬驱逐阈值与软驱逐阈值类似，硬驱逐阈值没有缓冲时间，当 <code>节点总资源 - kube 系统组件预留 - 宿主机系统服务预留 - Pod 实际使用资源</code> 小于硬驱逐阈值的时候将会立即执行驱逐，没有等待时间，强制执行 KILL Pod。</p><p>(Pods 驱逐顺序下文会继续说明)</p></li></ul><h2 id="Pod-QoS-服务质量管理"><a href="#Pod-QoS-服务质量管理" class="headerlink" title="Pod QoS 服务质量管理"></a>Pod QoS 服务质量管理</h2><p>QoS 的英文全称为 <code>Quality of Service</code> ,中文名为”服务质量”。QOS 实现资源有效调度和分配，从而提高资源利用率。<code>kubernetes</code> 针对不同服务的预期资源要求，通过 QoS（Quality of Service）来对 Pod 进行服务质量管理。</p><p>对于 Pod 来说，服务质量体现在两个指标上：一个指标是 CPU，另一个指标是内存。</p><p>如果未对资源进行限制，一些以 Pod 运行的关键服务进程，可能因为内存资源紧张触发 OOM 而被系统 kill 掉，或者被限制 CPU 使用导致进程被暂停。在 kubernetes 中，每个 Pod 都有个 QoS 标记，通过这个 Qos 标记来对 Pod 进行服务质量管理。在实际运行过程中，当节点资源紧张的时候，kubernetes 根据 Pod 具有的不同 QoS 标记，采取不同的处理策略。</p><blockquote><p><strong>已知问题</strong>: QOS 目前不支持 swap，所有 QoS 策略基于 swap 禁止的基础上。</p></blockquote><h3 id="QOS-级别"><a href="#QOS-级别" class="headerlink" title="QOS 级别"></a>QOS 级别</h3><table><thead><tr><th align="left">QoS 级别</th><th>QoS 介绍</th></tr></thead><tbody><tr><td align="left">BestEffort</td><td>Pod 中的所有容器都没有指定 CPU 和内存的 requests 和 limits，那么这个 Pod 的 QoS 就是 BestEffort 级别</td></tr><tr><td align="left">Burstable</td><td>Pod 中只要有一个容器，这个容器 requests 和 limits 的设置同其他容器设置的不一致，那么这个 Pod 的 QoS 就是 Burstable 级别</td></tr><tr><td align="left">Guaranteed</td><td>Pod 中所有容器都必须统一设置了 limits，并且设置参数都一致，如果有一个容器要设置 requests，那么所有容器都要设置，并设置参数同 limits 一致，那么这个 Pod 的 QoS 就是 Guaranteed 级别</td></tr></tbody></table><h3 id="资源回收策略"><a href="#资源回收策略" class="headerlink" title="资源回收策略"></a>资源回收策略</h3><p>当 kubernetes 集群中某个节点上可用资源比较小时，kubernetes 提供了资源回收策略保证被调度到该节点 pod 服务正常运行。当节点上的内存或者 CPU 资源耗尽时，可能会造成该节点上正在运行的 pod 服务不稳定。Kubernetes 通过 kubelet 来进行回收策略控制，保证节点上 pod 在节点资源比较小时可以稳定运行。</p><ol><li><p>可压缩资源：CPU</p><p> 当 Pod 使用的 CPU 超过设置的 <code>limits</code> 值，Pod 中进程使用 CPU 会被限制，但不会被 kill。</p></li><li><p>不可压缩资源：memory、storage</p><p> Kubernetes 通过 cgroup 设置 Pod QoS 级别，当资源不足时先 kill 优先级低的 Pod，在实际使用过程中，通过 OOM 分数值来实现，OOM 分数值从 0-1000。</p><p> <strong>OOM 分数值根据 <code>OOM_ADJ</code> 参数计算得出：</strong></p><table><thead><tr><th>Name</th><th>OOM_ADJ</th></tr></thead><tbody><tr><td>sshd 等系统进程（sshd／dmevented &#x2F; systemd-udevd）</td><td>-1000</td></tr><tr><td>K8S 管理进程（kubelet&#x2F;docker&#x2F; journalctl）</td><td>-999</td></tr><tr><td>Guaranteed Pod</td><td>-998</td></tr><tr><td>其它进程（内核 init 进程等）</td><td>0</td></tr><tr><td>Burstable Pod</td><td>min(max(2, 1000 – <br/>(1000 * memoryRequestBytes) &#x2F; machineMemoryCapacityBytes), 999)</td></tr><tr><td>BestEffort Pod</td><td>1000</td></tr></tbody></table><p> OOM_ADJ 参数值越大，计算出来 OOM 分数越高，表明该 Pod 优先级就越低，当出现资源竞争时会越早被 kill 掉。对于 OOM_ADJ 参数是 <code>-1000</code> 的，表示永远不会因为 OOM 而被 kill 掉。</p></li><li><p>QoS Pods 驱逐顺序</p><p> 如果节点资源不足要驱逐 Pod 或 OOM Kill 进程，将按以下顺序进行驱逐：</p><ul><li>Best-Effort 类型：该类型 Pods 会最先被驱逐或者被 Kill；</li><li>Burstable 类型：在没有 Best-Effort Pod 可以被驱逐时，该类型 Pods 会被驱逐或者 kill 掉(其中较大预留但资源使用较少的 Pod 会最后被驱逐或者 Kill)。</li><li>Guaranteed 类型：系统用完了全部内存、且没有 Burstable 与 Best-Effort container 可以被 kill，该类型的 Pods 会被 kill 掉。</li></ul></li></ol><blockquote><p>注：如果 Pod 进程因使用超过 limites 值而非 Node 资源紧张导致的 Kill，系统倾向于在原节点上重启该 Container，或在原节点或者其他节点重新创建一个 Pod。</p></blockquote><h2 id="Pod-优先级"><a href="#Pod-优先级" class="headerlink" title="Pod 优先级"></a>Pod 优先级</h2><p>RKE-Kubernets 集群核心组件以原生 docker 容器运行，因为主机资源固定，那么可以通过<strong>kubepods cgroup</strong>限制应用 Pod 进程使用的资源最大量，从而保证 RKE-Kubernets 集群核心组件和宿主机系统服务不受资源不足的影响。</p><p>但是有一些 Kubernets 系统组件，比如 DNS，它们也是以 Pod 方式运行并绑定在<strong>kubepods cgroup</strong>中。如果其他应用 Pod 进程使用了<strong>kubepods cgroup</strong>限制的最大内存资源，将会触发系统 <code>OOM</code> 或者因为资源紧张导致服务运行不正常。</p><p><strong>根据 <code>Pod QoS 服务质量</code> 的特性，在节点资源不足时，会先驱逐优先级最低的 Pod。因此，为了保证 Kubernets 系统组件的正常运行防止被驱逐，需要提升 Kubernets 系统组件的优先级。</strong></p><p>Kubernets 具有提高 Pod 优先级的功能，从 1.8 到 1.10 版本，默认没有开启 Pod 优先级和抢占。为了启用该功能，需要在 API server 和 scheduler 的启动参数中设置：</p><p><code>--feature-gates=PodPriority=true</code></p><p>在 API server 中还需要设置如下启动参数：</p><p><code>--runtime-config=scheduling.k8s.io/v1alpha1=true</code></p><p>Pod 优先级指明 pod 的相对重要程度。在 1.9 之前的版本中，如果 pod 因为资源问题无法调度，则 kubernetes 尝试抢占低优先级 pod 资源，将它们排挤掉，为高优先级 pod 提供运行条件。</p><p>在 1.9 及之后的版本中，pod 优先级会影响 pod 的调度顺序及当节点资源不足时的驱逐顺序。即调度时优先部署高优先级 pod，当节点资源不足时先行驱逐低优先级 pod。</p><p>在 1.11 之前的版本中，pod 优先级是 alpha 特性，在 1.11 版本中变成 beta 特性，并保证在后续版本继续支持。alpha 版本中默认禁止，需要明确打开，beta 版本默认打开，关系如下表：</p><table><thead><tr><th>Kubernetes Version</th><th>Priority and Preemption State</th><th>默认启用</th></tr></thead><tbody><tr><td>1.8</td><td>alpha</td><td>no</td></tr><tr><td>1.9</td><td>alpha</td><td>no</td></tr><tr><td>1.10</td><td>alpha</td><td>no</td></tr><tr><td>1.11</td><td>beta</td><td>yes</td></tr><tr><td>1.14</td><td>stable</td><td>yes</td></tr></tbody></table><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="docker-service-配置"><a href="#docker-service-配置" class="headerlink" title="docker.service 配置"></a>docker.service 配置</h3><p>对于 CentOS 系统，docker.service 默认位于 <code>/usr/lib/systemd/system/docker.service</code>；</p><p>对于 Ubuntu 系统，docker.service 默认位于 <code>/lib/systemd/system/docker.service</code>。</p><p>编辑 <code>docker.service</code>，添加以下参数。</p><ul><li><p>防止 docker 服务被 OOM KILL</p><p>docker 服务属于整个容器平台的核心基础服务。在宿主机系统内存不足时会触发 OOM KILL，docker 服务不是系统服务，因此 docker 服务进程很可能会被系统 KILL。为了防止 docker 进程被 KILL，可以在 <code>docker.service</code> 中配置 <code>OOMScoreAdjust=-1000</code> 以禁止被 OOM KILL。</p></li><li><p>防止 docker 服务内存溢出</p><p>docker 服务有时候出现异常，会出现占用很多内存资源的情况。为了防止 docker 服务占用整个节点资源，需要对服务做内存限制。在 docker.service 中添加 <code>MemoryLimit=xxG</code> 以限制 docker 服务使用最大内存。</p></li><li><p>开启 iptables 转发链</p><p>因为目前是通过 iptables 进行转发通信，而 iptables FORWARD 链默认是丢弃模式(Chain FORWARD (policy DROP)。为了保证通信正常，在启动 docker 前自动把 <code>iptables FORWARD</code> 链打开。</p><p><code>ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT</code> (centos)</p><p><code>ExecStartPost=/sbin/iptables -P FORWARD ACCEPT</code> (ubuntu)</p></li><li><p>docker 推荐配置</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">mkdir -p /etc/docker/</span><br><span class="line">touch /etc/docker/daemon.json</span><br><span class="line"></span><br><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;log-driver&quot;</span><span class="punctuation">:</span> <span class="string">&quot;json-file&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;log-opts&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;max-size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;100m&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;max-file&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;oom-score-adjust&quot;</span><span class="punctuation">:</span> <span class="number">-1000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;max-concurrent-downloads&quot;</span><span class="punctuation">:</span> <span class="number">10</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;max-concurrent-uploads&quot;</span><span class="punctuation">:</span> <span class="number">10</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;registry-mirrors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;https://7bezldxe.mirror.aliyuncs.com&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;storage-driver&quot;</span><span class="punctuation">:</span> <span class="string">&quot;overlay2&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;storage-opts&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;overlay2.override_kernel_check=true&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h3 id="RKE-配置-Kubernetes-集群资源预留"><a href="#RKE-配置-Kubernetes-集群资源预留" class="headerlink" title="RKE 配置 Kubernetes 集群资源预留"></a>RKE 配置 Kubernetes 集群资源预留</h3><p>  rke 参考配置文件，rke 版本大于等于 <code>v0.2.4</code></p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">address:</span> <span class="string">&lt;节点</span> <span class="string">IP&gt;</span></span><br><span class="line">      <span class="attr">user:</span> <span class="string">&lt;user&gt;</span></span><br><span class="line">      <span class="attr">role:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">controlplane</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">etcd</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">worker</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ignore_docker_version:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">ssh_key_path:</span> <span class="string">&lt;修改为实际路径&gt;</span></span><br><span class="line"><span class="attr">ssh_agent_auth:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#private_registries:</span></span><br><span class="line"><span class="comment">#    - url: registry.cn-shanghai.aliyuncs.com</span></span><br><span class="line"><span class="comment">##      user:</span></span><br><span class="line"><span class="comment">##      password:</span></span><br><span class="line"><span class="comment">#      is_default: true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">cluster_name:</span> <span class="string">demo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">    <span class="attr">etcd:</span></span><br><span class="line">      <span class="comment">## rke 版本大于等于 0.2.x 或 rancher 版本大于等于 2.2.0 时使用</span></span><br><span class="line">      <span class="attr">backup_config:</span></span><br><span class="line">        <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">interval_hours:</span> <span class="number">12</span></span><br><span class="line">        <span class="attr">retention:</span> <span class="number">6</span></span><br><span class="line">       <span class="attr">quota-backend-bytes:</span> <span class="string">&#x27;4294967296&#x27;</span></span><br><span class="line">       <span class="attr">auto-compaction-retention:</span> <span class="number">240</span> <span class="comment">#(单位小时)</span></span><br><span class="line">    <span class="attr">kube-api:</span></span><br><span class="line">      <span class="attr">service_cluster_ip_range:</span> <span class="number">10.43</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">      <span class="attr">service_node_port_range:</span> <span class="number">30000</span><span class="number">-32767</span></span><br><span class="line">      <span class="attr">pod_security_policy:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">extra_args:</span></span><br><span class="line">        <span class="attr">audit-log-path:</span> <span class="string">&quot;-&quot;</span></span><br><span class="line">        <span class="attr">delete-collection-workers:</span> <span class="number">3</span></span><br><span class="line">        <span class="attr">v:</span> <span class="number">4</span></span><br><span class="line">    <span class="attr">kube-controller:</span></span><br><span class="line">      <span class="attr">cluster_cidr:</span> <span class="number">10.42</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">      <span class="attr">service_cluster_ip_range:</span> <span class="number">10.43</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">      <span class="attr">extra_args:</span></span><br><span class="line">        <span class="comment">## 控制器定时与节点通信以检查通信是否正常，周期默认 5s</span></span><br><span class="line">        <span class="attr">node-monitor-period:</span> <span class="string">&#x27;5s&#x27;</span></span><br><span class="line">        <span class="comment">## 当节点通信失败后，再等一段时间 kubernetes 判定节点为 notready 状态。</span></span><br><span class="line">        <span class="comment">## 这个时间段必须是 kubelet 的 nodeStatusUpdateFrequency(默认 10s)的 N 倍，</span></span><br><span class="line">        <span class="comment">## 其中 N 表示允许 kubelet 同步节点状态的重试次数，默认 40s。</span></span><br><span class="line">        <span class="attr">node-monitor-grace-period:</span> <span class="string">&#x27;20s&#x27;</span></span><br><span class="line">        <span class="comment">## 再持续通信失败一段时间后，kubernetes 判定节点为 unhealthy 状态，默认 1m0s。</span></span><br><span class="line">        <span class="attr">node-startup-grace-period:</span> <span class="string">&#x27;30s&#x27;</span></span><br><span class="line">        <span class="comment">## 再持续失联一段时间，kubernetes 开始迁移失联节点的 Pod，默认 5m0s。</span></span><br><span class="line">        <span class="attr">pod-eviction-timeout:</span> <span class="string">&#x27;1m&#x27;</span></span><br><span class="line">    <span class="attr">kubelet:</span></span><br><span class="line">      <span class="attr">cluster_domain:</span> <span class="string">cluster.local</span></span><br><span class="line">      <span class="attr">cluster_dns_server:</span> <span class="number">10.43</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">      <span class="attr">fail_swap_on:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">extra_args:</span></span><br><span class="line">        <span class="comment">## 修改节点最大 Pod 数量</span></span><br><span class="line">        <span class="attr">max-pods:</span> <span class="string">&quot;250&quot;</span></span><br><span class="line">        <span class="comment">## 密文和配置映射同步时间，默认 1 分钟</span></span><br><span class="line">        <span class="attr">sync-frequency:</span> <span class="string">&#x27;3s&#x27;</span></span><br><span class="line">        <span class="comment">## Kubelet 进程可以打开的文件数（默认 1000000）,根据节点配置情况调整</span></span><br><span class="line">        <span class="attr">max-open-files:</span> <span class="string">&#x27;2000000&#x27;</span></span><br><span class="line">        <span class="comment">## 与 apiserver 会话时的并发数，默认是 10</span></span><br><span class="line">        <span class="attr">kube-api-burst:</span> <span class="string">&#x27;30&#x27;</span></span><br><span class="line">        <span class="comment">## 与 apiserver 会话时的 QPS,默认是 5</span></span><br><span class="line">        <span class="attr">kube-api-qps:</span> <span class="string">&#x27;15&#x27;</span></span><br><span class="line">        <span class="comment">## kubelet 默认一次拉取一个镜像，设置为 false 可以同时拉取多个镜像，</span></span><br><span class="line">        <span class="comment">## 前提是存储驱动要为 overlay2，对应的 Dokcer 也需要增加下载并发数</span></span><br><span class="line">        <span class="attr">serialize-image-pulls:</span> <span class="string">&#x27;false&#x27;</span></span><br><span class="line">        <span class="comment">## 拉取镜像的最大并发数，registry-burst 不能超过 registry-qps ，</span></span><br><span class="line">        <span class="comment">## 仅当 registry-qps 大于 0(零)时生效，(默认 10)。如果 registry-qps 为 0 则不限制(默认 5)。</span></span><br><span class="line">        <span class="attr">registry-burst:</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">        <span class="attr">registry-qps:</span> <span class="string">&#x27;0&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">cgroups-per-qos:</span> <span class="string">&#x27;true&#x27;</span></span><br><span class="line">        <span class="comment"># 这里一定要为 cgroupfs</span></span><br><span class="line">        <span class="attr">cgroup-driver:</span> <span class="string">&#x27;cgroupfs&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">enforce-node-allocatable:</span> <span class="string">&#x27;pods,kube-reserved&#x27;</span> <span class="comment"># &#x27;pods,kube-reserved,system-reserved&#x27;</span></span><br><span class="line">        <span class="attr">system-reserved:</span> <span class="string">&#x27;cpu=1,memory=500Mi&#x27;</span></span><br><span class="line">        <span class="comment"># 根据实际资源调整</span></span><br><span class="line">        <span class="attr">kube-reserved:</span> <span class="string">&#x27;cpu=1,memory=1Gi&#x27;</span></span><br><span class="line">        <span class="comment"># RKE 集群需要设置为 `/docker`</span></span><br><span class="line">        <span class="attr">kube-reserved-cgroup:</span> <span class="string">&#x27;/docker&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置进行 pod 驱逐的阈值，这个参数只支持内存和磁盘。通过--eviction-hard 标志预留一些内存后，当 Allocatable 可用内存降至保留值以下时，kubelet 将会对 pod 进行驱逐。</span></span><br><span class="line">        <span class="comment">## 硬阈值，当值小于 100M 就成触发驱逐</span></span><br><span class="line">        <span class="attr">eviction-hard:</span> <span class="string">&#x27;memory.available&lt;100Mi&#x27;</span></span><br><span class="line">        <span class="comment"># 软阈值，当可用值小于 500Mi，</span></span><br><span class="line">        <span class="attr">eviction-soft:</span> <span class="string">&#x27;memory.available&lt;500Mi&#x27;</span></span><br><span class="line">        <span class="attr">eviction-soft-grace-period:</span> <span class="string">&#x27;memory.available=1m30s&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">eviction-max-pod-grace-period:</span> <span class="string">&#x27;30&#x27;</span></span><br><span class="line">        <span class="attr">eviction-pressure-transition-period:</span> <span class="string">&#x27;30s&#x27;</span></span><br><span class="line"></span><br><span class="line">      <span class="attr">extra_binds:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Currently, only authentication strategy supported is x509.</span></span><br><span class="line"><span class="comment"># You can optionally create additional SANs (hostnames or IPs) to add to</span></span><br><span class="line"><span class="comment">#  the API server PKI certificate.</span></span><br><span class="line"><span class="comment"># This is useful if you want to use a load balancer for the control plane servers.</span></span><br><span class="line"><span class="attr">authentication:</span></span><br><span class="line">    <span class="attr">strategy:</span> <span class="string">&quot;x509|webhook&quot;</span></span><br><span class="line">    <span class="attr">sans:</span></span><br><span class="line">      <span class="comment"># 此处配置备用域名或 IP，当主域名或者 IP 无法访问时，可通过备用域名或 IP 访问</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;192.168.1.100&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;www.test.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Kubernetes 认证模式</span></span><br><span class="line"><span class="comment">## Use `mode: rbac` 启用 RBAC</span></span><br><span class="line"><span class="comment">## Use `mode: none` 禁用 认证</span></span><br><span class="line"><span class="attr">authorization:</span></span><br><span class="line">    <span class="attr">mode:</span> <span class="string">rbac</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add-ons are deployed using kubernetes jobs. RKE will give up on trying to get the job status after this timeout in seconds..</span></span><br><span class="line"><span class="attr">addon_job_timeout:</span> <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有几个网络插件可以选择：`flannel、canal、calico`，Rancher2 默认 canal</span></span><br><span class="line"><span class="attr">network:</span></span><br><span class="line">    <span class="attr">plugin:</span> <span class="string">canal</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">      <span class="attr">canal_iface:</span> <span class="string">eth0</span></span><br><span class="line">      <span class="attr">flannel_backend_type:</span> <span class="string">&quot;vxlan&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 目前只支持 nginx ingress controller</span></span><br><span class="line"><span class="comment">## 可以设置 `provider: none` 来禁用 ingress controller</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ingress:</span></span><br><span class="line">    <span class="attr">provider:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">node_selector:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">ingress</span></span><br><span class="line"><span class="comment"># 配置 dns 上游 dns 服务器</span></span><br><span class="line"><span class="comment">## 可用 rke 版本 v0.2.0</span></span><br><span class="line"><span class="attr">dns:</span></span><br><span class="line">    <span class="attr">provider:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">upstreamnameservers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">114.114</span><span class="number">.114</span><span class="number">.114</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">1.2</span><span class="number">.4</span><span class="number">.8</span></span><br></pre></td></tr></table></figure><h3 id="配置-Pod-优先级"><a href="#配置-Pod-优先级" class="headerlink" title="配置 Pod 优先级"></a>配置 Pod 优先级</h3><p>PriorityClass 是一个不受命名空间约束的对象，它定义了优先级类名与优先级整数值的映射。它的名称通过 PriorityClass 对象 <strong>metadata</strong> 中的 name 字段指定。value 值越大，优先级越高。</p><p>PriorityClass 对象的值可以是小于或者等于 10 亿的 32 位整数值。更大的数值被保留给那些通常不应该取代或者驱逐的关键的系统级 Pod 使用。集群管理员应该为它们想要的每个此类映射创建一个 PriorityClass 对象。</p><p><strong>注意:</strong> 优先级配置需要在集群基础组件配置完成后再执行。</p><ol><li><p><strong>配置 PriorityClass</strong></p> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PriorityClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">high-priority-system-pod</span></span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">preemptionPolicy:</span> <span class="string">PreemptLowerPriority</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&quot;This priority class should be used for XYZ service pods only.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">metadata.name：名称</span></span><br><span class="line"><span class="string">value：小于或者等于</span> <span class="number">10</span> <span class="string">亿的</span> <span class="number">32</span> <span class="string">位任意整数值，数字越大优先级越高，超过一亿的数字被系统保留，用于指派给系统组件。</span></span><br><span class="line"><span class="string">globalDefault：是否应用于全局</span> <span class="string">pod</span> <span class="string">策略</span></span><br><span class="line"><span class="string">description：描述信息</span></span><br><span class="line"><span class="attr">preemptionPolicy:</span> <span class="string">抢占功能。设置为</span> <span class="string">Never</span> <span class="string">表示不抢占，默认为</span> <span class="string">PreemptLowerPriority，1.15</span> <span class="string">之后默认禁用抢占功能。</span></span><br></pre></td></tr></table></figure><ul><li>如果升级现有集群启用此功能，那些已经存在系统里面的 Pod 的优先级将会设置为 0。</li><li>此外，将一个<strong>PriorityClass</strong>的 globalDefault 设置为 true，不会改变系统中已经存在的 Pod 的优先级。也就是说，PriorityClass 的值只能用于在 PriorityClass 添加之后创建的那些 Pod 。</li><li>如果您删除一个 PriorityClass，那些使用了该 PriorityClass 的 Pod 将会保持不变，但是，该 PriorityClass 的名称不能在新创建 Pod 时使用。</li></ul><p></p></li><li><p><strong>设置 Pod priority</strong></p><p> 在配置 Pod 时，设置其<strong>priorityClass Name</strong>字段为可用 PriorityClass 名称，则在创建 Pod 时由允入控制器将名称转换成对应数字。如果没有找到相应的 PriorityClass，Pod 将会被拒绝创建。示例如下：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">APP_NS=kube-system</span><br><span class="line"></span><br><span class="line">APP=canal</span><br><span class="line">WORKLOAD_TYPE=daemonsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line">APP=kube-dns</span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line">APP=kube-dns-autoscaler</span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line">APP=metrics-server</span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f -</span><br><span class="line"></span><br><span class="line"><span class="comment">##################</span></span><br><span class="line">APP_NS=cattle-system</span><br><span class="line"></span><br><span class="line">APP=rancher <span class="comment"># 仅 local 集群执行</span></span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f -</span><br><span class="line"></span><br><span class="line">APP=cattle-cluster-agent</span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f -</span><br><span class="line"></span><br><span class="line">APP=cattle-node-agent</span><br><span class="line">WORKLOAD_TYPE=daemonsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line"><span class="comment">##################</span></span><br><span class="line">APP_NS=ingress-nginx</span><br><span class="line"></span><br><span class="line">APP=nginx-ingress-controller</span><br><span class="line">WORKLOAD_TYPE=daemonsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line"><span class="comment">##################</span></span><br><span class="line"><span class="comment"># 启用集群监控和告警的集群，执行力以下命令</span></span><br><span class="line">APP_NS=cattle-prometheus</span><br><span class="line"></span><br><span class="line">APP=prometheus-cluster-monitoring</span><br><span class="line">WORKLOAD_TYPE=statefulsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line">APP=alertmanager-cluster-alerting</span><br><span class="line">WORKLOAD_TYPE=statefulsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cgroup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8S 节点不可用时快速迁移 Pods</title>
      <link href="/kubernetes/fast-migration-pod-when-node-unavailable/"/>
      <url>/kubernetes/fast-migration-pod-when-node-unavailable/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/fast-migration-pod-when-node-unavailable/" target="_blank" title="https://www.xtplayer.cn/kubernetes/fast-migration-pod-when-node-unavailable/">https://www.xtplayer.cn/kubernetes/fast-migration-pod-when-node-unavailable/</a></p><p>当节点宕机或者节点网络异常时，我们希望该节点上的 Pod 能够快速迁移到其他节点，并提供服务。但是测试发现节点 Down 掉以后，Pod 并不会立即迁移到其他节点。使用不同的 Kubernetes 版本测试，在默认配置下，需要等待将近 6 到 10 分钟节点上的 Pod 才会开始迁移。</p><p>这个现象与 Kubelet 的状态更新机制密切相关，下面我们来分析下 Kubelet 状态更新的基本流程。</p><ul><li><p>kubelet</p><p>kubelet 自身会定期更新状态到 <strong>apiserver</strong>，通过参数 <code>--node-status-update-frequency</code> 配置上报频率，默认 <strong>10s</strong> 上报一次。</p></li><li><p>kube-controller-manager</p><ol><li><strong>kube-controller-manager</strong> 会定时去检查 kubelet 的状态，可通过 <code>--node-monitor-period</code> 自定义这个时间 ，默认是 <strong>5s</strong>。</li><li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>notready</code> 状态，这段时长可通过 <code>--node-monitor-grace-period</code> 参数配置，默认 <strong>40s</strong>。</li><li>当 node 状态为 <code>notready</code> 一段时间后，kubernetes 判定 node 为 <code>unhealthy</code> 状态，这段时长可通过 <code>--node-startup-grace-period</code> 参数配置，默认 <strong>1m0s</strong>。</li><li>当 node 状态为 <code>unhealthy</code> 一段时间后，kubernetes 开始删除原 node 上的 Pod，这段时长可通过 <code>--pod-eviction-timeout</code> 参数配置，默认 <strong>5m0s</strong>。</li></ol></li><li><p>kube-apiserver  (Kubernetes v1.13 版本及以后版本)</p><p>​Kubernetes v1.13 及以后版本默认开启了 <strong>基于污点的驱逐</strong>，默认 <strong>300</strong> 秒。节点不同状态时会自动添加不同的污点，因而 Pod 的驱逐时间将受到节点状态的影响。</p><ol><li><p><code>default-not-ready-toleration-seconds</code>: 表示 <code>notReady:NoExecute</code> 容忍的时间。<code>notReady:NoExecute</code> 默认添加到所有 Pod（默认 300 秒）。</p></li><li><p><code>default-unreachable-toleration-seconds</code>: 表示 <code>unreachable:NoExecute</code> 容忍的时间。<code>unreachable:NoExecute</code> 默认添加到所有 Pod（默认 300 秒）。</p></li></ol></li></ul><h2 id="Kubernetes-小于-v1-13"><a href="#Kubernetes-小于-v1-13" class="headerlink" title="Kubernetes 小于 v1.13"></a>Kubernetes 小于 v1.13</h2><p>在 Kubernetes 小于 v1.13 版本之前，默认没有启用基于污点的驱逐，因此 Pod 最终迁移的时间由 <code>node-monitor-period</code>、<code>node-monitor-grace-period</code>、<code>node-startup-grace-period</code>、<code>pod-eviction-timeout</code> 几个参数同时决定。</p><h3 id="rke-配置示例"><a href="#rke-配置示例" class="headerlink" title="rke 配置示例"></a>rke 配置示例</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">kube-controller:</span></span><br><span class="line">     <span class="string">&lt;其他参数已省略&gt;</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">node-monitor-period:</span> <span class="string">&quot;5s&quot;</span>  <span class="comment"># (默认 5s)</span></span><br><span class="line">      <span class="attr">node-monitor-grace-period:</span> <span class="string">&quot;40s&quot;</span> <span class="comment"># Node Notready (默认 40s)</span></span><br><span class="line">      <span class="attr">node-startup-grace-period:</span> <span class="string">&quot;1m0s&quot;</span> <span class="comment"># Node Unhealthy (默认 1m0s)</span></span><br><span class="line">      <span class="attr">pod-eviction-timeout:</span> <span class="string">&quot;5m0s&quot;</span> <span class="comment"># 开始驱逐 Pod (默认 5m0s)</span></span><br></pre></td></tr></table></figure><h2 id="Kubernetes-大于等于-v1-13-小于-v1-18"><a href="#Kubernetes-大于等于-v1-13-小于-v1-18" class="headerlink" title="Kubernetes 大于等于 v1.13 小于 v1.18"></a>Kubernetes 大于等于 v1.13 小于 v1.18</h2><h3 id="基于污点的驱逐"><a href="#基于污点的驱逐" class="headerlink" title="基于污点的驱逐"></a>基于污点的驱逐</h3><ol><li><p>在 kubernetes 1.13 之前，基于污点的驱逐（TaintBasedEvictions）处于 <strong>Alpha</strong> 阶段，默认是禁用状态（TaintBasedEvictions&#x3D;false）。所以在 kubernetes 1.13 之前的版本中只需要配置 <strong>kube-controller</strong> 的参数即可控制节点不可用时 Pod 的迁移时间。</p></li><li><p>从 kubernetes 1.13 开始，基于污点的驱逐（TaintBasedEvictions）处于 <strong>Beta</strong> 阶段，默认是被开启状态（TaintBasedEvictions&#x3D;true）。当修改 <strong>kube-controller</strong> 参数后手动停止一个节点，发现 Pod 还是需要等很久才能被迁移。</p></li><li><p>在 基于污点的驱逐（TaintBasedEvictions）开启的状态下，<code>node-startup-grace-period</code> 和 <code>pod-eviction-timeout</code> 参数配置的时间不再生效。相应的我们可以在 Pod 的 yaml 配置文件中看到如下的参数：</p> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoExecute</span></span><br><span class="line">  <span class="attr">key:</span> <span class="string">node.kubernetes.io/not-ready</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">  <span class="attr">tolerationSeconds:</span> <span class="number">300</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoExecute</span></span><br><span class="line">  <span class="attr">key:</span> <span class="string">node.kubernetes.io/unreachable</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">  <span class="attr">tolerationSeconds:</span> <span class="number">300</span></span><br></pre></td></tr></table></figure></li><li><p>因为 kubernetes 节点控制器根据 kubelet 的不同状态，将自动为节点添加不同的 <strong>污点</strong>。当节点具有 <code>node.kubernetes.io/not-ready</code> 或者 <code>node.kubernetes.io/unreachable</code> 污点时，将会等待 <code>tolerationSeconds</code> 所配置的时长。<code>tolerations</code> 是由 API SERVER 控制生成，从 Kubernetes v1.13 开始可以通过以下参数来配置 <code>tolerationSeconds</code> 时间值。</p><ul><li><p>kube-apiserver  (Kubernetes v1.13 版本及以后)</p><ol><li><p><code>default-not-ready-toleration-seconds</code>: 表示 <code>notReady:NoExecute</code> 容忍的时间。<code>notReady:NoExecute</code> 默认添加到所有 Pod。默认 300 秒）</p></li><li><p><code>default-unreachable-toleration-seconds</code>: 表示 <code>unreachable:NoExecute</code> 容忍的时间。<code>unreachable:NoExecute</code> 默认添加到所有 Pod。（默认 300 秒）</p></li></ol><blockquote><p><strong>注意：</strong> <code>notReady</code> 和 <code>unreachable</code> 两个污点匹配到一个就会触发驱逐，<code>notReady</code> 的时间可以适当设置长一点。</p></blockquote></li></ul></li></ol><h3 id="节点污点"><a href="#节点污点" class="headerlink" title="节点污点"></a>节点污点</h3><p>从 kubernetes 1.6 开始，kubernetes 的节点控制器根据 kubelet 的不同状态，将自动为节点添加不同的<strong>污点</strong>。</p><p>这类污点有：</p><ul><li><code>node.kubernetes.io/not-ready</code>： 节点未就绪。对应着 NodeCondition <code>Ready</code> 为 <code>False</code> 的情况。</li><li><code>node.kubernetes.io/unreachable</code>： 节点不可触达。对应着 NodeCondition <code>Ready</code> 为 <code>Unknown</code> 的情况。</li><li><code>node.kubernetes.io/out-of-disk</code>：节点磁盘空间已满。</li><li><code>node.kubernetes.io/memory-pressure</code>：节点内存吃紧。</li><li><code>node.kubernetes.io/disk-pressure</code>：节点磁盘吃紧。</li><li><code>node.kubernetes.io/network-unavailable</code>：节点网络不可用。</li><li><code>node.kubernetes.io/unschedulable</code>：节点不可调度。</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 是由 “外部” 云服务商启动的，该污点用来标识某个节点当前为不可用的状态。在 “云控制器”（cloud-controller-manager）初始化这个节点以后，kubelet 将此污点移除。</li></ul><h3 id="rke-配置示例-1"><a href="#rke-配置示例-1" class="headerlink" title="rke 配置示例"></a>rke 配置示例</h3><p>在 Kubernetes 大于等于 v1.13 小于 v1.18 的版本中，基于污点的驱逐（TaintBasedEvictions）处于 <strong>Beta</strong> 阶段，可选择以下两种方式来配置。</p><ul><li><p>禁用 基于污点的驱逐</p>  <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">kube-controller:</span></span><br><span class="line">     <span class="string">&lt;其他参数已省略&gt;</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">node-monitor-period:</span> <span class="string">&quot;5s&quot;</span>  <span class="comment"># (默认 5s)</span></span><br><span class="line">      <span class="attr">node-monitor-grace-period:</span> <span class="string">&quot;40s&quot;</span> <span class="comment"># Node Notready (默认 40s)</span></span><br><span class="line">      <span class="attr">node-startup-grace-period:</span> <span class="string">&quot;1m0s&quot;</span> <span class="comment"># Node Unhealthy (默认 1m0s)</span></span><br><span class="line">      <span class="attr">pod-eviction-timeout:</span> <span class="string">&quot;5m0s&quot;</span> <span class="comment"># 开始驱逐 Pod (默认 5m0s)</span></span><br><span class="line">      <span class="attr">feature-gates:</span> <span class="string">&quot;TaintBasedEvictions=false&quot;</span> <span class="comment"># 禁用 基于污点的驱逐</span></span><br></pre></td></tr></table></figure></li><li><p>启用 基于污点的驱逐</p>  <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">kube-api:</span></span><br><span class="line">    <span class="string">&lt;其他参数已省略&gt;</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">default-not-ready-toleration-seconds:</span> <span class="string">&#x27;60&#x27;</span> <span class="comment"># 默认 300 秒</span></span><br><span class="line">      <span class="attr">default-unreachable-toleration-seconds:</span> <span class="string">&#x27;30&#x27;</span> <span class="comment"># 默认 300 秒</span></span><br><span class="line">  <span class="attr">kube-controller:</span></span><br><span class="line">     <span class="string">&lt;其他参数已省略&gt;</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">node-monitor-period:</span> <span class="string">&quot;5s&quot;</span>  <span class="comment"># (默认 5s)</span></span><br><span class="line">      <span class="attr">node-monitor-grace-period:</span> <span class="string">&quot;40s&quot;</span> <span class="comment"># Node Notready (默认 40s)</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="Kubernetes-大于等于-v1-18"><a href="#Kubernetes-大于等于-v1-18" class="headerlink" title="Kubernetes 大于等于 v1.18"></a>Kubernetes 大于等于 v1.18</h2><p>在 Kubernetes 1.18 及以上版本中，<code>TaintBasedEvictions</code> 默认设置为 <code>true</code> 并且不能修改。如果强制在 rke 配置中指定 <code>TaintBasedEvictions=false</code>，如下配置：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">kube-controller:</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">feature-gates:</span> <span class="string">&quot;TaintBasedEvictions=false&quot;</span></span><br></pre></td></tr></table></figure><p>那么 <code>kube-controller-manager</code> 将无法正常运行，日志中会看到如下的错误提示：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error: invalid argument <span class="string">&quot;TaintBasedEvictions=false&quot;</span> <span class="keyword">for</span> <span class="string">&quot;--feature-gates&quot;</span> flag: cannot <span class="built_in">set</span> feature gate TaintBasedEvictions to <span class="literal">false</span>, feature is locked to <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="rke-配置示例-2"><a href="#rke-配置示例-2" class="headerlink" title="rke 配置示例"></a>rke 配置示例</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">kube-api:</span></span><br><span class="line">    <span class="string">&lt;其他参数已省略&gt;</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">default-not-ready-toleration-seconds:</span> <span class="string">&#x27;60&#x27;</span> <span class="comment"># 默认 300 秒</span></span><br><span class="line">      <span class="attr">default-unreachable-toleration-seconds:</span> <span class="string">&#x27;30&#x27;</span> <span class="comment"># 默认 300 秒</span></span><br><span class="line">  <span class="attr">kube-controller:</span></span><br><span class="line">    <span class="string">&lt;其他参数已省略&gt;</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">node-monitor-period:</span> <span class="string">&quot;5s&quot;</span></span><br><span class="line">      <span class="attr">node-monitor-grace-period:</span> <span class="string">&quot;30s&quot;</span> <span class="comment"># Unavailable</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> TaintBasedEvictions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 Longhorn 优雅地恢复运行中的容器应用</title>
      <link href="/longhorn/longhorn-recovery-app/"/>
      <url>/longhorn/longhorn-recovery-app/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Longhorn 是基于 Kubernetes 的轻量级分布式块存储系统，它完全开源，并且已经捐献给 CNCF。随着云原生应用的普及，越来越多的服务提供容器运行时，数据的持久化存储问题渐渐显现出来，我们要做的不仅仅是数据的持久化，还要考虑备份的准确性，迁移的复杂性等。</p><p>Longhorn 提供的分布式块存储可以在 Kubernetes 中直接使用持久化存储，它可以为数据卷在不同主机提供多副本服务，以保证数据的可靠性，它提供简洁的 UI 可以直接管理存储节点、数据卷，轻松实现数据卷的备份&#x2F;定时备份，您还可以使用容灾备份功能，在不同集群创建容灾备份卷，并在发生紧急情况时及时进行故障转移。</p><p>在这篇文章中，我们将把 MySQL 作为 Pod 部署到集群 A 中，并使用 Longhorn 作为持久化存储卷，然后依托 Rancher 多集群管理的特性，结合 Longhorn 的容灾备份功能，演示一下如何优雅地将集群 A 中的 MySQL 应用及数据迁移到集群 B 中。</p><img src="/longhorn/longhorn-recovery-app/640.png" class="" title="img"><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li>Rancher (HA 部署) - v2.4.8</li><li>Longhorn（通过 Rancher UI Catalog 部署） - v1.0.2</li><li>MySQL - 8</li><li>私有 S3 - minio 部署</li><li>业务集群 A、B</li></ul><h2 id="部署-longhorn"><a href="#部署-longhorn" class="headerlink" title="部署 longhorn"></a>部署 longhorn</h2><ol><li><p>在 Rancher 中，进入集群 A 的 system 项目，在应用商店中可以一键启动 Longhorn。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718549.png" class="" title="img"></li><li><p>等待应用商店部署成功后，可以根据应用商店链接进入到 Longhorn UI。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718482.png" class="" title="img"> <img src="/longhorn/longhorn-recovery-app/640-20200915191718568.png" class="" title="img"></li><li><p>接下来，我们在集群 B 中以同样方式启动 Longhorn，等待 Longhorn 正常运行。</p></li></ol><h2 id="Longhorn-配置"><a href="#Longhorn-配置" class="headerlink" title="Longhorn 配置"></a>Longhorn 配置</h2><h3 id="创建-s3-访问的-secret（用于-longhorn-备份使用）"><a href="#创建-s3-访问的-secret（用于-longhorn-备份使用）" class="headerlink" title="创建 s3 访问的 secret（用于 longhorn 备份使用）"></a>创建 s3 访问的 secret（用于 longhorn 备份使用）</h3><ol><li><p>访问 Rancher，进入集群 A 的 system 项目，选择 资源 &gt; 密文 菜单，在 longhorn-system 命名空间中创建访问 s3 的 secret，需要增加的参数包括：</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718487.png" class="" title="img"></li><li><p>访问 Rancher，进入集群 B 的 system 项目，选择 <strong>资源 &gt; 密文</strong> 菜单，在 longhorn-system 命名空间中创建与上面相同的 secret 内容。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915192133450.png" class="" title="img"></li></ol><h3 id="配置-longhorn-backup-target"><a href="#配置-longhorn-backup-target" class="headerlink" title="配置 longhorn backup target"></a>配置 longhorn backup target</h3><p>接下来我们需要在 A 和 B 两个集群的 Longhorn 中配置相同的 backup target。</p><ol><li><p>从应用商店的链接跳转到 longhorn UI，点击 <strong>Setting &gt; General</strong> 菜单，找到 Backup 分类，配置备份目标。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915192206425.png" class="" title="img"></li><li><p>目前 Longhorn 支持 NFS&#x2F;S3 两种备份方式，在本例中，我们使用 minio 搭建了一个私有 S3 服务，这里我们使用 s3 方式配置。</p><blockquote><p>PS: 如果使用 NFS，则需要保证 NFS server 支持 NFSv4</p></blockquote> <img src="/longhorn/longhorn-recovery-app/640-20200915192217444.png" class="" title="img"></li></ol><h2 id="部署-MySQL-应用"><a href="#部署-MySQL-应用" class="headerlink" title="部署 MySQL 应用"></a>部署 MySQL 应用</h2><ol><li><p>在集群 A 中部署一个 MySQL 应用，并在 MySQL 中创建测试表 article 并插入一条数据。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718562.png" class="" title="img"></li><li><p>在集群 A 的 Longhorn 中对这个卷进行备份。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718567.png" class="" title="img"></li><li><p>备份成功后，可以在集群 B 的 Longhorn 中看到备份信息（因为两个集群的 Longhorn 配置了相同的 backup target，所以备份信息是共享的）。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915192240575.png" class="" title="img"></li></ol><h2 id="创建容灾备份卷"><a href="#创建容灾备份卷" class="headerlink" title="创建容灾备份卷"></a>创建容灾备份卷</h2><ol><li><p>访问集群 B 的 Longhorn UI，使用集群 A 的 Volume 的备份，在集群 B 中创建一个容灾备份卷。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718587-0168638.png" class="" title="img"> <img src="/longhorn/longhorn-recovery-app/640-20200915192333671.png" class="" title="img"></li><li><p>这里 Volume 的名称会根据备份卷自动填充，<strong>不建议手动修改。</strong>保存后，访问 Volume 页面可以看到我们新创建的容灾备份卷。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718633.png" class="" title="img"></li><li><p>在集群 A 的 MySQL 应用中再次插入几条数据。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718605.png" class="" title="img"></li><li><p>访问集群 A 的 Longhorn UI，对这个卷再次进行备份。这时可以看到集群 B 中的容灾备份卷图标变成了灰色，代表这个卷正在同步集群 A 中 Volume 的最新备份数据，此时无法激活和使用容灾备份卷。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718654.png" class="" title="img"></li><li><p>等待一会，图标变成了蓝色，代表已经与集群 A 中 Volume 的最新备份同步成功。接下来我们就可以使用这个卷了。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718618.png" class="" title="img"><p> 如果在集群 A 中对 MySQL 卷设置了自动备份，集群 B 中的 Longhorn 会定时轮询最新的备份，将增量数据信息自动同步到容灾备份卷，以保持与集群 A 中 Volume 的数据一致。</p></li></ol><h2 id="在新集群中恢复-mysql-应用"><a href="#在新集群中恢复-mysql-应用" class="headerlink" title="在新集群中恢复 mysql 应用"></a>在新集群中恢复 mysql 应用</h2><p>假设此时我们的集群 A 已经无法使用了，我们可以在集群 B 使用最新的备份卷快速恢复 MySQL 应用。</p><ol><li><p>首先，我们将集群 B 的容灾备份卷激活。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915192420639.png" class="" title="img"></li><li><p>等待卷状态变成 Detached 以后，选择创建 PV&#x2F;PVC。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915192431536.png" class="" title="img"></li><li><p>这里<strong>不建议修改 PV 和 PVC 的名称</strong>，namespace 可根据实际集群 B 中的 namespace 名称进行填写。保存会，会在集群 B 的指定 namespace 中创建 PVC。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915192440569.png" class="" title="img"></li><li><p>访问 Rancher 可以看到 PVC 已经创建成功。</p> <img src="/longhorn/longhorn-recovery-app/640-20200915192448739.png" class="" title="img"></li><li><p>使用恢复的 PVC 创建 MySQL 应用，再查询一下数据，可以看到数据也恢复过来啦！大功告成！</p> <img src="/longhorn/longhorn-recovery-app/640-20200915191718641.png" class="" title="img"></li></ol><h2 id="总-结"><a href="#总-结" class="headerlink" title="总 结"></a>总 结</h2><p>随着云原生应用的普及，越来越多的服务可以依托 Kubernetes 运行，保证服务的稳定性和可靠性也渐渐成为难题，依托 Longhorn 的跨集群容灾备份功能，在 Rancher 中可以自动完成应用的编排、数据迁移，随时优雅的切换业务应用运行环境。</p>]]></content>
      
      
      <categories>
          
          <category> longhorn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longhorn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>coredns 解析外部域名</title>
      <link href="/coredns/coredns-host/"/>
      <url>/coredns/coredns-host/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/coredns/coredns-host/" target="_blank" title="https://www.xtplayer.cn/coredns/coredns-host/">https://www.xtplayer.cn/coredns/coredns-host/</a></p><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>有时候 kubernetes 集群中的应用需要通过域名访问集群外部的一些服务，虽然 ip 是可以正常通信，但是因为没有可用解析域名的 DNS 服务器，从而导致了应用无法正常访问外部域名。</p><h2 id="解决方案一"><a href="#解决方案一" class="headerlink" title="解决方案一"></a>解决方案一</h2><p>对于单个应用，可以给 Pod 的 hosts（&#x2F;etc&#x2F;hosts）添加条目。</p><p>编辑应用，点击右下角的 <strong>显示高级选项</strong>，在网络选项中设置 &#x2F;etc&#x2F;hosts</p><img src="/coredns/coredns-host/image-20200915132819787.png" class="" title="image-20200915132819787"><h2 id="解决方案二"><a href="#解决方案二" class="headerlink" title="解决方案二"></a>解决方案二</h2><p>对于方案一，少数应用或者临时测试是没有问题。但是对于大量应用需要同时修改配置，工作量就挺大了。这个时候就需要一个中间的 DNS 代理服务，只需要修改一次代理服务即可。</p><p>kubernetes 集群中内置了 DNS 服务器软件。在 rancher 或者 rke 集群中，不同的版本内置的 DNS 服务器软件不同。下表列出了对应关系：</p><blockquote><p><a href="https://rancher.com/docs/rke/latest/en/config-options/add-ons/dns/">https://rancher.com/docs/rke/latest/en/config-options/add-ons/dns/</a></p></blockquote><table><thead><tr><th align="left">RKE 版本</th><th align="left">KUBERNETES 版本</th><th align="left">默认 DNS PROVIDER</th></tr></thead><tbody><tr><td align="left">v0.2.5 and higher</td><td align="left">v1.14.0 and higher</td><td align="left">CoreDNS</td></tr><tr><td align="left">v0.2.5 and higher</td><td align="left">v1.13.x and lower</td><td align="left">kube-dns</td></tr><tr><td align="left">v0.2.4 and lower</td><td align="left">all</td><td align="left">kube-dns</td></tr></tbody></table><blockquote><p><strong>注意：</strong>不建议再使用 <code>kube-dns</code>。可以升级 RKE 或者 Rancher 版本，然后升级 kubernetes 版本，DNS 服务器软件将会自动更新。</p></blockquote><h3 id="hosts-插件"><a href="#hosts-插件" class="headerlink" title="hosts 插件"></a>hosts 插件</h3><p>Pod 的 <code>/etc/resolv.conf</code> 配置中默认的 DNS 为 CoreDNS（10.43.0.10）。应用访问外部域名的时候，会先把请求发送给 CoreDNS。</p><ol><li>在 CoreDNS 中未配置 hosts 插件的情况下<ul><li>如果 CoreDNS Server 内部数据库中有对应的 DNS 记录，则返回域名对应的 IP 给应用，应用则可以正常连接，相反则无法连接。</li></ul></li><li>在 CoreDNS 中配置 hosts 插件的情况下<ul><li>如果 CoreDNS Server 内部数据库中有对应的 DNS 记录，则返回域名对应的 IP 给应用。</li><li>如果 CoreDNS 内部数据库中没有对应的 DNS 记录，根据 hosts 的配置，如果匹配上对应的主机名，则把对应的 IP 返回给应用。</li></ul></li></ol><h3 id="配置格式"><a href="#配置格式" class="headerlink" title="配置格式"></a>配置格式</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">hosts</span> [<span class="string">FILE</span> [<span class="string">ZONES...</span>]] &#123;</span><br><span class="line">    [<span class="string">INLINE</span>]</span><br><span class="line">    <span class="string">ttl</span> <span class="string">SECONDS</span></span><br><span class="line">    <span class="string">no_reverse</span></span><br><span class="line">    <span class="string">reload</span> <span class="string">DURATION</span></span><br><span class="line">    <span class="string">fallthrough</span> [<span class="string">ZONES...</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>FILE：需要读取与解析的 hosts 文件；如果省略，默认取值 “&#x2F;etc&#x2F;hosts”；每 5s 扫描一次 hosts 文件的变更。</p></li><li><p>ZONES：如果为空，取配置块中的 zone。</p></li><li><p>INLINE：宿主机 hosts 文件在 corefile 中的内联；在 “fallthrough” 之前的所有 “INLINE” 都可视为 hosts 文件的附加内容，hosts 文件中相同条目将被覆盖，以 ”INLINE” 为准。</p></li><li><p>ttl：更改生成的 DNS 记录的 TTL（正向和反向），默认值为 3600 秒（1 小时）。</p></li><li><p>no_reverse：禁用自动生成主机的 <code>in-addr.arpa</code> 或 <code>ip6.arpa</code> 条目。</p></li><li><p>reload：文件重新加载之间的时间间隔</p></li><li><p>fallthrough：如果 zone 匹配且无法生成记录，将请求传递给下一个插件；如果省略，对所有 zones 有效，如果列出特定 zone，则只有列出的 zone 受到影响。</p></li></ul><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><p>依次访问 <strong>system 项目|资源|配置映射</strong>，在 <strong>kube-system</strong> 命名空间中找到 <strong>coredns</strong>，点击右侧省略号菜单并选择编辑。</p><h3 id="配置示例"><a href="#配置示例" class="headerlink" title="配置示例"></a>配置示例</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    health &#123;</span><br><span class="line">      lameduck 5s</span><br><span class="line">    &#125;</span><br><span class="line">    ready</span><br><span class="line">    hosts &#123;</span><br><span class="line">            <span class="comment"># 1 个 hostname 映射 1 个 ip；</span></span><br><span class="line">            172.30.200.21   www.test1.local</span><br><span class="line">            172.30.200.22   www.test2.local</span><br><span class="line">            172.30.200.23   www.test3.local</span><br><span class="line">            fallthrough</span><br><span class="line">        &#125;</span><br><span class="line">    kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">      pods insecure</span><br><span class="line">      fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9153</span><br><span class="line">    forward . <span class="string">&quot;/etc/resolv.conf&quot;</span></span><br><span class="line">    cache 30</span><br><span class="line">    loop</span><br><span class="line">    reload</span><br><span class="line">    loadbalance</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="更多参数"><a href="#更多参数" class="headerlink" title="更多参数"></a>更多参数</h2><p><a href="https://coredns.io/plugins/kubernetes/">https://coredns.io/plugins/kubernetes/</a></p>]]></content>
      
      
      <categories>
          
          <category> coredns </category>
          
      </categories>
      
      
        <tags>
            
            <tag> coredns </tag>
            
            <tag> coredns 最佳实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PLEG is not healthy（二）</title>
      <link href="/kubernetes/kubelet/pleg-is-not-healthy-2/"/>
      <url>/kubernetes/kubelet/pleg-is-not-healthy-2/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/kubelet/pleg-is-not-healthy-2/" target="_blank" title="https://www.xtplayer.cn/kubernetes/kubelet/pleg-is-not-healthy-2/">https://www.xtplayer.cn/kubernetes/kubelet/pleg-is-not-healthy-2/</a></p><p>上一篇文章（<a href="/Kubernetes/kubelet/pleg-is-not-healthy/">PLEG is not healthy (一)</a>）我们讲了 <code>PLEG is not healthy</code>  发生的原因，以及分析了 PLEG 的工作原理。</p><p>这篇文章再来说说实际遇到 <code>PLEG is not healthy</code>  时如何快速处理。</p><h2 id="PLEG-is-not-healthy-现象"><a href="#PLEG-is-not-healthy-现象" class="headerlink" title="PLEG is not healthy 现象"></a>PLEG is not healthy 现象</h2><p><code>Healthy()</code> 函数会以 “PLEG” 的形式添加到 <code>runtimeState</code> 中，Kubelet 在一个同步循环（<code>SyncLoop()</code> 函数）中会定期（默认是 10s）调用 <code>Healthy()</code> 函数。<code>Healthy()</code> 函数会检查 <code>relist</code> 进程（PLEG 的关键任务）是否在 3 分钟内完成。如果 relist 进程的完成时间超过 3 分钟，就会报告 <strong>PLEG is not healthy</strong>。</p><p>概括一点就是 kubelet 会每隔 10 秒去获取当前节点容器的健康状态，如果超过 3 分钟还没有完成，就会提示 <strong>PLEG is not healthy</strong>。 </p><p><strong>PLEG is not healthy</strong> 最直观的现象如下图：</p><img src="/kubernetes/kubelet/pleg-is-not-healthy-2/image-20200915103827616.png" class="" title="image-20200915103827616"><p>出现  <strong>PLEG is not healthy</strong> 后，节点无法再正常的调度运行 Pod。</p><h2 id="PLEG-is-not-healthy-分析"><a href="#PLEG-is-not-healthy-分析" class="headerlink" title="PLEG is not healthy 分析"></a>PLEG is not healthy 分析</h2><p>当出现 PLEG is not healthy，在 kubelet 日志中应该可以看到类似如下的错误日志：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">E0708 16:45:53.999538    2103 pod_workers.go:190] Error syncing pod d9cbbfb8-4ae6-4758-b3d4-fc106db98316 (<span class="string">&quot;wordpress-6bcf994cbd-8fbzd_default(d9cbbfb8-4ae6-4758-b3d4-fc106db98316)&quot;</span>), skipping: failed to <span class="string">&quot;StartContainer&quot;</span> <span class="keyword">for</span> <span class="string">&quot;wordpress&quot;</span> with CrashLoopBackOff: <span class="string">&quot;Back-off 5m0s restarting failed container=wordpress pod=wordpress-6bcf994cbd-8fbzd_default(d9cbbfb8-4ae6-4758-b3d4-fc106db98316)&quot;</span></span><br><span class="line"></span><br><span class="line">I0708 16:45:35.881181    2103 kubelet.go:2101] Failed to delete pod <span class="string">&quot;rancher-logging-fluentd-linux-qw6cv_cattle-logging(8536a1ac-d253-485c-9f29-fecd7bd4a42d)&quot;</span>, err: pod not found</span><br><span class="line"></span><br><span class="line">I0708 16:45:35.911218    2103 kubelet.go:1888] SyncLoop (ADD, <span class="string">&quot;api&quot;</span>): <span class="string">&quot;rancher-logging-fluentd-linux-5h7nc_cattle-logging(8c99d435-96fa-4f51-a1c5-0e7df963b83f)&quot;</span></span><br><span class="line"></span><br><span class="line">I0708 16:45:35.881181    2103 kubelet.go:2101] Failed killing the pod <span class="string">&quot;rancher-logging-fluentd-linux-qw6cv_cattle-logging(8536a1ac-d253-485c-9f29-fecd7bd4a42d)&quot;</span>, err: pod not found</span><br><span class="line"></span><br><span class="line">E0729 11:21:02.178079    9575 remote_runtime.go:321] ContainerStatus <span class="string">&quot;052b011f5fcc54536223afb2d544344c58e62b96bc836a64140c5e36af004ceb&quot;</span> from runtime service failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded</span><br><span class="line"></span><br><span class="line">E0729 11:21:02.178130    9575 kuberuntime_manager.go:917] getPodContainerStatuses <span class="keyword">for</span> pod <span class="string">&quot;bushuzu-nettools-88854d496-9nvlk_p000(472cb4d7-caf0-11ea-ab9d-ac1f6ba4811c)&quot;</span> failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded</span><br><span class="line"></span><br><span class="line">E0729 11:21:50.993186    9575 kubelet_pods.go:1093] Failed killing the pod <span class="string">&quot;bushuzu-nettools-88854d496-9nvlk&quot;</span>: failed to <span class="string">&quot;KillContainer&quot;</span> <span class="keyword">for</span> <span class="string">&quot;bushuzu-nettools&quot;</span> with KillContainerError: <span class="string">&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;</span></span><br><span class="line"></span><br><span class="line">I0729 11:20:59.245243    9575 kubelet.go:1823] skipping pod synchronization - PLEG is not healthy: pleg was last seen active 3m57.138893648s ago; threshold is 3m0s.</span><br></pre></td></tr></table></figure><h2 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h2><p>可能因为某些 K8S 依赖或者 docker 自身的原因，导致 Pod 或容器一直未能删除。</p><ol><li><p>强制删除 Pod</p><p><code>kubectl -n &lt;命名空间&gt; delete pod --force --grace-period=0 xxx</code></p></li><li><p>强制删除容器</p><p>对于日志报 KillContainerError 错误的，可以根据日志的提示，在主机上执行 <code>docker ps -a | grep &lt;Pod 名称&gt;</code> 来查询容器 ID，然后通过 <code>docker rm -f &lt;容器 id&gt;</code> 来删除容器。</p></li><li><p>通过 Pid 删除容器</p><p>有时候在第 2 步中执行 <code>docker rm -f &lt;容器 id&gt;</code> 删除容器时没有任何响应，这种情况可以通过 KILL PID 的方式来删除容器。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">C_ID_NAME=&lt;第 2 步中查询到的容器 ID&gt;</span><br><span class="line"></span><br><span class="line">C_PID=$(ps -ef | grep -v grep | grep `docker ps -a | grep <span class="variable">$&#123;C_ID_NAME&#125;</span> | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>` | awk <span class="string">&#x27;&#123;print $2 &#125;&#x27;</span>)</span><br><span class="line">C_PPID=$(ps --ppid <span class="variable">$C_PID</span>|grep -v PID|awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$C_PPID</span> <span class="variable">$C_PID</span></span><br><span class="line"><span class="built_in">kill</span> -9 <span class="variable">$C_PPID</span> <span class="variable">$C_PID</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><p>从自动化运维角度看，以上的操作建议做成脚本定时在主机上运行，通过分析 kubelet 日志中的关键字来自动处理。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> kubelet </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PLEG </tag>
            
            <tag> kubelet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PLEG is not healthy (一)</title>
      <link href="/kubernetes/kubelet/pleg-is-not-healthy/"/>
      <url>/kubernetes/kubelet/pleg-is-not-healthy/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/kubelet/pleg-is-not-healthy/" target="_blank" title="https://www.xtplayer.cn/kubernetes/kubelet/pleg-is-not-healthy/">https://www.xtplayer.cn/kubernetes/kubelet/pleg-is-not-healthy/</a></p><h2 id="PLEG-是什么？"><a href="#PLEG-是什么？" class="headerlink" title="PLEG 是什么？"></a>PLEG 是什么？</h2><p>PLEG 全称叫 <code>Pod Lifecycle Event Generator</code>，即 Pod 生命周期事件生成器。实际上它只是 <code>Kubelet</code> 中的一个模块，主要职责就是通过匹配每个的 Pod 事件级别来调整容器运行时的状态，并将调整的结果写入缓存，使 <code>Pod</code> 的缓存保持最新状态。</p><h2 id="PLEG-出现的背景"><a href="#PLEG-出现的背景" class="headerlink" title="PLEG 出现的背景"></a>PLEG 出现的背景</h2><p>在 Kubernetes 中，每个节点上都运行着守护进程 <code>Kubelet</code> 来管理节点上的容器，调整容器的实际状态以匹配 <code>spec</code> 中定义的状态。具体来说，Kubelet 需要对两个地方的更改做出及时的回应:</p><ul><li>Pod spec 中定义的状态</li><li>容器运行时的状态</li></ul><p>对于 Pod，Kubelet 会从多个数据来源 <code>watch</code> Pod spec 中的变化。对于容器，Kubelet 会定期（例如，10s）轮询容器运行时，以获取所有容器的最新状态。</p><p>随着 Pod 和容器数量的增加，轮询会产生不可忽略的开销，并且会由于 Kubelet 的并行操作而加剧这种开销（为每个 Pod 分配一个 <code>goruntine</code>，用来获取容器的状态）。轮询带来的周期性大量并发请求会导致较高的 CPU 使用率峰值（即使 Pod 的定义和容器的状态没有发生改变），降低性能。最后容器运行时可能不堪重负，从而降低系统的可靠性，限制 Kubelet 的可扩展性。</p><p>为了降低 Pod 的管理开销，提升 Kubelet 的性能和可扩展性，引入了 PLEG，改进了之前的工作方式：</p><ul><li>减少空闲期间的不必要工作（例如 Pod 的定义和容器的状态没有发生更改）。</li><li>减少获取容器状态的并发请求数量。</li></ul><p>整体的工作流程如下图所示，虚线部分是 PLEG 的工作内容:</p><img src="/kubernetes/kubelet/pleg-is-not-healthy/ykjn32j5n8.png" class="" title="ykjn32j5n8"><h2 id="PLEG-is-not-healthy-是如何发生？"><a href="#PLEG-is-not-healthy-是如何发生？" class="headerlink" title="PLEG is not healthy 是如何发生？"></a>PLEG is not healthy 是如何发生？</h2><p><code>Healthy()</code> 函数会以 “PLEG” 的形式添加到 <code>runtimeState</code> 中，Kubelet 在一个同步循环（<code>SyncLoop()</code> 函数）中会定期（默认是 10s）调用 <code>Healthy()</code> 函数。<code>Healthy()</code> 函数会检查 <code>relist</code> 进程（PLEG 的关键任务）是否在 3 分钟内完成。如果 relist 进程的完成时间超过 3 分钟，就会报告 <strong>PLEG is not healthy</strong>。</p><img src="/kubernetes/kubelet/pleg-is-not-healthy/0lwunp8jgm.png" class="" title="0lwunp8jgm"><h2 id="PLEG-工作流程"><a href="#PLEG-工作流程" class="headerlink" title="PLEG 工作流程"></a>PLEG 工作流程</h2><p>以下流程的每一步通过源代码解释其相关的工作原理，源代码基于 Kubernetes 1.11。</p><h3 id="Healthy-函数"><a href="#Healthy-函数" class="headerlink" title="Healthy() 函数"></a>Healthy() 函数</h3><p>healthy() 函数的相关代码：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - Healthy()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The threshold needs to be greater than the relisting period + the</span></span><br><span class="line"><span class="comment">// relisting time, which can vary significantly. Set a conservative</span></span><br><span class="line"><span class="comment">// threshold to avoid flipping between healthy and unhealthy.</span></span><br><span class="line">relistThreshold = <span class="number">3</span> * time.<span class="property">Minute</span></span><br><span class="line">:</span><br><span class="line">func (g *<span class="title class_">GenericPLEG</span>) <span class="title class_">Healthy</span>() (bool, error) &#123;</span><br><span class="line">  relistTime := g.<span class="title function_">getRelistTime</span>()</span><br><span class="line">  elapsed := g.<span class="property">clock</span>.<span class="title class_">Since</span>(relistTime)</span><br><span class="line">  <span class="keyword">if</span> elapsed &gt; relistThreshold &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>, fmt.<span class="title class_">Errorf</span>(<span class="string">&quot;pleg was last seen active %v ago; threshold is %v&quot;</span>, elapsed, relistThreshold)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>, nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/kubelet.go - NewMainKubelet()</span></span><br><span class="line">func <span class="title class_">NewMainKubelet</span>(kubeCfg *kubeletconfiginternal.<span class="property">KubeletConfiguration</span>, ...</span><br><span class="line">:</span><br><span class="line">  klet.<span class="property">runtimeState</span>.<span class="title function_">addHealthCheck</span>(<span class="string">&quot;PLEG&quot;</span>, klet.<span class="property">pleg</span>.<span class="property">Healthy</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/kubelet.go - syncLoop()</span></span><br><span class="line">func (kl *<span class="title class_">Kubelet</span>) <span class="title function_">syncLoop</span>(<span class="params">updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler</span>) &#123;</span><br><span class="line">:</span><br><span class="line"><span class="comment">// The resyncTicker wakes up kubelet to checks if there are any pod workers</span></span><br><span class="line"><span class="comment">// that need to be sync&#x27;d. A one-second period is sufficient because the</span></span><br><span class="line"><span class="comment">// sync interval is defaulted to 10s.</span></span><br><span class="line">:</span><br><span class="line">  <span class="keyword">const</span> (</span><br><span class="line">    base   = <span class="number">100</span> * time.<span class="property">Millisecond</span></span><br><span class="line">    max    = <span class="number">5</span> * time.<span class="property">Second</span></span><br><span class="line">    factor = <span class="number">2</span></span><br><span class="line">  )</span><br><span class="line">  duration := base</span><br><span class="line">  <span class="keyword">for</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> rs := kl.<span class="property">runtimeState</span>.<span class="title function_">runtimeErrors</span>(); <span class="title function_">len</span>(rs) != <span class="number">0</span> &#123;</span><br><span class="line">          glog.<span class="title class_">Infof</span>(<span class="string">&quot;skipping pod synchronization - %v&quot;</span>, rs)</span><br><span class="line">          <span class="comment">// exponential backoff</span></span><br><span class="line">          time.<span class="title class_">Sleep</span>(duration)</span><br><span class="line">          duration = time.<span class="title class_">Duration</span>(math.<span class="title class_">Min</span>(<span class="title function_">float64</span>(max), factor*<span class="title function_">float64</span>(duration)))</span><br><span class="line">          <span class="keyword">continue</span></span><br><span class="line">      &#125;</span><br><span class="line">    :</span><br><span class="line">  &#125;</span><br><span class="line">:</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/runtime.go - runtimeErrors()</span></span><br><span class="line">func (s *runtimeState) <span class="title function_">runtimeErrors</span>() []string &#123;</span><br><span class="line">:</span><br><span class="line">    <span class="keyword">for</span> _, hc := range s.<span class="property">healthChecks</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> ok, err := hc.<span class="title function_">fn</span>(); !ok &#123;</span><br><span class="line">            ret = <span class="title function_">append</span>(ret, fmt.<span class="title class_">Sprintf</span>(<span class="string">&quot;%s is not healthy: %v&quot;</span>, hc.<span class="property">name</span>, err))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">:</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="relist-函数"><a href="#relist-函数" class="headerlink" title="relist 函数"></a>relist 函数</h3><p>上文提到 <code>healthy()</code> 函数会检查 <strong>relist</strong> 的完成时间，但 <strong>relist</strong> 究竟是用来干嘛的呢？解释 relist 之前，要先解释一下 Pod 的生命周期事件。Pod 的生命周期事件是在 Pod 层面上对底层容器状态改变的抽象，使其与底层的容器运行时无关，这样就可以让 Kubelet 不受底层容器运行时的影响。</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">type <span class="title class_">PodLifeCycleEventType</span> string</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">    <span class="title class_">ContainerStarted</span>      <span class="title class_">PodLifeCycleEventType</span> = <span class="string">&quot;ContainerStarted&quot;</span></span><br><span class="line">    <span class="title class_">ContainerStopped</span>      <span class="title class_">PodLifeCycleEventType</span> = <span class="string">&quot;ContainerStopped&quot;</span></span><br><span class="line">    <span class="title class_">NetworkSetupCompleted</span> <span class="title class_">PodLifeCycleEventType</span> = <span class="string">&quot;NetworkSetupCompleted&quot;</span></span><br><span class="line">    <span class="title class_">NetworkFailed</span>         <span class="title class_">PodLifeCycleEventType</span> = <span class="string">&quot;NetworkFailed&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// PodLifecycleEvent is an event reflects the change of the pod state.</span></span><br><span class="line">type <span class="title class_">PodLifecycleEvent</span> struct &#123;</span><br><span class="line">    <span class="comment">// The pod ID.</span></span><br><span class="line">    <span class="variable constant_">ID</span> types.<span class="property">UID</span></span><br><span class="line">    <span class="comment">// The type of the event.</span></span><br><span class="line">    <span class="title class_">Type</span> <span class="title class_">PodLifeCycleEventType</span></span><br><span class="line">    <span class="comment">// The accompanied data which varies based on the event type.</span></span><br><span class="line">    <span class="title class_">Data</span> interface&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以 Docker 为例，在 Pod 中启动一个 <strong>infra</strong> 容器就会在 Kubelet 中注册一个 <code>NetworkSetupCompleted</code> Pod 生命周期事件。</p><ul><li>那么 PLEG 是如何知道新启动了一个 infra 容器呢？</li></ul><p>它会定期重新列出节点上的所有容器（例如：<strong>docker ps</strong>），并与上一次的容器列表进行对比，以此来判断容器状态的变化。其实这就是 <code>relist()</code> 函数干的事情，尽管这种方法和以前的 Kubelet 轮询类似，但现在只有一个线程，就是 PLEG。现在不需要所有的线程并发获取容器的状态，只有相关的线程会被唤醒用来同步容器状态。而且 <strong>relist</strong> 与容器运行时无关，也不需要外部依赖。</p><p>下面来看一下 <code>relist()</code> 函数的内部实现，完整的流程如下图所示：</p><img src="/kubernetes/kubelet/pleg-is-not-healthy/pleg-process.png" class="" title="img"><p>注意图中的 <strong>RPC</strong> 调用部分，后文将会详细说明。完整的源代码 <a href="https://github.com/openshift/origin/blob/release-3.11/vendor/k8s.io/kubernetes/pkg/kubelet/pleg/generic.go#L180-L284%E3%80%82">https://github.com/openshift/origin/blob/release-3.11/vendor/k8s.io/kubernetes/pkg/kubelet/pleg/generic.go#L180-L284。</a></p><ol><li>尽管每秒钟调用一次 <code>relist</code>，但它的完成时间仍然有可能超过 1s。因为下一次调用 <code>relist</code> 必须得等上一次 relist 执行结束，设想一下，如果容器运行时响应缓慢，或者一个周期内有大量的容器状态发生改变，那么 <code>relist</code> 的完成时间将不可忽略，假设是 5s，那么下一次调用 <code>relist</code> 将要等到 6s 之后。</li></ol><img src="/kubernetes/kubelet/pleg-is-not-healthy/pleg-start-relist.png" class="" title="img"><p>相关的源代码如下：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/kubelet.go - NewMainKubelet()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Generic PLEG relies on relisting for discovering container events.</span></span><br><span class="line"><span class="comment">// A longer period means that kubelet will take longer to detect container</span></span><br><span class="line"><span class="comment">// changes and to update pod status. On the other hand, a shorter period</span></span><br><span class="line"><span class="comment">// will cause more frequent relisting (e.g., container runtime operations),</span></span><br><span class="line"><span class="comment">// leading to higher cpu usage.</span></span><br><span class="line"><span class="comment">// Note that even though we set the period to 1s, the relisting itself can</span></span><br><span class="line"><span class="comment">// take more than 1s to finish if the container runtime responds slowly</span></span><br><span class="line"><span class="comment">// and/or when there are many container changes in one cycle.</span></span><br><span class="line">plegRelistPeriod = time.<span class="property">Second</span> * <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules.</span></span><br><span class="line"><span class="comment">// No initialization of Kubelet and its modules should happen here.</span></span><br><span class="line">func <span class="title class_">NewMainKubelet</span>(kubeCfg *kubeletconfiginternal.<span class="property">KubeletConfiguration</span>, ...</span><br><span class="line">:</span><br><span class="line">  klet.<span class="property">pleg</span> = pleg.<span class="title class_">NewGenericPLEG</span>(klet.<span class="property">containerRuntime</span>, plegChannelCapacity, plegRelistPeriod, klet.<span class="property">podCache</span>, clock.<span class="property">RealClock</span>&#123;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - Start()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Start spawns a goroutine to relist periodically.</span></span><br><span class="line">func (g *<span class="title class_">GenericPLEG</span>) <span class="title class_">Start</span>() &#123;</span><br><span class="line">  go wait.<span class="title class_">Until</span>(g.<span class="property">relist</span>, g.<span class="property">relistPeriod</span>, wait.<span class="property">NeverStop</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - relist()</span></span><br><span class="line">func (g *<span class="title class_">GenericPLEG</span>) <span class="title function_">relist</span>(<span class="params"></span>) &#123;</span><br><span class="line">... <span class="variable constant_">WE</span> <span class="variable constant_">WILL</span> <span class="variable constant_">REVIEW</span> <span class="variable constant_">HERE</span> ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>回到上面那幅图，<strong>relist</strong> 函数第一步就是记录 <code>Kubelet</code> 的相关指标（例如 <code>kubelet_pleg_relist_latency_microseconds</code>），然后通过 CRI 从容器运行时获取当前的 Pod 列表（包括停止的 Pod）。该 Pod 列表会和之前的 Pod 列表进行比较，检查哪些状态发生了变化，然后同时生成相关的 <strong>Pod 生命周期事件</strong>和<strong>更改后的状态</strong>。</li></ol><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - relist()</span></span><br><span class="line">  :</span><br><span class="line">  <span class="comment">// get a current timestamp</span></span><br><span class="line">  timestamp := g.<span class="property">clock</span>.<span class="title class_">Now</span>()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// kubelet_pleg_relist_latency_microseconds for prometheus metrics</span></span><br><span class="line">    defer <span class="title function_">func</span>(<span class="params"></span>) &#123;</span><br><span class="line">        metrics.<span class="property">PLEGRelistLatency</span>.<span class="title class_">Observe</span>(metrics.<span class="title class_">SinceInMicroseconds</span>(timestamp))</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get all the pods.</span></span><br><span class="line">    podList, err := g.<span class="property">runtime</span>.<span class="title class_">GetPods</span>(<span class="literal">true</span>)</span><br><span class="line">  :</span><br></pre></td></tr></table></figure><p>其中 <code>GetPods()</code> 函数的调用堆栈如下图所示：</p><img src="/kubernetes/kubelet/pleg-is-not-healthy/pleg-getpods.png" class="" title="img"><p>相关的源代码如下：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/kuberuntime/kuberuntime_manager.go - GetPods()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// GetPods returns a list of containers grouped by pods. The boolean parameter</span></span><br><span class="line"><span class="comment">// specifies whether the runtime returns all containers including those already</span></span><br><span class="line"><span class="comment">// exited and dead containers (used for garbage collection).</span></span><br><span class="line">func (m *kubeGenericRuntimeManager) <span class="title class_">GetPods</span>(all bool) ([]*kubecontainer.<span class="property">Pod</span>, error) &#123;</span><br><span class="line">    pods := <span class="title function_">make</span>(map[kubetypes.<span class="property">UID</span>]*kubecontainer.<span class="property">Pod</span>)</span><br><span class="line">    sandboxes, err := m.<span class="title function_">getKubeletSandboxes</span>(all)</span><br><span class="line">:</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/kuberuntime/kuberuntime_sandbox.go - getKubeletSandboxes()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// getKubeletSandboxes lists all (or just the running) sandboxes managed by kubelet.</span></span><br><span class="line">func (m *kubeGenericRuntimeManager) <span class="title function_">getKubeletSandboxes</span>(all bool) ([]*runtimeapi.<span class="property">PodSandbox</span>, error) &#123;</span><br><span class="line">:</span><br><span class="line">    resp, err := m.<span class="property">runtimeService</span>.<span class="title class_">ListPodSandbox</span>(filter)</span><br><span class="line">:</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/remote/remote_runtime.go - ListPodSandbox()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ListPodSandbox returns a list of PodSandboxes.</span></span><br><span class="line">func (r *<span class="title class_">RemoteRuntimeService</span>) <span class="title class_">ListPodSandbox</span>(filter *runtimeapi.<span class="property">PodSandboxFilter</span>) ([]*runtimeapi.<span class="property">PodSandbox</span>, error) &#123;</span><br><span class="line">:</span><br><span class="line">    resp, err := r.<span class="property">runtimeClient</span>.<span class="title class_">ListPodSandbox</span>(ctx, &amp;runtimeapi.<span class="property">ListPodSandboxRequest</span>&#123;</span><br><span class="line">:</span><br><span class="line">    <span class="keyword">return</span> resp.<span class="property">Items</span>, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>获取所有的 Pod 列表后，<code>relist</code> 的完成时间就会更新成当前的时间戳。也就是说，<code>Healthy()</code> 函数可以根据这个时间戳来评估 relist 是否超过了 3 分钟。</li></ol><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - relist()</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// update as a current timestamp</span></span><br><span class="line">  g.<span class="title function_">updateRelistTime</span>(timestamp)</span><br></pre></td></tr></table></figure><ol><li><p>将当前的 Pod 列表和上一次 relist 的 Pod 列表进行对比之后，就会针对每一个变化生成相应的 Pod 级别的事件。</p><p>其中 <code>generateEvents()</code> 函数（<code>computeEvents()</code> 函数会调用它）用来生成相应的 Pod 级别的事件（例如 <code>ContainerStarted</code>、<code>ContainerDied</code> 等等），然后通过 <code>updateEvents()</code> 函数来更新事件。</p><p>相关的源代码如下：</p></li></ol><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - relist()</span></span><br><span class="line"></span><br><span class="line">  pods := kubecontainer.<span class="title class_">Pods</span>(podList)</span><br><span class="line">  g.<span class="property">podRecords</span>.<span class="title function_">setCurrent</span>(pods)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Compare the old and the current pods, and generate events.</span></span><br><span class="line">  eventsByPodID := map[types.<span class="property">UID</span>][]*<span class="title class_">PodLifecycleEvent</span>&#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> pid := range g.<span class="property">podRecords</span> &#123;</span><br><span class="line">    oldPod := g.<span class="property">podRecords</span>.<span class="title function_">getOld</span>(pid)</span><br><span class="line">    pod := g.<span class="property">podRecords</span>.<span class="title function_">getCurrent</span>(pid)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get all containers in the old and the new pod.</span></span><br><span class="line">    allContainers := <span class="title function_">getContainersFromPods</span>(oldPod, pod)</span><br><span class="line">    <span class="keyword">for</span> _, container := range allContainers &#123;</span><br><span class="line">          events := <span class="title function_">computeEvents</span>(oldPod, pod, &amp;container.<span class="property">ID</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">for</span> _, e := range events &#123;</span><br><span class="line">                <span class="title function_">updateEvents</span>(eventsByPodID, e)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>​<code>computeEvents()</code> 函数的内容如下：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - computeEvents()</span></span><br><span class="line"></span><br><span class="line">func <span class="title function_">computeEvents</span>(oldPod, newPod *kubecontainer.<span class="property">Pod</span>, cid *kubecontainer.<span class="property">ContainerID</span>) []*<span class="title class_">PodLifecycleEvent</span> &#123;</span><br><span class="line">:</span><br><span class="line">    <span class="keyword">return</span> <span class="title function_">generateEvents</span>(pid, cid.<span class="property">ID</span>, oldState, newState)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - generateEvents()</span></span><br><span class="line"></span><br><span class="line">func <span class="title function_">generateEvents</span>(podID types.<span class="property">UID</span>, cid string, oldState, newState plegContainerState) []*<span class="title class_">PodLifecycleEvent</span> &#123;</span><br><span class="line">:</span><br><span class="line">    glog.<span class="title function_">V</span>(<span class="number">4</span>).<span class="title class_">Infof</span>(<span class="string">&quot;GenericPLEG: %v/%v: %v -&gt; %v&quot;</span>, podID, cid, oldState, newState)</span><br><span class="line">    <span class="keyword">switch</span> newState &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">plegContainerRunning</span>:</span><br><span class="line">      <span class="keyword">return</span> []*<span class="title class_">PodLifecycleEvent</span>&#123;&#123;<span class="attr">ID</span>: podID, <span class="title class_">Type</span>: <span class="title class_">ContainerStarted</span>, <span class="title class_">Data</span>: cid&#125;&#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">plegContainerExited</span>:</span><br><span class="line">      <span class="keyword">return</span> []*<span class="title class_">PodLifecycleEvent</span>&#123;&#123;<span class="attr">ID</span>: podID, <span class="title class_">Type</span>: <span class="title class_">ContainerDied</span>, <span class="title class_">Data</span>: cid&#125;&#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">plegContainerUnknown</span>:</span><br><span class="line">      <span class="keyword">return</span> []*<span class="title class_">PodLifecycleEvent</span>&#123;&#123;<span class="attr">ID</span>: podID, <span class="title class_">Type</span>: <span class="title class_">ContainerChanged</span>, <span class="title class_">Data</span>: cid&#125;&#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">plegContainerNonExistent</span>:</span><br><span class="line">      <span class="keyword">switch</span> oldState &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="attr">plegContainerExited</span>:</span><br><span class="line">        <span class="comment">// We already reported that the container died before.</span></span><br><span class="line">        <span class="keyword">return</span> []*<span class="title class_">PodLifecycleEvent</span>&#123;&#123;<span class="attr">ID</span>: podID, <span class="title class_">Type</span>: <span class="title class_">ContainerRemoved</span>, <span class="title class_">Data</span>: cid&#125;&#125;</span><br><span class="line">      <span class="attr">default</span>:</span><br><span class="line">        <span class="keyword">return</span> []*<span class="title class_">PodLifecycleEvent</span>&#123;&#123;<span class="attr">ID</span>: podID, <span class="title class_">Type</span>: <span class="title class_">ContainerDied</span>, <span class="title class_">Data</span>: cid&#125;, &#123;<span class="attr">ID</span>: podID, <span class="title class_">Type</span>: <span class="title class_">ContainerRemoved</span>, <span class="title class_">Data</span>: cid&#125;&#125;</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="attr">default</span>:</span><br><span class="line">      <span class="title function_">panic</span>(fmt.<span class="title class_">Sprintf</span>(<span class="string">&quot;unrecognized container state: %v&quot;</span>, newState))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li><p>relist 的最后一个任务是检查是否有与 Pod 关联的事件，并按照下面的流程更新 <code>podCache</code>。</p><p><code>updateCache()</code> 将会检查每个 Pod，并在单个循环中依次对其进行更新。因此，如果在同一个 relist 中更改了大量的 Pod，那么 updateCache 过程将会成为瓶颈。最后，更新后的 Pod 生命周期事件将会被发送到 <code>eventChannel</code>。</p><p>某些远程客户端还会调用每一个 Pod 来获取 Pod 的 spec 定义信息，这样一来，Pod 数量越多，延时就可能越高，因为 Pod 越多就会生成越多的事件。</p></li></ol><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - relist()</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// If there are events associated with a pod, we should update the</span></span><br><span class="line">  <span class="comment">// podCache.</span></span><br><span class="line">  <span class="keyword">for</span> pid, events := range eventsByPodID &#123;</span><br><span class="line">    pod := g.<span class="property">podRecords</span>.<span class="title function_">getCurrent</span>(pid)</span><br><span class="line">    <span class="keyword">if</span> g.<span class="title function_">cacheEnabled</span>(<span class="params"></span>) &#123;</span><br><span class="line">      <span class="comment">// updateCache() will inspect the pod and update the cache. If an</span></span><br><span class="line">      <span class="comment">// error occurs during the inspection, we want PLEG to retry again</span></span><br><span class="line">      <span class="comment">// in the next relist. To achieve this, we do not update the</span></span><br><span class="line">      <span class="comment">// associated podRecord of the pod, so that the change will be</span></span><br><span class="line">      <span class="comment">// detect again in the next relist.</span></span><br><span class="line">      <span class="comment">// <span class="doctag">TODO:</span> If many pods changed during the same relist period,</span></span><br><span class="line">      <span class="comment">// inspecting the pod and getting the PodStatus to update the cache</span></span><br><span class="line">      <span class="comment">// serially may take a while. We should be aware of this and</span></span><br><span class="line">      <span class="comment">// parallelize if needed.</span></span><br><span class="line">      <span class="keyword">if</span> err := g.<span class="title function_">updateCache</span>(pod, pid); err != nil &#123;</span><br><span class="line">        glog.<span class="title class_">Errorf</span>(<span class="string">&quot;PLEG: Ignoring events for pod %s/%s: %v&quot;</span>, pod.<span class="property">Name</span>, pod.<span class="property">Namespace</span>, err)</span><br><span class="line">        :</span><br><span class="line">      &#125;</span><br><span class="line">      :</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Update the internal storage and send out the events.</span></span><br><span class="line">    g.<span class="property">podRecords</span>.<span class="title function_">update</span>(pid)</span><br><span class="line">    <span class="keyword">for</span> i := range events &#123;</span><br><span class="line">      <span class="comment">// Filter out events that are not reliable and no other components use yet.</span></span><br><span class="line">      <span class="keyword">if</span> events[i].<span class="property">Type</span> == <span class="title class_">ContainerChanged</span> &#123;</span><br><span class="line">           <span class="keyword">continue</span></span><br><span class="line">      &#125;</span><br><span class="line">      g.<span class="property">eventChannel</span> &lt;- events[i]</span><br><span class="line">     &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>​<code>updateCache()</code> 的详细调用堆栈如下图所示，其中 <code>GetPodStatus()</code> 用来获取 Pod 的 spec 定义信息：</p><img src="/kubernetes/kubelet/pleg-is-not-healthy/pleg-updatecache.png" class="" title="img"><p>​相关代码如下：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">//// pkg/kubelet/pleg/generic.go - updateCache()</span></span><br><span class="line"></span><br><span class="line">func (g *<span class="title class_">GenericPLEG</span>) <span class="title function_">updateCache</span>(pod *kubecontainer.<span class="property">Pod</span>, pid types.<span class="property">UID</span>) error &#123;</span><br><span class="line">:</span><br><span class="line">    timestamp := g.<span class="property">clock</span>.<span class="title class_">Now</span>()</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Consider adding a new runtime method</span></span><br><span class="line">    <span class="comment">// GetPodStatus(pod *kubecontainer.Pod) so that Docker can avoid listing</span></span><br><span class="line">    <span class="comment">// all containers again.</span></span><br><span class="line">    status, err := g.<span class="property">runtime</span>.<span class="title class_">GetPodStatus</span>(pod.<span class="property">ID</span>, pod.<span class="property">Name</span>, pod.<span class="property">Namespace</span>)</span><br><span class="line">  :</span><br><span class="line">    g.<span class="property">cache</span>.<span class="title class_">Set</span>(pod.<span class="property">ID</span>, status, err, timestamp)</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/kuberuntime/kuberuntime_manager.go - GetPodStatus()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// GetPodStatus retrieves the status of the pod, including the</span></span><br><span class="line"><span class="comment">// information of all containers in the pod that are visible in Runtime.</span></span><br><span class="line">func (m *kubeGenericRuntimeManager) <span class="title class_">GetPodStatus</span>(uid kubetypes.<span class="property">UID</span>, name, namespace string) (*kubecontainer.<span class="property">PodStatus</span>, error) &#123;</span><br><span class="line">  podSandboxIDs, err := m.<span class="title function_">getSandboxIDByPodUID</span>(uid, nil)</span><br><span class="line">  :</span><br><span class="line">    <span class="keyword">for</span> idx, podSandboxID := range podSandboxIDs &#123;</span><br><span class="line">        podSandboxStatus, err := m.<span class="property">runtimeService</span>.<span class="title class_">PodSandboxStatus</span>(podSandboxID)</span><br><span class="line">    :</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get statuses of all containers visible in the pod.</span></span><br><span class="line">    containerStatuses, err := m.<span class="title function_">getPodContainerStatuses</span>(uid, name, namespace)</span><br><span class="line">  :</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/kuberuntime/kuberuntime_sandbox.go - getSandboxIDByPodUID()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// getPodSandboxID gets the sandbox id by podUID and returns ([]sandboxID, error).</span></span><br><span class="line"><span class="comment">// Param state could be nil in order to get all sandboxes belonging to same pod.</span></span><br><span class="line">func (m *kubeGenericRuntimeManager) <span class="title function_">getSandboxIDByPodUID</span>(podUID kubetypes.<span class="property">UID</span>, state *runtimeapi.<span class="property">PodSandboxState</span>) ([]string, error) &#123;</span><br><span class="line">  :</span><br><span class="line">  sandboxes, err := m.<span class="property">runtimeService</span>.<span class="title class_">ListPodSandbox</span>(filter)</span><br><span class="line">  :</span><br><span class="line">  <span class="keyword">return</span> sandboxIDs, nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/remote/remote_runtime.go - PodSandboxStatus()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// PodSandboxStatus returns the status of the PodSandbox.</span></span><br><span class="line">func (r *<span class="title class_">RemoteRuntimeService</span>) <span class="title class_">PodSandboxStatus</span>(podSandBoxID string) (*runtimeapi.<span class="property">PodSandboxStatus</span>, error) &#123;</span><br><span class="line">    ctx, cancel := <span class="title function_">getContextWithTimeout</span>(r.<span class="property">timeout</span>)</span><br><span class="line">    defer <span class="title function_">cancel</span>()</span><br><span class="line"></span><br><span class="line">    resp, err := r.<span class="property">runtimeClient</span>.<span class="title class_">PodSandboxStatus</span>(ctx, &amp;runtimeapi.<span class="property">PodSandboxStatusRequest</span>&#123;</span><br><span class="line">        <span class="title class_">PodSandboxId</span>: podSandBoxID,</span><br><span class="line">    &#125;)</span><br><span class="line">  :</span><br><span class="line">    <span class="keyword">return</span> resp.<span class="property">Status</span>, nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//// pkg/kubelet/kuberuntime/kuberuntime_container.go - getPodContainerStatuses()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// getPodContainerStatuses gets all containers&#x27; statuses for the pod.</span></span><br><span class="line">func (m *kubeGenericRuntimeManager) <span class="title function_">getPodContainerStatuses</span>(uid kubetypes.<span class="property">UID</span>, name, namespace string) ([]*kubecontainer.<span class="property">ContainerStatus</span>, error) &#123;</span><br><span class="line">  <span class="comment">// Select all containers of the given pod.</span></span><br><span class="line">  containers, err := m.<span class="property">runtimeService</span>.<span class="title class_">ListContainers</span>(&amp;runtimeapi.<span class="property">ContainerFilter</span>&#123;</span><br><span class="line">    <span class="title class_">LabelSelector</span>: map[string]string&#123;types.<span class="property">KubernetesPodUIDLabel</span>: <span class="title function_">string</span>(uid)&#125;,</span><br><span class="line">  &#125;)</span><br><span class="line">  :</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> optimization: set maximum number of containers per container name to examine.</span></span><br><span class="line">  <span class="keyword">for</span> i, c := range containers &#123;</span><br><span class="line">    status, err := m.<span class="property">runtimeService</span>.<span class="title class_">ContainerStatus</span>(c.<span class="property">Id</span>)</span><br><span class="line">    :</span><br><span class="line">  &#125;</span><br><span class="line">  :</span><br><span class="line">  <span class="keyword">return</span> statuses, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="监控-relist"><a href="#监控-relist" class="headerlink" title="监控 relist"></a>监控 relist</h3><p>我们可以通过监控 Kubelet 的指标来了解 <code>relist</code> 的延时。<code>relist</code> 的调用周期是 1s，那么 <strong>relist 的完成时间 + 1s</strong> 就等于 <code>kubelet_pleg_relist_interval_microseconds</code> 指标的值。你也可以监控容器运行时每个操作的延时，这些指标在排查故障时都能提供线索。</p><img src="/kubernetes/kubelet/pleg-is-not-healthy/pleg-kubelet-metrics-table.png" class="" title="img"><p>你可以在每个节点上通过访问 URL <code>https://127.0.0.1:10250/metrics</code> 来获取 Kubelet 的指标。</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"># <span class="variable constant_">HELP</span> kubelet_pleg_relist_interval_microseconds <span class="title class_">Interval</span> <span class="keyword">in</span> microseconds between relisting <span class="keyword">in</span> <span class="variable constant_">PLEG</span>.</span><br><span class="line"># <span class="variable constant_">TYPE</span> kubelet_pleg_relist_interval_microseconds summary</span><br><span class="line">kubelet_pleg_relist_interval_microseconds&#123;quantile=<span class="string">&quot;0.5&quot;</span>&#125; <span class="number">1.054052e+06</span></span><br><span class="line">kubelet_pleg_relist_interval_microseconds&#123;quantile=<span class="string">&quot;0.9&quot;</span>&#125; <span class="number">1.074873e+06</span></span><br><span class="line">kubelet_pleg_relist_interval_microseconds&#123;quantile=<span class="string">&quot;0.99&quot;</span>&#125; <span class="number">1.126039e+06</span></span><br><span class="line">kubelet_pleg_relist_interval_microseconds_count <span class="number">5146</span></span><br><span class="line"></span><br><span class="line"># <span class="variable constant_">HELP</span> kubelet_pleg_relist_latency_microseconds <span class="title class_">Latency</span> <span class="keyword">in</span> microseconds <span class="keyword">for</span> relisting pods <span class="keyword">in</span> <span class="variable constant_">PLEG</span>.</span><br><span class="line"># <span class="variable constant_">TYPE</span> kubelet_pleg_relist_latency_microseconds summary</span><br><span class="line">kubelet_pleg_relist_latency_microseconds&#123;quantile=<span class="string">&quot;0.5&quot;</span>&#125; <span class="number">53438</span></span><br><span class="line">kubelet_pleg_relist_latency_microseconds&#123;quantile=<span class="string">&quot;0.9&quot;</span>&#125; <span class="number">74396</span></span><br><span class="line">kubelet_pleg_relist_latency_microseconds&#123;quantile=<span class="string">&quot;0.99&quot;</span>&#125; <span class="number">115232</span></span><br><span class="line">kubelet_pleg_relist_latency_microseconds_count <span class="number">5106</span></span><br><span class="line"></span><br><span class="line"># <span class="variable constant_">HELP</span> kubelet_runtime_operations <span class="title class_">Cumulative</span> number <span class="keyword">of</span> runtime operations by operation type.</span><br><span class="line"># <span class="variable constant_">TYPE</span> kubelet_runtime_operations counter</span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;container_status&quot;</span>&#125; <span class="number">472</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;create_container&quot;</span>&#125; <span class="number">93</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;exec&quot;</span>&#125; <span class="number">1</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;exec_sync&quot;</span>&#125; <span class="number">533</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;image_status&quot;</span>&#125; <span class="number">579</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;list_containers&quot;</span>&#125; <span class="number">10249</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;list_images&quot;</span>&#125; <span class="number">782</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;list_podsandbox&quot;</span>&#125; <span class="number">10154</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;podsandbox_status&quot;</span>&#125; <span class="number">315</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;pull_image&quot;</span>&#125; <span class="number">57</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;remove_container&quot;</span>&#125; <span class="number">49</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;run_podsandbox&quot;</span>&#125; <span class="number">28</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;start_container&quot;</span>&#125; <span class="number">93</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;status&quot;</span>&#125; <span class="number">1116</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;stop_container&quot;</span>&#125; <span class="number">9</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;stop_podsandbox&quot;</span>&#125; <span class="number">33</span></span><br><span class="line">kubelet_runtime_operations&#123;operation_type=<span class="string">&quot;version&quot;</span>&#125; <span class="number">564</span></span><br><span class="line"></span><br><span class="line"># <span class="variable constant_">HELP</span> kubelet_runtime_operations_latency_microseconds <span class="title class_">Latency</span> <span class="keyword">in</span> microseconds <span class="keyword">of</span> runtime operations. <span class="title class_">Broken</span> down by operation type.</span><br><span class="line"># <span class="variable constant_">TYPE</span> kubelet_runtime_operations_latency_microseconds summary</span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;container_status&quot;</span>,quantile=<span class="string">&quot;0.5&quot;</span>&#125; <span class="number">12117</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;container_status&quot;</span>,quantile=<span class="string">&quot;0.9&quot;</span>&#125; <span class="number">26607</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;container_status&quot;</span>,quantile=<span class="string">&quot;0.99&quot;</span>&#125; <span class="number">27598</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds_count&#123;operation_type=<span class="string">&quot;container_status&quot;</span>&#125; <span class="number">486</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;list_containers&quot;</span>,quantile=<span class="string">&quot;0.5&quot;</span>&#125; <span class="number">29972</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;list_containers&quot;</span>,quantile=<span class="string">&quot;0.9&quot;</span>&#125; <span class="number">47907</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;list_containers&quot;</span>,quantile=<span class="string">&quot;0.99&quot;</span>&#125; <span class="number">80982</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds_count&#123;operation_type=<span class="string">&quot;list_containers&quot;</span>&#125; <span class="number">10812</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;list_podsandbox&quot;</span>,quantile=<span class="string">&quot;0.5&quot;</span>&#125; <span class="number">18053</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;list_podsandbox&quot;</span>,quantile=<span class="string">&quot;0.9&quot;</span>&#125; <span class="number">28116</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;list_podsandbox&quot;</span>,quantile=<span class="string">&quot;0.99&quot;</span>&#125; <span class="number">68748</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds_count&#123;operation_type=<span class="string">&quot;list_podsandbox&quot;</span>&#125; <span class="number">10712</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;podsandbox_status&quot;</span>,quantile=<span class="string">&quot;0.5&quot;</span>&#125; <span class="number">4918</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;podsandbox_status&quot;</span>,quantile=<span class="string">&quot;0.9&quot;</span>&#125; <span class="number">15671</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds&#123;operation_type=<span class="string">&quot;podsandbox_status&quot;</span>,quantile=<span class="string">&quot;0.99&quot;</span>&#125; <span class="number">18398</span></span><br><span class="line">kubelet_runtime_operations_latency_microseconds_count&#123;operation_type=<span class="string">&quot;podsandbox_status&quot;</span>&#125; <span class="number">323</span></span><br></pre></td></tr></table></figure><p>可以通过 Prometheus 对其进行监控：</p><img src="/kubernetes/kubelet/pleg-is-not-healthy/pleg-prometheus-metrics.png" class="" title="img"><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>造成 <strong>PLEG is not healthy</strong> 的因素有很多，出现最多的原因：</p><ul><li>RPC 调用过程中容器运行时响应超时（有可能是性能下降，死锁或者出现了 bug）。</li><li>节点上的 Pod 数量太多，导致 <code>relist</code> 无法在 3 分钟内完成。事件数量和延时与 Pod 数量成正比，与节点资源无关。</li><li>relist 出现了死锁，该 <a href="https://github.com/kubernetes/kubernetes/issues/72482">bug</a> 已在 Kubernetes 1.14 中修复。</li><li>获取 Pod 的网络堆栈信息时 CNI 出现了 bug。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes/">https://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes/</a></p><p><a href="https://github.com/kubernetes/kubernetes/issues/72482">https://github.com/kubernetes/kubernetes/issues/72482</a></p><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md</a></p><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/runtime-pod-cache.md">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/runtime-pod-cache.md</a></p><p><a href="https://github.com/openshift/origin/blob/release-3.11/vendor/k8s.io/kubernetes/pkg/kubelet/pleg/generic.go#L180-L284">https://github.com/openshift/origin/blob/release-3.11/vendor/k8s.io/kubernetes/pkg/kubelet/pleg/generic.go#L180-L284</a></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> kubelet </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PLEG </tag>
            
            <tag> kubelet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher2 配置 okta 认证</title>
      <link href="/rancher/authentication/rancher2-okta-authentication/"/>
      <url>/rancher/authentication/rancher2-okta-authentication/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/authentication/rancher2-okta-authentication/" target="_blank" title="https://www.xtplayer.cn/rancher/authentication/rancher2-okta-authentication/">https://www.xtplayer.cn/rancher/authentication/rancher2-okta-authentication/</a></p><p>版本支持: <code>Rancher v2.2.0+</code></p><blockquote><p><strong>注意</strong> 在开始之前，请熟悉 <a href="/rancher/authentication/rancher2-authentication/">外部身份验证配置和主要用户</a> 的概念。</p></blockquote><h2 id="okta-应用配置"><a href="#okta-应用配置" class="headerlink" title="okta 应用配置"></a>okta 应用配置</h2><p>这里以 okta 官方 demo 为例，访问 <code>https://www.okta.com/free-trial/</code> 可以申请 30 天的试用账号。</p><h3 id="创建应用"><a href="#创建应用" class="headerlink" title="创建应用"></a>创建应用</h3><ol><li><p>登录控制台后，依次点击 1、2、3、4 来创建应用。</p><blockquote><p>注意：深红色的 3 是系统的提示。</p></blockquote><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910215214878.png" class="" title="image-20200910215214878"><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910215328163.png" class="" title="image-20200910215328163"></li><li><p>选择 <code>web</code> 和 <code>SAML 2.0</code>，并点击创建。</p><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910215405283.png" class="" title="image-20200910215405283"></li></ol><h3 id="SAML-配置"><a href="#SAML-配置" class="headerlink" title="SAML 配置"></a>SAML 配置</h3><h4 id="常规设置"><a href="#常规设置" class="headerlink" title="常规设置"></a>常规设置</h4><p>   设置 APP 名称，其他参数可以忽略，然后点击下一步。</p>   <img src="/rancher/authentication/rancher2-okta-authentication/image-20200910215606931.png" class="" title="image-20200910215606931"><h4 id="配置-SAML"><a href="#配置-SAML" class="headerlink" title="配置 SAML"></a>配置 SAML</h4><ol><li><p>SAML Settings</p><ul><li><p>Single sign on URL</p><p> <code>https://yourRancherHostURL/v1-saml/okta/saml/acs</code></p></li><li><p>Audience URI (SP Entity ID)</p><p> <code>https://yourRancherHostURL/v1-saml/okta/saml/metadata</code></p></li><li><p>其他参数保持默认</p></li></ul></li><li><p>USER ATTRIBUTE (用户属性)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">属性 1:</span><br><span class="line">Name: userName</span><br><span class="line">Name Format: Unspecified</span><br><span class="line">Value: user.username</span><br><span class="line"></span><br><span class="line">属性 2:</span><br><span class="line">Name: displayName</span><br><span class="line">Name Format: Unspecified</span><br><span class="line">Value: user.firstName + <span class="string">&quot; &quot;</span> + user.lastName</span><br></pre></td></tr></table></figure><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910220338784.png" class="" title="image-20200910220338784"></li><li><p>GROUP ATTRIBUTE (组属性)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Name: <span class="built_in">groups</span></span><br><span class="line">Name Format: Unspecified</span><br><span class="line">Filter: Regex</span><br><span class="line">Value: .*</span><br></pre></td></tr></table></figure><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910220535424.png" class="" title="image-20200910220535424"></li><li><p>反馈</p><p>根据自己喜好选择，最后点击<strong>完成</strong>。</p><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910221418441.png" class="" title="image-20200910221418441"></li></ol><h3 id="拷贝元数据-XML"><a href="#拷贝元数据-XML" class="headerlink" title="拷贝元数据 XML"></a>拷贝元数据 XML</h3><ol><li><p>上一步骤点击完成之后会自动跳转到 <strong>sign on</strong> 界面。</p><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910221946473.png" class="" title="image-20200910221946473"></li><li><p>点击页面中部的 <code>View Setup Instructions</code></p><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910222128307.png" class="" title="image-20200910222128307"></li><li><p>在弹出窗口的下部，找到 <strong>Optional</strong> 并复制里面的代码备用。</p><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910222202004.png" class="" title="image-20200910222202004"></li></ol><h3 id="分配用户和组"><a href="#分配用户和组" class="headerlink" title="分配用户和组"></a>分配用户和组</h3><p>要想用户或者组能登录 Rancher，需要分配用户或者组到应用。</p>   <img src="/rancher/authentication/rancher2-okta-authentication/image-20200910223727612.png" class="" title="image-20200910223727612"><h2 id="Rancher-认证配置"><a href="#Rancher-认证配置" class="headerlink" title="Rancher 认证配置"></a>Rancher 认证配置</h2><h3 id="创建自签名-ssl-证书"><a href="#创建自签名-ssl-证书" class="headerlink" title="创建自签名 ssl 证书"></a>创建自签名 ssl 证书</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout myservice.key -out myservice.crt -subj <span class="string">&quot;/C=CN&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>注意:</strong> <code>-days</code> 可以适当增大。</p></blockquote><h3 id="OKTA-参数配置"><a href="#OKTA-参数配置" class="headerlink" title="OKTA 参数配置"></a>OKTA 参数配置</h3><p>依次进入 <strong>全局|安全|认证</strong>，选择 <strong>OKTA</strong>。</p><ul><li>显示用户: <code>displayName</code></li><li>用户名: <code>userName</code></li><li>UID: <code>userName</code></li><li>组: <code>groups</code></li><li>Rancher API 地址: 当前集群 <code>rancher server url</code></li><li>私钥: 填写 <code>myservice.key</code> 中的信息</li><li>证书: 填写 <code>myservice.crt</code> 中的信息</li><li>元数据 XML: 复制粘贴前面步骤中保存的<strong>元数据 XML</strong></li></ul><h3 id="最后的配置结果"><a href="#最后的配置结果" class="headerlink" title="最后的配置结果"></a>最后的配置结果</h3><img src="/rancher/authentication/rancher2-okta-authentication/image-20200910231038382.png" class="" title="image-20200910231038382">]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> authentication </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>采用 GitOps 的 11 大原因</title>
      <link href="/git/top-11-reasons-to-use-gitops/"/>
      <url>/git/top-11-reasons-to-use-gitops/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/git/top-11-reasons-to-use-gitops/" target="_blank" title="https://www.xtplayer.cn/git/top-11-reasons-to-use-gitops/">https://www.xtplayer.cn/git/top-11-reasons-to-use-gitops/</a></p><p>Kubernetes 允许我们单纯地使用声明性的配置文件来管理我们的应用部署和其他基础设施组件（例如，我们现在都是 YAML 开发者）。这使我们能够把所有这些文件放到 Git 仓库中，然后把它挂到流水线上（Jenkins、GitLab 等），流水线会把这些变化应用到集群上，然后就有了 GitOps。如果你还不了解 GitOps 是什么，可以查看我们之前发布过的文章：<a href="http://mp.weixin.qq.com/s?__biz=MzIyMTUwMDMyOQ==&mid=2247494372&idx=1&sn=70b69c43fdba0324e7a44c4dd146d764&chksm=e8396c22df4ee534942cf24bfdbbcebfb5e88df82ac32dc5ef19fc59b5201c65d60919f818b9&scene=21#wechat_redirect">GitOps 初阶指南：将 DevOps 扩展至 K8S</a></p><img src="/git/top-11-reasons-to-use-gitops/640.png" class="" title="img"><p>为了使工作正常进行，我们必须确保改变集群的唯一方法是在 Git 仓库上提交。GitOps 并不是专门针对 Kubernetes 的，同样的原理也可以应用于任何其他声明式配置管理的环境。</p><p>可以说，很多企业已经开始采用 GitOps 了，但现在是业界开始充分认识到其潜力的时候。所以，让我们深入了解一下它如此出色的原因吧！</p><ol><li><p>存储环境变更历史记录</p><p>只有通过更新相应 Git 仓库中的配置，才能改变应用环境。这将创建一个完整的状态变化的历史记录，包括谁做了更改和为什么更改的记录。你可以通过正在使用的 Git 用户界面来读取历史记录。</p></li><li><p>轻松回滚到之前的状态</p><p>一旦我们所有的变更都被存储为 Git 历史记录，就可以很容易地将一个环境回滚到之前的任何状态。通过还原一些 commit，我们可以回到以前的工作状态。</p></li><li><p>保障部署安全</p><p>一旦对集群的所有更改都通过 GitOps repo，用户和持续集成(CI)流程就不需要再访问集群了。这大大降低了攻击面，尤其是还可以减少对 Kubernetes API 的网络访问。</p><p>部署过程无论如何实现，都可以在集群内部运行，并从 Git 中拉取配置。其对 API 的访问使用基于角色的访问控制（RBAC）进行限制。这极大地提高了集群的安全性，防止任何恶意的远程更改在 API 服务器上。</p></li><li><p>轻量化审批程序</p><p>在修改生产环境时，开发人员总是不受信任。因此在许多公司中都建立了四眼审批流程（four-eyes approval processes），不论是出于什么原因建立的审批流程，GitOps 都提供了一个简单的方法来实现它们。</p><p>具体实现方式取决于你使用的 Git 服务器，但重点是给开发人员在 Git repo 上创建拉取请求的权利，同时给另一组人审查和合并的权利。大多数 Git 服务器都有一个很好的 UI 来检查修改和批准拉取请求——所以这个解决方案不仅便宜，而且对用户也相当友好。</p></li><li><p>模块化架构</p><p>GitOps 有 3 个部分：Git repo、部署流程以及一个在 Git repo 中自动更新版本的过程。这三者可以相互独立演化或替换。</p><p>一边是一个组件在 Git repo 写入，另一边是一个组件在读取。Git repo 的结构成为这些组件之间的桥梁。由于这是一个相当松散的耦合，两边可以用不同的方式甚至不同的技术栈来实现。</p></li><li><p>独立于工具的架构</p><p>第 5 点中提到的模块化可以看出 GitOps 架构是一个可以灵活组装最佳工具的架构。当然，任何流行的 Git 服务器都可以完成 Git 部分的工作，FluxCD 或 ArgoCD 可以负责将 repo 同步到集群。JenkinsX 是一个处理这个过程所有部分的工具，包括创建 Git repos，并在构建新的 Docker 镜像时用新版本更新它们。</p></li><li><p>复用现有知识</p><p>将 Git 置于部署流程的核心，可以充分利用大多数开发人员和运维人员已经掌握的 Git 知识。不需要新的工具来浏览部署历史或实施审批流程。所有的流程都是用大家都熟悉的工具来完成的。</p></li><li><p>比较不同的环境</p><p>当你有一个从开发到用户接受度测试（UAT）再到生产的环境链时，跟踪这些环境之间的差异是一件很麻烦的事情。多亏了存储在 Git repos 中的声明式配置，它使得处理环境间差异就像比较一组 YAML 文件一样简单。</p><p>我们有非常棒的工具来做这件事，所以这将不再是一个问题。更重要的是，从头开始创建一个新的环境，就像复制和粘贴这些文件到一个新的 repo 中一样简单。</p></li><li><p>开箱即用的备份</p><p>由于你的环境状态存储在 Git 中，如果 Kubernetes 上的 etcd 发生了什么事情，你也永远不会丢失数据。因为它是你集群状态的自然备份。</p></li><li><p>像应用程序代码一样测试你的更改</p><p>你可以用测试应用程序代码的方式来测试环境中可能出现的破坏性变化。将更改放在一个分支上，然后在其上运行 CI 流水线。你的 CI 工具将能够运行测试，并根据测试结果将 Git 中的 pull-request 状态设置为绿色或红色。一旦所有的东西都经过测试和审查，你就可以合并到 master。</p><p>这听起来非常简单，但自动化测试是基础设施管理中经常被忽视的任务。虽然 GitOps 并没有让它变得更容易，但至少它为你提供了与你在其他地方使用的相同的熟悉工作流程。</p></li><li><p>高可用部署基础设施</p><p>部署基础设施保持一致很重要。Git repo 服务器通常已经以复制、高可用的方式进行了设置。源代码是所有开发人员在大多数时间都需要访问的东西，所以使用 Git 作为部署的源码并不会给 Git 本身增加额外的负担。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GitOps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Could not generate persistent MAC address </title>
      <link href="/linux/network/could-not-generate-persistent-mac-address/"/>
      <url>/linux/network/could-not-generate-persistent-mac-address/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/network/could-not-generate-persistent-mac-address/" target="_blank" title="https://www.xtplayer.cn/linux/network/could-not-generate-persistent-mac-address/">https://www.xtplayer.cn/linux/network/could-not-generate-persistent-mac-address/</a></p><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>在 docker 容器环境中，可能会看到很多如下的错误日志：</p><p><code>Could not generate persistent MAC address for tap0: No such file or directory</code></p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>这是一个容器环境下的 BUG：<a href="https://github.com/systemd/systemd/issues/3374">https://github.com/systemd/systemd/issues/3374</a> 。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>编辑以下文件，如果这个文件没有则新建，然后配置以下参数。</p><p><code>/etc/systemd/network/99-default.link</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Link]</span><br><span class="line">NamePolicy=kernel database onboard slot path</span><br><span class="line">MACAddressPolicy=none</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MACAddressPolicy=</span><br><span class="line">  如何设置网卡的 MAC 地址：</span><br><span class="line"></span><br><span class="line">persistent</span><br><span class="line">  如果内核使用了网卡硬件固有的 MAC 地址(绝大多数网卡都有)， 那么啥也不做， 直接使用内核的 MAC 地址。 否则， 将会随机新生成一个 确保在多次启动之间保持固定不变的 MAC 地址(针对给定的主板与网卡)。 自动生成 MAC 地址的特性 要求网卡必须存在 ID_NET_NAME_* 属性， 否则无法自动生成 MAC 地址。</span><br><span class="line"></span><br><span class="line">random</span><br><span class="line">  如果内核使用了随机生成的 MAC 地址(而不是网卡硬件固有的 MAC 地址)， 那么啥也不做，直接使用内核的 MAC 地址。 否则，将在网卡每次出现的时候(一般在启动过程中)随机新生成一个 MAC 地址。 无论使用上述哪种方式生成的 MAC 地址， 都将设置 <span class="string">&quot;unicast&quot;</span> 与 <span class="string">&quot;locally administered&quot;</span> 位。</span><br><span class="line"></span><br><span class="line">none</span><br><span class="line">  无条件的直接使用内核的 MAC 地址。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>etcd 配置优化</title>
      <link href="/etcd/etcd-optimize/"/>
      <url>/etcd/etcd-optimize/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/etcd/etcd-optimize/" target="_blank" title="https://www.xtplayer.cn/etcd/etcd-optimize/">https://www.xtplayer.cn/etcd/etcd-optimize/</a></p><blockquote><p><strong>注意：</strong> 以下操作中所指的 K8S 集群，均是 rancher 或者 rke 创建的集群。</p></blockquote><h2 id="存储空间配额"><a href="#存储空间配额" class="headerlink" title="存储空间配额"></a>存储空间配额</h2><p><code>--quota-backend-bytes</code></p><p>存储空间配额可以理解为 ETCD 数据库大小，默认限制 2G（推荐最大 8G）。当数据写入耗尽存储空间时，ETCD 会引发整个集群范围的警告，该警告将会导致集群切换为维护模式，<strong>维护模式</strong> 仅接受键值读取和删除，不支持写入。</p><p>所以，在创建集群时候建议修改配额大小。但是这个配额值不宜过设置大，建议在主机内存的 <code>60% - 70%</code>。</p><h3 id="手动释放配额空间"><a href="#手动释放配额空间" class="headerlink" title="手动释放配额空间"></a>手动释放配额空间</h3><p>如果在 ETCD 服务日志中看到类似以下的日志，那么说明 ETCD 配额空间可能已经满了，需要手动去清理并释放空间。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error:  rpc error: code = 8 desc = etcdserver: mvcc: database space exceeded</span><br></pre></td></tr></table></figure><ol><li><p>执行以下命令查看配额空间具体使用信息</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在 ETCD 主机上执行以下命令</span></span><br><span class="line">docker <span class="built_in">exec</span> -ti etcd sh -c <span class="string">&#x27;ETCDCTL_API=3 etcdctl --write-out=table endpoint status&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 信息输出</span></span><br><span class="line">rancher@alihost-01:~$ docker <span class="built_in">exec</span> -ti etcd sh -c <span class="string">&#x27;ETCDCTL_API=3 etcdctl --write-out=table endpoint status&#x27;</span></span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://192.168.1.224:2379 | 16e3472080d104d4 |   3.4.3 |  1040 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |        50 |  612649586 |          612649586 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure></li><li><p>通过以下操作命令来手动释放配额空间</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 登录 ETCD 主机，然后登录 ETCD 容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -ti etcd sh</span><br><span class="line"><span class="comment"># 获取当前的修订版本</span></span><br><span class="line">rev=$(ETCDCTL_API=3 etcdctl --endpoints=:2379 endpoint status --write-out=<span class="string">&quot;json&quot;</span> | egrep -o <span class="string">&#x27;&quot;revision&quot;:[0-9]*&#x27;</span> | egrep -o <span class="string">&#x27;[0-9].*&#x27;</span>)</span><br><span class="line"><span class="comment"># 压缩所有旧的修订</span></span><br><span class="line">ETCDCTL_API=3 etcdctl compact <span class="variable">$rev</span></span><br><span class="line"><span class="comment"># 碎片整理，释放空间</span></span><br><span class="line">ETCDCTL_API=3 etcdctl defrag</span><br><span class="line"><span class="comment"># 解除警报（每个 ETCD 实例都要执行）</span></span><br><span class="line">ETCDCTL_API=3 etcdctl alarm disarm</span><br></pre></td></tr></table></figure></li></ol><h2 id="历史版本清理"><a href="#历史版本清理" class="headerlink" title="历史版本清理"></a>历史版本清理</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--auto-compaction-mode</span><br><span class="line">--auto-compaction-retention</span><br></pre></td></tr></table></figure><p>ETCD 会存储多版本数据，随着写入的主键增加，历史版本将会越来越多，并且 ETCD 默认不会自动清理历史数据。数据达到 <code>--quota-backend-bytes</code> 设置的配额值时就无法写入数据，必须要压缩并清理历史数据才能继续写入。</p><p>所以，为了避免配额空间耗尽的问题，在创建集群时候建议默认开启 <strong>历史版本清理</strong> 功能。</p><ul><li><p>3.3.0 之前的版本</p><p>3.3.0 之前的版本，只能按周期 <code>periodic</code> 来压缩。比如设置 <code>--auto-compaction-retention=72h</code>，那么就会每 72 小时进行一次数据压缩。</p></li><li><p>3.3.0 之后的版本</p><p>比如在 v3.3.0， v3.3.1 和 v3.3.2 中，可以通过 <code>--auto-compaction-mode</code> 设置压缩模式，可以选择 <code>revision</code> 或者 <code>periodic</code> 来压缩数据，默认为 <code>periodic</code>。</p></li></ul><h2 id="Raft-日志保留"><a href="#Raft-日志保留" class="headerlink" title="Raft 日志保留"></a>Raft 日志保留</h2><p><code>--snapshot-count</code> 指定有多少条事务（transaction）被提交时，触发快照保存到磁盘。在存盘之前，Raft 条目将一直保存在内存中。从 v3.2 版本开始，<code>--snapshot-count</code> 条数从 <code>10000</code> 改为 <code>100000</code>，因此这将占用很大一部分内存资源。</p><p>如果节点总内存资源不多，或者是单 etcd 实例运行，则可以把 <code>--snapshot-count</code> 适当的缩减，比如设置为 <code>--snapshot-count=50000</code></p><h2 id="快照备份"><a href="#快照备份" class="headerlink" title="快照备份"></a>快照备份</h2><p>ETCD 具有快照备份功能，建议生产环境中开启 ETCD 自动备份功能。通过 rancher 创建的集群中，默认开启自动备份功能。对于 rke 创建的集群，需要在配置文件中添加对应的配置。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">etcd:</span></span><br><span class="line"><span class="comment"># 开启自动备份</span></span><br><span class="line">    <span class="comment">## rke 版本小于 0.2.x 或 rancher 版本小于 v2.2.0 时使用</span></span><br><span class="line">    <span class="attr">snapshot:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">creation:</span> <span class="string">5m0s</span></span><br><span class="line">    <span class="attr">retention:</span> <span class="string">24h</span></span><br><span class="line">    <span class="comment">## rke 版本大于等于 0.2.x 或 rancher 版本大于等于 v2.2.0 时使用(两段配置二选一)</span></span><br><span class="line">    <span class="attr">backup_config:</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">true</span>           <span class="comment"># 设置 true 启用 ETCD 自动备份，设置 false 禁用；</span></span><br><span class="line">      <span class="attr">interval_hours:</span> <span class="number">12</span>      <span class="comment"># 快照创建间隔时间，不加此参数，默认 5 分钟；</span></span><br><span class="line">      <span class="attr">retention:</span> <span class="number">6</span>            <span class="comment"># etcd 备份保留份数；</span></span><br></pre></td></tr></table></figure><h2 id="RKE-ETCD-参考配置"><a href="#RKE-ETCD-参考配置" class="headerlink" title="RKE ETCD 参考配置"></a>RKE ETCD 参考配置</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">etcd:</span></span><br><span class="line">    <span class="comment"># 开启自动备份</span></span><br><span class="line">    <span class="comment">## rke 版本小于 0.2.x 或 rancher 版本小于 v2.2.0 时使用</span></span><br><span class="line">    <span class="attr">snapshot:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">creation:</span> <span class="string">5m0s</span></span><br><span class="line">    <span class="attr">retention:</span> <span class="string">24h</span></span><br><span class="line">    <span class="comment">## rke 版本大于等于 0.2.x 或 rancher 版本大于等于 v2.2.0 时使用(两段配置二选一)</span></span><br><span class="line">    <span class="attr">backup_config:</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">true</span>           <span class="comment"># 设置 true 启用 ETCD 自动备份，设置 false 禁用；</span></span><br><span class="line">      <span class="attr">interval_hours:</span> <span class="number">12</span>      <span class="comment"># 快照创建间隔时间，不加此参数，默认 5 分钟；</span></span><br><span class="line">      <span class="attr">retention:</span> <span class="number">6</span>            <span class="comment"># etcd 备份保留份数；</span></span><br><span class="line">    <span class="attr">extra_env:</span></span><br><span class="line">    <span class="comment"># 扩展参数 https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md</span></span><br><span class="line">    <span class="attr">extra_args:</span></span><br><span class="line">      <span class="attr">heartbeat-interval:</span> <span class="string">&#x27;500&#x27;</span> <span class="comment"># 心跳检查间隔，ETCD 默认 100 毫秒，网络质量不好的环境，适当增加这个值</span></span><br><span class="line">      <span class="attr">election-timeout:</span> <span class="string">&#x27;5000&#x27;</span> <span class="comment"># 选举超时，ETCD 默认 1000 毫秒，网络质量不好的环境，适当增加这个值</span></span><br><span class="line">      <span class="attr">auto-compaction-mode:</span> <span class="string">periodic</span></span><br><span class="line">      <span class="attr">auto-compaction-retention:</span> <span class="string">60m</span></span><br><span class="line">      <span class="comment"># 修改空间配额为$((6*1024*1024*1024))，默认 2G,最大 8G</span></span><br><span class="line">      <span class="attr">quota-backend-bytes:</span> <span class="string">&#x27;6442450944&#x27;</span></span><br><span class="line">      <span class="attr">snapshot-count:</span> <span class="number">50000</span> <span class="comment"># 触发快照写入磁盘的提交事务数。默认值: &quot;100000&quot;，在写入磁盘前事务保存在内存中，如果节点内存紧张，可以减少这个值。</span></span><br><span class="line">      <span class="attr">max-request-bytes:</span> <span class="number">10485760</span> <span class="comment"># 服务器接受的最大客户端请求大小（以字节为单位），默认 1.5M，建议增加大小，比如设置为 10M。</span></span><br><span class="line">      <span class="attr">log-level:</span> <span class="string">info</span> <span class="comment"># supports debug, info, warn, error, panic, or fatal.</span></span><br><span class="line">      <span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">enable-pprof:</span> <span class="literal">false</span> <span class="comment"># Enable runtime profiling data via HTTP server. Address is at client URL + &quot;/debug/pprof/&quot;, default: false.</span></span><br><span class="line">    <span class="attr">extra_binds:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;/etc/localtime:/etc/localtime&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> etcd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> etcd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mds、mds_stores、mdworker 占用大量 cpu 和内存</title>
      <link href="/macos/mds-stores-use-high-cpu/"/>
      <url>/macos/mds-stores-use-high-cpu/</url>
      
        <content type="html"><![CDATA[<h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>有时候忽然发现风扇转到特别厉害，打开的应用程序并不多。打开活动监视器，可以看到 mds、mds_stores、mdworker 占用很高的 cpu 和 内存 资源。</p><img src="/macos/mds-stores-use-high-cpu/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3MDkzNDY1,size_16,color_FFFFFF,t_70-20200901230345861.png" class="" title="image"><p><strong>Spotlight</strong> 中文名称为 <strong>聚焦</strong>，就是按下  <strong>Command + 空格</strong> 弹窗的那个搜索框。</p><img src="/macos/mds-stores-use-high-cpu/20181227133718960.png" class="" title="image"><p>系统偏好设置位置：</p><img src="/macos/mds-stores-use-high-cpu/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3MDkzNDY1,size_16,color_FFFFFF,t_70-20200901230345883.png" class="" title="image"><p><code>mds、mds_stores、mdworker</code> 是 <strong>Spotlight</strong> 的组成部分：</p><ul><li><p>mds</p><p>元数据服务器。</p></li><li><p>mdworker</p><p>元数据服务器 worker 进程，主要负责文件的索引。</p></li><li><p>mds_stores</p><p>元数据存储服务。</p></li></ul><h2 id="在-Mac-上使用“聚焦”"><a href="#在-Mac-上使用“聚焦”" class="headerlink" title="在 Mac 上使用“聚焦”"></a>在 Mac 上使用“聚焦”</h2><p>利用“聚焦”，您可以在 Mac 上查找应用、文稿及其他文件。您还可以利用“聚焦建议”来获取新闻、体育、影片、天气等信息。</p><h3 id="使用“聚焦”进行搜索"><a href="#使用“聚焦”进行搜索" class="headerlink" title="使用“聚焦”进行搜索"></a>使用“聚焦”进行搜索</h3><ol><li><p>点按菜单栏右上角的 <img src="/macos/mds-stores-use-high-cpu/macos-spotlight-search-menu-bar-icon.png" class="" title="img">，或按下 Command-空格键。</p><img src="/macos/mds-stores-use-high-cpu/macos-high-sierra-menu-bar-spotlight-crop.jpg" class="" title="菜单栏中的“聚焦”图标"></li><li><p>输入您要查找的内容。您可以搜索类似“Apple Store 商店”或“来自艾米莉的电子邮件”这样的内容。<img src="/macos/mds-stores-use-high-cpu/macos-high-sierra-spotlight-search-results-maps.jpg" class="" title="“聚焦”中的地图搜索"></p></li><li><p>要打开结果列表中的某个项目，请连按这个项目。或者，要快速浏览结果，请使用向上箭头或向下箭头键。</p></li></ol><h3 id="按文件类型或位置进行搜索"><a href="#按文件类型或位置进行搜索" class="headerlink" title="按文件类型或位置进行搜索"></a>按文件类型或位置进行搜索</h3><ul><li>要按文件类型进行搜索，请使用“种类”一词以及文件类型。例如，键入“种类:文件夹”或“种类:音频”。</li><li>要在 Mac 上显示某个文件的位置，请从结果列表中选取这个文件，然后按住 Command。预览窗口的底部将显示这个文件的位置。要打开这个文件的位置，请按下 Command-R。</li><li>要在 Finder 中查看来自 Mac 的所有结果，请滚动到结果列表的底部，然后连按“在访达中全部显示”。</li></ul><img src="/macos/mds-stores-use-high-cpu/macos-high-sierra-spotlight-search-results-kind.jpg" class="" title="文件类型搜索"><h3 id="获取定义以及计算和换算结果"><a href="#获取定义以及计算和换算结果" class="headerlink" title="获取定义以及计算和换算结果"></a>获取定义以及计算和换算结果</h3><p>“聚焦” 可以显示词典定义、计算结果、度量单位换算结果等内容。了解<a href="https://support.apple.com/zh-cn/guide/mac-help/narrow-the-scope-of-your-searches-mh15155/10.13">使用“聚焦”进行搜索的更多方式。</a></p><p>下面列举了一些您可以完成的操作示例：</p><ul><li>要获取定义，请输入字词或短语，然后点按“定义”部分中的结果。</li><li>要获取计算结果，请在搜索栏中输入类似“2+2”这样的内容。</li><li>要转换度量单位，请输入类似“25 磅”或“32 英尺换算成米”这样的内容。</li></ul><img src="/macos/mds-stores-use-high-cpu/macos-high-sierra-spotlight-search-results-definition.jpg" class="" title="“聚焦”中的定义查找"><h3 id="查找影片放映时间、天气和附近的地点"><a href="#查找影片放映时间、天气和附近的地点" class="headerlink" title="查找影片放映时间、天气和附近的地点"></a>查找影片放映时间、天气和附近的地点</h3><p>您可以使用“聚焦”来搜索影片放映时间、天气以及附近的地点。</p><p>下面列举了一些您可以完成的操作示例：</p><ul><li>要获取放映时间，请输入您要观看的影片名称。要查看您附近放映的影片，请输入“放映时间”。</li><li>要获取当地的天气信息，请输入“天气”。</li><li>要查找您附近的餐馆，请输入类似“就餐场所”这样的内容，然后点按“地图”部分中的结果。</li></ul><img src="/macos/mds-stores-use-high-cpu/macos-high-sierra-spotlight-search-results-weather.jpg" class="" title="img"><h2 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h2><p>以上是  <strong>聚焦</strong> 的功能说明，因为我很少使用这些功能，所以我选择了禁止  <strong>聚焦</strong> 索引文件。</p><p><code>sudo mdutil -a -i off</code></p><p>如果需要再次使用，可以通过以下命令重新开启。</p><p><code>sudo mdutil -a -i on</code></p>]]></content>
      
      
      <categories>
          
          <category> macos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> macos </tag>
            
            <tag> mds_stores </tag>
            
            <tag> mdworker </tag>
            
            <tag> mds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>project.management.cattle.io not found</title>
      <link href="/rancher/project-management-cattle-io-not-found/"/>
      <url>/rancher/project-management-cattle-io-not-found/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/project-management-cattle-io-not-found/" target="_blank" title="https://www.xtplayer.cn/rancher/project-management-cattle-io-not-found/">https://www.xtplayer.cn/rancher/project-management-cattle-io-not-found/</a></p><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>有时候可能会在 rancher server 中看到很多如下的错误提示：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2019/07/16 06:33:23 [ERROR] NamespaceController default [resourceQuotaSyncController] failed with : project.management.cattle.io <span class="string">&quot;cluster-vp4z8/project-zj5k9&quot;</span> not found, [resourceQuotaUsedLimitController] failed with : project.management.cattle.io <span class="string">&quot;cluster-vp4z8/project-zj5k9&quot;</span> not found</span><br><span class="line">2019/07/16 06:33:39 [ERROR] NamespaceController kube-system [resourceQuotaSyncController] failed with : project.management.cattle.io <span class="string">&quot;cluster-rvvmm/project-r6l6q&quot;</span> not found, [resourceQuotaUsedLimitController] failed with : project.management.cattle.io <span class="string">&quot;cluster-rvvmm/project-r6l6q&quot;</span> not found</span><br></pre></td></tr></table></figure><p>出现问题时，尝试通过在 local 集群或者单 rancher 容器中运行以下命令去查询集群 ID 、项目 ID、命名空间 ID。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get cluster --all-namespaces</span><br><span class="line">kubectl get projects.management.cattle.io  --all-namespaces</span><br><span class="line">kubectl get ns --all-namespaces</span><br></pre></td></tr></table></figure><p>对比查询结果，均不存在错误提示中的 集群 ID、项目 ID、命名空间 ID。</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><ol><li><p>新的集群第一次导入时，会自动给 K8S 和 rancher 相关的<strong>命名空间</strong>添加 <strong>annotations:</strong> <code>field.cattle.io/projectId</code> 和 <strong>labels:</strong> <code>field.cattle.io/projectId</code>，rancher 通过这两个参数来控制和更新<strong>命名空间</strong>。</p></li><li><p>找一个命名空间查看其 yaml 配置，比如：<code>kubectl get ns kube-system -oyaml</code></p> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">field.cattle.io/projectId:</span> <span class="string">local:p-zkhr5</span></span><br><span class="line">    <span class="attr">lifecycle.cattle.io/create.namespace-auth:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">&quot;2019-03-24T05:56:17Z&quot;</span></span><br><span class="line">  <span class="attr">finalizers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">controller.cattle.io/namespace-auth</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">field.cattle.io/projectId:</span> <span class="string">p-zkhr5</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">&quot;383933986&quot;</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/kube-system</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">8a3306c1-4df9-11e9-906a-00163e145e40</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">finalizers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">phase:</span> <span class="string">Active</span></span><br></pre></td></tr></table></figure><p> 可以看到 <strong>labels</strong> 和 <strong>annotations</strong> 中带有与 集群 ID 和项目 ID 相关的参数。</p></li><li><p>截止 <code>rancher v2.4.6 、v2.3.9</code>，重复导入的 K8S 集群，原集群中的<strong>命名空间</strong>不会自动移动到 <strong>Rancher projects</strong> 下。访问 Rancher UI ，在 <code>集群|项目/命名空间</code> 下可以看到<strong>命名空间</strong>都是在 <strong>全部</strong> 下。</p> <img src="/rancher/project-management-cattle-io-not-found/image-20200901122920458.png" class="" title="image-20200901122920458"> <img src="/rancher/project-management-cattle-io-not-found/image-20200901123045488.png" class="" title="image-20200901123045488"></li><li><p>重复导入的 K8S 集群，因为不会自动把<strong>命名空间</strong>移动到 <strong>Rancher projects</strong> 下，所以命名空间 yaml 配置的参数没有自动更新，<code>resourceQuotaSyncController</code> 是根据这两个参数来控制命名空间，因此在 Rancher server 中看到了最开始的错误日志。</p></li></ol><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>要解决这个问题，只需要让命名空间更新  <strong>labels</strong> 和 <strong>annotations</strong> 两个参数即可。</p><ol><li>把如下图的命名空间移动到 <strong>system</strong> 项目下。</li></ol><img src="/rancher/project-management-cattle-io-not-found/image-20200901125942233.png" class="" title="image-20200901125942233"><ol><li>把 <strong>default</strong> 命名空间移动到 default 项目下。</li></ol><img src="/rancher/project-management-cattle-io-not-found/image-20200901125949646.png" class="" title="image-20200901125949646"><ol><li>其他的命名空间，根据实际的规划来移动到对应的项目下。</li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> project </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自签名 SSL 证书</title>
      <link href="/ssl/self-signed-ssl/"/>
      <url>/ssl/self-signed-ssl/</url>
      
        <content type="html"><![CDATA[<h2 id="HTTP-over-SSL"><a href="#HTTP-over-SSL" class="headerlink" title="HTTP over SSL"></a>HTTP over SSL</h2><p>要保证 Web 浏览器到服务器的安全连接，HTTPS 几乎是唯一选择。HTTPS 其实就是 HTTP over SSL，也就是让 HTTP 连接建立在 SSL 安全连接之上。</p><p>SSL 使用证书来创建安全连接，有两种验证模式:</p><ol><li>仅客户端验证服务器的证书，客户端自己不提供证书；</li><li>客户端和服务器都互相验证对方的证书。</li></ol><p>一般第二种方式用于网上银行等安全性要求较高的网站，普通的 Web 网站只采用第一种方式。</p><ul><li>客户端如何验证服务器的证书呢？</li></ul><p>web 服务的证书必须经过某<code>权威</code>证书的签名，而这个<code>权威</code>证书又可能经过更权威的证书签名。这么一级一级追溯上去，最顶层那个最权威的证书就称为<strong>根证书</strong>，根证书一般内置在浏览器中。这样，浏览器就可以利用自己自带的根证书去验证某个 web 服务的证书是否有效。如果要提供一个有效的证书，web 服务的证书必须从 <code>VeriSign</code> 这样的证书颁发机构签名。这样，浏览器就可以验证通过，否则浏览器给出一个证书无效的警告。一般安全要求较高的内网环境，可以通过创建自签名 SSL 证书来加密通信。</p><h2 id="数字证书-Certificate"><a href="#数字证书-Certificate" class="headerlink" title="数字证书(Certificate)"></a>数字证书(Certificate)</h2><p>在 HTTPS 的传输过程中，有一个非常关键的角色—<code>数字证书</code>。那什么是数字证书？又有什么作用呢？</p><p>所谓数字证书，是一种用于电脑的身份识别机制。由数字证书颁发机构(CA)对使用私钥创建的签名请求文件做的签名(盖章)，表示 CA 结构对证书持有者的认可。</p><h3 id="数字证书拥有以下几个优点"><a href="#数字证书拥有以下几个优点" class="headerlink" title="数字证书拥有以下几个优点:"></a>数字证书拥有以下几个优点:</h3><ul><li>使用数字证书能够提高用户的可信度；</li><li>数字证书中的公钥，能够与服务端的私钥配对使用，实现数据传输过程中的加密和解密；</li><li>在证认使用者身份期间，使用者的敏感个人数据并不会被传输至证书持有者的网络系统上；</li></ul><h3 id="证书类型"><a href="#证书类型" class="headerlink" title="证书类型"></a>证书类型</h3><p>x509 的证书编码格式有两种:</p><ol><li><p>PEM(Privacy-enhanced Electronic Mail) 是明文格式的，以 <strong>—–BEGIN CERTIFICATE——</strong> 开头，已 <strong>—–END CERTIFICATE——</strong> 结尾。中间是经过 <strong>base64</strong> 编码的内容。查看这类证书的信息的命令为: <code>openssl x509 -noout -text -in server.pem</code>。其实<strong>PEM</strong>就是把<strong>DER</strong>的内容进行了一次<strong>base64</strong>编码</p></li><li><p>DER 是二进制格式的证书，查看这类证书的信息的命令为: <code>openssl x509 -noout -text -inform der -in server.der</code></p></li></ol><h3 id="扩展名"><a href="#扩展名" class="headerlink" title="扩展名"></a>扩展名</h3><ul><li><code>.crt</code>：证书文件，可以是<strong>DER</strong>(二进制)编码的，也可以是 PEM( ASCII Base64 编码的)，在类 unix 系统中比较常见。</li><li><code>.cer</code>：也是证书，常见于 Windows 系统。编码类型同样可以是 DER 或者 PEM 的，windows 下有工具可以转换 crt 到 cer。</li><li><code>.csr</code>：证书签名请求文件，一般是生成请求以后发送给 CA，然后 CA 会给您签名并发回证书。</li><li><code>.key</code>：一般公钥或者密钥都会用这种扩展名，可以是 DER 编码的或者是 PEM 编码的。<ul><li>查看<strong>DER</strong>编码的(公钥或密钥)的文件的命令为: <code>openssl rsa -inform DER -noout -text -in xxx.key</code>。</li><li>查看<strong>PEM</strong>编码的(公钥或者密钥)的文件的命令为: <code>openssl rsa -inform PEM -noout -text -in xxx.key</code>;</li></ul></li><li><code>.p12</code>：证书文件，包含一个<strong>X509</strong>证书和一个被密码保护的私钥。</li></ul><h2 id="自签名证书及自签名类型"><a href="#自签名证书及自签名类型" class="headerlink" title="自签名证书及自签名类型"></a>自签名证书及自签名类型</h2><p>当由于某种原因（比如不想购买权威 ssl 证书，或者仅用于测试环境等情况)，这时可以生成一个自签名证书来使用。使用这个自签名证书的时候，会在客户端浏览器报一个警告，<strong>签名证书授权未知或不可信</strong>(signing certificate authority is unknown and not trusted)。</p><h3 id="自签名类型"><a href="#自签名类型" class="headerlink" title="自签名类型"></a>自签名类型</h3><ol><li><p>自签名证书</p></li><li><p>私有 CA 签名证书</p><blockquote><p>自签名证书的 <code>Issuer</code> 和 <code>Subject</code> 是相同的。</p></blockquote></li></ol><h3 id="自签名区别"><a href="#自签名区别" class="headerlink" title="自签名区别:"></a>自签名区别:</h3><ol><li><p>自签名的证书无法被吊销，私有 CA 签名的证书可以被吊销。</p></li><li><p>如果您的规划需要创建多个证书，那么使用私有 CA 签名的方法比较合适。因为只要给所有的客户端都安装相同的 CA 证书，那么以该 CA 证书签名过的证书，客户端都是信任的，也就只需要安装一次 CA 证书就够了。</p></li><li><p>如果您使用用自签名证书，您需要给所有的客户端安装该证书才会被信任。如果您需要第二个证书，则需要给所有客户端安装第二个 CA 证书才会被信任。</p></li></ol><h2 id="生成私有-CA-自签名证书"><a href="#生成私有-CA-自签名证书" class="headerlink" title="生成私有 CA 自签名证书"></a>生成私有 CA 自签名证书</h2><p>可以通过以下脚本一键生成 ssl 自签名证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash -e</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">help</span></span> ()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; ================================================================ &#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-domain: 生成 ssl 证书需要的主域名，如不指定则默认为 www.rancher.local，如果是 ip 访问服务，则可忽略；&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-trusted-ip: 一般 ssl 证书只信任域名的访问请求，有时候需要使用 ip 去访问 server，那么需要给 ssl 证书添加扩展 IP，多个 IP 用逗号隔开；&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-trusted-domain: 如果想多个域名访问，则添加扩展域名（SSL_TRUSTED_DOMAIN）,多个扩展域名用逗号隔开；&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-size: ssl 加密位数，默认 2048；&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-cn: 国家代码(2 个字母的代号),默认 CN;&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; 使用示例:&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; ./create_self-signed-cert.sh --ssl-domain=www.test.com --ssl-trusted-domain=www.test2.com \ &#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --ssl-trusted-ip=1.1.1.1,2.2.2.2,3.3.3.3 --ssl-size=2048 --ssl-date=3650&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; ================================================================&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&quot;<span class="variable">$1</span>&quot;</span> <span class="keyword">in</span></span><br><span class="line">    -h|--<span class="built_in">help</span>) <span class="built_in">help</span>; <span class="built_in">exit</span>;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$1</span> == <span class="string">&#x27;&#x27;</span> ]];<span class="keyword">then</span></span><br><span class="line">    <span class="built_in">help</span>;</span><br><span class="line">    <span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">CMDOPTS=<span class="string">&quot;$*&quot;</span></span><br><span class="line"><span class="keyword">for</span> OPTS <span class="keyword">in</span> <span class="variable">$CMDOPTS</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    key=$(<span class="built_in">echo</span> <span class="variable">$&#123;OPTS&#125;</span> | awk -F<span class="string">&quot;=&quot;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line">    value=$(<span class="built_in">echo</span> <span class="variable">$&#123;OPTS&#125;</span> | awk -F<span class="string">&quot;=&quot;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span> )</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;<span class="variable">$key</span>&quot;</span> <span class="keyword">in</span></span><br><span class="line">        --ssl-domain) SSL_DOMAIN=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-trusted-ip) SSL_TRUSTED_IP=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-trusted-domain) SSL_TRUSTED_DOMAIN=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-size) SSL_SIZE=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-date) SSL_DATE=<span class="variable">$value</span> ;;</span><br><span class="line">        --ca-date) CA_DATE=<span class="variable">$value</span> ;;</span><br><span class="line">        --ssl-cn) CN=<span class="variable">$value</span> ;;</span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CA 相关配置</span></span><br><span class="line">CA_DATE=<span class="variable">$&#123;CA_DATE:-3650&#125;</span></span><br><span class="line">CA_KEY=<span class="variable">$&#123;CA_KEY:-cakey.pem&#125;</span></span><br><span class="line">CA_CERT=<span class="variable">$&#123;CA_CERT:-cacerts.pem&#125;</span></span><br><span class="line">CA_DOMAIN=cattle-ca</span><br><span class="line"></span><br><span class="line"><span class="comment"># ssl 相关配置</span></span><br><span class="line">SSL_CONFIG=<span class="variable">$&#123;SSL_CONFIG:-<span class="variable">$PWD</span>/openssl.cnf&#125;</span></span><br><span class="line">SSL_DOMAIN=<span class="variable">$&#123;SSL_DOMAIN:-&#x27;www.rancher.local&#x27;&#125;</span></span><br><span class="line">SSL_DATE=<span class="variable">$&#123;SSL_DATE:-3650&#125;</span></span><br><span class="line">SSL_SIZE=<span class="variable">$&#123;SSL_SIZE:-2048&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 国家代码(2 个字母的代号),默认 CN;</span></span><br><span class="line">CN=<span class="variable">$&#123;CN:-CN&#125;</span></span><br><span class="line"></span><br><span class="line">SSL_KEY=<span class="variable">$SSL_DOMAIN</span>.key</span><br><span class="line">SSL_CSR=<span class="variable">$SSL_DOMAIN</span>.csr</span><br><span class="line">SSL_CERT=<span class="variable">$SSL_DOMAIN</span>.crt</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ---------------------------- \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m       | 生成 SSL Cert |       \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ---------------------------- \033[0m&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -e ./<span class="variable">$&#123;CA_KEY&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 1. 发现已存在 CA 私钥，备份&quot;</span><span class="variable">$&#123;CA_KEY&#125;</span><span class="string">&quot;为&quot;</span><span class="variable">$&#123;CA_KEY&#125;</span><span class="string">&quot;-bak，然后重新创建 \033[0m&quot;</span></span><br><span class="line">    <span class="built_in">mv</span> <span class="variable">$&#123;CA_KEY&#125;</span> <span class="string">&quot;<span class="variable">$&#123;CA_KEY&#125;</span>&quot;</span>-bak</span><br><span class="line">    openssl genrsa -out <span class="variable">$&#123;CA_KEY&#125;</span> <span class="variable">$&#123;SSL_SIZE&#125;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 1. 生成新的 CA 私钥 <span class="variable">$&#123;CA_KEY&#125;</span> \033[0m&quot;</span></span><br><span class="line">    openssl genrsa -out <span class="variable">$&#123;CA_KEY&#125;</span> <span class="variable">$&#123;SSL_SIZE&#125;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -e ./<span class="variable">$&#123;CA_CERT&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 2. 发现已存在 CA 证书，先备份&quot;</span><span class="variable">$&#123;CA_CERT&#125;</span><span class="string">&quot;为&quot;</span><span class="variable">$&#123;CA_CERT&#125;</span><span class="string">&quot;-bak，然后重新创建 \033[0m&quot;</span></span><br><span class="line">    <span class="built_in">mv</span> <span class="variable">$&#123;CA_CERT&#125;</span> <span class="string">&quot;<span class="variable">$&#123;CA_CERT&#125;</span>&quot;</span>-bak</span><br><span class="line">    openssl req -x509 -sha256 -new -nodes -key <span class="variable">$&#123;CA_KEY&#125;</span> -days <span class="variable">$&#123;CA_DATE&#125;</span> -out <span class="variable">$&#123;CA_CERT&#125;</span> -subj <span class="string">&quot;/C=<span class="variable">$&#123;CN&#125;</span>/CN=<span class="variable">$&#123;CA_DOMAIN&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 2. 生成新的 CA 证书 <span class="variable">$&#123;CA_CERT&#125;</span> \033[0m&quot;</span></span><br><span class="line">    openssl req -x509 -sha256 -new -nodes -key <span class="variable">$&#123;CA_KEY&#125;</span> -days <span class="variable">$&#123;CA_DATE&#125;</span> -out <span class="variable">$&#123;CA_CERT&#125;</span> -subj <span class="string">&quot;/C=<span class="variable">$&#123;CN&#125;</span>/CN=<span class="variable">$&#123;CA_DOMAIN&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 3. 生成 Openssl 配置文件 <span class="variable">$&#123;SSL_CONFIG&#125;</span> \033[0m&quot;</span></span><br><span class="line"><span class="built_in">cat</span> &gt; <span class="variable">$&#123;SSL_CONFIG&#125;</span> &lt;&lt;<span class="string">EOM</span></span><br><span class="line"><span class="string">[req]</span></span><br><span class="line"><span class="string">req_extensions = v3_req</span></span><br><span class="line"><span class="string">distinguished_name = req_distinguished_name</span></span><br><span class="line"><span class="string">[req_distinguished_name]</span></span><br><span class="line"><span class="string">[ v3_req ]</span></span><br><span class="line"><span class="string">basicConstraints = CA:FALSE</span></span><br><span class="line"><span class="string">keyUsage = nonRepudiation, digitalSignature, keyEncipherment</span></span><br><span class="line"><span class="string">extendedKeyUsage = clientAuth, serverAuth</span></span><br><span class="line"><span class="string">EOM</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -n <span class="variable">$&#123;SSL_TRUSTED_IP&#125;</span> || -n <span class="variable">$&#123;SSL_TRUSTED_DOMAIN&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">cat</span> &gt;&gt; <span class="variable">$&#123;SSL_CONFIG&#125;</span> &lt;&lt;<span class="string">EOM</span></span><br><span class="line"><span class="string">subjectAltName = @alt_names</span></span><br><span class="line"><span class="string">[alt_names]</span></span><br><span class="line"><span class="string">EOM</span></span><br><span class="line">    IFS=<span class="string">&quot;,&quot;</span></span><br><span class="line">    dns=(<span class="variable">$&#123;SSL_TRUSTED_DOMAIN&#125;</span>)</span><br><span class="line">    dns+=(<span class="variable">$&#123;SSL_DOMAIN&#125;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;!dns[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">      <span class="built_in">echo</span> DNS.$((i+<span class="number">1</span>)) = <span class="variable">$&#123;dns[$i]&#125;</span> &gt;&gt; <span class="variable">$&#123;SSL_CONFIG&#125;</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> [[ -n <span class="variable">$&#123;SSL_TRUSTED_IP&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">        ip=(<span class="variable">$&#123;SSL_TRUSTED_IP&#125;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;!ip[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">          <span class="built_in">echo</span> IP.$((i+<span class="number">1</span>)) = <span class="variable">$&#123;ip[$i]&#125;</span> &gt;&gt; <span class="variable">$&#123;SSL_CONFIG&#125;</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 4. 生成服务 SSL KEY <span class="variable">$&#123;SSL_KEY&#125;</span> \033[0m&quot;</span></span><br><span class="line">openssl genrsa -out <span class="variable">$&#123;SSL_KEY&#125;</span> <span class="variable">$&#123;SSL_SIZE&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 5. 生成服务 SSL CSR <span class="variable">$&#123;SSL_CSR&#125;</span> \033[0m&quot;</span></span><br><span class="line">openssl req -sha256 -new -key <span class="variable">$&#123;SSL_KEY&#125;</span> -out <span class="variable">$&#123;SSL_CSR&#125;</span> -subj <span class="string">&quot;/C=<span class="variable">$&#123;CN&#125;</span>/CN=<span class="variable">$&#123;SSL_DOMAIN&#125;</span>&quot;</span> -config <span class="variable">$&#123;SSL_CONFIG&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 6. 生成服务 SSL CERT <span class="variable">$&#123;SSL_CERT&#125;</span> \033[0m&quot;</span></span><br><span class="line">openssl x509 -sha256 -req -<span class="keyword">in</span> <span class="variable">$&#123;SSL_CSR&#125;</span> -CA <span class="variable">$&#123;CA_CERT&#125;</span> \</span><br><span class="line">    -CAkey <span class="variable">$&#123;CA_KEY&#125;</span> -CAcreateserial -out <span class="variable">$&#123;SSL_CERT&#125;</span> \</span><br><span class="line">    -days <span class="variable">$&#123;SSL_DATE&#125;</span> -extensions v3_req \</span><br><span class="line">    -extfile <span class="variable">$&#123;SSL_CONFIG&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 7. 证书制作完成 \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 8. 以 YAML 格式输出结果 \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;----------------------------------------------------------&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ca_key: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$CA_KEY</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ca_cert: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$CA_CERT</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ssl_key: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$SSL_KEY</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ssl_csr: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$SSL_CSR</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ssl_cert: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$SSL_CERT</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 9. 附加 CA 证书到 Cert 文件 \033[0m&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$&#123;CA_CERT&#125;</span> &gt;&gt; <span class="variable">$&#123;SSL_CERT&#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;ssl_cert: |&quot;</span></span><br><span class="line"><span class="built_in">cat</span> <span class="variable">$SSL_CERT</span> | sed <span class="string">&#x27;s/^/  /&#x27;</span></span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;\033[32m ====&gt; 10. 重命名服务证书 \033[0m&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;cp <span class="variable">$&#123;SSL_DOMAIN&#125;</span>.key tls.key&quot;</span></span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$&#123;SSL_DOMAIN&#125;</span>.key tls.key</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;cp <span class="variable">$&#123;SSL_DOMAIN&#125;</span>.crt tls.crt&quot;</span></span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$&#123;SSL_DOMAIN&#125;</span>.crt tls.crt</span><br></pre></td></tr></table></figure><h3 id="脚本说明"><a href="#脚本说明" class="headerlink" title="脚本说明"></a>脚本说明</h3><ul><li>复制以上代码另存为 <code>create_self-signed-cert.sh</code> 或者其他您喜欢的文件名。</li><li>脚本参数</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--ssl-domain: 生成 ssl 证书需要的主域名，如不指定则默认为 www.rancher.local，如果是 ip 访问服务，则可忽略；</span><br><span class="line">--ssl-trusted-ip: 一般 ssl 证书只信任域名的访问请求，有时候需要使用 ip 去访问 server，那么需要给 ssl 证书添加扩展 IP，多个 IP 用逗号隔开；</span><br><span class="line">--ssl-trusted-domain: 如果想多个域名访问，则添加扩展域名（TRUSTED_DOMAIN）,多个 TRUSTED_DOMAIN 用逗号隔开；</span><br><span class="line">--ssl-size: ssl 加密位数，默认 2048；</span><br><span class="line">--ssl-cn: 国家代码(2 个字母的代号),默认 CN；</span><br><span class="line">使用示例:</span><br><span class="line">./create_self-signed-cert.sh --ssl-domain=www.test.com --ssl-trusted-domain=www.test2.com \</span><br><span class="line">--ssl-trusted-ip=1.1.1.1,2.2.2.2,3.3.3.3 --ssl-size=2048 --ssl-date=3650</span><br></pre></td></tr></table></figure><h2 id="验证证书"><a href="#验证证书" class="headerlink" title="验证证书"></a>验证证书</h2><blockquote><p>注意: 因为使用的是自签名证书，浏览器会提示证书的颁发机构是未知的。</p></blockquote><p>把生成的 ca 证书和去除密码的私钥文件部署到 web 服务器后，执行以下命令验证:</p><h3 id="通过-openssl-本地校验"><a href="#通过-openssl-本地校验" class="headerlink" title="通过 openssl 本地校验"></a>通过 openssl 本地校验</h3><p><code>openssl verify -CAfile cacerts.pem tls.crt</code> 应该返回状态为 <code>ok</code></p><img src="/ssl/self-signed-ssl/image-20190328180337080.247019b5.png" class="" title="image-20190328180337080"><p><code>openssl x509 -in tls.crt -noout -text</code> 执行后查看对应的域名和扩展 iP 是否正确</p><img src="/ssl/self-signed-ssl/image-20190328180442226.0c3ec171.png" class="" title="image-20190328180442226"><img src="/ssl/self-signed-ssl/image-20190328180455485.a3cf66b9.png" class="" title="image-20190328180455485"><h3 id="通过远程-web-服务在线验证"><a href="#通过远程-web-服务在线验证" class="headerlink" title="通过远程 web 服务在线验证"></a>通过远程 web 服务在线验证</h3><blockquote><p><strong>注意:</strong> 修改为你自己的域名</p></blockquote><ul><li><p>不加 CA 证书验证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl s_client -connect demo.rancher.com:443 -servername demo.rancher.com</span><br></pre></td></tr></table></figure><img src="/ssl/self-signed-ssl/image-20180805213034249.png" class="" title="image-20180805213034249"></li><li><p>添加 CA 证书验证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl s_client -connect demo.rancher.com:443 -servername demo.rancher.com -CAfile server-ca.crt</span><br></pre></td></tr></table></figure><img src="/ssl/self-signed-ssl/image-20180805213123781.3920c247.png" class="" title="image-20180805213123781"></li></ul>]]></content>
      
      
      <categories>
          
          <category> ssl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher v2.4.10 发布</title>
      <link href="/rancher/release/2410/"/>
      <url>/rancher/release/2410/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/release/2410/" target="_blank" title="https://www.xtplayer.cn/rancher/release/2410/">https://www.xtplayer.cn/rancher/release/2410/</a></p><blockquote><p><strong>仅解决了 <a href="https://github.com/rancher/rancher/releases/tag/v2.4.9">v2.4.9 中</a>没有最新系统 chart 的离线设置问题。请注意，发行说明与 v2.4.9 相同。</strong></p></blockquote><h2 id="重要"><a href="#重要" class="headerlink" title="重要"></a>重要</h2><ul><li><p><strong>请查看 <a href="https://github.com/rancher/rancher/releases/tag/v2.4.0">v2.4.0 发行说明</a> 以获取重要的更新&#x2F;重大更改。</strong></p></li><li><p><strong>Kubernetes 1.18 现在是默认版本[ <a href="https://github.com/rancher/rancher/issues/25117">＃25117</a> ]</strong> -Kubernetes 1.18 现在是默认版本。每当升级到任何 Kubernetes 版本时，请查看 Kubernetes 发行说明以了解任何重大更改。</p></li><li><p><strong>使用单个 Docker 容器安装的用户</strong>-Docker<strong>容器中的</strong>Etcd 已从 3.3 升级到 3.4，因此您<em>必须</em>在升级之前进行备份，以便能够回滚到 v2.3.x 版本。没有此备份，您将无法回滚。</p></li><li><p>**使用带有 RHEL&#x2F;CentOS 节点的节点池的用户[ <a href="https://github.com/rancher/rancher/issues/18065">＃18065</a> ]**：RHEL&#x2F;CentOS 节点的默认存储驱动程序已更新为 <code>overlay2</code>。如果您的节点模板未指定存储驱动程序，则将使用新的更新的默认值（<code>overlay</code>）而不是旧的默认值（<code>devicemapper</code>）来配置任何新节点。如果需要继续使用 <code>devicemapper</code> 作为存储驱动程序选项，请编辑节点模板以将存储驱动程序显式设置为 &#96;devicemapper。</p></li><li><p><strong>运行 Windows 集群的用户[ <a href="https://github.com/rancher/rancher/issues/25582">＃25582</a> ]</strong> -Windows 从 2 月 11 日开始启动了安全补丁。升级之前，请更新您的节点以包括此安全补丁，否则，升级将失败，直到应用补丁为止。</p></li><li><p><strong>Rancher 启动的集群需要额外的 500MB 空间</strong>-默认情况下，Rancher 启动的集群已在集群上启用了审核日志记录。</p></li><li><p><strong>Rancher 启动了 Kubernetes 集群，其升级行为已更改[ <a href="https://github.com/rancher/rancher/issues/23897">＃23897</a> ]</strong> - 请参考零宕机时间升级功能以了解更多信息。</p></li></ul><h2 id="版本号"><a href="#版本号" class="headerlink" title="版本号"></a>版本号</h2><p>以下版本是最新且稳定的版本：</p><table><thead><tr><th>类型</th><th>rancher 版本</th><th>docker 标签</th><th>Helm Repo</th><th>舵图版本</th></tr></thead><tbody><tr><td>Latest</td><td>v2.4.10</td><td><code>rancher/rancher:latest</code></td><td>server-charts&#x2F;latest</td><td>v2.4.10</td></tr><tr><td>Stable</td><td>v2.4.10</td><td><code>rancher/rancher:stable</code></td><td>server-charts&#x2F;stable</td><td>v2.4.10</td></tr></tbody></table><p>请查看我们的<a href="https://rancher.com/docs/rancher/v2.x/en/installation/server-tags/">版本文档</a>以获取有关版本控制和标记约定的更多详细信息。</p><h2 id="增强功能"><a href="#增强功能" class="headerlink" title="增强功能"></a>增强功能</h2><ul><li><p>引入了带有新的 Ingress 镜像[ <a href="https://github.com/rancher/ingress-nginx/releases/tag/nginx-0.35.0-rancher2">nginx-0.35.0-rancher2</a> ]的新 Kubernetes 版本，以解决在关闭节点电源时不更新负载均衡器 IP 的问题。[ <a href="https://github.com/rancher/rancher/issues/28230">＃28230</a> ]，[ <a href="https://github.com/rancher/rancher/issues/13862">＃13862</a> ]</p></li><li><p>引入了用于为 Nginx 入口控制器配置不同网络模式的选项。[ <a href="https://github.com/rancher/rke/issues/1876">rancher&#x2F;rke＃1876</a> ]，[ <a href="https://github.com/rancher/rancher/issues/28329">＃28329</a> ]</p></li><li><p>引入了一个标志，用于更改为 Rancher 部署的图表保留的 Helm 版本历史的数量。[ <a href="https://github.com/rancher/rancher/issues/28765">＃28765</a> ]</p></li><li><p><code>v0.1.4</code> 在 prometheus-auth [ <a href="https://github.com/rancher/rancher/issues/29290">＃29290</a> ]中引入了带有更新的 Alpine Linux 镜像的新监视图（），并支持自定义污点。[ <a href="https://github.com/rancher/rancher/issues/27253">＃27253</a> ]</p></li><li><p>经过验证的对以下操作系统的支持：</p><table><thead><tr><th>操作系统</th><th>问题</th></tr></thead><tbody><tr><td>RHEL 7.9</td><td><a href="https://github.com/rancher/rancher/issues/29736">＃29736</a></td></tr><tr><td>Oracle Linux 7.9</td><td><a href="https://github.com/rancher/rancher/issues/29737">＃29737</a></td></tr><tr><td>CentOS 的 7.8</td><td><a href="https://github.com/rancher/rancher/issues/29738">＃29738</a></td></tr><tr><td>SLES 15 SP2</td><td><a href="https://github.com/rancher/rancher/issues/29739">＃29739</a></td></tr></tbody></table></li></ul><h2 id="从-v2-4-8-开始修复的主要错误"><a href="#从-v2-4-8-开始修复的主要错误" class="headerlink" title="从 v2.4.8 开始修复的主要错误"></a>从 v2.4.8 开始修复的主要错误</h2><ul><li>带有云提供商的 vSphere 集群现在在删除节点之前先耗尽它们。[ <a href="https://github.com/rancher/rancher/issues/18221">＃18221</a> ]，[ <a href="https://github.com/rancher/rancher/issues/24690">＃24690</a> ]</li><li>修复了删除 etcd 领导者会导致集群不健康的问题[ <a href="https://github.com/rancher/rancher/issues/24547">＃24547</a> ]</li><li>修复了 CLI 中用户提供的值未正确应用的问题[ <a href="https://github.com/rancher/rancher/issues/27841">＃27841</a> ]，[ <a href="https://github.com/rancher/rancher/issues/28000">＃28000</a> ]</li><li>修复了如果在集群上启用了项目网络隔离，则无法正确填充入口的负载均衡器 IP 的问题。[ <a href="https://github.com/rancher/rancher/issues/26677">＃26677</a> ]</li><li>修复了 azureAD 中导致内存泄漏无法获取用户组的问题。[ <a href="https://github.com/rancher/rancher/issues/29055">＃29055</a> ]</li><li>修复了应用程序安装中的一个问题，因此当 helm 安装失败时，应用程序不会继续重试并填充磁盘空间。[ <a href="https://github.com/rancher/rancher/issues/27146">＃27146</a> ]</li></ul><h2 id="从-v2-4-8-开始修复的小错误"><a href="#从-v2-4-8-开始修复的小错误" class="headerlink" title="从 v2.4.8 开始修复的小错误"></a>从 v2.4.8 开始修复的小错误</h2><ul><li>s3 etcd 备份验证现在在下游用户集群而不是 rancher 的管理集群上进行了验证[ <a href="https://github.com/rancher/rancher/issues/28739">＃28739</a> ]</li><li>修复了 Vsphere 中的一个问题，该问题由于数据存储区带有空字符而导致节点设置失败[ <a href="https://github.com/rancher/rancher/issues/27699">＃27699</a> ]</li><li>修复了从 API [ <a href="https://github.com/rancher/rancher/issues/28998">＃28998</a> ]耗尽节点的问题</li><li>修复了 Rancher Logging 中的一个问题，该问题会导致从 UI 编辑为文件时配置丢失。[ <a href="https://github.com/rancher/rancher/issues/26559">＃26559</a> ]</li><li>修复了在集群创建期间无法将只读角色分配给成员的问题。[ <a href="https://github.com/rancher/rancher/issues/23061">＃23061</a> ]</li><li>修复了无法在 vSphere 节点模板中选择子资源组的问题。[ <a href="https://github.com/rancher/rancher/issues/24507">＃24507</a> ]</li><li>修复了更新的 cert-manager <code>v1.0.0</code>[ <a href="https://github.com/rancher/rancher/issues/29056">＃29056</a> ]没有种类的 Issuer 错误消息的问题</li><li>修复了在 Oracle Linux 7.8 上安装 Docker [ <a href="https://github.com/rancher/rancher/issues/27691">＃27691</a> ]</li><li>修复了 UI 中由于大小写敏感而不支持 AKS 网络策略的问题[ <a href="https://github.com/rancher/rancher/issues/28880">＃28880</a> ]</li></ul><h2 id="其他注意事项"><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a>其他注意事项</h2><ul><li>OKE 和 OCI 节点驱动程序现在可以从 OCI API 动态填充值。[ <a href="https://github.com/rancher/rancher/issues/29621">＃29621</a> ]，[ <a href="https://github.com/rancher/rancher/issues/27051">＃27051</a> ]</li><li>现在，AWS 驱动程序会从 AMI 中检测根设备名称。[ <a href="https://github.com/rancher/rancher/issues/29568">＃29568</a> ]</li><li>现在，新的 E2 计算形状已添加到 OKE 驱动程序列表中。<a href="https://github.com/rancher/rancher/issues/29382">＃29382</a> ]</li><li>添加了对 vSphere &#x2F; ESXi 7.0 的支持[ <a href="https://github.com/rancher/rancher/issues/29519">＃29519</a> ]，[ <a href="https://github.com/rancher/rancher/issues/27732">＃27732</a> ]</li><li>为 EKS 集群添加了 Kubernetes 1.18 <a href="https://github.com/rancher/rancher/issues/29508">＃29508</a> ]</li></ul><h3 id="气隙的安装和升级"><a href="#气隙的安装和升级" class="headerlink" title="气隙的安装和升级"></a>气隙的安装和升级</h3><p>在 v2.4.0 中，气隙安装不再需要镜像系统图表 git repo。请遵循有关如何<a href="https://rancher.com/docs/rancher/v2.x/en/installation/air-gap/install-rancher">安装 Rancher 的说明以使用打包的系统图表</a>。</p><h3 id="其他升级"><a href="#其他升级" class="headerlink" title="其他升级"></a>其他升级</h3><h3 id="已知的主要问题"><a href="#已知的主要问题" class="headerlink" title="已知的主要问题"></a>已知的主要问题</h3><ul><li>当对启用了 Grafana 的持久性存储使用监视时，升级监视会导致 Pod 无法启动。问题中提供了解决方法步骤。[ <a href="https://github.com/rancher/rancher/issues/27450">＃27450</a> ]</li><li>使用监控时，升级 Kubernetes 版本会删除<em>“ API 服务器请求率”</em>指标[ <a href="https://github.com/rancher/rancher/issues/27267">＃27267</a> ]</li><li>将新的图表版本添加到 helm 3 目录中时，升级过程可能默认为 helm 2，从而导致 api 错误。解决问题。[ <a href="https://github.com/rancher/rancher/issues/27252">27252</a> ]</li><li>打开项目网络隔离后，从以前的 Rancher 版本升级到 2.4.10 将导致 CPU &#x2F;日志记录增加。解决方法是关闭 PNI。<a href="https://github.com/rancher/rancher/issues/30052">＃30052</a>。在<a href="https://github.com/rancher/rancher/issues/30045">＃30045 中</a>跟踪修订。</li></ul><h2 id="版本号-1"><a href="#版本号-1" class="headerlink" title="版本号"></a>版本号</h2><h3 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h3><ul><li>rancher&#x2F;rancher: v2.4.10</li><li>rancher&#x2F;rancher-agent: v2.4.10</li></ul><h3 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h3><ul><li>cli - <a href="https://github.com/rancher/cli/releases/tag/v2.4.7">v2.4.7</a></li><li>rke - <a href="https://github.com/rancher/rke/releases/tag/v1.1.11">v1.1.11</a></li></ul><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><ul><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.18.6-rancher1">1.18.10</a>（默认）</li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.17.9-rancher1">1.17.13</a></li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.16.13-rancher1">1.16.15</a></li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.15.12-rancher2">1.15.12</a></li></ul><h2 id="升级和回滚"><a href="#升级和回滚" class="headerlink" title="升级和回滚"></a>升级和回滚</h2><p>Rancher 支持升级和回滚。请注意您要<a href="https://rancher.com/docs/rancher/v2.x/en/upgrades/">升级</a>或<a href="https://rancher.com/docs/rancher/v2.x/en/backups/rollbacks/">回滚</a>以更改 Rancher 版本的版本。</p><p>请注意，在升级到 v2.3.0 +时，由于增加了对 Kubernetes 系统组件的容忍度，对 Rancher 启动的 Kubernetes 集群的任何编辑都将导致所有系统组件重新启动。制定相应的计划。</p><p>如果您具有使用自签名证书的 Rancher HA 安装，则对 cert-manager 的最新更改需要升级。如果您使用的是早于 v0.9.1 的 cert-manager，请参阅有关如何升级 cert-manager<a href="https://rancher.com/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/">的文档</a>。</p><p><strong>重要：</strong> 回滚时，我们希望您回滚到升级时的状态。升级后的任何更改都不会反映出来。</p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> release </category>
          
      </categories>
      
      
        <tags>
            
            <tag> release </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher v2.4.6 发布</title>
      <link href="/rancher/release/246/"/>
      <url>/rancher/release/246/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/release/246/" target="_blank" title="https://www.xtplayer.cn/rancher/release/246/">https://www.xtplayer.cn/rancher/release/246/</a></p><h2 id="重要更新"><a href="#重要更新" class="headerlink" title="重要更新"></a>重要更新</h2><ul><li><p><strong>请查看 <a href="https://github.com/rancher/rancher/releases/tag/v2.4.0">v2.4.0 发行说明</a> 以获取重要更新&#x2F;重大更改。</strong></p></li><li><p><strong>Kubernetes 1.18 现在是默认版本[ <a href="https://github.com/rancher/rancher/issues/25117">＃25117</a> ]</strong></p><p>Kubernetes 1.18 现在是默认版本。每当升级到任何 Kubernetes 版本时，请查看 Kubernetes 发行说明以了解任何重大更改。</p></li><li><p><strong>使用单个 Docker 容器安装的用户</strong></p><p>Docker <strong>容器中的</strong> Etcd 已从 3.3 升级到 3.4，因此您<em>必须</em>在升级之前进行备份，以便能够回滚到 v2.3.x 版本。没有备份，您将无法回滚。</p></li><li><p><strong>使用带有 RHEL &#x2F; CentOS 节点的节点池用户[ <a href="https://github.com/rancher/rancher/issues/18065">＃18065</a> ]</strong></p><p>RHEL &#x2F; CentOS 节点的默认存储驱动程序已更新为 <code>overlay2</code>，如果您的节点模板未指定存储驱动程序，则将使用新的更新的默认值（<code>overlay</code>）而不是旧的默认值（<code>devicemapper</code>）来配置任何新节点。如果需要继续使用 <code>devicemapper</code> 作为存储驱动程序选项，请编辑节点模板以将存储驱动程序显式设置为 <code>devicemapper</code>。</p></li><li><p><strong>运行 Windows 集群的用户[ <a href="https://github.com/rancher/rancher/issues/25582">＃25582</a> ]</strong></p><p>Windows 从 2 月 11 日开始启动了安全补丁。升级之前，请更新您的节点以包括此安全补丁，否则，升级将失败，直到应用该补丁为止。</p></li><li><p><strong>Rancher 启动的集群需要额外的 500MB 空间</strong></p><p>默认情况下，Rancher 启动的集群已在集群上启用了审核日志记录。</p></li><li><p><strong>Rancher 启动了 Kubernetes 集群，其升级行为已更改[ <a href="https://github.com/rancher/rancher/issues/23897">＃23897</a> ]</strong></p><p>请参考零宕机升级功能以了解更多信息。</p></li></ul><h2 id="版本号"><a href="#版本号" class="headerlink" title="版本号"></a>版本号</h2><p>以下版本是最新且稳定的版本：</p><table><thead><tr><th>类型</th><th>Rancher 版本</th><th>Docker 镜像标签</th><th>Helm Repo</th><th>Helm Chart 版本</th></tr></thead><tbody><tr><td>最新</td><td>v2.4.6</td><td><code>rancher/rancher:latest</code></td><td>server-charts&#x2F;latest</td><td>v2.4.6</td></tr><tr><td>稳定</td><td>v2.4.6</td><td><code>rancher/rancher:stable</code></td><td>server-charts&#x2F;stable</td><td>v2.4.6</td></tr></tbody></table><p>请查看我们的<a href="https://rancher.com/docs/rancher/v2.x/en/installation/server-tags/">版本文档</a>以获取有关版本控制和标签约定的更多详细信息。</p><h2 id="增强功能"><a href="#增强功能" class="headerlink" title="增强功能"></a>增强功能</h2><ul><li>Kubeconfig API 令牌现在支持自定义 TTL [ <a href="https://github.com/rancher/rancher/pull/28378">＃28378</a> ]</li></ul><p><strong>更新 Kubernetes 版本</strong></p><blockquote><p>注意：这些是<a href="https://forums.rancher.com/t/kubernetes-v1-16-13-v1-17-9-v1-18-6-addresses-security-announcement/17971">较早提供的</a>，但已正式打包到此发行版中。</p></blockquote><ul><li>更新为使用 <code>v1.16.13-rancher1-2</code>， <code>v1.17.9-rancher1-2</code> 并 <code>v1.18.6-rancher1-2</code></li><li>Kubernetes 相关的 CVE [ <a href="https://github.com/rancher/rancher/issues/27950">＃27950</a> ]</li></ul><ol><li><p><a href="https://discuss.kubernetes.io/t/security-advisory-cve-2020-8557-node-disk-dos-by-writing-to-container-etc-hosts/11870">CVE-2020-8557</a></p><p>节点磁盘 DOS 通过写入到容器&#x2F;etc&#x2F;hosts</p></li><li><p><a href="https://github.com/kubernetes/kubernetes/issues/92315">CVE-2020-8558</a></p><p>节点设置允许相邻主机绕过本地主机边界</p></li><li><p><a href="https://discuss.kubernetes.io/t/security-advisory-cve-2020-8559-privilege-escalation-from-compromised-node-to-cluster/11873">CVE-2020-8559</a></p><p>从受感染节点到集群的特权升级</p></li></ol><ul><li>更新 Canal 的 Flannel 版本到 v0.12.0 [ <a href="https://github.com/rancher/rancher/issues/27577">＃27577</a> ]</li><li>增加 Minio 的内存限制[ <a href="https://github.com/rancher/rancher/issues/28025">＃28025</a> ]</li></ul><h2 id="修复了自-v2-4-5-以来的主要-bug"><a href="#修复了自-v2-4-5-以来的主要-bug" class="headerlink" title="修复了自 v2.4.5 以来的主要 bug"></a>修复了自 v2.4.5 以来的主要 bug</h2><ul><li><p>Windows 集群现在支持前缀路径，并且 Windows 仅通过新的 win_ configs 来配置 args&#x2F;env&#x2F;binds [ <a href="https://github.com/rancher/rancher/issues/28143">＃28143</a> <a href="https://github.com/rancher/rancher/issues/25108">＃25108</a> ]</p></li><li><p>现在，删除 RKE 模板更加可靠[ <a href="https://github.com/rancher/rancher/issues/26861">＃26861</a> ]</p></li><li><p>从某些 S3 存储提供程序（如 NetApp StorageGRID [ <a href="https://github.com/rancher/rancher/issues/27608">#27608</a> ]）修复了 ETCD 快照还原</p></li><li><p>修复了从本地目录安装 Helm Charts 时的验证错误[ <a href="https://github.com/rancher/rancher/issues/23832">＃23832</a> ]</p></li><li><p><code>Timeout: Too large resource version</code> 已修复列表错误[ <a href="https://github.com/rancher/rancher/issues/28477">＃28477</a> ]</p></li><li><p>Kiali Traffic Graphs 现在可以在 1.5.x [ <a href="https://github.com/rancher/rancher/issues/28109">＃28109</a> ]中使用</p></li><li><p>Rancher Kubernetes 身份验证代理现在利用转发代理[ <a href="https://github.com/rancher/rancher/issues/25488">＃25488</a> ]</p></li><li><p>修复了 cli 和 –format 标志[ <a href="https://github.com/rancher/rancher/issues/27661">＃27661</a> ]的错误</p></li><li><p>修复了 wit cli 和 –values 标志的错误[ <a href="https://github.com/rancher/rancher/pull/27346">＃27346</a> ]</p></li><li><p>修复了编辑多集群应用程序的错误[ <a href="https://github.com/rancher/rancher/issues/27416">＃27416</a> ]</p></li><li><p>修复了节点 Draining [ <a href="https://github.com/rancher/rancher/issues/23333">＃23333</a> ] 的错误</p></li><li><p>修复了未创建节点池的用户无法缩放节点池的错误[ <a href="https://github.com/rancher/rancher/issues/27031">＃27031</a> ]</p></li><li><p>通过编码 Azure AD 令牌[ <a href="https://github.com/rancher/rancher/pull/27774">＃27774</a> ] 修复了一个错误</p></li><li><p><code>context deadline exceeded</code>[ <a href="https://github.com/rancher/rancher/issues/27736">＃27736</a> ]</p></li><li><p>RKE 快照现在存储用于备份还原的证书<a href="https://github.com/rancher/rke/issues/1336">＃1336</a></p></li><li><p>Config Maps and Secrets 现在支持名称中带有 <code>.</code>[ <a href="https://github.com/rancher/rancher/issues/25955">＃25955</a> ]</p></li><li><p>固定的 UI 问题 [ <a href="https://github.com/rancher/rancher/issues/27849">＃27849</a> <a href="https://github.com/rancher/rancher/issues/27769">＃27769</a> <a href="https://github.com/rancher/rancher/issues/27705">＃27705</a> <a href="https://github.com/rancher/rancher/issues/27439">＃27439</a> <a href="https://github.com/rancher/rancher/issues/27416">＃27416</a> <a href="https://github.com/rancher/rancher/issues/27333">＃27333</a> <a href="https://github.com/rancher/rancher/issues/27021">＃27021</a> <a href="https://github.com/rancher/rancher/issues/26865">＃26865</a> <a href="https://github.com/rancher/rancher/issues/26827">＃26827</a> <a href="https://github.com/rancher/rancher/issues/26469">＃150469</a> <a href="https://github.com/rancher/rancher/issues/15037">＃15037</a> <a href="https://github.com/rancher/rancher/issues/26469">#4037</a> <a href="https://github.com/rancher/ui/pull/4047">＃4047</a> ]</p></li></ul><h2 id="其他注意事项"><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a>其他注意事项</h2><ul><li>Rancher Machine 创建的节点现在默认为 overlay2 文件系统[ <a href="https://github.com/rancher/rancher/issues/27414">＃27414</a> ]</li><li>默认 CIS 扫描版本为 1.5 [ <a href="https://github.com/rancher/rancher/issues/27446">＃27446</a> ]</li><li>EC2 节点模板现在支持 KMS 加密密钥[ <a href="https://github.com/rancher/rancher/issues/27965">＃27965</a> ]</li><li>EC2 节点模板现在支持 MetadataOptions [ <a href="https://github.com/rancher/rancher/issues/25078">＃25078</a> ]</li><li>为 Microsoft 团队添加了通知程序[ <a href="https://github.com/rancher/rancher/issues/15802">＃15802</a> ]</li><li>集群 API lister 现在使用认证缓存来提高速度[ <a href="https://github.com/rancher/rancher/issues/27192">＃27192</a> ]</li></ul><h2 id="其他升级"><a href="#其他升级" class="headerlink" title="其他升级"></a>其他升级</h2><h3 id="已知的重大问题"><a href="#已知的重大问题" class="headerlink" title="已知的重大问题"></a>已知的重大问题</h3><ul><li>当在启用了 Grafana 的持久存储的情况下使用监控时，升级监控会导致 Pod 启动失败。issue [ <a href="https://github.com/rancher/rancher/issues/27450">＃27450</a> ] 中提供了解决步骤。</li><li>使用监控时，升级 Kubernetes 版本会删除 <code>API 服务器请求速率</code>指标[ <a href="https://github.com/rancher/rancher/issues/27267">＃27267</a> ]</li><li>将新的 cahrt 版本添加到 helm3 目录中时，升级过程可能默认为 helm2，从而导致 api 错误。解决问题。[ <a href="https://github.com/rancher/rancher/issues/27252">27252</a> ]</li></ul><h3 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h3><ul><li>cli- <a href="https://github.com/rancher/cli/releases/tag/v2.4.6">v2.4.6</a></li><li>rke- <a href="https://github.com/rancher/rke/releases/tag/v1.1.6">v1.1.6</a></li></ul><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><ul><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.18.6-rancher1">1.18.6</a>（默认）</li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.17.9-rancher1">1.17.9</a></li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.16.13-rancher1">1.16.13</a></li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.15.12-rancher2">1.15.12</a></li></ul><h2 id="完整镜像列表"><a href="#完整镜像列表" class="headerlink" title="完整镜像列表"></a>完整镜像列表</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">busybox</span><br><span class="line">rancher/calico-cni:v3.13.4</span><br><span class="line">rancher/calico-ctl:v3.13.4</span><br><span class="line">rancher/calico-kube-controllers:v3.13.4</span><br><span class="line">rancher/calico-node:v3.13.4</span><br><span class="line">rancher/calico-pod2daemon-flexvol:v3.13.4</span><br><span class="line">rancher/cluster-proportional-autoscaler:1.3.0</span><br><span class="line">rancher/cluster-proportional-autoscaler:1.7.1</span><br><span class="line">rancher/configmap-reload:v0.3.0-rancher2</span><br><span class="line">rancher/coredns-coredns:1.3.1</span><br><span class="line">rancher/coredns-coredns:1.6.2</span><br><span class="line">rancher/coredns-coredns:1.6.5</span><br><span class="line">rancher/coredns-coredns:1.6.9</span><br><span class="line">rancher/coreos-configmap-reload:v0.0.1</span><br><span class="line">rancher/coreos-etcd:v3.3.10-rancher1</span><br><span class="line">rancher/coreos-etcd:v3.3.15-rancher1</span><br><span class="line">rancher/coreos-etcd:v3.4.3-rancher1</span><br><span class="line">rancher/coreos-flannel:v0.12.0</span><br><span class="line">rancher/coreos-kube-state-metrics:v1.9.5</span><br><span class="line">rancher/coreos-prometheus-config-reloader:v0.38.1</span><br><span class="line">rancher/coreos-prometheus-operator:v0.38.1</span><br><span class="line">rancher/eks-operator:v0.1.0-rc24</span><br><span class="line">rancher/flannel-cni:v0.3.0-rancher6</span><br><span class="line">rancher/fluentd:v0.1.19</span><br><span class="line">rancher/grafana-grafana:6.7.4</span><br><span class="line">rancher/grafana-grafana:7.1.0</span><br><span class="line">rancher/hyperkube:v1.15.12-rancher2</span><br><span class="line">rancher/hyperkube:v1.16.13-rancher1</span><br><span class="line">rancher/hyperkube:v1.17.9-rancher1</span><br><span class="line">rancher/hyperkube:v1.18.6-rancher1</span><br><span class="line">rancher/istio-1.5-migration:0.1.1</span><br><span class="line">rancher/istio-citadel:1.5.9</span><br><span class="line">rancher/istio-coredns-plugin:0.2-istio-1.1</span><br><span class="line">rancher/istio-galley:1.5.9</span><br><span class="line">rancher/istio-kubectl:1.1.5</span><br><span class="line">rancher/istio-kubectl:1.4.6</span><br><span class="line">rancher/istio-kubectl:1.5.9</span><br><span class="line">rancher/istio-mixer:1.5.9</span><br><span class="line">rancher/istio-node-agent-k8s:1.5.9</span><br><span class="line">rancher/istio-pilot:1.5.9</span><br><span class="line">rancher/istio-proxyv2:1.5.9</span><br><span class="line">rancher/istio-sidecar_injector:1.5.9</span><br><span class="line">rancher/jaegertracing-all-in-one:1.14</span><br><span class="line">rancher/jenkins-jnlp-slave:3.35-4</span><br><span class="line">rancher/jetstack-cert-manager-controller:v0.8.1</span><br><span class="line">rancher/k3s-upgrade:v1.17.11-k3s1</span><br><span class="line">rancher/k3s-upgrade:v1.18.8-k3s1</span><br><span class="line">rancher/k8s-dns-dnsmasq-nanny:1.15.0</span><br><span class="line">rancher/k8s-dns-dnsmasq-nanny:1.15.2</span><br><span class="line">rancher/k8s-dns-kube-dns:1.15.0</span><br><span class="line">rancher/k8s-dns-kube-dns:1.15.2</span><br><span class="line">rancher/k8s-dns-node-cache:1.15.7</span><br><span class="line">rancher/k8s-dns-sidecar:1.15.0</span><br><span class="line">rancher/k8s-dns-sidecar:1.15.2</span><br><span class="line">rancher/kiali-kiali:v1.17</span><br><span class="line">rancher/klipper-helm:v0.2.3</span><br><span class="line">rancher/klipper-helm:v0.2.5</span><br><span class="line">rancher/klipper-lb:v0.1.2</span><br><span class="line">rancher/kube-api-auth:v0.1.4</span><br><span class="line">rancher/kubectl:v1.18.0</span><br><span class="line">rancher/kubernetes-external-dns:v0.6.0</span><br><span class="line">rancher/library-traefik:1.7.19</span><br><span class="line">rancher/local-path-provisioner:v0.0.11</span><br><span class="line">rancher/log-aggregator:v0.1.6</span><br><span class="line">rancher/metrics-server:v0.3.3</span><br><span class="line">rancher/metrics-server:v0.3.4</span><br><span class="line">rancher/metrics-server:v0.3.6</span><br><span class="line">rancher/minio-minio:RELEASE.2020-07-13T18-09-56Z</span><br><span class="line">rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1</span><br><span class="line">rancher/nginx-ingress-controller:nginx-0.32.0-rancher1</span><br><span class="line">rancher/nginx:1.17.4-alpine</span><br><span class="line">rancher/opa-gatekeeper:v3.1.0-beta.7</span><br><span class="line">rancher/openzipkin-zipkin:2.14.2</span><br><span class="line">rancher/pause:3.1</span><br><span class="line">rancher/pipeline-jenkins-server:v0.1.4</span><br><span class="line">rancher/pipeline-tools:v0.1.14</span><br><span class="line">rancher/plugins-docker:18.09</span><br><span class="line">rancher/prom-alertmanager:v0.20.0</span><br><span class="line">rancher/prom-node-exporter:v0.18.1</span><br><span class="line">rancher/prom-prometheus:v2.12.0</span><br><span class="line">rancher/prom-prometheus:v2.17.2</span><br><span class="line">rancher/prometheus-auth:v0.2.0</span><br><span class="line">rancher/pstauffer-curl:v1.0.3</span><br><span class="line">rancher/rancher-agent:v2.4.6</span><br><span class="line">rancher/rancher:v2.4.6</span><br><span class="line">rancher/rke-tools:v0.1.64</span><br><span class="line">rancher/security-scan:v0.1.14</span><br><span class="line">rancher/sonobuoy-sonobuoy:v0.16.3</span><br><span class="line">rancher/system-upgrade-controller:v0.6.2</span><br><span class="line">rancher/thanos:v0.10.1</span><br><span class="line">rancher/webhook-receiver:v0.2.4</span><br><span class="line">registry:2</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> release </category>
          
      </categories>
      
      
        <tags>
            
            <tag> release </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher v2.4.8 发布</title>
      <link href="/rancher/release/248/"/>
      <url>/rancher/release/248/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/release/248/" target="_blank" title="https://www.xtplayer.cn/rancher/release/248/">https://www.xtplayer.cn/rancher/release/248/</a></p><p>Rancher 2.4.8 版本于 2020 年 9 月 4 日发布。Rancher 2.4.8 解决了 Rancher v2.4.6 中新引入的设置（auth-token-max-ttl-minutes）有关的问题。除了以下变化，版本说明与 v2.4.6 相同。点击<a href="https://github.com/rancher/rancher/releases/tag/v2.4.8">这里</a>查看英文版版本说明。</p><ul><li><p>修正了一个 UI 问题，在这个问题上，列出 KMS 密钥需要权限才能修改 AWS 节点模板<a href="https://github.com/rancher/rancher/issues/28724">#28724</a>。</p></li><li><p>修正了 k8s go-client 与旧版 k8s 不兼容的问题，它可能会引起 <code>Timeout: Too large resource version</code> 错误。注意：该消息对集群没有已知的直接影响<a href="https://github.com/rancher/rancher/issues/28623">#28623</a>。</p></li><li><p>修正了新设置的默认值（auth-token-max-ttl-minutes）</p><ul><li><p>修正了在 v2.4.6 中创建的新 API 令牌会在 24 小时内默认过期的问题。该设置现在默认为 0，允许令牌永不过期，与 v2.4.5 类似<a href="https://github.com/rancher/rancher/issues/28668">#28668</a></p></li><li><p>修正了一个用户界面问题，如果 API 令牌以非零 TTL 创建，他们会有错误的到期时间戳，并且在 v2.4.6 中引入的新设置 auth-token-max-ttl-minutes 被设置为 0（永不过期）<a href="https://github.com/rancher/rancher/issues/28678">#28678</a></p></li></ul></li></ul><h2 id="解决安全漏洞"><a href="#解决安全漏洞" class="headerlink" title="解决安全漏洞"></a>解决安全漏洞</h2><ul><li><p>升级了 Rancher 使用的 Helm 版本，现在使用的是 Helm v2.16.8，以解决 <a href="https://github.com/helm/helm/releases/tag/v2.16.8">CVE-2020-7919</a> [<a href="https://github.com/rancher/rancher/pull/27568">PR-27568</a>]这个问题。</p></li><li><p>升级了 Rancher 的监控工具 Grafana 的版本，以解决<a href="https://grafana.com/blog/2020/06/03/grafana-6.7.4-and-7.0.2-released-with-important-security-fix/">CVE-2020-13379</a> [<a href="https://github.com/rancher/rancher/issues/27418">#27418</a>]这个问题。</p></li><li><p>升级了 Rancher 中的 Istio 的版本，以解决<a href="https://istio.io/latest/news/releases/1.4.x/announcing-1.4.9/">ISTIO-SECURITY-2020-005</a> [<a href="https://github.com/rancher/rancher/issues/27064">#27064</a>]这个问题。</p></li></ul><h2 id="重要说明"><a href="#重要说明" class="headerlink" title="重要说明"></a>重要说明</h2><ul><li><p><strong>Kubernetes 1.18 现在为默认版本 [<a href="https://github.com/rancher/rancher/issues/25117">#25117</a>]：</strong> Kubernetes 1.18 现在为默认版本。在任何时候升级 Kubernetes 版本之前，请阅读 Kubernetes 的版本说明，来了解重要变更。</p></li><li><p><strong>使用单节点 Docker 安装的用户请注意：</strong> Docker 容器中的 Etcd 已从 3.3 升级到了 3.4，因此，您必须在升级之前进行备份，以便能够回滚到 v2.3.x 版本。没有此备份，将无法回滚。</p></li><li><p><strong>使用节点池功能，并使用 RHEL&#x2F;CentOS 的用户请注意 [<a href="https://github.com/rancher/rancher/issues/18065">#18065</a>]：</strong> RHEL &#x2F; CentOS 节点的默认存储驱动已更新为 <code>overlay2</code>。如果您的节点模板未指定存储驱动，则在使用该模版创建新节点时，将使用默认值 <code>overlay2</code> 而不是之前的 <code>devicemapper</code> 作为存储驱动。如果您需要继续使用 <code>devicemapper</code> 作为存储驱动选项，请编辑节点模板，并将存储驱动显式设置为 <code>devicemapper</code></p></li><li><p><strong>使用 Windows 集群的用户请注意 [<a href="https://github.com/rancher/rancher/issues/25582">#25582</a>]：</strong> Windows 已于 2 月 11 日发布了安全补丁程序。升级之前，请更新您的节点以包括此安全补丁程序，否则，升级将失败，直到您使用该补丁程序为止。</p></li><li><p><strong>Rancher 部署的集群（RKE 集群）需要额外的 500MB 磁盘空间</strong> Rancher 部署的集群（RKE 集群）默认开启了审计日志功能。</p></li><li><p><strong>Rancher 部署的集群（RKE 集群）升级方式发生了变化 [<a href="https://github.com/rancher/rancher/issues/23897">#23897</a>]：</strong> 请参考零宕机升级功能以了解更多信息。</p></li></ul><h2 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h2><p>下面的版本为当前的最新版和稳定版：</p><table><thead><tr><th align="left">类型</th><th align="left">Rancher 版本</th><th align="left">Docker 标签</th><th align="left">Helm 仓库</th><th align="left">Helm Chart 版本</th></tr></thead><tbody><tr><td align="left">最新版</td><td align="left">v2.4.8</td><td align="left"><code>rancher/rancher:latest</code></td><td align="left">server-charts&#x2F;latest</td><td align="left">v2.4.8</td></tr><tr><td align="left">稳定版</td><td align="left">v2.4.8</td><td align="left"><code>rancher/rancher:stable</code></td><td align="left">server-charts&#x2F;stable</td><td align="left">v2.4.8</td></tr></tbody></table><h2 id="功能和优化"><a href="#功能和优化" class="headerlink" title="功能和优化"></a>功能和优化</h2><p>Kubeconfig API 令牌现在支持 TTL[<a href="https://github.com/rancher/rancher/issues/28378">#28378</a>]。</p><h2 id="自-v2-4-5-以来修复的主要问题"><a href="#自-v2-4-5-以来修复的主要问题" class="headerlink" title="自 v2.4.5 以来修复的主要问题"></a>自 v2.4.5 以来修复的主要问题</h2><ul><li><p>Windows 集群现在支持前缀路径，并且通过新的 win_configs 只支持 Windows 的 args&#x2F;env&#x2F;binds[<a href="https://github.com/rancher/rancher/issues/28413">#28413</a>]、[<a href="https://github.com/rancher/rancher/issues/25108">#25108</a>]。</p></li><li><p>改进了删除 RKE 模板的操作[<a href="https://github.com/rancher/rancher/issues/26861">26861</a>]。</p></li><li><p>修正了一些 S3 Bucket 供应商（如 NetApp StorageGRID）的 ETCD 快照恢复[<a href="https://github.com/rancher/rancher/issues/27608">27608</a>]。</p></li></ul><h2 id="自-v2-4-5-以来修复的小问题"><a href="#自-v2-4-5-以来修复的小问题" class="headerlink" title="自 v2.4.5 以来修复的小问题"></a>自 v2.4.5 以来修复的小问题</h2><ul><li><p>修正了从本地目录安装 Helm 图表时的授权错误 <a href="https://github.com/rancher/rancher/issues/23832">#23832</a></p></li><li><p>超时：在 k8s v1.18 上修正了资源版本列表过大的错误 <a href="https://github.com/rancher/rancher/issues/28477">#28477</a></p></li><li><p>Kiali Traffic Graphs 1.5.x 可用 <a href="https://github.com/rancher/rancher/issues/28109">#28109</a></p></li><li><p>Rancher Kubernetes Auth Proxy 现在利用转发代理 <a href="https://github.com/rancher/rancher/issues/25488">#25488</a></p></li><li><p>修正了 cli 和–format 标志的一个错误 <a href="https://github.com/rancher/rancher/issues/27661">#27661</a></p></li><li><p>修正了 cli 和–value 标志的一个错误 <a href="https://github.com/rancher/rancher/issues/27346">#27346</a></p></li><li><p>修正了一个编辑多集群应用程序的错误 <a href="https://github.com/rancher/rancher/issues/27416">#27416</a></p></li><li><p>修正了一个 draining 节点的错误 <a href="https://github.com/rancher/rancher/issues/23333">#23333</a></p></li><li><p>修正了一个节点池的错误，非创建者也可以缩放节点池<a href="https://github.com/rancher/rancher/issues/27031">#27031</a></p></li><li><p>修正了一个加密 Azure AD 密钥的错误。 <a href="https://github.com/rancher/rancher/issues/27774">#27774</a></p></li><li><p>超出上下文最后期限而导致日志被淹没的超时延长 <a href="https://github.com/rancher/rancher/issues/27736">#27736</a></p></li><li><p>RKE 快照现在可以存储用于备份还原的证书 <a href="https://github.com/rancher/rke/issues/">#1336</a></p></li><li><p>配置地图和秘密的名称中现在接受“.”符号 <a href="https://github.com/rancher/rancher/issues/25955">#25955</a></p></li><li><p>修正了一些用户界面问题</p></li></ul><h2 id="其他说明"><a href="#其他说明" class="headerlink" title="其他说明"></a>其他说明</h2><ul><li><p>Rancher 机器创建的节点现在默认为 overlay2 文件系统 <a href="https://github.com/rancher/rancher/issues/27414">#27414</a></p></li><li><p>默认 CIS 扫描版本改为 v1.5 <a href="https://github.com/rancher/rancher/issues/27446">#27446</a></p></li><li><p>EC2 节点模板现在支持 KMS 加密密钥 <a href="https://github.com/rancher/rancher/issues/27965">#27965</a></p></li><li><p>EC2 Node 模板现在支持 MetadataOptions <a href="https://github.com/rancher/rancher/issues/25078">#25078</a></p></li><li><p>新增通知方式：Microsoft Teams <a href="https://github.com/rancher/rancher/issues/15802">#15802</a></p></li><li><p>Cluster API lister 现在使用 auth cache 来提高速度 <a href="https://github.com/rancher/rancher/issues/27192">#27192</a></p></li></ul><h3 id="离线安装和升级"><a href="#离线安装和升级" class="headerlink" title="离线安装和升级"></a>离线安装和升级</h3><p>在 v2.4.0 的版本里，离线安装将不在需要手动同步 system charts 的源代码仓库了。</p><h3 id="主要的已知问题"><a href="#主要的已知问题" class="headerlink" title="主要的已知问题"></a>主要的已知问题</h3><ul><li><p>启用 Grafana 和持久存储时，升级监控工具会导致 pod 无法启动。解决方法请参考[<a href="https://github.com/rancher/rancher/issues/27450">#27450</a>]。</p></li><li><p>将新的 chart 版本添加到 Helm3 应用商店的时候，升级时默认使用的是 Helm2 而不是 Helm3，导致出现 API 错误。解决方法请参考[<a href="https://github.com/rancher/rancher/issues/27252">27252</a>]。</p></li><li><p>使用监控时，升级 Kubernetes 版本会删除 API Server Request Rate 监控参数<a href="https://github.com/rancher/rancher/issues/27267">#27267</a>]。</p></li></ul><h2 id="版本信息"><a href="#版本信息" class="headerlink" title="版本信息"></a>版本信息</h2><h3 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h3><ul><li>rancher&#x2F;rancher:v2.4.8</li><li>rancher&#x2F;rancher-agent:v2.4.8</li></ul><h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h3><ul><li>cli - <a href="https://github.com/rancher/rancher/releases/tag/v2.4.8">v2.4.8</a></li><li>rke - <a href="https://github.com/rancher/rke/releases/tag/v1.1.6">v1.1.6</a></li></ul><h3 id="Kubernete-版本"><a href="#Kubernete-版本" class="headerlink" title="Kubernete 版本"></a>Kubernete 版本</h3><ul><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.18.6-rancher1">1.18.6</a>(默认版本）</li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.17.9-rancher1">1.17.9</a></li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.16.13-rancher1">1.16.13</a></li><li><a href="https://github.com/rancher/hyperkube/releases/tag/v1.15.12-rancher2">1.15.12</a></li></ul><h2 id="升级和回滚"><a href="#升级和回滚" class="headerlink" title="升级和回滚"></a>升级和回滚</h2><p>请注意，在升级到 v2.3.0 或者以上版本时，第一次修改通过 Rancher v2.3.0 之前版本部署的 RKE 集群时，由于要向系统组件中加入 Tolerations，该集群全部的系统组件将会自动重启。请事先规划好。</p><p><strong>重要：</strong>回滚时，我们建议回滚至升级前的状态，Rancher 不会保留回滚前的修改。</p><h2 id="Assets"><a href="#Assets" class="headerlink" title="Assets"></a>Assets</h2><p>请在<a href="https://github.com/rancher/rancher/releases/tag/v2.4.8">这里</a>获取该版本的 Assets。</p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> release </category>
          
      </categories>
      
      
        <tags>
            
            <tag> release </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点+权威证书+四层负载均衡安装 Rancher</title>
      <link href="/rancher/install/single-node-install-authority-ssl-l4/"/>
      <url>/rancher/install/single-node-install-authority-ssl-l4/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl-l4/" target="_blank" title="https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl-l4/">https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl-l4/</a></p><p>权威 ssl 证书与默认自签名 ssl 证书或者自己生成的自签名证书有所不同，在权威 ssl 证书模式下，agent 通过 IP 访问无法通过证书的校验。因为 ssl 证书在生成的时候会与 IP 或者域名有一个绑定关系，而权威证书生成时候是基于申请证书时使用的域名，所以当用其他 IP 访问时候则无法验证通过。默认自签名 ssl 证书或者自己生成的自签名证书，在创建的时候有传递相应的 IP，所以自签名可以正常认证。</p><blockquote><p>要使用权威证书安装 rancher，需要使用申请 ssl 证书时用的域名来访问 rancher。</p></blockquote><h2 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker_data_dir=xxxx <span class="comment"># 定义绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/data <span class="comment"># rancher 数据目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog <span class="comment"># 审计日志目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl <span class="comment"># 自定义 CA 证书目录</span></span><br><span class="line"></span><br><span class="line">docker run -d --restart=unless-stopped \</span><br><span class="line">-p 10080:80 -p 10443:443 \ ①</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/data:/var/lib/rancher \ ②</span><br><span class="line"><span class="comment"># 审计日志配置 ③</span></span><br><span class="line">-e AUDIT_LEVEL=3 \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog:/var/log/auditlog \</span><br><span class="line"><span class="comment"># 以下两行为自定义 CA 根证书 ④</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/ssl/certs/ca-additional.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/rancher/ssl/ca-additional.pem \</span><br><span class="line"><span class="comment"># 以下两行为配置权威 ssl 证书 ⑤</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/tls.crt:/etc/rancher/ssl/cert.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/tls.key:/etc/rancher/ssl/key.pem \</span><br><span class="line">rancher/rancher:stable --no-cacerts</span><br></pre></td></tr></table></figure><blockquote><ol><li>如果要在安装 rancher 的主机上安装 K8S 集群，那么在 ① 处需要修改主机端口为非 80 和非 443，比如：-p 10080:80 -p 10443:443 ；</li><li>为了保证数据持久保存，需要在 ② 处映射 rancher 数据到主机路径；</li><li>如果不需要开启升级日志，则设置 AUDIT_LEVEL&#x3D;0；</li><li>rancher 内置的应用商店，是利用 git 工具去获取 git 服务器 repo 中 chart。如果 git 服务器用的自签名 ssl 证书，那么需要给 rancher 配置自签名 CA 证书用以访问 git 服务器时做认证。所以需要在 ④ 处把自签名 CA 映射到 rancher 容器中；</li><li>第 ⑤ 处的证书文件建议保持一致。并且在 <strong>cert.pem</strong> 文件中需要包含中间链路证书，常见包含中间链路证书的 <strong>cert.pem</strong> 文件中，至少有两个以 —BEGIN CERTIFICATE— 开头并以 —END CERTIFICATE— 结尾。</li></ol></blockquote><h2 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h2><p>离线安装方法与在线安装方法基本相同，前提是已经将镜像同步到离线镜像仓库了，可以参考<a href="/rancher/rancher-install-offline-images">准备离线镜像</a>来同步镜像。除此之外，有以下几点注意：</p><ol><li><p>修改 <code>rancher/rancher:stable</code> 为离线镜像仓库中的镜像地址，比如：<code>192.168.100.100/rancher/rancher:stable</code></p></li><li><p>Rancher <code>v2.3.0</code> 之后的版本默认内置了 system-chart，比如：监控。如果你的环境中没有 git 服务器用来同步 GitHub 上的 system-chart，那么可以通过设置以下环境变量来使用 Rancher 内置的 system-chart。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-e CATTLE_SYSTEM_CATALOG=bundled \</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong>如果将来要切换为外部 Git 仓库的 system-chart，需要修改变量 <code>CATTLE_SYSTEM_CATALOG=external</code>，不能去掉这个变量。</p></blockquote></li><li><p>在 Rancher 运行起来后进入 Rancher UI，进入<strong>Settings</strong>视图，查找 <code>system-default-registry</code> 并点击 <strong>Edit</strong>。</p> <p> 将<strong>值</strong>改为您的私有仓库地址， 例如：<code>registry.yourdomain.com:port</code>， 不要添加 <code>http:// 或 https://</code> 前缀。</p> </li></ol><h2 id="外部负载均衡"><a href="#外部负载均衡" class="headerlink" title="外部负载均衡"></a>外部负载均衡</h2><p>有时候主机只有一个内网 IP 而这个 <strong>IP</strong> 我们又无法直接访问，对于这种场景，需要有一个既可以访问 <strong>rancher server</strong>，又可以被用户直接访问的工具来代理。比较常见的，比如 <strong>nginx</strong> 的反向代理，下文所指的负载均衡均以 <strong>nginx</strong> 为例。</p><h3 id="NGINX-四层代理参考配置"><a href="#NGINX-四层代理参考配置" class="headerlink" title="NGINX 四层代理参考配置"></a>NGINX 四层代理参考配置</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">worker_processes</span> <span class="number">4</span><span class="string">;</span></span><br><span class="line"><span class="string">worker_rlimit_nofile</span> <span class="number">40000</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line"><span class="string">events</span> &#123;</span><br><span class="line">    <span class="string">worker_connections</span> <span class="number">8192</span><span class="string">;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">stream</span> &#123;</span><br><span class="line">    <span class="string">upstream</span> <span class="string">rancher_servers_http</span> &#123;</span><br><span class="line">        <span class="string">least_conn;</span></span><br><span class="line">        <span class="string">server</span> <span class="string">&lt;IP_NODE_1&gt;:10080</span> <span class="string">max_fails=3</span> <span class="string">fail_timeout=5s;</span> <span class="comment"># 根据实际 rancher 映射端口修改</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span>     <span class="number">80</span><span class="string">;</span></span><br><span class="line">        <span class="string">proxy_pass</span> <span class="string">rancher_servers_http;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">upstream</span> <span class="string">rancher_servers_https</span> &#123;</span><br><span class="line">        <span class="string">least_conn;</span></span><br><span class="line">        <span class="string">server</span> <span class="string">&lt;IP_NODE_1&gt;:10443</span> <span class="string">max_fails=3</span> <span class="string">fail_timeout=5s;</span> <span class="comment"># 根据实际 rancher 映射端口修改</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span>     <span class="number">443</span><span class="string">;</span></span><br><span class="line">        <span class="string">proxy_pass</span> <span class="string">rancher_servers_https;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
            <tag> 单节点安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点+权威证书+七层负载均衡安装 Rancher</title>
      <link href="/rancher/install/single-node-install-authority-ssl-l7/"/>
      <url>/rancher/install/single-node-install-authority-ssl-l7/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl-l7/" target="_blank" title="https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl-l7/">https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl-l7/</a></p><p>权威 ssl 证书与默认自签名 ssl 证书或者自己生成的自签名证书有所不同，在权威 ssl 证书模式下，agent 通过 IP 访问无法通过证书的校验。因为 ssl 证书在生成的时候会与 IP 或者域名有一个绑定关系，而权威证书生成时候是基于申请证书时使用的域名，所以当用其他 IP 访问时候则无法验证通过。默认自签名 ssl 证书或者自己生成的自签名证书，在创建的时候有传递相应的 IP，所以自签名可以正常认证。</p><blockquote><p>要使用权威证书安装 rancher，需要使用申请 ssl 证书时用的域名来访问 rancher。</p></blockquote><h2 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker_data_dir=xxxx <span class="comment"># 定义绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/data <span class="comment"># rancher 数据目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog <span class="comment"># 审计日志目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl <span class="comment"># 自定义 CA 证书目录</span></span><br><span class="line"></span><br><span class="line">docker run -d --restart=unless-stopped \</span><br><span class="line">-p 10080:80 \ ①</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/data:/var/lib/rancher \ ②</span><br><span class="line"><span class="comment"># 审计日志配置 ③</span></span><br><span class="line">-e AUDIT_LEVEL=3 \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog:/var/log/auditlog \</span><br><span class="line"><span class="comment"># 以下两行为自定义 CA 根证书 ④</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/ssl/certs/ca-additional.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/rancher/ssl/ca-additional.pem \</span><br><span class="line">rancher/rancher:stable --no-cacerts</span><br></pre></td></tr></table></figure><blockquote><ol><li>如果要在安装 rancher 的主机上安装 K8S 集群，那么在 ① 处需要修改主机端口为非 80，比如：-p 10080:80 ;</li><li>为了保证数据持久保存，需要在 ② 处映射 rancher 数据到主机路径；</li><li>如果不需要开启升级日志，则设置 AUDIT_LEVEL&#x3D;0；</li><li>rancher 内置的应用商店，是利用 git 工具去获取 git 服务器 repo 中 chart。如果 git 服务器用的自签名 ssl 证书，那么需要给 rancher 配置自签名 CA 证书用以访问 git 服务器时做认证。所以需要在 ④ 处把自签名 CA 映射到 rancher 容器中；</li><li>第 ⑤ 处的证书文件建议保持一致。并且在 <strong>cert.pem</strong> 文件中需要包含中间链路证书，常见包含中间链路证书的 <strong>cert.pem</strong> 文件中，至少有两个以 —BEGIN CERTIFICATE— 开头并以 —END CERTIFICATE— 结尾。</li></ol></blockquote><h2 id="单节点离线安装"><a href="#单节点离线安装" class="headerlink" title="单节点离线安装"></a>单节点离线安装</h2><p>离线安装方法与在线安装方法基本相同，前提是已经将镜像同步到离线镜像仓库了，可以参考<a href="/rancher/rancher-install-offline-images">准备离线镜像</a>来同步镜像。除此之外，有以下几点注意：</p><ol><li><p>修改 <code>rancher/rancher:stable</code> 为离线镜像仓库中的镜像地址，比如：<code>192.168.100.100/rancher/rancher:stable</code></p></li><li><p>Rancher <code>v2.3.0</code> 之后的版本默认内置了 system-chart，比如：监控。如果你的环境中没有 git 服务器用来同步 GitHub 上的 system-chart，那么可以通过设置以下环境变量来使用 Rancher 内置的 system-chart。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-e CATTLE_SYSTEM_CATALOG=bundled \</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong>如果将来要切换为外部 Git 仓库的 system-chart，需要修改变量 <code>CATTLE_SYSTEM_CATALOG=external</code>，不能去掉这个变量。</p></blockquote></li><li><p>在 Rancher 运行起来后进入 Rancher UI，进入<strong>Settings</strong>视图，查找 <code>system-default-registry</code> 并点击 <strong>Edit</strong>。</p> <p> 将<strong>值</strong>改为您的私有仓库地址， 例如：<code>registry.yourdomain.com:port</code>， 不要添加 <code>http:// 或 https://</code> 前缀。</p> </li></ol><h2 id="外部负载均衡"><a href="#外部负载均衡" class="headerlink" title="外部负载均衡"></a>外部负载均衡</h2><p>有时候主机只有一个内网 IP 而这个 <strong>IP</strong> 我们又无法直接访问，对于这种场景，需要有一个既可以访问 <strong>rancher server</strong>，又可以被用户直接访问的工具来代理。比较常见的，比如 <strong>nginx</strong> 的反向代理，下文所指的负载均衡均以 <strong>nginx</strong> 为例。</p><h3 id="NGINX-七层代理参考配置"><a href="#NGINX-七层代理参考配置" class="headerlink" title="NGINX 七层代理参考配置"></a>NGINX 七层代理参考配置</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">worker_processes</span> <span class="number">4</span><span class="string">;</span></span><br><span class="line"><span class="string">worker_rlimit_nofile</span> <span class="number">40000</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line"><span class="string">events</span> &#123;</span><br><span class="line">    <span class="string">worker_connections</span> <span class="number">8192</span><span class="string">;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">http</span> &#123;</span><br><span class="line">    <span class="string">upstream</span> <span class="string">rancher</span> &#123;</span><br><span class="line">        <span class="string">server</span> <span class="string">IP_NODE_1:10080;</span> <span class="comment"># 根据实际 rancher 映射端口修改</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">map</span> <span class="string">$http_upgrade</span> <span class="string">$connection_upgrade</span> &#123;</span><br><span class="line">        <span class="string">default</span> <span class="string">Upgrade;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;</span>      <span class="string">close;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span> <span class="number">443</span> <span class="string">ssl</span> <span class="string">http2;</span> <span class="comment"># 如果是升级或者全新安装 v2.2.2,需要禁止 http2，其他版本不需修改。</span></span><br><span class="line">        <span class="string">server_name</span> <span class="string">&lt;与</span> <span class="string">ssl</span> <span class="string">证书匹配的域名</span> <span class="string">&gt;;</span> <span class="comment"># 修改此处的域名</span></span><br><span class="line">        <span class="string">ssl_certificate</span> <span class="string">&lt;更换证书&gt;;</span>    <span class="comment"># 设置 ssl 证书路径</span></span><br><span class="line">        <span class="string">ssl_certificate_key</span> <span class="string">&lt;更换证书私钥&gt;;</span> <span class="comment"># 设置 ssl 证书 key 路径</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/</span> &#123;</span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">Host</span> <span class="string">$host;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">X-Forwarded-Proto</span> <span class="string">$scheme;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">X-Forwarded-Port</span> <span class="string">$server_port;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">X-Forwarded-For</span> <span class="string">$proxy_add_x_forwarded_for;</span></span><br><span class="line">            <span class="string">proxy_pass</span> <span class="string">http://rancher;</span></span><br><span class="line">            <span class="string">proxy_http_version</span> <span class="number">1.1</span><span class="string">;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">Upgrade</span> <span class="string">$http_upgrade;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">Connection</span> <span class="string">$connection_upgrade;</span></span><br><span class="line">            <span class="comment"># This allows the ability for the execute shell window to remain open for up to 15 minutes.</span></span><br><span class="line">            <span class="comment">## Without this parameter, the default is 1 minute and will automatically close.</span></span><br><span class="line">            <span class="string">proxy_read_timeout</span> <span class="string">900s;</span></span><br><span class="line">            <span class="string">proxy_buffering</span> <span class="string">off;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span> <span class="number">80</span><span class="string">;</span></span><br><span class="line">        <span class="string">server_name</span> <span class="string">FQDN;</span></span><br><span class="line">        <span class="string">return</span> <span class="number">301</span> <span class="string">https://$server_name$request_uri;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
            <tag> 单节点安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点+权威证书安装 Rancher</title>
      <link href="/rancher/install/single-node-install-authority-ssl/"/>
      <url>/rancher/install/single-node-install-authority-ssl/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl/" target="_blank" title="https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl/">https://www.xtplayer.cn/rancher/install/single-node-install-authority-ssl/</a></p><p>权威 ssl 证书与默认自签名 ssl 证书或者自己生成的自签名证书有所不同，在权威 ssl 证书模式下，agent 通过 IP 访问无法通过证书的校验。因为 ssl 证书在生成的时候会与 IP 或者域名有一个绑定关系，而权威证书生成时候是基于申请证书时使用的域名，所以当用其他 IP 访问时候则无法验证通过。默认自签名 ssl 证书或者自己生成的自签名证书，在创建的时候有传递相应的 IP，所以自签名可以正常认证。</p><blockquote><p>要使用权威证书安装 rancher，需要使用申请 ssl 证书时用的域名来访问 rancher。</p></blockquote><h2 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker_data_dir=xxxx <span class="comment"># 定义绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/data <span class="comment"># rancher 数据目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog <span class="comment"># 审计日志目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl <span class="comment"># 自定义 CA 证书目录</span></span><br><span class="line"></span><br><span class="line">docker run -d --restart=unless-stopped \</span><br><span class="line">-p 80:80 -p 443:443 \ ①</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/data:/var/lib/rancher \ ②</span><br><span class="line"><span class="comment"># 审计日志配置 ③</span></span><br><span class="line">-e AUDIT_LEVEL=3 \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog:/var/log/auditlog \</span><br><span class="line"><span class="comment"># 以下两行为自定义 CA 根证书 ④</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/ssl/certs/ca-additional.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/rancher/ssl/ca-additional.pem \</span><br><span class="line"><span class="comment"># 以下两行为配置权威 ssl 证书 ⑤</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/tls.crt:/etc/rancher/ssl/cert.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/tls.key:/etc/rancher/ssl/key.pem \</span><br><span class="line">rancher/rancher:stable --no-cacerts</span><br></pre></td></tr></table></figure><blockquote><ol><li>如果要在安装 rancher 的主机上安装 K8S 集群，那么在 ① 处需要修改主机端口为非 80 和非 443，比如：-p 10080:80 -p 10443:443 ；</li><li>为了保证数据持久保存，需要在 ② 处映射 rancher 数据到主机路径；</li><li>如果不需要开启升级日志，则设置 AUDIT_LEVEL&#x3D;0；</li><li>rancher 内置的应用商店，是利用 git 工具去获取 git 服务器 repo 中 chart。如果 git 服务器用的自签名 ssl 证书，那么需要给 rancher 配置自签名 CA 证书用以访问 git 服务器时做认证。所以需要在 ④ 处把自签名 CA 映射到 rancher 容器中；</li><li>第 ⑤ 处的证书文件建议保持一致。并且在 <strong>cert.pem</strong> 文件中需要包含中间链路证书，常见包含中间链路证书的 <strong>cert.pem</strong> 文件中，至少有两个以 —BEGIN CERTIFICATE— 开头并以 —END CERTIFICATE— 结尾。</li></ol></blockquote><h2 id="单节点离线安装"><a href="#单节点离线安装" class="headerlink" title="单节点离线安装"></a>单节点离线安装</h2><p>离线安装方法与在线安装方法基本相同，前提是已经将镜像同步到离线镜像仓库了，可以参考<a href="/rancher/rancher-install-offline-images">准备离线镜像</a>来同步镜像。除此之外，有以下几点注意：</p><ol><li><p>修改 <code>rancher/rancher:stable</code> 为离线镜像仓库中的镜像地址，比如：<code>192.168.100.100/rancher/rancher:stable</code></p></li><li><p>Rancher <code>v2.3.0</code> 之后的版本默认内置了 system-chart，比如：监控。如果你的环境中没有 git 服务器用来同步 GitHub 上的 system-chart，那么可以通过设置以下环境变量来使用 Rancher 内置的 system-chart。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-e CATTLE_SYSTEM_CATALOG=bundled \</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong>如果将来要切换为外部 Git 仓库的 system-chart，需要修改变量 <code>CATTLE_SYSTEM_CATALOG=external</code>，不能去掉这个变量。</p></blockquote></li><li><p>在 Rancher 运行起来后进入 Rancher UI，进入<strong>Settings</strong>视图，查找 <code>system-default-registry</code> 并点击 <strong>Edit</strong>。</p> <p> 将<strong>值</strong>改为您的私有仓库地址， 例如：<code>registry.yourdomain.com:port</code>， 不要添加 <code>http:// 或 https://</code> 前缀。</p> </li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
            <tag> 单节点安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点+自定义自签证书+四层负载均衡安装 Rancher</title>
      <link href="/rancher/install/single-node-install-custom-ssl-l4/"/>
      <url>/rancher/install/single-node-install-custom-ssl-l4/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl-l4/" target="_blank" title="https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl-l4/">https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl-l4/</a></p><h2 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h2><p>Rancher 安装可以使用自己生成的自签名证书，如果没有自签名证书，可通过脚本<a href="/ssl/self-signed-ssl/">一键生成自签名 ssl 证书</a>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker_data_dir=xxxx <span class="comment"># 定义绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/data <span class="comment"># rancher 数据目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog <span class="comment"># 审计日志目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl <span class="comment"># 自定义 CA 证书目录</span></span><br><span class="line"></span><br><span class="line">docker run -d --restart=unless-stopped \</span><br><span class="line">-p 10080:80 -p 10443:443 \ ①</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/data:/var/lib/rancher \ ②</span><br><span class="line"><span class="comment"># 审计日志配置 ③</span></span><br><span class="line">-e AUDIT_LEVEL=3 \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog:/var/log/auditlog \</span><br><span class="line"><span class="comment"># 以下两行为自定义 CA 根证书 ④</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/ssl/certs/ca-additional.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/rancher/ssl/ca-additional.pem \</span><br><span class="line"><span class="comment"># 以下三行为配置自己的 ssl 证书 ⑤</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/tls.crt:/etc/rancher/ssl/cert.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/tls.key:/etc/rancher/ssl/key.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/cacerts.pem:/etc/rancher/ssl/cacerts.pem \</span><br><span class="line">rancher/rancher:stable</span><br></pre></td></tr></table></figure><blockquote><ol><li>如果要在安装 rancher 的主机上安装 K8S 集群，那么在 ① 处需要修改主机端口为非 80 和非 443，比如：-p 10080:80 -p 10443:443 ；</li><li>为了保证数据持久保存，需要在 ② 处映射 rancher 数据到主机路径；</li><li>如果不需要开启升级日志，则设置 AUDIT_LEVEL&#x3D;0；</li><li>rancher 内置的应用商店，是利用 git 工具去获取 git 服务器 repo 中 chart。如果 git 服务器用的自签名 ssl 证书，那么需要给 rancher 配置自签名 CA 证书用以访问 git 服务器时做认证。所以需要在 ④ 处把自签名 CA 映射到 rancher 容器中；</li><li>第 ⑤ 处的证书文件建议保持一致；</li></ol></blockquote><h2 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h2><p>离线安装方法与在线安装方法基本相同，根据 ssl 证书类型选择不同的安装方法，但是有个前提是已经将镜像同步到离线镜像仓库了，可以参考<a href="/rancher/rancher-install-offline-images">准备离线镜像</a>来同步镜像。除此之外，有以下几点注意：</p><ol><li><p>修改 <code>rancher/rancher:stable</code> 为离线镜像仓库中的镜像地址，比如：<code>192.168.100.100/rancher/rancher:stable</code></p></li><li><p>Rancher <code>v2.3.0</code> 之后的版本默认内置了 system-chart，比如：监控。如果你的环境中没有 git 服务器用来同步 GitHub 上的 system-chart，那么可以通过设置以下环境变量来使用 Rancher 内置的 system-chart。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-e CATTLE_SYSTEM_CATALOG=bundled \</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong>如果将来要切换为外部 Git 仓库的 system-chart，需要修改变量 <code>CATTLE_SYSTEM_CATALOG=external</code>，不能去掉这个变量。</p></blockquote></li><li><p>在 Rancher 运行起来后进入 Rancher UI，进入<strong>Settings</strong>视图，查找 <code>system-default-registry</code> 并点击 <strong>Edit</strong>。</p> <p> 将<strong>值</strong>改为您的私有仓库地址， 例如：<code>registry.yourdomain.com:port</code>， 不要添加 <code>http:// 或 https://</code> 前缀。</p> </li></ol><h2 id="外部负载均衡"><a href="#外部负载均衡" class="headerlink" title="外部负载均衡"></a>外部负载均衡</h2><p>有时候主机只有一个内网 IP 而这个 <strong>IP</strong> 我们又无法直接访问，对于这种场景，需要有一个既可以访问 <strong>rancher server</strong>，又可以被用户直接访问的工具来代理。比较常见的，比如 <strong>nginx</strong> 的反向代理，下文所指的负载均衡均以 <strong>nginx</strong> 为例。</p><h3 id="NGINX-四层代理参考配置"><a href="#NGINX-四层代理参考配置" class="headerlink" title="NGINX 四层代理参考配置"></a>NGINX 四层代理参考配置</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">worker_processes</span> <span class="number">4</span><span class="string">;</span></span><br><span class="line"><span class="string">worker_rlimit_nofile</span> <span class="number">40000</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line"><span class="string">events</span> &#123;</span><br><span class="line">    <span class="string">worker_connections</span> <span class="number">8192</span><span class="string">;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">stream</span> &#123;</span><br><span class="line">    <span class="string">upstream</span> <span class="string">rancher_servers_http</span> &#123;</span><br><span class="line">        <span class="string">least_conn;</span></span><br><span class="line">        <span class="string">server</span> <span class="string">&lt;IP_NODE_1&gt;:10080</span> <span class="string">max_fails=3</span> <span class="string">fail_timeout=5s;</span> <span class="comment"># 根据实际 rancher 映射端口修改</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span>     <span class="number">80</span><span class="string">;</span></span><br><span class="line">        <span class="string">proxy_pass</span> <span class="string">rancher_servers_http;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">upstream</span> <span class="string">rancher_servers_https</span> &#123;</span><br><span class="line">        <span class="string">least_conn;</span></span><br><span class="line">        <span class="string">server</span> <span class="string">&lt;IP_NODE_1&gt;:10443</span> <span class="string">max_fails=3</span> <span class="string">fail_timeout=5s;</span> <span class="comment"># 根据实际 rancher 映射端口修改</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span>     <span class="number">443</span><span class="string">;</span></span><br><span class="line">        <span class="string">proxy_pass</span> <span class="string">rancher_servers_https;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
            <tag> 单节点安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点+自定义自签证书+七层负载均衡安装 Rancher</title>
      <link href="/rancher/install/single-node-install-custom-ssl-l7/"/>
      <url>/rancher/install/single-node-install-custom-ssl-l7/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl-l7/" target="_blank" title="https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl-l7/">https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl-l7/</a></p><h2 id="单节点在线安装"><a href="#单节点在线安装" class="headerlink" title="单节点在线安装"></a>单节点在线安装</h2><p>Rancher 安装可以使用自己生成的自签名证书，如果没有自签名证书，可通过脚本<a href="/ssl/self-signed-ssl/">一键生成自签名 ssl 证书</a>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker_data_dir=xxxx <span class="comment"># 定义绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/data <span class="comment"># rancher 数据目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog <span class="comment"># 审计日志目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl <span class="comment"># 自定义 CA 证书目录</span></span><br><span class="line"></span><br><span class="line">docker run -d --restart=unless-stopped \</span><br><span class="line">-p 10080:80 \ ①</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/data:/var/lib/rancher \ ②</span><br><span class="line"><span class="comment"># 审计日志配置 ③</span></span><br><span class="line">-e AUDIT_LEVEL=3 \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog:/var/log/auditlog \</span><br><span class="line"><span class="comment"># 以下两行为自定义 CA 根证书 ④</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/ssl/certs/ca-additional.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/rancher/ssl/ca-additional.pem \</span><br><span class="line"><span class="comment"># 自签名 SSL CA 证书 ⑤</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/cacerts.pem:/etc/rancher/ssl/cacerts.pem \</span><br><span class="line">rancher/rancher:stable</span><br></pre></td></tr></table></figure><blockquote><ol><li>如果要在安装 rancher 的主机上安装 K8S 集群，那么在 ① 处需要修改主机端口为非 80，比如：-p 10080:80；</li><li>为了保证数据持久保存，需要在 ② 处映射 rancher 数据到主机路径；</li><li>如果不需要开启升级日志，则设置 AUDIT_LEVEL&#x3D;0；</li><li>rancher 内置的应用商店，是利用 git 工具去获取 git 服务器 repo 中 chart。如果 git 服务器用的自签名 ssl 证书，那么需要给 rancher 配置自签名 CA 证书用以访问 git 服务器时做认证。所以需要在 ④ 处把自签名 CA 映射到 rancher 容器中；</li><li>第 ⑤ 处的需要将自签名 SSL 证书的 CA 证书 配置到 rancher，用于 ssl 证书的校验；</li></ol></blockquote><h2 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h2><p>离线安装方法与在线安装方法基本相同，前提是已经将镜像同步到离线镜像仓库了，可以参考<a href="/rancher/rancher-install-offline-images">准备离线镜像</a>来同步镜像。除此之外，有以下几点注意：</p><ol><li><p>修改 <code>rancher/rancher:stable</code> 为离线镜像仓库中的镜像地址，比如：<code>192.168.100.100/rancher/rancher:stable</code></p></li><li><p>Rancher <code>v2.3.0</code> 之后的版本默认内置了 system-chart，比如：监控。如果你的环境中没有 git 服务器用来同步 GitHub 上的 system-chart，那么可以通过设置以下环境变量来使用 Rancher 内置的 system-chart。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-e CATTLE_SYSTEM_CATALOG=bundled \</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong>如果将来要切换为外部 Git 仓库的 system-chart，需要修改变量 <code>CATTLE_SYSTEM_CATALOG=external</code>，不能去掉这个变量。</p></blockquote></li><li><p>在 Rancher 运行起来后进入 Rancher UI，进入<strong>Settings</strong>视图，查找 <code>system-default-registry</code> 并点击 <strong>Edit</strong>。</p> <p> 将<strong>值</strong>改为您的私有仓库地址， 例如：<code>registry.yourdomain.com:port</code>， 不要添加 <code>http:// 或 https://</code> 前缀。</p> </li></ol><h2 id="外部负载均衡"><a href="#外部负载均衡" class="headerlink" title="外部负载均衡"></a>外部负载均衡</h2><p>有时候主机只有一个内网 IP 而这个 <strong>IP</strong> 我们又无法直接访问，对于这种场景，需要有一个既可以访问 <strong>rancher server</strong>，又可以被用户直接访问的工具来代理。比较常见的，比如 <strong>nginx</strong> 的反向代理，下文所指的负载均衡均以 <strong>nginx</strong> 为例。</p><h3 id="NGINX-七层代理参考配置"><a href="#NGINX-七层代理参考配置" class="headerlink" title="NGINX 七层代理参考配置"></a>NGINX 七层代理参考配置</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">worker_processes</span> <span class="number">4</span><span class="string">;</span></span><br><span class="line"><span class="string">worker_rlimit_nofile</span> <span class="number">40000</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line"><span class="string">events</span> &#123;</span><br><span class="line">    <span class="string">worker_connections</span> <span class="number">8192</span><span class="string">;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">http</span> &#123;</span><br><span class="line">    <span class="string">upstream</span> <span class="string">rancher</span> &#123;</span><br><span class="line">        <span class="string">server</span> <span class="string">IP_NODE_1:10080;</span> <span class="comment"># 根据实际 rancher 映射端口修改</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">map</span> <span class="string">$http_upgrade</span> <span class="string">$connection_upgrade</span> &#123;</span><br><span class="line">        <span class="string">default</span> <span class="string">Upgrade;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;</span>      <span class="string">close;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span> <span class="number">443</span> <span class="string">ssl</span> <span class="string">http2;</span> <span class="comment"># 如果是升级或者全新安装 v2.2.2,需要禁止 http2，其他版本不需修改。</span></span><br><span class="line">        <span class="string">server_name</span> <span class="string">FQDN;</span></span><br><span class="line">        <span class="string">ssl_certificate</span> <span class="string">&lt;更换证书&gt;;</span></span><br><span class="line">        <span class="string">ssl_certificate_key</span> <span class="string">&lt;更换证书私钥&gt;;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/</span> &#123;</span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">Host</span> <span class="string">$host;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">X-Forwarded-Proto</span> <span class="string">$scheme;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">X-Forwarded-Port</span> <span class="string">$server_port;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">X-Forwarded-For</span> <span class="string">$proxy_add_x_forwarded_for;</span></span><br><span class="line">            <span class="string">proxy_pass</span> <span class="string">http://rancher;</span></span><br><span class="line">            <span class="string">proxy_http_version</span> <span class="number">1.1</span><span class="string">;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">Upgrade</span> <span class="string">$http_upgrade;</span></span><br><span class="line">            <span class="string">proxy_set_header</span> <span class="string">Connection</span> <span class="string">$connection_upgrade;</span></span><br><span class="line">            <span class="comment"># This allows the ability for the execute shell window to remain open for up to 15 minutes.</span></span><br><span class="line">            <span class="comment">## Without this parameter, the default is 1 minute and will automatically close.</span></span><br><span class="line">            <span class="string">proxy_read_timeout</span> <span class="string">900s;</span></span><br><span class="line">            <span class="string">proxy_buffering</span> <span class="string">off;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span> <span class="number">80</span><span class="string">;</span></span><br><span class="line">        <span class="string">server_name</span> <span class="string">FQDN;</span></span><br><span class="line">        <span class="string">return</span> <span class="number">301</span> <span class="string">https://$server_name$request_uri;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
            <tag> 单节点安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点+自定义自签证书安装 Rancher</title>
      <link href="/rancher/install/single-node-install-custom-ssl/"/>
      <url>/rancher/install/single-node-install-custom-ssl/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl/" target="_blank" title="https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl/">https://www.xtplayer.cn/rancher/install/single-node-install-custom-ssl/</a></p><h2 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h2><p>Rancher 安装也可以使用自己生成的自签名证书，如果没有自签名证书，可通过脚本<a href="/ssl/self-signed-ssl/">一键生成自签名 ssl 证书</a>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker_data_dir=xxxx <span class="comment"># 定义绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/data <span class="comment"># rancher 数据目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog <span class="comment"># 审计日志目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl <span class="comment"># 自定义 CA 证书目录</span></span><br><span class="line"></span><br><span class="line">docker run -d --restart=unless-stopped \</span><br><span class="line">-p 80:80 -p 443:443 \ ①</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/data:/var/lib/rancher \ ②</span><br><span class="line"><span class="comment"># 审计日志配置 ③</span></span><br><span class="line">-e AUDIT_LEVEL=3 \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog:/var/log/auditlog \</span><br><span class="line"><span class="comment"># 以下两行为自定义 CA 根证书 ④</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/ssl/certs/ca-additional.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/rancher/ssl/ca-additional.pem \</span><br><span class="line"><span class="comment"># 以下三行为配置自己的 ssl 证书 ⑤</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/tls.crt:/etc/rancher/ssl/cert.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/tls.key:/etc/rancher/ssl/key.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/cacerts.pem:/etc/rancher/ssl/cacerts.pem \</span><br><span class="line">rancher/rancher:stable</span><br></pre></td></tr></table></figure><blockquote><ol><li>如果要在安装 rancher 的主机上安装 K8S 集群，那么在 ① 处需要修改主机端口为非 80 和黑 443，比如：-p 10080:80 -p 10443:443 ；</li><li>为了保证数据持久保存，需要在 ② 处映射 rancher 数据到主机路径；</li><li>如果不需要开启升级日志，则设置 AUDIT_LEVEL&#x3D;0；</li><li>rancher 内置的应用商店，是利用 git 工具去获取 git 服务器 repo 中 chart。如果 git 服务器用的自签名 ssl 证书，那么需要给 rancher 配置自签名 CA 证书用以访问 git 服务器时做认证。所以需要在 ④ 处把自签名 CA 映射到 rancher 容器中；</li><li>第 ⑤ 处的证书文件建议保持一致；</li></ol></blockquote><h2 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h2><p>离线安装方法与在线安装方法基本相同，前提是已经将镜像同步到离线镜像仓库了，可以参考<a href="/rancher/rancher-install-offline-images">准备离线镜像</a>来同步镜像。除此之外，有以下几点注意：</p><ol><li><p>修改 <code>rancher/rancher:stable</code> 为离线镜像仓库中的镜像地址，比如：<code>192.168.100.100/rancher/rancher:stable</code></p></li><li><p>Rancher <code>v2.3.0</code> 之后的版本默认内置了 system-chart，比如：监控。如果你的环境中没有 git 服务器用来同步 GitHub 上的 system-chart，那么可以通过设置以下环境变量来使用 Rancher 内置的 system-chart。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-e CATTLE_SYSTEM_CATALOG=bundled \</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong>如果将来要切换为外部 Git 仓库的 system-chart，需要修改变量 <code>CATTLE_SYSTEM_CATALOG=external</code>，不能去掉这个变量。</p></blockquote></li><li><p>在 Rancher 运行起来后进入 Rancher UI，进入<strong>Settings</strong>视图，查找 <code>system-default-registry</code> 并点击 <strong>Edit</strong>。</p> <p> 将<strong>值</strong>改为您的私有仓库地址， 例如：<code>registry.yourdomain.com:port</code>， 不要添加 <code>http:// 或 https://</code> 前缀。</p> </li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
            <tag> 单节点安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点+默认自签证书+四层负载均衡安装 Rancher</title>
      <link href="/rancher/install/single-node-install-default-ssl-l4/"/>
      <url>/rancher/install/single-node-install-default-ssl-l4/</url>
      
        <content type="html"><![CDATA[<p>默认情况下，Rancher 会自动生成一个用于加密的自签名证书。可以直接通过运行 Docker 命令来安装 Rancher，而不需要任何其他参数。</p><h2 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker_data_dir=xxxx <span class="comment"># 定义绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/data <span class="comment"># rancher 数据目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog <span class="comment"># 审计日志目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl <span class="comment"># 自定义 CA 证书目录</span></span><br><span class="line"></span><br><span class="line">docker run -d --restart=unless-stopped \</span><br><span class="line">-p 10080:80 -p 10443:443 \ ①</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/data:/var/lib/rancher \ ②</span><br><span class="line"><span class="comment"># 审计日志配置 ③</span></span><br><span class="line">-e AUDIT_LEVEL=3 \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog:/var/log/auditlog \</span><br><span class="line"><span class="comment"># 以下两行配置自定义 CA 根证书 ④</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/ssl/certs/ca-additional.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/rancher/ssl/ca-additional.pem \</span><br><span class="line">rancher/rancher:stable</span><br></pre></td></tr></table></figure><blockquote><ol><li>如果要在安装 rancher 的主机上安装 K8S 集群，那么在 ① 处需要修改主机端口为非 80 和黑 443，比如：-p 10080:80 -p 10443:443 ；</li><li>为了保证数据持久保存，需要在 ② 处映射 rancher 数据到主机路径；</li><li>如果不需要开启升级日志，则设置 AUDIT_LEVEL&#x3D;0；</li><li>rancher 内置的应用商店，是利用 git 工具去获取 git 服务器 repo 中 chart。如果 git 服务器用的自签名 ssl 证书，那么需要给 rancher 配置自签名 CA 证书用以访问 git 服务器时做认证。所以需要在 ④ 处把自签名 CA 映射到 rancher 容器中；</li></ol></blockquote><h2 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h2><p>离线安装方法与在线安装方法基本相同，前提是已经将镜像同步到离线镜像仓库了，可以参考<a href="/rancher/rancher-install-offline-images">准备离线镜像</a>来同步镜像。除此之外，有以下几点注意：</p><ol><li><p>修改 <code>rancher/rancher:stable</code> 为离线镜像仓库中的镜像地址，比如：<code>192.168.100.100/rancher/rancher:stable</code></p></li><li><p>Rancher <code>v2.3.0</code> 之后的版本默认内置了 system-chart，比如：监控。如果你的环境中没有 git 服务器用来同步 GitHub 上的 system-chart，那么可以通过设置以下环境变量来使用 Rancher 内置的 system-chart。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-e CATTLE_SYSTEM_CATALOG=bundled \</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong>如果将来要切换为外部 Git 仓库的 system-chart，需要修改变量 <code>CATTLE_SYSTEM_CATALOG=external</code>，不能去掉这个变量。</p></blockquote></li><li><p>在 Rancher 运行起来后进入 Rancher UI，进入 <strong>Settings</strong> 视图，查找 <code>system-default-registry</code> 并点击 <strong>Edit</strong>。</p> <p> 将<strong>值</strong>改为您的私有仓库地址， 例如：<code>registry.yourdomain.com:port</code>， 不要添加 <code>http:// 或 https://</code> 前缀。</p> </li></ol><h2 id="外部负载均衡"><a href="#外部负载均衡" class="headerlink" title="外部负载均衡"></a>外部负载均衡</h2><p>有时候主机只有一个内网 IP 而这个 <strong>IP</strong> 我们又无法直接访问，对于这种场景，需要有一个既可以访问 <strong>rancher server</strong>，又可以被用户直接访问的工具来代理。比较常见的，比如 <strong>nginx</strong> 的反向代理，下文所指的负载均衡均以 <strong>nginx</strong> 为例。</p><h3 id="NGINX-四层代理参考配置"><a href="#NGINX-四层代理参考配置" class="headerlink" title="NGINX 四层代理参考配置"></a>NGINX 四层代理参考配置</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">worker_processes</span> <span class="number">4</span><span class="string">;</span></span><br><span class="line"><span class="string">worker_rlimit_nofile</span> <span class="number">40000</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line"><span class="string">events</span> &#123;</span><br><span class="line">    <span class="string">worker_connections</span> <span class="number">8192</span><span class="string">;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">stream</span> &#123;</span><br><span class="line">    <span class="string">upstream</span> <span class="string">rancher_servers_http</span> &#123;</span><br><span class="line">        <span class="string">least_conn;</span></span><br><span class="line">        <span class="string">server</span> <span class="string">&lt;IP_NODE_1&gt;:10080</span> <span class="string">max_fails=3</span> <span class="string">fail_timeout=5s;</span> <span class="comment"># 根据实际 rancher 映射端口修改</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span>     <span class="number">80</span><span class="string">;</span></span><br><span class="line">        <span class="string">proxy_pass</span> <span class="string">rancher_servers_http;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">upstream</span> <span class="string">rancher_servers_https</span> &#123;</span><br><span class="line">        <span class="string">least_conn;</span></span><br><span class="line">        <span class="string">server</span> <span class="string">&lt;IP_NODE_1&gt;:10443</span> <span class="string">max_fails=3</span> <span class="string">fail_timeout=5s;</span> <span class="comment"># 根据实际 rancher 映射端口修改</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">listen</span>     <span class="number">443</span><span class="string">;</span></span><br><span class="line">        <span class="string">proxy_pass</span> <span class="string">rancher_servers_https;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
            <tag> 单节点安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点+默认自签证书安装 Rancher</title>
      <link href="/rancher/install/single-node-install-default-ssl/"/>
      <url>/rancher/install/single-node-install-default-ssl/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/install/single-node-install-default-ssl/" target="_blank" title="https://www.xtplayer.cn/rancher/install/single-node-install-default-ssl/">https://www.xtplayer.cn/rancher/install/single-node-install-default-ssl/</a></p><p>默认情况下，Rancher 会自动生成一个用于加密的自签名证书。直接通过运行 Docker 命令来安装 Rancher，而不需要任何其他参数。</p><h2 id="单节点在线安装"><a href="#单节点在线安装" class="headerlink" title="单节点在线安装"></a>单节点在线安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker_data_dir=xxxx <span class="comment"># 定义绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/data <span class="comment"># rancher 数据目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog <span class="comment"># 审计日志目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl <span class="comment"># 自定义 CA 证书目录</span></span><br><span class="line"></span><br><span class="line">docker run -d --restart=unless-stopped \</span><br><span class="line">-p 80:80 -p 443:443 \ ①</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/data:/var/lib/rancher \ ②</span><br><span class="line"><span class="comment"># 审计日志配置 ③</span></span><br><span class="line">-e AUDIT_LEVEL=3 \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/auditlog:/var/log/auditlog \</span><br><span class="line"><span class="comment"># 以下两行为自定义 CA 根证书 ④</span></span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/ssl/certs/ca-additional.pem \</span><br><span class="line">-v <span class="variable">$&#123;docker_data_dir&#125;</span>/ssl/ca-additional.pem:/etc/rancher/ssl/ca-additional.pem \</span><br><span class="line">rancher/rancher:stable</span><br></pre></td></tr></table></figure><blockquote><ol><li>如果要在安装 rancher 的主机上安装 K8S 集群，那么在 ① 处需要修改主机端口为非 80 和黑 443，比如：-p 10080:80 -p 10443:443 ；</li><li>为了保证数据持久保存，需要在 ② 处映射 rancher 数据到主机路径；</li><li>如果不需要开启升级日志，则设置 AUDIT_LEVEL&#x3D;0；</li><li>rancher 内置的应用商店，是利用 git 工具去获取 git 服务器 repo 中 chart。如果 git 服务器用的自签名 ssl 证书，那么需要给 rancher 配置自签名 CA 证书用以访问 git 服务器时做认证。所以需要在 ④ 处把自签名 CA 映射到 rancher 容器中；</li></ol></blockquote><h2 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h2><p>离线安装方法与在线安装方法基本相同，前提是已经将镜像同步到离线镜像仓库了，可以参考<a href="/rancher/rancher-install-offline-images">准备离线镜像</a>来同步镜像。除此之外，有以下几点注意：</p><ol><li><p>修改 <code>rancher/rancher:stable</code> 为离线镜像仓库中的镜像地址，比如：<code>192.168.100.100/rancher/rancher:stable</code></p></li><li><p>Rancher <code>v2.3.0</code> 之后的版本默认内置了 system-chart，比如：监控。如果你的环境中没有 git 服务器用来同步 GitHub 上的 system-chart，那么可以通过设置以下环境变量来使用 Rancher 内置的 system-chart。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-e CATTLE_SYSTEM_CATALOG=bundled \</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong>如果将来要切换为外部 Git 仓库的 system-chart，需要修改变量 <code>CATTLE_SYSTEM_CATALOG=external</code>，不能去掉这个变量。</p></blockquote></li><li><p>在 Rancher 运行起来后进入 Rancher UI，进入<strong>Settings</strong>视图，查找 <code>system-default-registry</code> 并点击 <strong>Edit</strong>。</p> <img src="/rancher/install/single-node-install-default-ssl/edit-system-default-registry.png" class="" title="Edit"><p> 将<strong>值</strong>改为您的私有仓库地址， 例如：<code>registry.yourdomain.com:port</code>， 不要添加 <code>http:// 或 https://</code> 前缀。</p> <img src="/rancher/install/single-node-install-default-ssl/enter-system-default-registry.png" class="" title="Save"></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> install </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> install </tag>
            
            <tag> 单节点安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图形化的方式了解 Kubernetes</title>
      <link href="/kubernetes/learn-about-kubernetes-graphically/"/>
      <url>/kubernetes/learn-about-kubernetes-graphically/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/learn-about-kubernetes-graphically/" target="_blank" title="https://www.xtplayer.cn/kubernetes/learn-about-kubernetes-graphically/">https://www.xtplayer.cn/kubernetes/learn-about-kubernetes-graphically/</a></p><h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><p> 在我们尝试了解 Kubernetes 之前，让我们花一点时间来澄清<strong>容器</strong>是什么，以及它们为什么如此受欢迎。毕竟，在不知道容器是什么的情况下谈论容器编排器（Kubernetes）是没有意义的。</p><p>“容器” 是一个用来存放你放入的所有物品的容器。</p><p>像应用程序代码，依赖库以及它的依赖关系一直到内核。这里的关键概念是隔离。将所有内容与其余内容隔离开，以便你更好地控制它们。容器提供三种隔离类型：</p><ul><li>工作区隔离（流程、网络）</li><li>资源隔离（CPU、内存）</li><li>文件系统隔离（联合文件系统）</li></ul><p>考虑一下像 VM 一样的容器。它们精简，快速（启动）且体积小。而且，所有这些都没有构建起来。取而代之的是，他们使用 Linux 系统中存在的结构（例如：cgroups、namespaces）在其上构建了一个不错的抽象。</p><p>现在我们知道什么是容器了，很容易理解为什么它们很受欢迎。不仅可以分发应用程序的二进制&#x2F;代码，还可以以实用的方式交付运行应用程序所需的整个环境，因为可以将容器构建为非常小的单元。解决“在我的机器上工作”问题的完美解决方案。</p><h2 id="什么时候使用-Kubernetes"><a href="#什么时候使用-Kubernetes" class="headerlink" title="什么时候使用 Kubernetes"></a>什么时候使用 Kubernetes</h2><p>容器一切都很好，软件开发人员的生活现在要好很多。那么，为什么我们需要另一项技术，如 Kubernetes 这样的容器编排工具呢？</p><p>当进入某个状态时，你需要用到它来管理众多容器。</p><p>问：我的前端容器在哪里，我要运行几个？</p><p>答：很难说，使用容器编排工具。</p><p>问：如何使前端容器与新创建的后端容器对话？</p><p>答：对 IP 进行硬编码，或者，使用容器编排工具。</p><p>问：如何进行滚动升级？</p><p>答：在每个步骤中手动握住，或者，使用容器编排工具。</p><h2 id="为什么我更喜欢-Kubernetes"><a href="#为什么我更喜欢-Kubernetes" class="headerlink" title="为什么我更喜欢 Kubernetes"></a>为什么我更喜欢 Kubernetes</h2><p>有很多容器编排工具，例如 Docker Swarm，Mesos 和 Kubernetes。我的选择是 Kubernetes（因此有了本文），因为 Kubernetes 是…</p><p>就像乐高积木一样，它不仅具有大规模运行容器编排所需的组件，而且还具有使用自定义组件交换内部和外部交换不同组件的灵活性。想要拥有一个自定义的调度程序，也很方便。需要具有新的资源类型，编写一个 CRD。此外，社区非常活跃，并且工具迅速发展。</p><h2 id="Kubernetes-架构"><a href="#Kubernetes-架构" class="headerlink" title="Kubernetes 架构"></a>Kubernetes 架构</h2><p>每个 Kubernetes 集群都有两种类型的节点，主节点和工作节点。顾名思义，主节点是在工作程序运行有效负载（应用程序）的地方控制和监视群集。</p><p>集群可以与单个主节点一起工作，但是最好拥有三个以实现高可用性（称为 HA 群集）。</p><p>让我们仔细看一下主节点及其组成。</p><ul><li><p>etcd</p><p>数据库，用于存储有关 kubernetes 对象，其当前状态，访问信息和其他集群配置信息的所有数据。</p></li><li><p>API Server</p><p>RESTful API 服务器，公开端点以操作整个集群。主节点和工作节点中的几乎所有组件都与该服务器通信以执行其职责。</p></li><li><p>调度程序</p><p>负责决定哪个有效负载需要在哪台机器上运行。</p></li><li><p>控制管理器</p><p>这是一个控制循环，它监视集群的状态（通过调用 API 服务器来获取此数据）并采取措施将其置于预期状态。</p></li></ul><ul><li><p>kubelet</p><p>是工作节点的心脏。它与主节点 API 服务器通信并运行为其节点安排的容器。</p></li><li><p>kube-proxy</p><p>使用 IP 表&#x2F;IPVS 处理 Pod 的网络需求。</p></li><li><p>Pod</p><p>运行所有容器的 Kubernetes 的功劳。如果没有 Pod 的抽象，就无法在 kubernetes 中运行容器。Pod 添加了对容器之间的 kuberenetes 联网方式至关重要的功能。</p></li></ul><p>一个 Pod 可以有多个容器，并且在这些容器中运行的所有服务器都可以将彼此视为本地主机。这使得将应用程序的不同方面分离为单独的容器，并将它们全部作为一个容器加载在一起非常方便。有多种不同的 Pod 模式，例如 sidecar，proxy 和大使，可以满足不同的需求。</p><p>Pod 网络接口提供了一种将其与同一节点和其他工作节点中的其他 Pod 通信的机制。</p><p>而且，每个 Pod 都将分配有自己的 IP 地址，kube-proxy 将使用该 IP 地址来路由流量，而且此 IP 地址仅在群集中可见。</p><p>所有容器也都可以看到安装在容器内的卷，有时可以使用这些卷在容器之间进行异步通信。例如，假设你的应用是照片上传应用（例如 instagram），它可以将这些文件保存在一个卷中，而同一 Pod 中的另一个容器可以监视该卷中的新文件，并开始对其进行处理以创建多种尺寸，将它们上传到云存储。</p><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><p>在 Kubernetes 中，有很多 controller，例如 ReplicaSet、Replication Controllers、Deployments、StatefulSets 和 Service。这些是以一种或另一种方式控制 Pod 的对象。让我们看一些比较重要的 controller。</p><h4 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h4><p>ReplicaSet 做自己擅长的事情：复制 Pod</p><p>该 controller 的主要职责是创建给定 Pod 的副本，如果 Pod 因某种原因死亡，则会通知该 controller，并立即跳入操作以创建新的 Pod。</p><h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><p>Deployment 是一个高阶对象，它使用 ReplicaSet 来管理副本。它通过放大新的 ReplicaSet 和缩小（最终删除）现有的 ReplicaSet 来提供滚动升级。</p><h4 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h4><p>表示为无人机的服务，将数据包传递到相应的 Pod</p><p>服务是一个控制器对象，其主要职责是在将“数据包”分发到相应节点时充当负载平衡器。基本上，它是一种控制器构造，用于在工作节点之间对相似的 Pod（通常由 Pod 标签标识）进行分组。</p><p>假设你的“前端”应用程序想与“后端”应用程序通信，则每个应用程序可能有许多正在运行的实例。你不必担心对每个后端 Pod 的 IP 进行硬编码，而是将数据包发送到后端服务，然后由后端服务决定如何进行负载平衡并相应地转发。</p><p>PS：请注意，服务更像是一个虚拟实体，因为所有数据包路由均由 IP 表&#x2F;IPVS&#x2F;CNI 插件处理。它只是使它更容易被视为一个真正的实体，让它们脱颖而出以了解其在 Kubernetes 生态系统中的作用。</p><h4 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h4><p>进入一个浮动平台，所有数据包都通过该平台流入集群</p><p>Ingress controller 是与外界联系的单点，可以与集群中运行的所有服务进行对话。这使我们可以轻松地在单个位置设置安全策略，监控甚至记录日志。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>使用 kind 快速创建本地集群</title>
      <link href="/kubernetes/use-kind/"/>
      <url>/kubernetes/use-kind/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/use-kind/" target="_blank" title="https://www.xtplayer.cn/kubernetes/use-kind/">https://www.xtplayer.cn/kubernetes/use-kind/</a></p><h2 id="简-介"><a href="#简-介" class="headerlink" title="简 介"></a>简 介</h2><p>kind 是另一个 Kubernetes SIG 项目，但它与 minikube 有很大区别。它可以将集群迁移到 Docker 容器中，这与生成虚拟机相比，启动速度大大加快。简而言之，kind 是一个使用 Docker 容器节点运行本地 Kubernetes 集群的工具（CLI）。</p><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><p>想要顺利完成本教程，你需要在本地系统中准备好以下程序：</p><ul><li>Go</li><li>需要运行的 Docker 服务</li></ul><h2 id="安-装"><a href="#安-装" class="headerlink" title="安 装"></a>安 装</h2><ol><li>使用以下命令下载和安装 kind 二进制文件：</li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">GO111MODULE=”on” go get sigs.k8s.io/kind@v0.8.1</span><br></pre></td></tr></table></figure><ol><li>确保 kind 二进制文件是存在</li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kind version</span><br><span class="line"></span><br><span class="line">kind v0.8.1 go1.14.2 darwin/amd64</span><br></pre></td></tr></table></figure><p>现在，我们应该能够使用 kind CLI 来启动一个 Kubernetes 集群：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Usage:</span><br><span class="line">  kind [<span class="built_in">command</span>]Available Commands:</span><br><span class="line">  build       Build one of [node-image]</span><br><span class="line">  completion  Output shell completion code <span class="keyword">for</span> the specified shell</span><br><span class="line">  create      Creates one of [cluster]</span><br><span class="line">  delete      Deletes one of [cluster]</span><br><span class="line">  <span class="built_in">export</span>      Exports one of [kubeconfig, logs]</span><br><span class="line">  get         Gets one of [clusters, nodes, kubeconfig]</span><br><span class="line">  <span class="built_in">help</span>        Help about any <span class="built_in">command</span></span><br><span class="line">  load        Loads images into nodes</span><br><span class="line">  version     Prints the kind CLI version</span><br></pre></td></tr></table></figure><p>在本文中，我们主要说明 create、get 和 delete 命令。</p><h2 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h2><h3 id="创建默认版本-K8S-集群"><a href="#创建默认版本-K8S-集群" class="headerlink" title="创建默认版本 K8S 集群"></a>创建默认版本 K8S 集群</h3><p>执行以下命令即可创建一个集群：</p><p><code>kind create cluster</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kind create cluster</span><br><span class="line">Creating cluster <span class="string">&quot;kind&quot;</span> ...</span><br><span class="line"> ✓ Ensuring node image (kindest/node:v1.18.2)</span><br><span class="line"> ✓ Preparing nodes</span><br><span class="line"> ✓ Writing configuration</span><br><span class="line"> ✓ Starting control-plane</span><br><span class="line"> ✓ Installing CNI</span><br><span class="line"> ✓ Installing StorageClass</span><br><span class="line">Set kubectl context to <span class="string">&quot;kind-kind&quot;</span></span><br><span class="line">You can now use your cluster with:kubectl cluster-info --context kind-kind Have a <span class="built_in">nice</span> day!</span><br></pre></td></tr></table></figure><p>将通过拉取最新的 Kubernetes 节点（v 1.18.2）来创建一个 Kubernetes 集群。刚刚我们已经创建了一个 v 1.18.2 的 Kubernetes 集群。在创建集群的过程中如果我们没有 <code>--name</code> 参数，那么集群名称将会默认设置为 <strong>kind</strong>。</p><h3 id="创建指定版本-K8S-集群"><a href="#创建指定版本-K8S-集群" class="headerlink" title="创建指定版本 K8S 集群"></a>创建指定版本 K8S 集群</h3><p>我们可以通过传递 <code>--image</code> 参数来部署一个特定版本的 Kubernetes 集群。</p><p>使用的命令为：</p><p><code>kind create cluster --image kindest/node:v1.15.6</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kind create cluster --image kindest/node:v1.15.6 --name kind-1.15.6</span><br><span class="line">Creating cluster <span class="string">&quot;kind&quot;</span> ...</span><br><span class="line"> ✓ Ensuring node image (kindest/node:v1.15.6)</span><br><span class="line"> ✓ Preparing nodes</span><br><span class="line"> ✓ Writing configuration</span><br><span class="line"> ✓ Starting control-plane</span><br><span class="line"> ✓ Installing CNI</span><br><span class="line"> ✓ Installing StorageClass</span><br><span class="line">Set kubectl context to <span class="string">&quot;kind-kind&quot;</span></span><br><span class="line">You can now use your cluster with:kubectl cluster-info --context kind-kind Have a <span class="built_in">nice</span> day! </span><br></pre></td></tr></table></figure><h2 id="列出部署的集群"><a href="#列出部署的集群" class="headerlink" title="列出部署的集群"></a>列出部署的集群</h2><p>输入命令：</p><p><code>kind get clusters</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kind get clusters</span><br><span class="line">kind</span><br><span class="line">kind-1.15.6</span><br></pre></td></tr></table></figure><p>这应该列出我们此前创建的两个不同 K8S 版本的集群。</p><h2 id="为-kubectl-设置上下文配置"><a href="#为-kubectl-设置上下文配置" class="headerlink" title="为 kubectl 设置上下文配置"></a>为 kubectl 设置上下文配置</h2><p>创建集群之后，kubectl 配置默认指定了最近创建的 K8S 集群。</p><p>让我们来检查一下所有可用的上下文配置。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                               CLUSTER</span><br><span class="line">          kind-kind                          kind-kind</span><br><span class="line">*         kind-kind-1.15.6                   kind-kind-1.15.6</span><br></pre></td></tr></table></figure><p>从输出中，我们可以看到，kubectl 上下文配置目前已经被默认设置为最新的集群，即 kind-1.15.6（上下文名称是以 kind 为前缀的）。</p><p>要将 kubectl 上下文设置为版本是 1.18.2 的 kind 集群，我们需要进行如下操作：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kubectl config set-context kind-kind</span><br><span class="line"></span><br><span class="line">Context <span class="string">&quot;kind-kind&quot;</span> modified.</span><br></pre></td></tr></table></figure><p>要验证 kubectl 是否指向正确的集群，我们需要检查节点：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kubectl get nodes</span><br><span class="line">NAME                        STATUS   ROLES    AGE     VERSION</span><br><span class="line">kind-1.18.2-control-plane   Ready    master   8m20s   v1.18.2</span><br></pre></td></tr></table></figure><h2 id="删除集群"><a href="#删除集群" class="headerlink" title="删除集群"></a>删除集群</h2><h3 id="删除某个集群"><a href="#删除某个集群" class="headerlink" title="删除某个集群"></a>删除某个集群</h3><p>要删除一个特定的群集，可以通过传递<code>—name</code> 参数去执行删除命令。</p><p>命令为：</p><p><code>kind delete cluster --name kind</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kind delete cluster --name kind</span><br><span class="line"></span><br><span class="line">Deleting cluster <span class="string">&quot;kind&quot;</span> ...</span><br></pre></td></tr></table></figure><h3 id="删除所有集群"><a href="#删除所有集群" class="headerlink" title="删除所有集群"></a>删除所有集群</h3><p>如果你想一次性删除所有集群，请执行：</p><p><code>kind delete clusters –all</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; kind delete clusters --all</span><br><span class="line"></span><br><span class="line">Deleted clusters: [<span class="string">&quot;kind-1.15.6&quot;</span>]</span><br></pre></td></tr></table></figure><h2 id="kind-的优势是什么"><a href="#kind-的优势是什么" class="headerlink" title="kind 的优势是什么"></a>kind 的优势是什么</h2><p>kind（Kubernetes in Docker）是一个基于 Docker 构建的 Kubernetes 集群的工具。它经过 CNCF 认证，并且支持多节点集群，包括高可用集群。并且支持 Linux、macOS 以及 Windows 操作系统，操作简单，学习成本低，非常适合用来在本地搭建基于 Kubernetes 的开发&#x2F;测试环境。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ipv6/conf/eth0/accept_dad: no such file or directory</title>
      <link href="/kubernetes/pod-not-run/"/>
      <url>/kubernetes/pod-not-run/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/pod-not-run/" target="_blank" title="https://www.xtplayer.cn/kubernetes/pod-not-run/">https://www.xtplayer.cn/kubernetes/pod-not-run/</a></p><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>在早期 rancher kubernetes 版本中，在新集群创建应用后可能会出现大批量的 Pod 处于 <code>ContainerCreating</code> 状态。如图：</p>  <img src="/kubernetes/pod-not-run/image-20200825114802247.png" class="" title="image-20200825114802247"><p>通过 <code>kubectl describe</code> 查看应用的事件，提示无法获取 IP 地址：</p>  <img src="/kubernetes/pod-not-run/image-20200825115430865.png" class="" title="image-20200825115430865"><p>查看 kubelet 服务日志，可以看到以下的错误信息：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">E0824 08:04:39.776000   20094 cni.go:331] Error adding kube-system_coredns-59db856b69-b2jck/ead1d2646611bcee3bdb9e0900a53139af282232f6354b156b92a91bdcc3cfee to network flannel/cbr0: open /proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory</span><br><span class="line">E0824 08:04:39.809938   20094 cni.go:352] Error deleting kube-system_coredns-59db856b69-b2jck/ead1d2646611bcee3bdb9e0900a53139af282232f6354b156b92a91bdcc3cfee from network flannel/cbr0: failed to get IP addresses <span class="keyword">for</span> <span class="string">&quot;eth0&quot;</span>: &lt;nil&gt;</span><br><span class="line">E0824 08:04:39.863051   20094 cni.go:331] Error adding kube-system_kubernetes-dashboard-776548b567-j4d86/b6fb00ab9b815ea8a04242b263bfce958b15d0e3efeacc62e5132de8e5e95397 to network flannel/cbr0: open /proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory</span><br><span class="line">E0824 08:04:39.863229   20094 cni.go:331] Error adding kube-system_coredns-autoscaler-5b4f4f8f6b-zs7qq/aa3e232e5364e2d90fda2dd75e10da3b34a998812d06d22a4549995de26e7157 to network flannel/cbr0: open /proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory</span><br><span class="line">E0824 08:04:39.886022   20094 cni.go:331] Error adding kube-system_metrics-server-66c5c9947c-6k852/012960c8e4c6c525072acb93ed1795fc6b7f66003bf7da855d35da618d9310eb to network flannel/cbr0: open /proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory</span><br></pre></td></tr></table></figure><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>根据以上信息可以确定，因为主机不支持 <strong>ipv6</strong> 或者 <strong>ipv6</strong> 功能被禁用，导致 flannel 在初始化容器网卡的时候一直无法初始化容器网卡的 IPV6 相关配置，从而导致容器无法正常获取 IP 地址。</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>经过查询可以找到这是一个已知的 BUG，在不考虑更新 rancher kubernetes 版本的前提下，则需要在主机上强制启用 <strong>ipv6</strong> 功能。</p><h3 id="centos"><a href="#centos" class="headerlink" title="centos"></a>centos</h3><ol><li>编辑 <code>/etc/default/grub</code>，如果之前配置了 <code>ipv6.disable=1</code>，则把它改为 <code>ipv6.disable=0</code>；如果之前没有添加 <code>ipv6.disable</code> 这个参数，则在现有的配置上添加 <code>ipv6.disable=0</code>。</li></ol><p>  示例如下：</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">GRUB_CMDLINE_LINUX=<span class="string">&quot;xxxxxx ipv6.disable=0&quot;</span></span><br></pre></td></tr></table></figure><ol><li><p>然后执行以下配置更新 grub 配置</p><p><code>grub2-mkconfig -o /boot/grub2/grub.cfg</code></p></li><li><p>最后 <code>reboot</code> 重启系统</p></li></ol><h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><ol><li>编辑 <code>/etc/default/grub</code>，如果之前配置了 <code>ipv6.disable=1</code>，则把它改为 <code>ipv6.disable=0</code>；如果之前没有添加 <code>ipv6.disable</code> 这个参数，则在现有的配置上添加 <code>ipv6.disable=0</code>。</li></ol><p>  示例如下：</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">GRUB_CMDLINE_LINUX=<span class="string">&quot;xxxxxx ipv6.disable=0&quot;</span></span><br></pre></td></tr></table></figure><ol><li><p>然后执行以下配置更新 grub 配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">update-grub</span><br></pre></td></tr></table></figure></li><li><p>最后 <code>reboot</code> 重启系统</p></li></ol><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>  <a href="https://github.com/containernetworking/cni/issues/569">https://github.com/containernetworking/cni/issues/569</a></p><p>  <a href="https://github.com/rancher/flannel-cni/pull/8">https://github.com/rancher/flannel-cni/pull/8</a></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pod </tag>
            
            <tag> flannel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker Cgroup 版本对应</title>
      <link href="/docker/docker-cgroup/"/>
      <url>/docker/docker-cgroup/</url>
      
        <content type="html"><![CDATA[<p>centos8&#x2F;redhat8、Fedora 31 或者 Linux 内核 4.5 以上，cgroup 默认为 v2 版本，而当前 docker 只支持 以上，cgroup v1。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker: Error response from daemon: cgroups: cannot found cgroup mount destination: unknown.</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux authentication token manipulation 错误</title>
      <link href="/linux/authentication-token-manipulation/"/>
      <url>/linux/authentication-token-manipulation/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/authentication-token-manipulation/" target="_blank" title="https://www.xtplayer.cn/linux/authentication-token-manipulation/">https://www.xtplayer.cn/linux/authentication-token-manipulation/</a></p><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>修改密码时出现以下错误：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@alihost-01:~# passwd</span><br><span class="line">Enter new UNIX password:</span><br><span class="line">Retype new UNIX password:</span><br><span class="line">passwd: Authentication token manipulation error</span><br><span class="line">passwd: password unchanged</span><br><span class="line">root@alihost-01:~#</span><br></pre></td></tr></table></figure><h2 id="问题分析与解决"><a href="#问题分析与解决" class="headerlink" title="问题分析与解决"></a>问题分析与解决</h2><ol><li><p>先查看 <code>/etc/shadow</code>  文件的权限，发现都是正常的权限</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> -l /etc/shadow</span><br><span class="line">-rw-r----- 1 root shadow 1025 Feb  11 22:11 /etc/shadow</span><br></pre></td></tr></table></figure></li><li><p>尝试强制修改 <code>/etc/shadow</code> 文件权限，报如下错误</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@alihost-01:~<span class="comment"># sudo chmod 0640 /etc/shadow</span></span><br><span class="line"><span class="built_in">chmod</span>: changing permissions of <span class="string">&#x27;/etc/shadow&#x27;</span>: Operation not permitted</span><br></pre></td></tr></table></figure></li><li><p>用 lsattr 命令查看 <code>/etc/passwd</code>  的隐藏权限，发现被设置了 a 权限</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@alihost-01:~<span class="comment"># lsattr /etc/shadow</span></span><br><span class="line">-----a-------e-- /etc/shadow</span><br></pre></td></tr></table></figure></li><li><p>通过 <code>chattr</code> 去除 a 权限</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@alihost-01:~<span class="comment"># chattr -a /etc/shadow</span></span><br></pre></td></tr></table></figure></li><li><p>再次查看 <code>/etc/passwd</code> 的隐藏权限</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@alihost-01:~<span class="comment"># lsattr /etc/shadow</span></span><br><span class="line">-------------e-- /etc/shadow</span><br></pre></td></tr></table></figure></li><li><p>再次修改密码</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@alihost-01:~<span class="comment"># passwd</span></span><br><span class="line">Enter new UNIX password:</span><br><span class="line">Retype new UNIX password:</span><br><span class="line">passwd: password updated successfully</span><br><span class="line">root@alihost-01:~<span class="comment">#</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Ingress 常用配置（持续更新）</title>
      <link href="/kubernetes/ingress-configuration-demo/"/>
      <url>/kubernetes/ingress-configuration-demo/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/ingress-configuration-demo/" target="_blank" title="https://www.xtplayer.cn/kubernetes/ingress-configuration-demo/">https://www.xtplayer.cn/kubernetes/ingress-configuration-demo/</a></p><p>本文列举一些常用的 ingress 功能配置，并持续更新。</p><h2 id="Ingress-配置修改方法"><a href="#Ingress-配置修改方法" class="headerlink" title="Ingress 配置修改方法"></a>Ingress 配置修改方法</h2><h3 id="通过-Rancher-UI-配置"><a href="#通过-Rancher-UI-配置" class="headerlink" title="通过 Rancher UI 配置"></a>通过 Rancher UI 配置</h3><p>依次进入 <strong>system 项目|配置映射</strong>，然后在 <strong>ingress-nginx</strong> 命名空间部分找到 <strong>nginx-configuration</strong>配置映射并编辑 nginx-configuration。</p><p>然后在 <strong>配置映射</strong> 中以键值对形式添加参数，比如： <code>server-tokens=false</code>。</p><blockquote><p><strong>注意:</strong> 配置映射设置的参数是作用于全局，如果想局部生效，可通过对应的 <strong>注释</strong> 去配置相应参数，配置方法文章后面会说明。</p></blockquote><img src="/kubernetes/ingress-configuration-demo/image-20200813114245277.png" class="" title="image-20200813114245277"><h3 id="通过-kubectl-命令行修改"><a href="#通过-kubectl-命令行修改" class="headerlink" title="通过 kubectl 命令行修改"></a>通过 kubectl 命令行修改</h3><p>执行以下命令进入配置映射文件的编辑模式</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl -n ingress-nginx  edit configmaps nginx-configuration</span><br></pre></td></tr></table></figure><p>然后在 <code>data</code> 字段中添加相应参数，比如： <code>server-tokens: &quot;false&quot;</code>，注意双引号。</p><img src="/kubernetes/ingress-configuration-demo/image-20200813114512096.png" class="" title="image-20200813114512096"><h2 id="kubectl-ingress-nginx-plugin"><a href="#kubectl-ingress-nginx-plugin" class="headerlink" title="kubectl ingress-nginx plugin"></a>kubectl ingress-nginx plugin</h2><p>ingress 大概从 0.26.0（或 v0.24.0）版本开始不再支持在 nginx.conf 配置文件中直接显示 backend。</p><blockquote><p>参考链接: <a href="https://github.com/kubernetes/ingress-nginx/blob/e825af86e11790a335e0e4b4360d52ce13cd7a9c/rootfs/etc/nginx/template/nginx.tmpl#L456">https://github.com/kubernetes/ingress-nginx/blob/e825af86e11790a335e0e4b4360d52ce13cd7a9c/rootfs/etc/nginx/template/nginx.tmpl#L456</a></p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">upstream upstream_balancer &#123;</span><br><span class="line">    <span class="comment">### Attention!!!</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># We no longer create &quot;upstream&quot; section for every backend.</span></span><br><span class="line">    <span class="comment"># Backends are handled dynamically using Lua. If you would like to debug</span></span><br><span class="line">    <span class="comment"># and see what backends ingress-nginx has in its memory you can</span></span><br><span class="line">    <span class="comment"># install our kubectl plugin https://kubernetes.github.io/ingress-nginx/kubectl-plugin.</span></span><br><span class="line">    <span class="comment"># Once you have the plugin you can use &quot;kubectl ingress-nginx backends&quot; command to</span></span><br><span class="line">    <span class="comment"># inspect current backends.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line"></span><br><span class="line">    server 0.0.0.1; <span class="comment"># placeholder</span></span><br><span class="line"></span><br><span class="line">    balancer_by_lua_block &#123;</span><br><span class="line">      balancer.balance()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>backend 数据保存在内存中，将通过 Lua 动态的去生成 Nginx 配置文件。为了 debug ingress 的配置，ingress 推出了一款 kubectl 插件，可以通过此插件查看相应的配置信息。</p><h3 id="安装-krew"><a href="#安装-krew" class="headerlink" title="安装 krew"></a>安装 krew</h3><p>Krew 可以理解为 kubectl 插件的包管理工具。借助 Krew，可以轻松地使用 kubectl plugin 查询、安装和管理插件，使用类似 apt、dnf 或 brew。</p><ul><li>macOS&#x2F;Linux</li></ul><ol><li><p>需要提前安装 git 工具</p></li><li><p>在终端中运行以下命令以下载并安装 krew</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(</span><br><span class="line">  <span class="built_in">set</span> -x;</span><br><span class="line">  <span class="built_in">cd</span> <span class="string">&quot;<span class="subst">$(mktemp -d)</span>&quot;</span> &amp;&amp;</span><br><span class="line">  curl -fsSLO <span class="string">&quot;https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.&#123;tar.gz,yaml&#125;&quot;</span> &amp;&amp;</span><br><span class="line">  tar zxvf krew.tar.gz &amp;&amp;</span><br><span class="line">  KREW=./krew-<span class="string">&quot;<span class="subst">$(uname | tr &#x27;[:upper:]&#x27; &#x27;[:lower:]&#x27;)</span>_amd64&quot;</span> &amp;&amp;</span><br><span class="line">  <span class="string">&quot;<span class="variable">$KREW</span>&quot;</span> install --manifest=krew.yaml --archive=krew.tar.gz &amp;&amp;</span><br><span class="line">  <span class="string">&quot;<span class="variable">$KREW</span>&quot;</span> update</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>在<code>.bashrc</code> 或<code>.zshrc</code> 文件中添加以下内容将 <code>$HOME/.krew/bin</code> 目录添加到 PATH 环境变量。</p></li><li><p>运行 <code>kubectl krew</code> 验证命令是否运行正常。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;<span class="variable">$&#123;KREW_ROOT:-<span class="variable">$HOME</span>/.krew&#125;</span>/bin:<span class="variable">$PATH</span>&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>其他运行环境请参考：<a href="https://krew.sigs.k8s.io/docs/user-guide/setup/install/">https://krew.sigs.k8s.io/docs/user-guide/setup/install/</a></p></blockquote></li></ol><h3 id="安装-kubectl-ingress-nginx-plugin"><a href="#安装-kubectl-ingress-nginx-plugin" class="headerlink" title="安装 kubectl ingress-nginx plugin"></a>安装 kubectl ingress-nginx plugin</h3><ol><li><p>运行以下命令安装 kubectl ingress-nginx plugin</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl krew install ingress-nginx</span><br></pre></td></tr></table></figure></li><li><p>安装完成后，可通过以下命令查看插件安装是否正常</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl ingress-nginx --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">A kubectl plugin <span class="keyword">for</span> inspecting your ingress-nginx deployments</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  ingress-nginx [<span class="built_in">command</span>]</span><br><span class="line"></span><br><span class="line">Available Commands:</span><br><span class="line">  backends    Inspect the dynamic backend information of an ingress-nginx instance</span><br><span class="line">  certs       Output the certificate data stored <span class="keyword">in</span> an ingress-nginx pod</span><br><span class="line">  conf        Inspect the generated nginx.conf</span><br><span class="line">  <span class="built_in">exec</span>        Execute a <span class="built_in">command</span> inside an ingress-nginx pod</span><br><span class="line">  general     Inspect the other dynamic ingress-nginx information</span><br><span class="line">  <span class="built_in">help</span>        Help about any <span class="built_in">command</span></span><br><span class="line">  info        Show information about the ingress-nginx service</span><br><span class="line">  ingresses   Provide a short summary of all of the ingress definitions</span><br><span class="line">  lint        Inspect kubernetes resources <span class="keyword">for</span> possible issues</span><br><span class="line">  logs        Get the kubernetes logs <span class="keyword">for</span> an ingress-nginx pod</span><br><span class="line">  ssh         ssh into a running ingress-nginx pod</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line">      --as string                      Username to impersonate <span class="keyword">for</span> the operation</span><br><span class="line">      --as-group stringArray           Group to impersonate <span class="keyword">for</span> the operation, this flag can be repeated to specify multiple <span class="built_in">groups</span>.</span><br><span class="line">      --cache-dir string               Default HTTP cache directory (default <span class="string">&quot;/Users/hxl/.kube/http-cache&quot;</span>)</span><br><span class="line">      --certificate-authority string   Path to a cert file <span class="keyword">for</span> the certificate authority</span><br><span class="line">      --client-certificate string      Path to a client certificate file <span class="keyword">for</span> TLS</span><br><span class="line">      --client-key string              Path to a client key file <span class="keyword">for</span> TLS</span><br><span class="line">      --cluster string                 The name of the kubeconfig cluster to use</span><br><span class="line">      --context string                 The name of the kubeconfig context to use</span><br><span class="line">  -h, --<span class="built_in">help</span>                           <span class="built_in">help</span> <span class="keyword">for</span> ingress-nginx</span><br><span class="line">      --insecure-skip-tls-verify       If <span class="literal">true</span>, the server<span class="string">&#x27;s certificate will not be checked for validity. This will make your HTTPS connections insecure</span></span><br><span class="line"><span class="string">      --kubeconfig string              Path to the kubeconfig file to use for CLI requests.</span></span><br><span class="line"><span class="string">  -n, --namespace string               If present, the namespace scope for this CLI request</span></span><br><span class="line"><span class="string">      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value    of zero means don&#x27;</span>t <span class="built_in">timeout</span> requests. (default <span class="string">&quot;0&quot;</span>)</span><br><span class="line">  -s, --server string                  The address and port of the Kubernetes API server</span><br><span class="line">      --tls-server-name string         Server name to use <span class="keyword">for</span> server certificate validation. If it is not provided, the hostname used to contact the server is used</span><br><span class="line">      --token string                   Bearer token <span class="keyword">for</span> authentication to the API server</span><br><span class="line">      --user string                    The name of the kubeconfig user to use</span><br><span class="line"></span><br><span class="line">Use <span class="string">&quot;ingress-nginx [command] --help&quot;</span> <span class="keyword">for</span> more information about a <span class="built_in">command</span>.</span><br></pre></td></tr></table></figure></li></ol><h3 id="kubectl-ingress-nginx-plugin-用法"><a href="#kubectl-ingress-nginx-plugin-用法" class="headerlink" title="kubectl ingress-nginx plugin 用法"></a>kubectl ingress-nginx plugin 用法</h3><blockquote><p><strong>注意:</strong> 因为 rancher 或 rke 创建的集群中，ingress 控制器是以 DaemonSet 的方式运行。kubectl ingress-nginx plugin 查看配置时需要指定资源，默认只支持 <code>--deployment</code>。在 rancher 或 rke 创建的 ingress 默认都有 <code>app=ingress-nginx</code> 标签，所以这里用标签来指定资源。</p></blockquote><ul><li><p>查看 Nginx 配置</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl ingress-nginx --kubeconfig=xxxx -n ingress-nginx -l app=ingress-nginx conf</span><br></pre></td></tr></table></figure><p>  以上的命令将会输出全部规则的配置，可以通过指定域名来输出指定配置的配置。</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl ingress-nginx --kubeconfig=xxxx -n ingress-nginx -l app=ingress-nginx conf --host &lt;配置的域名&gt;</span><br></pre></td></tr></table></figure></li><li><p>查看 backends</p>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl ingress-nginx --kubeconfig=xxxx  -n ingress-nginx -l app=ingress-nginx backends</span><br></pre></td></tr></table></figure></li></ul><h2 id="永久重定向状态码-（http-redirect-code）"><a href="#永久重定向状态码-（http-redirect-code）" class="headerlink" title="永久重定向状态码 （http-redirect-code）"></a>永久重定向状态码 （http-redirect-code）</h2><p>ingress 中重定向的状态码默认是 <code>308</code>，某些情况下我们可能需要 <code>301</code> 状态码，可以按照以下方法配置。</p><h3 id="全局配置"><a href="#全局配置" class="headerlink" title="全局配置"></a>全局配置</h3><p>可通过在配置映射中添加 <code>http-redirect-code=301</code> 来修改全局重定向状态码，修改后所有 ingress 规则的重定向状态码都将是 <code>301</code>。</p><blockquote><p>参考链接：<a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#http-redirect-code">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#http-redirect-code</a></p></blockquote><h3 id="局部配置"><a href="#局部配置" class="headerlink" title="局部配置"></a>局部配置</h3><p>如果只想某个域名使用 <code>301</code> 状态码，那可以编辑对应的 ingress 规则添加以下<strong>注释</strong>。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/permanent-redirect-code:</span> <span class="string">&#x27;301&#x27;</span> <span class="comment"># 默认 308</span></span><br></pre></td></tr></table></figure><blockquote><p>参考链接：<a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#permanent-redirect-code">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#permanent-redirect-code</a></p></blockquote><h2 id="代理外部应用（域名（http、https）或-ip-端口）"><a href="#代理外部应用（域名（http、https）或-ip-端口）" class="headerlink" title="代理外部应用（域名（http、https）或 ip + 端口）"></a>代理外部应用（域名（http、https）或 ip + 端口）</h2><h3 id="创建外部服务"><a href="#创建外部服务" class="headerlink" title="创建外部服务"></a>创建外部服务</h3><p>假设有一个外部应用，是通过域名 <strong>123.com</strong> 进行访问。现在想在 <strong>ingress</strong> 中通过新的域名 <strong>abc.com</strong> 去代理这个外部应用。那么需要先把外部域名通过<strong>外部服务</strong>的方式引入 K8S 集群中，然后 ingress 去调用这个外部服务。</p><ol><li><p>在<strong>服务发现</strong>页面，添加 一条 DNS 记录（或者叫新建一个 SVC）</p> <img src="/kubernetes/ingress-configuration-demo/image-20200812095300200.png" class="" title="image-20200812095300200"></li><li><p>DNS 记录配置详情</p><ul><li><p>DNS 记录名称</p><p> 名称可以随意填写，但是不能以数字开头，不能大写</p></li><li><p>命名空间</p><p> DNS 记录所在的命名空间</p></li><li><p>解析类型</p><p> 选择解析类型，可以选择外部域名或者外部 ip</p></li><li><p>填写目标域名或者目标 ip (这里不用写端口)</p></li><li><p>端口映射</p><p> 这个参数很重要，需要根据外部服务访问的真实端口来配置。比如你的服务访问是通过 <code>http://123.com:9000</code> 来访问的，那么这里就需要填写 9000 端口。如果是通过 <code>http://123.com</code> 或者 <code>https://123.com</code> 访问，80 或 443 端口也需要填写。</p> <img src="/kubernetes/ingress-configuration-demo/image-20200812120853135.png" class="" title="image-20200812120853135"></li></ul></li></ol><h3 id="Ingress-规则创建"><a href="#Ingress-规则创建" class="headerlink" title="Ingress 规则创建"></a>Ingress 规则创建</h3><ol><li><p>在负载均衡页面添加一条新的 ingress 规则。如下图，添加 <strong>目标后端</strong> 的时候先删除默认的后端规则，然后点击<strong>服务</strong>。</p> <img src="/kubernetes/ingress-configuration-demo/image-20200812120431552.png" class="" title="image-20200812120431552"></li><li><p>因为只有选择 <strong>服务</strong> 此处才能选择之前创建的 <strong>DNS 记录</strong>。</p> <img src="/kubernetes/ingress-configuration-demo/image-20200812121036778.png" class="" title="image-20200812121036778"></li><li><p>最后点击保存</p></li></ol><h4 id="http-域名或者-ip-端口"><a href="#http-域名或者-ip-端口" class="headerlink" title="http 域名或者 ip + 端口"></a>http 域名或者 ip + 端口</h4><p>编辑刚刚创建的 ingress 规则，在 <strong>标签&#x2F;注释</strong> 中为 ingress 规则添加 <code>annotations</code>，通过 <code>annotations</code> 指定后端应用的访问地址。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/upstream-vhost:</span> <span class="number">123.</span><span class="string">com</span> <span class="string">或者</span> <span class="string">ip+端口</span></span><br></pre></td></tr></table></figure><img src="/kubernetes/ingress-configuration-demo/image-20200812121732765.png" class="" title="image-20200812121732765"><h4 id="https"><a href="#https" class="headerlink" title="https"></a>https</h4><p>对于外部服务为 <strong>https</strong> 访问又有两种情况，一种是权威的 ssl 证书，一种是自签名的 ssl 证书。不同类型需要添加不同的  <code>annotations</code> 配置。</p><ul><li><p>权威 ssl 证书</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/upstream-vhost:</span> <span class="number">123.</span><span class="string">com</span></span><br><span class="line"><span class="attr">nginx.ingress.kubernetes.io/secure-backends:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">nginx.ingress.kubernetes.io/backend-protocol:</span> <span class="string">HTTPS</span></span><br></pre></td></tr></table></figure></li><li><p>自签名 ssl 证书</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/upstream-vhost:</span> <span class="number">123.</span><span class="string">com</span></span><br><span class="line"><span class="attr">nginx.ingress.kubernetes.io/secure-backends:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">nginx.ingress.kubernetes.io/proxy-ssl-verify:</span> <span class="string">&quot;off&quot;</span></span><br><span class="line"><span class="attr">nginx.ingress.kubernetes.io/backend-protocol:</span> <span class="string">HTTPS</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="外部服务后端重写"><a href="#外部服务后端重写" class="headerlink" title="外部服务后端重写"></a>外部服务后端重写</h4><p>如果外部服务是以 <code>http(s)://123.com/path</code> 的 url 形式访问，那么需要为其配置<strong>后端重写</strong>才能保证能通过 <strong>abc.com</strong> 这种形式访问。</p><p>配置方法请参考 <a href="/kubernetes/ingress-configuration-demo/#%E5%90%8E%E7%AB%AF%E9%87%8D%E5%86%99">后端重写</a>。</p><h2 id="后端重写"><a href="#后端重写" class="headerlink" title="后端重写"></a>后端重写</h2><p><strong>后端重写</strong> 主要有两种使用场景：</p><ol><li>有多个后端应用，但是当前只有一个可用域名，那么需要通过 <code>&lt;单域名&gt;/&lt;Path&gt;</code> 的方式来区分。</li><li>多个应用通过子目录的方式部署在一个 web server 下，需要通过多个域名来访问应用。</li></ol><h3 id="单域名-Path"><a href="#单域名-Path" class="headerlink" title="单域名 + Path"></a>单域名 + Path</h3><img src="/kubernetes/ingress-configuration-demo/image-20200814144328115.png" class="" title="image-20200814144328115"><p>如上图，可用配置多个 <strong>Path</strong> 来区分多个服务。 但是当访问 <code>&lt;单域名&gt;/&lt;Path&gt;</code> 的时候，可能会出现 <code>404</code> 错误。因为当访问 <code>&lt;单域名&gt;/&lt;Path&gt;</code> 的时候会是去后端服务页面中找 <code>Path</code> 路径，可能 <code>Path</code> 是随意设置的，后端服务页面并不存在此页面，所以会提示 <code>404</code> 错误。并且我们要求访问 <code>&lt;单域名&gt;/&lt;Path&gt;</code> 的时候是访问后端服务的根目录，而不是子目录，所以需要通过后端重写的方式将请求转发到根目录。</p><p>在 ingress 可通过配置以下<strong>注释</strong>达到预期要求：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="string">/</span></span><br></pre></td></tr></table></figure><img src="/kubernetes/ingress-configuration-demo/image-20200814144446820.png" class="" title="image-20200814144446820"><h3 id="多域名-Path"><a href="#多域名-Path" class="headerlink" title="多域名 + Path"></a>多域名 + Path</h3><p>如果多个应用是以子目录的方式部署在同一个 web 服务器中，这种情况其实是可通过 <code>&lt;域名&gt;/&lt;Path&gt;</code> 的方式来访问的。但是有时候可能需要不加 <strong>Path</strong> 后缀，直接通过域名访问。这种需求也需要通过 <strong>后端重写</strong> 的方式来设置，其配置方法正好与 <code>单域名 + Path</code> 相反。</p><blockquote><p><strong>注意：</strong> 如果请求的资源不是全部在子站点中，那么页面重写过去之后，可能存在一些静态资源无法加载的问题，因此需要根据实际的应用架构来确定是否采用这种后端重写。</p></blockquote><ol><li><p>在 ingress 规则中添加以下注释来实现页面重写:</p> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="string">/test</span></span><br></pre></td></tr></table></figure></li><li><p>Path 处设置为 <code>/(.*)</code></p></li><li><p>完整 ingress yaml 示例：</p></li></ol><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="string">/test/$1</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-domain</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">www.test.local</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">ingress-541fa2c5687afb7ccb3c7c9a13fa7119</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/(.*)</span></span><br></pre></td></tr></table></figure><blockquote><p>参考链接：<a href="https://kubernetes.github.io/ingress-nginx/examples/rewrite/">https://kubernetes.github.io/ingress-nginx/examples/rewrite/</a></p></blockquote><h2 id="隐藏-Ingress-Nginx-版本号"><a href="#隐藏-Ingress-Nginx-版本号" class="headerlink" title="隐藏 Ingress - Nginx 版本号"></a>隐藏 Ingress - Nginx 版本号</h2><p>Ingress 是内置 NGINX 来提供负载均衡服务，默认显示 nginx 版本号。为了安全有时候会要求关闭版本号显示，可通过修改配置映射来隐藏版本号。</p><p>在配置映射中设置：</p><ol><li>Rancher UI 配置 <code>server-tokens=false</code></li><li>YAML 配置 <code>server-tokens: &quot;false&quot;</code></li></ol><blockquote><p>参考链接：<a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#server-tokens">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#server-tokens</a></p></blockquote><h2 id="自定义-http-和-https-端口"><a href="#自定义-http-和-https-端口" class="headerlink" title="自定义 http 和 https 端口"></a>自定义 http 和 https 端口</h2><p>在 Rancher 部署的 K8S 集群中，默认部署了 <strong>ingress</strong> 控制器。ingress 控制器以 <strong>DaemonSet</strong> 类型部署，并且以 <strong>host</strong> 网络模式运行。ingress 控制器默认会监听 <strong>80</strong> 和 <strong>443</strong> 两个端口来提供 web 服务，所以如果主机上原来已经有服务监听了 80 或者 443 端口，那么 ingress 控制器部署将会出现端口冲突。</p><p>根据部署方式的不同，大概有以下几种修改方式:</p><h3 id="更新工作负载修改-http-和-https-端口"><a href="#更新工作负载修改-http-和-https-端口" class="headerlink" title="更新工作负载修改 http 和 https 端口"></a>更新工作负载修改 http 和 https 端口</h3><ol><li><p>依次进入 <strong>目标集群|system 项目</strong>，在工作负载中找到 <strong>nginx-ingress-controller</strong>，点击右侧省略号菜单，选择编辑。</p> <img src="/kubernetes/ingress-configuration-demo/image-20200821222324354.png" class="" title="image-20200821222324354"></li><li><p>在配置详情页面中，点击右下角 <strong>显示高级选项</strong>，然后打开 <strong>命令</strong> 选项卡</p> <img src="/kubernetes/ingress-configuration-demo/image-20200821222512212.png" class="" title="image-20200821222512212"></li><li><p>在命令配置栏中添加配置参数，比如： <code>--http-port=8880 --https-port=8443</code></p> <img src="/kubernetes/ingress-configuration-demo/image-20200821222615959.png" class="" title="image-20200821222615959"></li></ol><h3 id="RKE-配置文件修改"><a href="#RKE-配置文件修改" class="headerlink" title="RKE 配置文件修改"></a>RKE 配置文件修改</h3><p>如果使用 rke 创建 K8S 集群，那么可以在 rke 配置文件中指定 ingress 的端口参数。</p>  <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">ingress:</span></span><br><span class="line">  <span class="attr">provider:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">options:</span></span><br><span class="line">    <span class="attr">map-hash-bucket-size:</span> <span class="string">&quot;128&quot;</span></span><br><span class="line">    <span class="attr">ssl-protocols:</span> <span class="string">SSLv2</span></span><br><span class="line">  <span class="attr">extra_args:</span></span><br><span class="line">    <span class="attr">enable-ssl-passthrough:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">http-port:</span> <span class="number">8880</span></span><br><span class="line">    <span class="attr">https-port:</span> <span class="number">8443</span></span><br></pre></td></tr></table></figure><h3 id="Rancher-自定义集群"><a href="#Rancher-自定义集群" class="headerlink" title="Rancher 自定义集群"></a>Rancher 自定义集群</h3><p>与 rke 集群相似，可以编辑集群的 YAML 配置文件，然后添加对应参数。</p>  <img src="/kubernetes/ingress-configuration-demo/image-20200821223131704.png" class="" title="image-20200821223131704">  <img src="/kubernetes/ingress-configuration-demo/image-20200821223223894.png" class="" title="image-20200821223223894"><h2 id="自定义-client-max-body-size-大小"><a href="#自定义-client-max-body-size-大小" class="headerlink" title="自定义 client_max_body_size 大小"></a>自定义 client_max_body_size 大小</h2><p>编辑 ingress 规则，在注释中添加:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/proxy-body-size:</span> <span class="string">32m</span></span><br></pre></td></tr></table></figure><img src="/kubernetes/ingress-configuration-demo/image-20200824151410783.png" class="" title="image-20200824151410783"><blockquote><p>如果想全局配置 proxy-body-size 大小，则在<a href="/Kubernetes/ingress-configuration-demo/#Ingress-%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%E6%96%B9%E6%B3%95">配置映射</a>文件中添加 <code>proxy-body-size=32m</code></p></blockquote><h2 id="自定义-Proxy-buffering-大小"><a href="#自定义-Proxy-buffering-大小" class="headerlink" title="自定义 Proxy buffering 大小"></a>自定义 Proxy buffering 大小</h2><ol><li><p>默认情况下，ingress 配置中禁用 Proxy buffering 。所以需要配置以下设置开启：</p> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/proxy-buffering:</span> <span class="string">&quot;on&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>如果想全局开启 Proxy buffering ，则在<a href="/Kubernetes/ingress-configuration-demo/#Ingress-%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%E6%96%B9%E6%B3%95">配置映射</a>文件中添加 <code>proxy-buffering=on</code>。</p></blockquote></li><li><p>默认情况下，Proxy buffering 大小设置为 <code>4k</code>，可通过以下配置修改大小：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/proxy-buffer-size:</span> <span class="string">&quot;8k&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>如果想全局配置 Proxy buffering 大小，则在<a href="/Kubernetes/ingress-configuration-demo/#Ingress-%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%E6%96%B9%E6%B3%95">配置映射</a>文件中设置 <code>proxy-buffer-size=8k</code></p></blockquote></li></ol><h2 id="开启-TLS-x2F-HTTPS"><a href="#开启-TLS-x2F-HTTPS" class="headerlink" title="开启 TLS&#x2F;HTTPS"></a>开启 TLS&#x2F;HTTPS</h2><p>默认情况下，如果 ingress 规则对象入口启用了 <strong>TLS&#x2F;HTTPS</strong>，则 ingress-controller 将使用 <strong>308</strong> 永久重定向将 <strong>HTTP</strong> 请求重定向到 <strong>HTTPS</strong> 请求。如果需修改 308 为 301，请参考：<a href="/Kubernetes/ingress-configuration-demo/#%E6%B0%B8%E4%B9%85%E9%87%8D%E5%AE%9A%E5%90%91%E7%8A%B6%E6%80%81%E7%A0%81-%EF%BC%88http-redirect-code%EF%BC%89">永久重定向状态码 （http-redirect-code）</a></p><p>某些情况下可能希望 http 和 https 同时使用，那么就需要禁止 TLS 自动重定向。可以在 ingress 规则中添加以下注释来禁止重定向：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/ssl-redirect:</span> <span class="string">&quot;false&quot;</span></span><br></pre></td></tr></table></figure><p>如果想全局禁止 TLS 自动重定向，可以在配置映射文件中配置： <code>ssl-redirect=false</code> 来禁止。</p><h2 id="记录客户端地址"><a href="#记录客户端地址" class="headerlink" title="记录客户端地址"></a>记录客户端地址</h2><h2 id="跨域配置"><a href="#跨域配置" class="headerlink" title="跨域配置"></a>跨域配置</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/cors-allow-methods:</span> <span class="string">&quot;PUT, GET, POST, OPTIONS&quot;</span></span><br><span class="line"><span class="string">nginx.ingress.kubernetes.io/cors-allow-headers：&quot;DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization&quot;</span></span><br><span class="line"><span class="attr">nginx.ingress.kubernetes.io/cors-allow-origin:</span> <span class="string">&quot;*&quot;</span></span><br></pre></td></tr></table></figure><h2 id="白名单"><a href="#白名单" class="headerlink" title="白名单"></a>白名单</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/whitelist-source-range:</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.0</span><span class="string">/24,192.168.2.8</span></span><br></pre></td></tr></table></figure><h2 id="请求速率限制"><a href="#请求速率限制" class="headerlink" title="请求速率限制"></a>请求速率限制</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/limit-rps:</span> <span class="string">&#x27;100&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="websocket-配置"><a href="#websocket-配置" class="headerlink" title="websocket 配置"></a>websocket 配置</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nginx.ingress.kubernetes.io/configuration-snippet:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    proxy_set_header Upgrade &quot;websocket&quot;;</span></span><br><span class="line"><span class="string">    proxy_set_header Connection &quot;Upgrade&quot;;</span></span><br><span class="line"><span class="string">    nginx.ingress.kubernetes.io/proxy-read-timeout 3600;</span></span><br><span class="line"><span class="string">    nginx.ingress.kubernetes.io/proxy-send-timeout 3600;</span></span><br></pre></td></tr></table></figure><h2 id="自定义-header"><a href="#自定义-header" class="headerlink" title="自定义 header"></a>自定义 header</h2><h3 id="全局配置-header"><a href="#全局配置-header" class="headerlink" title="全局配置 header"></a>全局配置 header</h3><ol><li><p>复制以下内容创建自定义 header ConfigMap 文件</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">X-Different-Name:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">X-Request-Start:</span> <span class="string">t=$&#123;msec&#125;</span></span><br><span class="line">  <span class="attr">X-Using-Nginx-Controller:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">x-custom-headers:</span> <span class="string">xxxxx</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">custom-headers</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ingress-nginx</span></span><br></pre></td></tr></table></figure><p>配置效果：</p><img src="/kubernetes/ingress-configuration-demo/g9kzTyf-MA0BAHgkxDuoxBsqRxrl1pPqtA.png" class="" title="img"></li><li><p>在 <strong>system 项目|配置映射</strong> 下找到 <strong>nginx-configuration</strong> 配置映射并编辑它，添加如下配置：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">enable-underscores-in-headers=true</span></span><br><span class="line"><span class="string">proxy-set-headers=ingress-nginx/custom-headers</span></span><br></pre></td></tr></table></figure><p>配置效果：</p><img src="/kubernetes/ingress-configuration-demo/0_xOiiB_xxLmsMILlG8ZNJbML0tMHwc2Sg.png" class="" title="img"></li><li><p><strong>注意</strong>：如果配置一直没有更新，则需要重启 ingress 控制器 Pod，以触发配置更新。</p></li></ol><h3 id="单个-ingress-规则配置自定义-header"><a href="#单个-ingress-规则配置自定义-header" class="headerlink" title="单个 ingress 规则配置自定义 header"></a>单个 ingress 规则配置自定义 header</h3><ol><li><p>在 <strong>system 项目|配置映射</strong> 下找到 <strong>nginx-configuration</strong> 配置映射并编辑它，添加如下配置：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">enable-underscores-in-headers=true</span></span><br></pre></td></tr></table></figure></li><li><p>编辑 ingress 规则的 YAML 配置，添加如下配置：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/configuration-snippet:</span> <span class="string">|</span></span><br><span class="line"><span class="string">      more_set_headers &quot;Hello: World&quot;;</span></span><br><span class="line"><span class="string">      more_set_headers &quot;Hello1: World1&quot;;</span></span><br></pre></td></tr></table></figure><p>配置效果：</p><img src="/kubernetes/ingress-configuration-demo/bxS0DWv9SNQD8tcPKm8abwjrQkI0jfVihA.png" class="" title="img"></li></ol><h3 id="自定义日志输出格式"><a href="#自定义日志输出格式" class="headerlink" title="自定义日志输出格式"></a>自定义日志输出格式</h3><ul><li><p>upstream log format</p><p>在配置映射文件中添加以下配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">log-format-upstream: <span class="string">&#x27;&#123;&quot;time&quot;: &quot;$time_iso8601&quot;, &quot;remote_addr&quot;: &quot;$proxy_protocol_addr&quot;, &quot;x_forward_for&quot;: &quot;$proxy_add_x_forwarded_for&quot;, &quot;request_id&quot;: &quot;$req_id&quot;, &quot;remote_user&quot;: &quot;$remote_user&quot;, &quot;bytes_sent&quot;: $bytes_sent, &quot;request_time&quot;: $request_time, &quot;status&quot;: $status, &quot;vhost&quot;: &quot;$host&quot;, &quot;request_proto&quot;: &quot;$server_protocol&quot;, &quot;path&quot;: &quot;$uri&quot;, &quot;request_query&quot;: &quot;$args&quot;, &quot;request_length&quot;: $request_length, &quot;duration&quot;: $request_time,&quot;method&quot;: &quot;$request_method&quot;, &quot;http_referrer&quot;: &quot;$http_referer&quot;, &quot;http_user_agent&quot;: &quot;$http_user_agent&quot; &#125;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>参考链接</strong>：<a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#log-format-upstream">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#log-format-upstream</a></p></blockquote></li><li><p>stream log format</p><blockquote><p><strong>参考链接</strong>：<a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#log-format-stream">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#log-format-stream</a></p></blockquote></li></ul><h3 id="更多参数参考"><a href="#更多参数参考" class="headerlink" title="更多参数参考"></a>更多参数参考</h3><p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#configuration-options/">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#configuration-options/</a></p><p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/</a></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> ingress </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自定义服务账户（Service Account）</title>
      <link href="/kubernetes/custom-serviceaccount/"/>
      <url>/kubernetes/custom-serviceaccount/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/kubernetes/custom-serviceaccount/" target="_blank" title="https://www.xtplayer.cn/kubernetes/custom-serviceaccount/">https://www.xtplayer.cn/kubernetes/custom-serviceaccount/</a></p><p>Service account 主要是为了方便 Pod 中的进程调用 Kubernetes API 而设计的，为服务提供了一种方便的认证机制。但它不提供授权，需要配合 RBAC 来为 Service Account 鉴权。</p><h2 id="用户帐户与服务帐户"><a href="#用户帐户与服务帐户" class="headerlink" title="用户帐户与服务帐户"></a>用户帐户与服务帐户</h2><p>Kubernetes 出于多种原因区分用户帐户和服务帐户的概念：</p><ul><li>用户帐户是为人类使用的，服务帐户用于在 Pods 中运行的进程。</li><li>用户帐户是全局的，其名称在集群的所有命名空间中必须是唯一的，将来的用户资源将不会被命名空间。服务帐户有命名空间。</li><li>通常，集群的用户帐户可能是从公司数据库同步的。在公司数据库中，新用户帐户的创建需要特殊的权限，并且与复杂的业务流程相关联。服务帐户的创建旨在更加轻量级，从而允许集群用户为特定任务（即最小特权原则）创建服务帐户。</li><li>用户账户和服务帐户的审核注意事项可能有所不同。</li><li>复杂系统的配置可以包括该系统组件的各种服务帐户的定义。因为可以临时创建服务帐户并使用命名空间名称，所以这种配置是可移植的。</li></ul><h2 id="服务帐户自动化"><a href="#服务帐户自动化" class="headerlink" title="服务帐户自动化"></a>服务帐户自动化</h2><p>Kubernetes 通过三个独立组件的协作实现了服务帐户自动化控制：</p><ul><li>服务帐户准入控制器</li><li>令牌控制器</li><li>服务帐户控制器</li></ul><h3 id="服务帐户准入控制器"><a href="#服务帐户准入控制器" class="headerlink" title="服务帐户准入控制器"></a>服务帐户准入控制器</h3><p>Pod 的修改是通过名为 <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Admission Controller</a> 的插件实现的。它是 apiserver 的一部分，它在创建或更新 Pods 时同步地对其进行修改。当这个插件被激活时(在大多数发行版上它是默认的)，当一个 Pod 被创建或修改时，它会执行以下操作:</p><ol><li>如果 Pod 没有设置 <strong>ServiceAccount</strong>，它将 <strong>ServiceAccount</strong> 设置为 <strong>默认值</strong>。</li><li>它确保 Pod 引用的 <strong>ServiceAccount</strong> 存在，否则将拒绝创建 Pod。</li><li>如果 Pod 不包含任何 <strong>ImagePullSecrets</strong>，则 <strong>ServiceAccount</strong> 的 <strong>ImagePullSecrets</strong> 被添加到 Pod 中。</li><li>它向 Pod 添加一个卷，其中包含用于 API 访问的令牌。</li><li>它向 Pod 的每个容器添加一个 <strong>volumeSource</strong>，并挂载在 <code>/var/run/secrets/kubernet.io/serviceaccount</code> 上。</li></ol><p>从 v1.13 开始，当启用了 <strong>BoundServiceAccountTokenVolume</strong> 功能时，您可以将服务帐户卷迁移到 <strong>Projected Volume</strong> 。服务帐户令牌将在 1 小时后，或者 pod 被删除后过期。查看有关<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-projected-volume-storage/">Projected Volume</a>更多详细信息。</p><h3 id="令牌控制器"><a href="#令牌控制器" class="headerlink" title="令牌控制器"></a>令牌控制器</h3><p>Token Controller 作为控制管理器的一部分，它是异步运行。它：</p><ul><li>监听 <strong>serviceAccount</strong> 创建并创建一个对应的 <strong>Secret</strong> 以允许访问 <strong>API</strong> 。</li><li>监听 <strong>serviceAccount</strong> 删除并删除所有相应的 <strong>ServiceAccountToken Secrets</strong>。</li><li>监听 <strong>Secrets</strong> 添加，并确保引用的 <strong>ServiceAccount</strong> 存在，并在需要时向该 <strong>Secrets</strong> 添加令牌。</li><li>监听 <strong>Secrets</strong> 删除，并在需要时从相应的 <strong>ServiceAccount</strong> 中删除引用。</li></ul><p>必须使用 <code>--service-account-private-key-file</code> 选项将 <strong>服务帐户</strong> 的私钥文件传递给控制管理器中的令牌控制器，私钥将用于对生成的服务帐户令牌进行签名。同样，您必须使用 <code>--service-account-key-file</code> 选项将相应的公钥传递给 <strong>kube-apiserver</strong> ，公钥将在身份验证期间用于验证令牌。</p><h4 id="创建额外的-API-令牌"><a href="#创建额外的-API-令牌" class="headerlink" title="创建额外的 API 令牌"></a>创建额外的 API 令牌</h4><p>控制器循环确保每个服务帐户都存在一个带有 API 令牌的 <strong>Secrets</strong>。要为服务帐户创建额外的 API 令牌，需要创建一个类型为<strong>ServiceAccountToken</strong> 的 <strong>secret</strong>，并带有引用服务帐户的 <strong>annotation</strong> ，控制器将用生成的令牌更新它:</p><p><strong>secret.json</strong>：</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;kind&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Secret&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;apiVersion&quot;</span><span class="punctuation">:</span> <span class="string">&quot;v1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;metadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysecretname&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;annotations&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;kubernetes.io/service-account.name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;myserviceaccount&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;kubernetes.io/service-account-token&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create -f ./secret.json</span><br><span class="line">kubectl describe secret mysecretname</span><br></pre></td></tr></table></figure><h4 id="删除服务帐户令牌"><a href="#删除服务帐户令牌" class="headerlink" title="删除服务帐户令牌"></a>删除服务帐户令牌</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl delete secret mysecretname</span><br></pre></td></tr></table></figure><h3 id="服务账户控制器"><a href="#服务账户控制器" class="headerlink" title="服务账户控制器"></a>服务账户控制器</h3><p>服务帐户控制器管理命名空间内的 <strong>ServiceAccount</strong>，并确保每个活动命名空间中都存在一个名为 <code>default</code> 的 <strong>ServiceAccount</strong>。</p><h3 id="默认的-Service-account"><a href="#默认的-Service-account" class="headerlink" title="默认的 Service account"></a>默认的 Service account</h3><p>默认情况下，创建 Pod 时，如果未指定 Service account ，则会在同一命名空间中自动为其分配名为 <strong>default</strong> 的服务帐户。查看 Pod 的原始 <strong>json</strong> 或 <strong>yaml</strong>（例如: <code>kubectl get pods/&lt;podname&gt; -o yaml</code>），则可以看到 <code>spec.serviceAccountName</code> 字段被自动设置为 <strong>default</strong>。</p><p>这个 <strong>default Service account</strong> 由 K8S 自动创建，默认没有绑定任何权限。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">priority: 0</span><br><span class="line">restartPolicy: Always</span><br><span class="line">schedulerName: default-scheduler</span><br><span class="line">securityContext: &#123;&#125;</span><br><span class="line">serviceAccount: default</span><br><span class="line">serviceAccountName: default</span><br><span class="line">terminationGracePeriodSeconds: 30</span><br><span class="line">tolerations:</span><br><span class="line">- effect: NoExecute</span><br><span class="line">  key: node.kubernetes.io/not-ready</span><br><span class="line">  operator: Exists</span><br><span class="line">  tolerationSeconds: 300</span><br></pre></td></tr></table></figure><h3 id="创建自定义-ServiceAccount"><a href="#创建自定义-ServiceAccount" class="headerlink" title="创建自定义 ServiceAccount"></a>创建自定义 ServiceAccount</h3><p>有两种方法创建 <strong>ServiceAccount</strong>：</p><ol><li><p>通过 <strong>yaml</strong> 文件创建 <strong>ServiceAccount</strong></p> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&lt;ServiceAccount-name&gt;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">&lt;ServiceAccount-namespace&gt;</span></span><br></pre></td></tr></table></figure><p> 保存以上内容为 <code>ServiceAccount.yaml</code>，然后运行以下命令创建 ServiceAccount：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f ServiceAccount.yaml</span><br></pre></td></tr></table></figure></li><li><p>直接通过 kubectl 命令行创建</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">namespace_name=xxx</span><br><span class="line">serviceaccount_name=xxx</span><br><span class="line"></span><br><span class="line">kubectl create serviceaccount --namespace=<span class="variable">$&#123;namespace_name&#125;</span>  <span class="variable">$&#123;serviceaccount_name&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="服务账户权限控制"><a href="#服务账户权限控制" class="headerlink" title="服务账户权限控制"></a>服务账户权限控制</h2><p>前文已说到服务账户本身不提供授权功能，如果要通过 ServiceAccount 去访问集群中某些资源，那么需要通过额外的授权插件和策略来实现授权。常用的授权模式，比如：<code>基于角色（Role）的访问控制（RBAC）</code>。</p><p>基于角色（Role）的访问控制(RBAC)是一种基于组织中各个用户的角色来调节对计算机或网络资源的访问的方法。RBAC 授权使用 <code>rbac.authorization.k8s.io</code> <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups">API Groups</a> 来驱动授权决策，允许您通过 <code>Kubernetes API</code> 动态配置策略。</p><h3 id="Role-和-ClusterRole"><a href="#Role-和-ClusterRole" class="headerlink" title="Role 和 ClusterRole"></a>Role 和 ClusterRole</h3><p>RBAC Role 或 ClusterRole 包含一组权限规则，权限可以累加。只有<strong>允许</strong>规则，没有 <strong>禁止</strong> 规则，默认全部<strong>禁止</strong>。</p><ul><li>Role 具有命名空间的限制，当您创建一个角色，你必须指定所属的命名空间。一个 <code>Role</code> 只可以用来对某一命名空间中的资源赋予访问权限。</li><li>相比之下，ClusterRole 是一种没有命名空间限制的资源。<code>ClusterRole</code> 可以授予的权限和 <code>Role</code> 相同， 但是因为 <code>ClusterRole</code> 属于集群范围，所以它也可以授予以下访问权限：<ul><li>集群范围资源 （比如 <strong>nodes</strong>）</li><li>非资源端点（比如 “&#x2F;<strong>healthz</strong>“）</li><li>跨命名空间访问的有名字空间作用域的资源（如 <strong>Pods</strong>），比如运行命令 <code>kubectl get pods --all-namespaces</code> 时需要此能力</li></ul></li></ul><h4 id="Role-示例"><a href="#Role-示例" class="headerlink" title="Role 示例"></a>Role 示例</h4><p>这是 default 命名空间的示例 Role，可用于授予对 <strong>Pods</strong> 的读取访问权限。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>] <span class="comment"># &quot;&quot; indicates the core API group</span></span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;list&quot;</span>]</span><br></pre></td></tr></table></figure><h4 id="ClusterRole-示例"><a href="#ClusterRole-示例" class="headerlink" title="ClusterRole 示例"></a>ClusterRole 示例</h4><p>这是一个 <strong>ClusterRole</strong> 示例，可用于授予对以下内容的读取访问权限： <strong>secrets</strong> 在任何特定的命名空间中，或所有命名空间中。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># &quot;namespace&quot; omitted since ClusterRoles are not namespaced</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secret-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># at the HTTP level, the name of the resource for accessing Secret</span></span><br><span class="line">  <span class="comment"># objects is &quot;secrets&quot;</span></span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;secrets&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;list&quot;</span>]</span><br></pre></td></tr></table></figure><h3 id="RoleBinding-和-ClusterRoleBinding"><a href="#RoleBinding-和-ClusterRoleBinding" class="headerlink" title="RoleBinding 和 ClusterRoleBinding"></a>RoleBinding 和 ClusterRoleBinding</h3><p><strong>RoleBinding</strong> 将角色中定义的权限授予一个用户或一组用户。它包含一个主题列表(用户、组或服务帐户)和一个对被授予角色的引用。<strong>RoleBinding</strong> 授予特定命名空间中的权限，而 <strong>ClusterRoleBinding</strong> 则在整个集群范围内授予访问权限。</p><p><strong>RoleBinding</strong> 可以引用同一命名空间中的任意 <strong>Role</strong>，也可以将 <strong>ClusterRole</strong> 绑定到 <strong>RoleBinding</strong> 的命名空间。</p><h4 id="RoleBinding-示例"><a href="#RoleBinding-示例" class="headerlink" title="RoleBinding 示例"></a>RoleBinding 示例</h4><p>以下是 <strong>RoleBinding</strong> 示例，该示例将 <code>pod-reader</code> 角色授予 <strong>default</strong> 命名空间中的用户 <strong>jane</strong>。这允许 <strong>jane</strong> 读取 <strong>default</strong> 命名空间中的<strong>Pod</strong>。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="comment"># This role binding allows &quot;jane&quot; to read pods in the &quot;default&quot; namespace.</span></span><br><span class="line"><span class="comment"># You need to already have a Role named &quot;pod-reader&quot; in that namespace.</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">read-pods</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="comment"># You can specify more than one &quot;subject&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">jane</span> <span class="comment"># &quot;name&quot; is case sensitive</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="comment"># &quot;roleRef&quot; specifies the binding to a Role / ClusterRole</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span> <span class="comment">#this must be Role or ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span> <span class="comment"># this must match the name of the Role or ClusterRole you wish to bind to</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><p><strong>RoleBinding</strong> 还可以引用 <strong>ClusterRole</strong>，以将该 <strong>ClusterRole</strong> 中定义的权限授予 <strong>RoleBinding</strong> 命名空间内的资源。这种引用允许您在集群中定义一组通用角色，然后在多个命名空间中重用它们。</p><p>例如以下的例子，即使以下 <strong>RoleBinding</strong> 引用了 <strong>ClusterRole</strong>，<strong>dave</strong>（名称区分大小写）也只能读取 <strong>development</strong> 命名空间中的<strong>Secrets</strong>，因为 <strong>RoleBinding</strong> 的命名空间（在 <strong>metadata</strong> 中）是 <strong>development</strong> ，<strong>RoleBinding</strong> 具有命名空间限制。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="comment"># This role binding allows &quot;dave&quot; to read secrets in the &quot;development&quot; namespace.</span></span><br><span class="line"><span class="comment"># You need to already have a ClusterRole named &quot;secret-reader&quot;.</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">read-secrets</span></span><br><span class="line">  <span class="comment"># The namespace of the RoleBinding determines where the permissions are granted.</span></span><br><span class="line">  <span class="comment"># This only grants permissions within the &quot;development&quot; namespace.</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">development</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">dave</span> <span class="comment"># Name is case sensitive</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secret-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><h4 id="ClusterRoleBinding-示例"><a href="#ClusterRoleBinding-示例" class="headerlink" title="ClusterRoleBinding 示例"></a>ClusterRoleBinding 示例</h4><p>要在整个集群上授予权限，可以使用 <strong>ClusterRoleBinding</strong>。以下 <strong>ClusterRoleBinding</strong> 允许 <strong>manager</strong> 组（Group）中的任意用户读取任意命名空间中的 secret。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="comment"># This cluster role binding allows anyone in the &quot;manager&quot; group to read secrets in any namespace.</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">read-secrets-global</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">manager</span> <span class="comment"># Name is case sensitive</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secret-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><p>更多的 <strong>rbac</strong> 使用方法，请参考：<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/">1.service-accounts-admin</a><br><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">2.configure-service-account</a><br><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">3.access-authn-authz-rbac</a></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> serviceaccount </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过 webhooks 自动更新应用</title>
      <link href="/rancher/webhooks/"/>
      <url>/rancher/webhooks/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/webhooks/" target="_blank" title="https://www.xtplayer.cn/rancher/webhooks/">https://www.xtplayer.cn/rancher/webhooks/</a></p><p>当流水线构建完镜像并成功推送到镜像仓库后，根据一些业务需求，可能希望自动去更新相应的应用。目前 rancher 2.x 自身不支持这样的自动触发更新的功能，所以需要一个附加的组件去触发自动更新。</p><h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>本工具基于 <code>https://github.com/adnanh/webhook.git</code>  定制， <code>Dockerfile</code> 地址: <code>https://github.com/cnrancher/docker-webhook.git</code>。</p><p>镜像下载地址: <code>registry.cn-shenzhen.aliyuncs.com/rancher/webhook:latest</code></p><ol><li><p>支持镜像仓库类型:</p><p> 阿里云镜像仓库: <code>https://cr.console.aliyun.com</code><br> Docker Hub: <code>http://hub.docker.com</code><br> 自定义 webhooks（比如 harbor）</p></li><li><p>支持邮件通知</p></li></ol><h2 id="准备配置文件"><a href="#准备配置文件" class="headerlink" title="准备配置文件"></a>准备配置文件</h2><blockquote><p>建议把 webhooks 作为系统服务运行在 <code>system</code> 项目下。</p></blockquote><ol><li><p>登录 Rancher UI 切换到 <code>system</code> 项目下，然后依次进入 <code>资源\配置映射</code>，点击页面右上角的 <code>添加配置映射</code>。</p></li><li><p>修改模板中对应的参数:</p><ul><li><code>&lt;webhooks_id&gt;</code>: 此 <code>webhooks-id</code> 具有唯一性，不能重复。建议设置为服务名，比如 <code>cnrancher_website</code>；</li><li><code>&lt;token&gt;</code>: 设置一个 token 值用于匹配校验；</li><li><code>&lt;workload&gt;</code>: 指定一个应用，书写格式为 <code>类型/Workload</code>,例如: <code>deployment/webhooks、daemonset/webhooks</code>；</li><li><code>&lt;namespaces&gt;</code>: 指定服务所在的命名空间；</li><li><code>&lt;container&gt;</code>: 指定容器名称，对于一个有多容器的 Pod，升级时需要指定容器名称；</li><li><code>&lt;MAIL_TO&gt;</code>: 收件人邮箱地址；</li><li><code>&lt;NET_TYPE&gt;</code>: 如果阿里云的镜像仓库，可在 url 中添加 <code>net_type</code> 指定网络类型: 1.公共网络: 不指定默认为公共网络，2.专有网络: <code>net_type=vpc</code>，3.经典网络: <code>net_type=internal</code>；</li></ul><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;webhooks-id&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;execute-command&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/webhooks.sh&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;command-working-directory&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/home&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;response-message&quot;</span><span class="punctuation">:</span> <span class="string">&quot;I got the payload!&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;include-command-output-in-response&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;include-command-output-in-response-on-error&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;trigger-rule-mismatch-http-response-code&quot;</span><span class="punctuation">:</span> <span class="number">500</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;response-headers&quot;</span><span class="punctuation">:</span></span><br><span class="line">        <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Access-Control-Allow-Origin&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;pass-arguments-to-command&quot;</span><span class="punctuation">:</span></span><br><span class="line">        <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;entire-payload&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">          <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;pass-environment-to-command&quot;</span><span class="punctuation">:</span></span><br><span class="line">        <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;envname&quot;</span><span class="punctuation">:</span> <span class="string">&quot;APP_NS&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ns&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;envname&quot;</span><span class="punctuation">:</span> <span class="string">&quot;APP_WORKLOAD&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;workload&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;envname&quot;</span><span class="punctuation">:</span> <span class="string">&quot;APP_CONTAINER&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;container&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;envname&quot;</span><span class="punctuation">:</span> <span class="string">&quot;REPO_TYPE&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;repo_type&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;envname&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NET_TYPE&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;net_type&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;envname&quot;</span><span class="punctuation">:</span> <span class="string">&quot;MAIL_TO&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;MAIL_TO&gt;&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;trigger-rule&quot;</span><span class="punctuation">:</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;and&quot;</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span></span><br><span class="line">                    <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;value&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;token&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span></span><br><span class="line">                        <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;token&quot;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span></span><br><span class="line">                    <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;value&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;namespaces&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span></span><br><span class="line">                        <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ns&quot;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span></span><br><span class="line">                    <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;value&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;workload&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span></span><br><span class="line">                        <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;workload&quot;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span></span><br><span class="line">                    <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;value&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;container&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span></span><br><span class="line">                        <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;url&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;container&quot;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure></li><li><p>填写<code>添加配置映射</code>参数，其中:</p><ul><li><code>名称</code>: 可以随意填写;</li><li><code>键</code>: 以<code>.json</code> 结尾的文件名，比如 <code>cnrancher.json</code>;</li><li><code>值</code>: 设置为上一步中修改的配置文件;</li><li>如果有多个服务，可以添加多个键值对，如图:</li></ul><img src="/rancher/webhooks/image-20190314173250612.80c369ba.png" class="" title="image-20190314173250612"></li></ol><h2 id="webhooks-安装"><a href="#webhooks-安装" class="headerlink" title="webhooks 安装"></a>webhooks 安装</h2><p>依次点击 <code>system 项目\工作负载\工作负载</code>，点击右侧部署服务。</p><ol><li><p>配置服务名称和镜像</p><p><code>registry.cn-shenzhen.aliyuncs.com/rancher/webhook</code></p><img src="/rancher/webhooks/image-20190314173915552.4b816c75.png" class="" title="image-20190314173915552"></li><li><p>对外服务</p><ul><li>服务默认监听端口为 <code>9000</code>，如果使用 NodePort 提供服务，则安以下方式配置；</li></ul><img src="/rancher/webhooks/image-20190923184019854.3cb7cbf2.png" class="" title="image-20190923184019854"><ul><li>如果使用负载均衡服务，在负载均衡页面添加相应规则，如果使用 https，记得配置 ssl 证书；</li></ul><img src="/rancher/webhooks/image-20190923184601819.0a1963f8.png" class="" title="image-20190923184601819"></li><li><p>配置环境变量</p><p><code>WEBHOOK_CMD=-template</code>: 系统命令；<br><code>MAIL_SMTP_PORT=</code>: 邮箱 SMTP 服务器端口；<br><code>MAIL_SMTP_SERVER=</code>: 邮箱 SMTP 服务器地址，(需要 base64 加密: echo &lt;SMTP 服务器地址&gt; | base64 )；<br><code>MAIL_FROM=</code> : 发件人邮箱，(需要 base64 加密: echo &lt;发件人邮箱&gt; | base64 )；<br><code>MAIL_PASSWORD=</code>: 发件人邮箱密码，(需要 base64 加密: echo &lt;密码&gt; | base64 )；<br><code>MAIL_CACERT=</code>: 自签名 CA 证书，邮箱服务器采用自签名 ssl 证书时使用(需要 base64 加密: cat &lt;ca 文件&gt; | base64 )；<br><code>MAIL_TLS_CHECK=</code>: 是否开启 TLS 认证(false&#x2F;true,默认 true)；</p><blockquote><p>请以文字说明为准，图片是早期版本截图，图片中有些地方未做 base64 加密</p></blockquote><ul><li>常用邮箱配置(qq,163 等)</li></ul><img src="/rancher/webhooks/image-20190315232842668.db3bee8c.png" class="" title="image-20190315232842668"><ul><li>自签名证书邮箱服务器</li></ul><img src="/rancher/webhooks/image-20190315233201822.0036b8a5.png" class="" title="image-20190315233201822"><ul><li>不启用 TLS 认证邮箱</li></ul><img src="/rancher/webhooks/image-20190315233320815.5c6c6f34.png" class="" title="image-20190315233320815"></li><li><p>配置健康检查</p><p>端口: <code>9000</code></p><img src="/rancher/webhooks/image-20190314174424562.24eec45c.png" class="" title="image-20190314174424562"></li><li><p>配置数据卷</p><ul><li>选择配置映射卷</li></ul><img src="/rancher/webhooks/image-20190314174524973.0e3afb15.png" class="" title="image-20190314174524973"><ul><li>配置映射名: 选择前面创建的配置映射;</li><li>容器路径: <code>/etc/webhook/source</code>;</li><li>其他参数保持默认;</li></ul><img src="/rancher/webhooks/image-20190314174834334.61433ce0.png" class="" title="image-20190314174834334"></li><li><p>最后点击启动，启动后查看日志，可以看到当前监听的服务</p><img src="/rancher/webhooks/image-20190314175308608.6b8b2964.png" class="" title="image-20190314175308608"></li><li><p>设置 <code>serviceaccounts</code></p><p>这一步相对比较重要，webhooks 服务需要 <code>serviceaccounts</code> 才可以正常的与 K8S 通信。因为目前 Rancher UI 不支持设置 <code>serviceaccounts</code>，所以需要编辑 <code>yaml</code> 文件来配置 <code>serviceaccounts</code>。为了方便，这里复用了 rancher 组件使用的 <code>serviceaccounts</code> 账号 <code>cattle</code>，具有集群管理员角色，您也可以根据需要定制 <code>serviceaccounts</code> 角色。</p><ul><li>如图，选择 <code>查看/编辑 YAML</code></li></ul><img src="/rancher/webhooks/image-20190314212013510.0907c0cb.png" class="" title="image-20190314212013510"><ul><li>在 <code>securityContext: &#123;&#125;</code> 下边添加 <code>serviceAccount: cattle</code> 和 <code>serviceAccountName: cattle</code>；</li></ul><img src="/rancher/webhooks/image-20190314212124745.a3a71d69.png" class="" title="image-20190314212124745"><ul><li>最后点击<code>保存</code></li></ul></li></ol><h2 id="webhooks-触发地址"><a href="#webhooks-触发地址" class="headerlink" title="webhooks 触发地址"></a>webhooks 触发地址</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http(s)://&lt;webhooks_url&gt;/hooks/\</span><br><span class="line">&lt;webhooks_id&gt;?\</span><br><span class="line">token=&lt;token&gt;&amp;\</span><br><span class="line">ns=&lt;namespaces&gt;&amp;\</span><br><span class="line">workload=&lt;workload&gt;&amp;\</span><br><span class="line">container=&lt;container&gt;&amp;\</span><br><span class="line">repo_type=&lt;repo_type&gt;</span><br></pre></td></tr></table></figure><ol><li><p>如果是阿里云的镜像仓库，可在 url 中添加 <code>net_type</code> 指定网络类型:</p><ul><li><p>公共网络:</p><p>如果不指定，则默认为公共网络拉取镜像</p></li><li><p>专有网络:</p><p><code>net_type=vpc</code></p></li><li><p>经典网络:</p><p><code>net_type=internal</code></p></li></ul><p>其中 <code>&lt;webhooks_id&gt;、&lt;namespaces&gt;、&lt;workload&gt;、&lt;container&gt;</code> 对应模板中的参数，<code>&lt;repo_type&gt;</code> 支持:<code>aliyun</code>、<code>dockerhub</code>、<code>custom</code>。</p></li></ol><h2 id="配置仓库触发"><a href="#配置仓库触发" class="headerlink" title="配置仓库触发"></a>配置仓库触发</h2><h3 id="Aliyun"><a href="#Aliyun" class="headerlink" title="Aliyun"></a>Aliyun</h3><ol><li><p>浏览器访问 <code>https://cr.console.aliyun.com</code> 进入容器镜像服务管理界面；</p></li><li><p>选择一个需要添加自动触发功能的仓库，点击右侧的管理；</p></li><li><p>在切换的新窗口左上角选择触发器；</p><img src="/rancher/webhooks/image-20190314181124857.d0b70607.png" class="" title="image-20190314181124857"></li><li><p>webhooks 触发消息示例：</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;push_data&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;digest&quot;</span><span class="punctuation">:</span>       <span class="string">&quot;sha256:f66daa126e9fcac4e2d0b7131e78ffd5d8e0012a1e6cb150a953e5be8da5d      980&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;pushed_at&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-03-13 23:38:07&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;tag&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;repository&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;date_created&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-03-05 13:47:43&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;webhook&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;namespace&quot;</span><span class="punctuation">:</span> <span class="string">&quot;rancher_cn&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;region&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cn-shanghai&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;repo_authentication_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NO_CERTIFIED&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;repo_full_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;rancher_cn/webhook&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;repo_origin_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NO_CERTIFIED&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;repo_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;PUBLIC&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><ol><li><p>浏览器访问 <code>https://cloud.docker.com/repository/list</code>，输入账号和密码后将进入仓库列表;</p></li><li><p>点击需要添加 webhooks 仓库，然后点击 webhooks;</p><img src="/rancher/webhooks/image-20190314182034766.7fd5c38b.png" class="" title="image-20190314182034766"></li><li><p>填写相关参数，点击右侧的加号；</p><img src="/rancher/webhooks/image-20190314182530479.57ced74e.png" class="" title="image-20190314182530479"></li><li><p>webhooks 触发消息示例：</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;push_data&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;pushed_at&quot;</span><span class="punctuation">:</span> <span class="number">1552553567</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;images&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;tag&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;pusher&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hongxiaolu&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;callback_url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;repository&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;status&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Active&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iperf3&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;is_trusted&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;full_description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;# iperf3\niperf3\n&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;repo_url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;owner&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hongxiaolu&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;is_official&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;is_private&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iperf3&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;namespace&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hongxiaolu&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;star_count&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;comment_count&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;date_created&quot;</span><span class="punctuation">:</span> <span class="number">1540013520</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;dockerfile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;# &quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;repo_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hongxiaolu/iperf3&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="自定义-webhooks"><a href="#自定义-webhooks" class="headerlink" title="自定义 webhooks"></a>自定义 webhooks</h3><p>如果是使用 Jenkins 自定义构建镜像，可以设置 <code>repo_type</code>&#x3D;<code>custom</code>。</p><p>在 Jenkins 构建 <code>task</code> 中，在镜像 <code>push</code> 操作后增加一个<code>执行 shell 命令</code>的步骤。这个操作主要是在镜像成功推送到镜像仓库后发出 <code>POST 消息</code>去触发 webhooks，这步中需要把上一步推送的<code>镜像仓库地址</code>,<code>镜像命名空间</code>,<code>镜像名</code>，以及镜像 <code>tag</code> 作为变量传递到这一步，这样在发送 <code>POST 消息</code>才可以把相关的镜像信息传递给 webhooks,从而触发服务升级。</p><p>示例 <code>POST 消息</code>:</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">curl -X POST \</span><br><span class="line">  &#x27;http(s)<span class="punctuation">:</span><span class="comment">//&lt;webhooks_url&gt;/hooks/&lt;webhooks_id&gt;?\</span></span><br><span class="line">  token=&lt;token&gt;&amp;\</span><br><span class="line">  ns=&lt;namespaces&gt;&amp;\</span><br><span class="line">  workload=&lt;workload&gt;&amp;\</span><br><span class="line">  container=&lt;container&gt;&amp;\</span><br><span class="line">  repo_type=custom&#x27; \</span><br><span class="line">  -H &#x27;Content-Type<span class="punctuation">:</span> application/json&#x27; \</span><br><span class="line">  -H &#x27;cache-control<span class="punctuation">:</span> no-cache&#x27; \</span><br><span class="line">  -d &#x27;<span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;push_data&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;tag&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;images_tag&#125;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;repository&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;repo_url&quot;</span><span class="punctuation">:</span> $<span class="punctuation">&#123;</span>images_repo_url<span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;images_name&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;namespace&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;images_namespace&#125;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span>&#x27;</span><br></pre></td></tr></table></figure><h2 id="触发-webhooks"><a href="#触发-webhooks" class="headerlink" title="触发 webhooks"></a>触发 webhooks</h2><ol><li><p>配置完以上参数，提交代码到 git 仓库后将会自动触发阿里云仓库或者 dockerhub 的自动构建，创建自动构建方法请自行查阅相关文档。</p><img src="/rancher/webhooks/image-20190314212701691.56020be2.png" class="" title="image-20190314212701691"><img src="/rancher/webhooks/image-20190314213123621.03b58f67.png" class="" title="image-20190314213123621"></li><li><p>当镜像构建完成并推送到仓库后，会触发 webhooks 消息到预先配置的地址，从触发器也可以查看历史记录。</p><img src="/rancher/webhooks/image-20190314213215303.64186b53.png" class="" title="image-20190314213215303"></li><li><p>webhooks 服务收到消息后，会马上触发服务的升级。查看 webhooks 服务的日志，可以看到已经成功升级。</p><img src="/rancher/webhooks/image-20190314213051682.0981509a.png" class="" title="image-20190314213051682"><img src="/rancher/webhooks/image-20190314205425237.c373646c.png" class="" title="image-20190314205425237"></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> webhooks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher server 管理员密码重置</title>
      <link href="/rancher/password-reset/"/>
      <url>/rancher/password-reset/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/password-reset/" target="_blank" title="https://www.xtplayer.cn/rancher/password-reset/">https://www.xtplayer.cn/rancher/password-reset/</a></p><p>目前 Rancher 不支持通过邮件或者其他 web 方式找回密码， 如果忘记 admin 密码，则需要通过在 Rancher server 容器中执行命令来重置密码，运行重置命令后将生成随机的字符串密码。</p><h2 id="Rancher-单节点安装"><a href="#Rancher-单节点安装" class="headerlink" title="Rancher 单节点安装"></a>Rancher 单节点安装</h2><p>在 Rancher 运行的主机上，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -ti &lt;container_id&gt; reset-password</span><br></pre></td></tr></table></figure><p>显示结果：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">New password <span class="keyword">for</span> default admin user (user-xxxxx):</span><br><span class="line">&lt;new_password&gt;</span><br></pre></td></tr></table></figure><h2 id="Rancher-HA-安装"><a href="#Rancher-HA-安装" class="headerlink" title="Rancher HA 安装"></a>Rancher HA 安装</h2><p>在安装有 <code>kubectl</code> 主机上，指定 kubeconfig 配置文件，然后运行以下命令，主机上需要安装 jq 工具。</p><blockquote><p>假设 kubectl 配置文件在当前目录下</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeconfig=./kube_config_rancher-cluster.yml</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig <span class="variable">$kubeconfig</span> -n cattle-system <span class="built_in">exec</span> -ti \</span><br><span class="line">  $(kubectl --kubeconfig <span class="variable">$kubeconfig</span> get pods -n cattle-system -o json | \</span><br><span class="line">  jq -r <span class="string">&#x27;.items [] | select(.spec.containers[].name==&quot;cattle-server&quot;) | .metadata.name&#x27;</span>) \</span><br><span class="line">  --reset-password</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">New password <span class="keyword">for</span> default admin user (user-xxxxx):</span><br><span class="line">&lt;new_password&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> 重置密码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher 单节点安装迁移 HA 安装</title>
      <link href="/rancher/backup-restore/single-to-ha/"/>
      <url>/rancher/backup-restore/single-to-ha/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/backup-restore/single-to-ha/" target="_blank" title="https://www.xtplayer.cn/rancher/backup-restore/single-to-ha/">https://www.xtplayer.cn/rancher/backup-restore/single-to-ha/</a></p><h2 id="Rancher-单节点安装"><a href="#Rancher-单节点安装" class="headerlink" title="Rancher 单节点安装"></a>Rancher 单节点安装</h2><blockquote><p>以下步骤创建用于演示迁移的 Rancher 单节点环境，如果您需要迁移正式环境可以跳过此步骤。</p></blockquote><ol><li><p>执行以下 docker 命令运行单个 Rancher Server 服务</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -d -p 8443:443 -p 8880:80 -v /home/rancher/:/var/lib/rancher/rancher/rancher:v2.3.0</span><br></pre></td></tr></table></figure></li><li><p>等容器初始化完成后，通过节点 IP 访问 Rancher Server UI，设置密码并登录。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016142653127.bfb75310.png" class="" title="image-20191016142653127"><img src="/rancher/backup-restore/single-to-ha/image-20191016142719866.e60dc1cc.png" class="" title="image-20191016142719866"></li></ol><h2 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h2><blockquote><p>以下步骤创建用于演示的业务集群，用来验证 Rancher 单节点安装迁移到 HA 后数据是否丢失，如果您需要迁移正式环境可以跳过此步骤。</p></blockquote><ol><li><p>登录 Rancher UI 后，添加一个自定义集群</p><img src="/rancher/backup-restore/single-to-ha/image-20191016142808830.c72a486a.png" class="" title="image-20191016142808830"></li><li><p>授权集群访问地址 设置为启用，FQDN 和证书可以不用填写。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016142850886.ba5a2e45.png" class="" title="image-20191016142850886"><blockquote><p>警告：这一步很关键。如果 Rancher 切换到 HA 后，因为地址或者 token 或者证书的变更，将会导致 Agent 无法连接 Rancher Server。在迁移到 HA 后，需要通过 kubectl 去编辑配置文件更新一些 Agent 相关的参数。默认 UI 上的 kube 配置文件是通过 Agent 代理连接到 K8S，如果 Agent 无法连接 Rancher Server，则通过这个 KUBE 配置文件无法访问 K8S 集群。开启授权集群访问地址功能会生成多个 Contexts Cluster，这些 Contexts Cluster 是直连 K8S，不通过 Agent 代理。如果业务集群未开启这个功能，可以通过编辑集群来开启这个功能。</p></blockquote></li><li><p>点击 下一步 ，根据预先分配的节点角色选择需要的角色，然后复制命令到主机终端执行。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016143133778.eb4ca75c.png" class="" title="image-20191016143133778"></li><li><p>集群部署完成后，进入集群首页，点击 <code>kubeconfig 文件</code>按钮。在弹窗页面中复制 kubeconfg 配置文件到文本编辑备用。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016143513361.5a28d714.png" class="" title="image-20191016143513361"><img src="/rancher/backup-restore/single-to-ha/image-20191016143636998.5937b767.png" class="" title="image-20191016143636998"></li></ol><h2 id="部署测试应用"><a href="#部署测试应用" class="headerlink" title="部署测试应用"></a>部署测试应用</h2><ol><li><p>进入 default 项目，随意部署一个测试应用。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016143950402.f80b226b.png" class="" title="image-20191016143950402"></li><li><p>进入应用商店，部署一个测试应用。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016144352807.226b19e3.png" class="" title="image-20191016144352807"></li></ol><h2 id="备份-Rancher-数据"><a href="#备份-Rancher-数据" class="headerlink" title="备份 Rancher 数据"></a>备份 Rancher 数据</h2><p>执行 <code>docker exec -ti &lt;Containers_ID&gt; bash</code> 进入 Rancher Server 容器，然后执行 <code>etcdctl snapshot save /var/lib/rancher/snapshot.db</code> 进行数据备份。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016144702116.b41e358f.png" class="" title="image-20191016144702116"><p>因为 <code>/var/lib/rancher/</code> 是映射到主机的 <code>/home/rancher/</code> 目录，所以备份的数据可以直接在主机的&#x2F;home&#x2F;rancher&#x2F;下获取。</p><h2 id="ETCD-数据恢复"><a href="#ETCD-数据恢复" class="headerlink" title="ETCD 数据恢复"></a>ETCD 数据恢复</h2><ol><li><p>将 Rancher 备份文件拷贝到需要部署 Rancher HA 的主机上，比如放在 <code>/home/</code> 目录，所有节点都要拷贝。如果节点之前安装过 K8S 集群，请确保节点已经初始化过（了解<a href="https://www.rancher.cn/docs/rancher/v2.x/cn/install-prepare/remove-node/">节点清理</a>）。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016145903388.e27b4dad.png" class="" title="image-20191016145903388"></li><li><p>获取 etcdctl</p><p>访问 <code>https://github.com/etcd-io/etcd/releases/</code> 下载 ETCD 二进制压缩包，解压后获得 etcdctl 二进制文件。拷贝 etcdctl 二进制文件到 Rancher HA 的主机上。并给文件可执行权限 <code>chmod +x etcdctl</code>。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016150137910.d1a12c57.png" class="" title="image-20191016150137910"></li><li><p>在 Rancher HA 所有节点执行以下命令将 ETCD 备份文件恢复到默认路径</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NODE_IP=<span class="string">&#x27;&#x27;</span> (当前节点 ip)</span><br><span class="line">ETCD_NAME=$( <span class="built_in">echo</span> etcd-$( <span class="built_in">echo</span> <span class="variable">$NODE_IP</span> | sed <span class="string">&#x27;s/\./-/g&#x27;</span> ) )</span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;<span class="subst">$( echo $ETCD_NAME=https://$&#123;NODE_IP&#125;:2380 )</span>&quot;</span></span><br><span class="line"></span><br><span class="line">ETCDCTL_API=3 etcdctl snapshot restore xxxxxxxxx \</span><br><span class="line">--data-dir=<span class="string">&quot;/var/lib/etcd&quot;</span> \</span><br><span class="line">--initial-cluster=<span class="variable">$&#123;ETCD_INITIAL_CLUSTER&#125;</span> \</span><br><span class="line">--initial-advertise-peer-urls=<span class="string">&quot;https://<span class="variable">$&#123;NODE_IP&#125;</span>:2380&quot;</span></span><br></pre></td></tr></table></figure><img src="/rancher/backup-restore/single-to-ha/image-20191016150556120.8d31391b.png" class="" title="image-20191016150556120"><p>以上操作将把数据恢复到 ETCD 默认存储路径。</p></li></ol><h2 id="LOCAL-K8S-集群部署"><a href="#LOCAL-K8S-集群部署" class="headerlink" title="LOCAL K8S 集群部署"></a>LOCAL K8S 集群部署</h2><ol><li><p>根据文档<a href="https://www.rancher.cn/docs/rke/latest/cn/example-yamls/">示例配置</a> 创建 RKE 配置文件。</p></li><li><p>执行 rke 命令创建 LOCAL K8S 集群</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke up --config cluster.yml</span><br></pre></td></tr></table></figure><img src="/rancher/backup-restore/single-to-ha/image-20191016152133693.57f266c1.png" class="" title="image-20191016152133693"></li><li><p>检查 K8S 集群运行状态</p><img src="/rancher/backup-restore/single-to-ha/image-20191016165114635.b64d8ccf.png" class="" title="image-20191016165114635"><img src="/rancher/backup-restore/single-to-ha/image-20191016165143557.52caeb15.png" class="" title="image-20191016165143557"></li></ol><h2 id="Rancher-HA-安装"><a href="#Rancher-HA-安装" class="headerlink" title="Rancher HA 安装"></a>Rancher HA 安装</h2><blockquote><p>警告: Rancher HA 的版本需要大于或者等于 Rancher 单节点的版本。</p></blockquote><ol><li><p>根据<a href="https://www.rancher.cn/docs/rancher/v2.x/cn/install-prepare/self-signed-ssl/">自签名 ssl 证书</a>文档创建自签名证书或者配置权威证书；</p></li><li><p>根据<a href="https://www.rancher.cn/docs/rancher/v2.x/cn/installation/">安装文档</a>进行 Rancher HA 安装；</p></li><li><p>安装完成后访问 Rancher UI，可以看到之前添加的 <code>test</code> 集群。（错误提示是因为 Rancher URL 改变，cluster Agent 无法连接 Rancher Server。）</p><img src="/rancher/backup-restore/single-to-ha/image-20191016170347456.6c38e55d.png" class="" title="image-20191016170347456"></li></ol><h2 id="Rancher-HA-配置"><a href="#Rancher-HA-配置" class="headerlink" title="Rancher HA 配置"></a>Rancher HA 配置</h2><ol><li><p>修改 Rancher URL 配置</p><p>在<code>全局|系统设置</code>中找到 <code>server-url</code>，如果 Rancher HA 地址与 Rancher 单节点地址不一致，则修改地址为 Rancher HA 地址。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016170849825.bb6a9895.png" class="" title="image-20191016170849825"></li><li><p>然后新开窗口，在浏览器地址栏输入 <code>https://rancher-url/v3/clusters/local/clusterregistrationtokens</code> 切换到注册命令接口页面。</p></li><li><p>在注册命令接口页找到 <code>insecureCommand</code> 字段，复制字段后面的 yaml 文件链接。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016185613510.4936d731.png" class="" title="image-20191016185613510"></li><li><p>通过命令 <code>curl -o local.yaml https://192.168.3.115:30303/v3/import/8vvmfwjwd6mkf8g2f769cnhh4s64s5jcnxgv5t6v5bdxk84tf6zztl.yaml</code> 把文件下载到本地，文件名建议以集群 ID 命名，用于区分。</p></li><li><p>相应的，其他业务集群也需要按照以上步骤，通过访问 <code>https://&lt;rancher_url&gt;/v3/clusters/&lt;cluster_id&gt;/clusterregistrationtokens</code> 去获取 <code>insecureCommand</code> 对应的 <code>yaml</code> 文件，然后下载 yaml 文件到本地。在切换到对应集群后，在地址栏可以看到 <code>&lt;cluster_id&gt;</code>。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016191846530.e09740ab.png" class="" title="image-20191016191846530"><blockquote><p>在更换地址后，<code>clusterregistrationtokens</code> 接口中可能会生成多组 <code>insecureCommand</code>，如下图。对应的，在 <code>insecureCommand</code> 上方有 <code>createdTS</code> 字段，<code>createdTS</code> 数值越大，对应的 yaml 配置最新，请选择最大数值 <code>createdTS</code> 下的 <code>insecureCommand</code> 后的 <code>yaml</code> 链接。</p></blockquote><img src="/rancher/backup-restore/single-to-ha/image-20191017182734715.a45ec16f.png" class="" title="image-20191017182734715"></li><li><p>根据前面步骤中保存的 kubecfg 配置文件，和上一步骤中保存的对应集群注册 <code>yaml 文件</code>，通过 kubectl 工具去执行。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--kubeconfig=xxxx <span class="comment"># 指定集群配置文件</span></span><br><span class="line">--context=xxxx  <span class="comment"># 切换授权集群访问地址（local 集群不用切换，local 集群配置默认是直连 K8S 集群）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># local 示例</span></span><br><span class="line">kubectl --kubeconfig=kube_config_cluster.yml apply -f local.yaml</span><br><span class="line"><span class="comment"># 业务集群示例</span></span><br><span class="line">kubectl --kubeconfig=kube_c-b49gh.yml --context test-node apply -f c-b49gh.yaml</span><br></pre></td></tr></table></figure></li><li><p>执行完以上命令后业务集群将自动连接 Rancher</p><img src="/rancher/backup-restore/single-to-ha/image-20191016195215819.13ff7ae2.png" class="" title="image-20191016195215819"><p>之前部署的应用正常运行，未丢失。</p><img src="/rancher/backup-restore/single-to-ha/image-20191016195309268.eba42a2e.png" class="" title="image-20191016195309268"></li></ol>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> backup-restore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 迁移 </tag>
            
            <tag> HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher-K8S 轮换证书</title>
      <link href="/rancher/rotate-cert/"/>
      <url>/rancher/rotate-cert/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/rotate-cert/" target="_blank" title="https://www.xtplayer.cn/rancher/rotate-cert/">https://www.xtplayer.cn/rancher/rotate-cert/</a></p><blockquote><p><strong>警告</strong> 如果您的证书已经过期，请先不要升级 <code>Rancher Server</code>，根据 <a href="/rancher/rotate-cert/#%E8%AF%81%E4%B9%A6%E5%B7%B2%E8%BF%87%E6%9C%9F">证书已过期导致无法连接 k8s</a> 进行处理。</p></blockquote><p>默认情况下，Kubernetes 集群使用 ssl 证书来加密通信，Rancher 自动为集群生成证书。在 <code>Rancher v2.0.14、v2.1.9</code> 之前的版本，Rancher 创建的集群 ssl 证书默认有效期为 1 年 (CA 证书默认 10 年)，在 <code>Rancher v2.0.14、v2.1.9</code> 以及更高的版本中，Rancher 创建的集群 ssl 证书默认为 10 年 (CA 证书默认 10 年)。</p><h2 id="通过-UI-轮换证书-业务集群"><a href="#通过-UI-轮换证书-业务集群" class="headerlink" title="通过 UI 轮换证书 (业务集群)"></a>通过 UI 轮换证书 (业务集群)</h2><p><em>可用版本: Rancher v2.2.0 +</em></p><p>在 Rancher v2.2.0 以及更高版本，可通过 UI 的证书轮换功能对集群证书进行更新，此功能适用于 <code>自定义安装的集群</code>。证书轮换之后，Kubernetes 组件将自动重新启动，重启不影响应用 Pod，重启时间需要 3 到 5 分钟。</p><ul><li>证书轮换可用于下列服务:<ul><li>etcd</li><li>kubelet</li><li>kube-apiserver</li><li>kube-proxy</li><li>kube-scheduler</li><li>kube-controller-manager</li></ul></li><li>通过 UI 轮换证书，目前支持:<ul><li>批量更新所有服务证书 (CA 证书不变)</li><li>更新某个指定服务 (CA 证书不变)</li></ul></li></ul><h3 id="重要-集群更新"><a href="#重要-集群更新" class="headerlink" title="(重要) 集群更新"></a>(重要) 集群更新</h3><p>如果 Rancher 版本是从 <code>v2.x.x 升级到 2.2.x</code>，则需要先做一次<code>集群更新</code>操作。</p><ol><li>进入<code>全局\集群</code>视图；</li><li>选择<code>目标集群</code>右侧的<code>省略号</code>菜单，选择升级；<img src="/rancher/rotate-cert/image-20190423132857924.f6e38a3c.png" class="" title="image-20190423132857924"></li><li>点击右侧<code>显示高级选项</code>，检查 <code>ETCD 备份轮换</code>功能是否开启，建议开启此功能；<img src="/rancher/rotate-cert/image-20190423133224065.bfafe04c.png" class="" title="image-20190423133224065"></li><li>在<code>授权集群访问地址</code>中，检查功能是否已开启，建议开始此功能，下边的域名可以不用填写；<img src="/rancher/rotate-cert/image-20190527112849881.44c90721.png" class="" title="image-20190527112849881"></li><li>最后点击<code>保存</code>，集群将自动进行更新<img src="/rancher/rotate-cert/image-20190423133309672.e0aade98.png" class="" title="image-20190423133309672"></li></ol><h3 id="轮换证书"><a href="#轮换证书" class="headerlink" title="轮换证书"></a>轮换证书</h3><ol><li><p>进入<code>全局\集群</code>视图；</p></li><li><p>选择对应集群右侧的<code>省略号</code>菜单，选择更新证书有效期；<img src="/rancher/rotate-cert/image-20190423112648449.7d8e6ccc.png" class="" title="image-20190423112648449"></p></li><li><p>选择更新所有服务证书，并点击保存<img src="/rancher/rotate-cert/image-20190423132218317.3d3387a0.png" class="" title="image-20190423132218317"></p></li><li><p>集群将自动更新证书<img src="/rancher/rotate-cert/image-20190423132305491.f0957394.png" class="" title="image-20190423132305491"></p></li><li><p>因为证书改变，相应的 <code>token</code> 也会变化，在集群证书更新完成后，需要对连接 <code> API SERVER</code> 的 Pod 进行重建，以获取新的 <code>token</code>。</p><ul><li>cattle-system&#x2F;cattle-cluster-agent</li><li>cattle-system&#x2F;cattle-node-agent</li><li>cattle-system&#x2F;kube-api-auth</li><li>ingress-nginx&#x2F;nginx-ingress-controller</li><li>kube-system&#x2F;canal</li><li>kube-system&#x2F;kube-dns</li><li>kube-system&#x2F;kube-dns-autoscaler</li><li>其他应用 Pod</li></ul></li></ol><h2 id="通过-UI-API-轮换证书-业务集群"><a href="#通过-UI-API-轮换证书-业务集群" class="headerlink" title="通过 UI API 轮换证书 (业务集群)"></a>通过 UI API 轮换证书 (业务集群)</h2><p><em>可用版本: Rancher v2.0.14+ v2.1.9+</em></p><p>对于 <code>Rancher v2.0.14、v2.1.9</code> 以及更高版本，可通过 API 对集群证书进行更新。API 证书轮换将会同时对所有组件证书进行更新，不支持指定组件更新证书。</p><ol><li><p>在<code>全局</code>视图中，定位到需要更新证书的集群，然后点击右侧省略号菜单，然后点击 <code>API 查看</code><img src="/rancher/rotate-cert/image-20190527122402294.33393d5f.png" class="" title="image-20190527122402294"></p></li><li><p>点击右上方的 <code>RotateCertificates</code> <img src="/rancher/rotate-cert/image-20190527122838260.efe768bd.png" class="" title="image-20190527122838260"></p></li><li><p>点击<strong>Show Request</strong></p></li><li><p>点击 <strong>Send Request</strong><img src="/rancher/rotate-cert/image-20190527123018599.66fd0aa8.png" class="" title="image-20190527123018599"></p></li><li><p>因为证书改变，相应的 <code>token</code> 也会变化，在集群证书更新完成后，需要对连接 <code> API SERVER</code> 的 Pod 进行重建，以获取新的 <code>token</code>。</p><ul><li>cattle-system&#x2F;cattle-cluster-agent</li><li>cattle-system&#x2F;cattle-node-agent</li><li>cattle-system&#x2F;kube-api-auth</li><li>ingress-nginx&#x2F;nginx-ingress-controller</li><li>kube-system&#x2F;canal</li><li>kube-system&#x2F;kube-dns</li><li>kube-system&#x2F;kube-dns-autoscaler</li><li>其他应用 Pod</li></ul></li></ol><h2 id="RKE-集群证书轮换-local-集群和业务集群通用"><a href="#RKE-集群证书轮换-local-集群和业务集群通用" class="headerlink" title="RKE 集群证书轮换 (local 集群和业务集群通用)"></a>RKE 集群证书轮换 (local 集群和业务集群通用)</h2><p><em>可用版本: rke v0.2.0+</em></p><blockquote><p><strong>注意</strong> 如果以前是通过 <code>rke v0.2.0</code> 之前的版本创建的 Kubernetes 集群，在轮换证书前先执行 <code>rke up</code> 操作。</p></blockquote><ul><li>通过 RKE 轮换证书，目前支持:<ul><li>批量更新所有服务证书 (CA 证书不变)</li><li>更新某个指定服务 (CA 证书不变)</li><li>轮换 CA 和所有服务证书</li></ul></li></ul><ol><li><p>批量更新所有服务证书 (CA 证书不变)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke cert rotate</span><br><span class="line"></span><br><span class="line">INFO[0000] Initiating Kubernetes cluster</span><br><span class="line">INFO[0000] Rotating Kubernetes cluster certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kubernetes API server certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Controller certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Scheduler certificates</span><br><span class="line">INFO[0001] [certificates] Generating Kube Proxy certificates</span><br><span class="line">INFO[0001] [certificates] Generating Node certificate</span><br><span class="line">INFO[0001] [certificates] Generating admin certificates and kubeconfig</span><br><span class="line">INFO[0001] [certificates] Generating Kubernetes API server proxy client certificates</span><br><span class="line">INFO[0001] [certificates] Generating etcd-xxxxx certificate and key</span><br><span class="line">INFO[0001] [certificates] Generating etcd-yyyyy certificate and key</span><br><span class="line">INFO[0002] [certificates] Generating etcd-zzzzz certificate and key</span><br><span class="line">INFO[0002] Successfully Deployed state file at [./cluster.rkestate]</span><br><span class="line">INFO[0002] Rebuilding Kubernetes cluster with rotated certificates</span><br><span class="line">.....</span><br><span class="line">INFO[0050] [worker] Successfully restarted Worker Plane..</span><br></pre></td></tr></table></figure></li><li><p>更新指定服务 (CA 证书不变)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke cert rotate --service kubelet</span><br><span class="line">INFO[0000] Initiating Kubernetes cluster</span><br><span class="line">INFO[0000] Rotating Kubernetes cluster certificates</span><br><span class="line">INFO[0000] [certificates] Generating Node certificate</span><br><span class="line">INFO[0000] Successfully Deployed state file at [./cluster.rkestate]</span><br><span class="line">INFO[0000] Rebuilding Kubernetes cluster with rotated certificates</span><br><span class="line">.....</span><br><span class="line">INFO[0033] [worker] Successfully restarted Worker Plane..</span><br></pre></td></tr></table></figure></li><li><p>轮换 CA 和所有服务证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rke cert rotate --rotate-ca</span><br><span class="line"></span><br><span class="line">INFO[0000] Initiating Kubernetes cluster</span><br><span class="line">INFO[0000] Rotating Kubernetes cluster certificates</span><br><span class="line">INFO[0000] [certificates] Generating CA kubernetes certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kubernetes API server certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Controller certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Scheduler certificates</span><br><span class="line">INFO[0000] [certificates] Generating Kube Proxy certificates</span><br><span class="line">INFO[0000] [certificates] Generating Node certificate</span><br><span class="line">INFO[0001] [certificates] Generating admin certificates and kubeconfig</span><br><span class="line">INFO[0001] [certificates] Generating Kubernetes API server proxy client certificates</span><br><span class="line">INFO[0001] [certificates] Generating etcd-xxxxx certificate and key</span><br><span class="line">INFO[0001] [certificates] Generating etcd-yyyyy certificate and key</span><br><span class="line">INFO[0001] [certificates] Generating etcd-zzzzz certificate and key</span><br><span class="line">INFO[0001] Successfully Deployed state file at [./cluster.rkestate]</span><br><span class="line">INFO[0001] Rebuilding Kubernetes cluster with rotated certificates</span><br></pre></td></tr></table></figure></li><li><p>因为证书改变，相应的 <code>token</code> 也会变化，在集群证书更新完成后，需要对连接 <code>API SERVER</code> 的 Pod 进行重建，以获取新的 <code>token</code>。</p><ul><li>cattle-system&#x2F;cattle-cluster-agent</li><li>cattle-system&#x2F;cattle-node-agent</li><li>cattle-system&#x2F;kube-api-auth</li><li>ingress-nginx&#x2F;nginx-ingress-controller</li><li>kube-system&#x2F;canal</li><li>kube-system&#x2F;kube-dns</li><li>kube-system&#x2F;kube-dns-autoscaler</li><li>其他应用 Pod</li></ul></li></ol><h2 id="单容器-Rancher-Server-证书更新"><a href="#单容器-Rancher-Server-证书更新" class="headerlink" title="单容器 Rancher Server 证书更新"></a>单容器 Rancher Server 证书更新</h2><h3 id="证书未过期"><a href="#证书未过期" class="headerlink" title="证书未过期"></a>证书未过期</h3><ul><li>v2.0.14+ 、v2.1.9+</li></ul><ol><li><p>正常升级 rancher 版本到 v2.0.14+ 、v2.1.9+；</p></li><li><p>执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rancher_server_id=xxx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> c -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/certs/bundle.json /var/lib/rancher/management-state/certs/bundle.json-bak</span><br><span class="line"></span><br><span class="line">docker restart <span class="variable">$&#123;rancher_server_id&#125;</span></span><br></pre></td></tr></table></figure></li></ol><ul><li>v2.2.0+</li></ul><ol><li><p>正常升级 rancher 版本到 v2.2.0+</p></li><li><p>执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rancher_server_id=xxx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/tls/localhost.crt /var/lib/rancher/management-state/tls/localhost.crt-bak</span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/tls/localhost.key /var/lib/rancher/management-state/tls/localhost.key-bak</span><br><span class="line">docker restart <span class="variable">$&#123;rancher_server_id&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="证书已过期"><a href="#证书已过期" class="headerlink" title="证书已过期"></a>证书已过期</h3><p>如果证书已过期，那么 rancher server 无法正常运行。即使升级到 Rancher v2.0.14+ 、v2.1.9+、v2.2.0+ 也不会更新证书。如果出现这种情况，可以把主机时间往后调整一些，ssl 证书有效时间验证是基于主机时间来验证。</p><ul><li><p>执行以下命令调整主机时间：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 关闭 ntp 同步，防止时间自动更新回来</span></span><br><span class="line">timedatectl set-ntp <span class="literal">false</span></span><br><span class="line"><span class="comment"># 修改节点时间</span></span><br><span class="line">timedatectl set-time <span class="string">&#x27;2019-01-01 00:00:00&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>注意: 有的虚拟机安装了 <code>vmtool</code> 工具并开启了虚拟机与主机时间同步功能，这种情况下需要停止 <code>vmtool</code> 进程，不然时间会自动更新回来。</p></blockquote></li><li><p>v2.0.14+ 、v2.1.9+</p></li></ul><ol><li><p>正常升级 rancher 版本到 v2.0.14+ 、v2.1.9+；</p></li><li><p>执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rancher_server_id=xxx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> c -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/certs/bundle.json /var/lib/rancher/management-state/certs/bundle.json-bak</span><br><span class="line"></span><br><span class="line">docker restart <span class="variable">$&#123;rancher_server_id&#125;</span></span><br></pre></td></tr></table></figure></li></ol><ul><li>v2.2.0+</li></ul><ol><li><p>正常升级 rancher 版本到 v2.2.0+</p></li><li><p>执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rancher_server_id=xxx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/tls/localhost.crt /var/lib/rancher/management-state/tls/localhost.crt-bak</span><br><span class="line">docker <span class="built_in">exec</span> -ti <span class="variable">$&#123;rancher_server_id&#125;</span> <span class="built_in">mv</span> /var/lib/rancher/management-state/tls/localhost.key /var/lib/rancher/management-state/tls/localhost.key-bak</span><br><span class="line">docker restart <span class="variable">$&#123;rancher_server_id&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="故障处理"><a href="#故障处理" class="headerlink" title="故障处理"></a>故障处理</h2><h3 id="提示-CA-证书为空"><a href="#提示-CA-证书为空" class="headerlink" title="提示 CA 证书为空"></a>提示 CA 证书为空</h3><p>如果执行更新证书后出现如下错误提示，因为没有执行集群更新操作</p><img src="/rancher/rotate-cert/image-20190423133555060.b403ce72.png" class="" title="image-20190423133555060"><p><strong>解决方法</strong></p><ol><li><p>选择对应问题集群，然后查看浏览器的集群 ID，如下图： <img src="/rancher/rotate-cert/image-20190423133810076.db389f73.png" class="" title="ran" alt="chimage-20190423133810076"></p></li><li><p>执行命令 <code>kubectl edit clusters &lt;clusters_ID&gt;</code></p><ul><li>如果 Rancher 是 HA 安装，直接在 local 集群中，通过 <code>rke</code> 生成的 <code>kube</code> 配置文件执行以上命令；</li><li>如果 Rancher 是单容器运行，通过 <code>docker exec -ti &lt;容器 ID&gt; bash</code> 进入容器中，然后执行 <code>apt install vim -y</code> 安装 vim 工具，然后再执行以上命令；</li></ul></li><li><p>删除 <code>spec.rancherKubernetesEngineConfig.rotateCertificates</code> 层级下的配置参数: <img src="/rancher/rotate-cert/image-20190423135522178.d83c0677.png" class="" title="image-20190423135522178"></p><p>修改为 <img src="/rancher/rotate-cert/image-20190423135604503.04e188d0.png" class="" title="image-20190423135604503"></p></li><li><p>输入 <code>:wq</code> 保存 yaml 文件后集群将自动更新，更新完成后再进行证书更新。</p></li></ol><h3 id="证书已过期导致无法连接-K8S-进行证书轮换"><a href="#证书已过期导致无法连接-K8S-进行证书轮换" class="headerlink" title="证书已过期导致无法连接 K8S 进行证书轮换"></a>证书已过期导致无法连接 K8S 进行证书轮换</h3><p>如果集群证书已经过期，那么即使升级到 <code>Rancher v2.0.14、v2.1.9</code> 以及更高版本也无法轮换证书。rancher 是通过 <code>Agent</code> 去更新证书，如果证书过期将无法与 <code>Agent</code> 连接。</p><p><strong>解决方法</strong></p><p>可以手动设置节点的时间，把时间往后调整一些。因为 <code>Agent</code> 只与 <code>K8S master</code> 和 <code>Rancher Server</code> 通信，如果 Rancher Server 证书未过期，那就只需调整 <code>K8S master</code> 节点时间。</p><p>调整命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 关闭 ntp 同步，不然时间会自动更新</span></span><br><span class="line">timedatectl set-ntp <span class="literal">false</span></span><br><span class="line"><span class="comment"># 修改节点时间</span></span><br><span class="line">timedatectl set-time <span class="string">&#x27;2019-01-01 00:00:00&#x27;</span></span><br></pre></td></tr></table></figure><p>然后再对 Rancher Server 进行升级，接着按照证书轮换步骤进行证书轮换，等到证书轮换完成后再把时间同步回来。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">timedatectl set-ntp <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>检查证书有效期:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl x509 -<span class="keyword">in</span> /etc/kubernetes/ssl/kube-apiserver.pem -noout -dates</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> 轮换证书 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>保存所有集群的 kubeconfig 配置文件</title>
      <link href="/rancher/save-all-kubecfg/"/>
      <url>/rancher/save-all-kubecfg/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/save-all-kubecfg/" target="_blank" title="https://www.xtplayer.cn/rancher/save-all-kubecfg/">https://www.xtplayer.cn/rancher/save-all-kubecfg/</a></p><h2 id="仅具有用户权限的配置文件"><a href="#仅具有用户权限的配置文件" class="headerlink" title="仅具有用户权限的配置文件"></a>仅具有用户权限的配置文件</h2><p>每一个分配集群资源权限的用户，都可以在集群页找到对应的 kubeconfig 配置文件。此 kubectl 配置文件， 仅可以操作自己有权限的资源。</p><img src="/rancher/save-all-kubecfg/image-20200807000938653.png" class="" title="image-20200807000938653"><p>你如果有很多集群有资源权限，并且你想统一保存 kubeconfig 文件，那么可以通过以下脚本来批量保存。</p><h3 id="前提要求"><a href="#前提要求" class="headerlink" title="前提要求"></a>前提要求</h3><p>需要在运行脚本的主机上安装 jq 工具</p><ol><li><p>centos</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y jq</span><br></pre></td></tr></table></figure></li><li><p>ubuntu</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt-get install jq -y</span><br></pre></td></tr></table></figure></li></ol><h3 id="脚本使用"><a href="#脚本使用" class="headerlink" title="脚本使用"></a>脚本使用</h3><ol><li><p>生成 API KEY，用于 Rancher UI API 的访问；</p><ul><li>切换到全局视图</li><li>右上角点击个人头像，接着点击 API &amp; KEY</li><li>点击 <strong>添加 Key</strong>，<ul><li>描述可以随便填写；</li><li>自动失效时间建议选择 1 天，一天后自动删除；</li><li>作用范围不用选择，默认全部范围；</li></ul></li><li>点击 创建 后复制 Bearer Token 备用</li></ul><img src="/rancher/save-all-kubecfg/image-20200807002511457.png" class="" title="image-20200807002511457"></li><li><p>保存以下内容为任意脚本；</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line"></span><br><span class="line">RANCHER_TOKEN=<span class="string">&quot;&lt;Bearer Token&gt;&quot;</span></span><br><span class="line">RANCHER_URL=<span class="string">&#x27;https://xxxx.rancher.com&#x27;</span>  <span class="comment"># 不要加后缀 /</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 具有用户权限的 kube_config</span></span><br><span class="line"><span class="comment">## 获取集群 ID 列表</span></span><br><span class="line">CLUSTER_ID_LIST=$(</span><br><span class="line">    curl -LSs \</span><br><span class="line">    -u <span class="string">&quot;<span class="variable">$&#123;RANCHER_TOKEN&#125;</span>&quot;</span> \</span><br><span class="line">    -X GET \</span><br><span class="line">    -H <span class="string">&#x27;Accept: application/json&#x27;</span> \</span><br><span class="line">    -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">    <span class="variable">$&#123;RANCHER_URL&#125;</span>/v3/clusters/ | jq -r .data[].<span class="built_in">id</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取全部集群的 kube_config</span></span><br><span class="line"><span class="keyword">for</span> CLUSTER_ID <span class="keyword">in</span> <span class="variable">$&#123;CLUSTER_ID_LIST&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    curl -LSs \</span><br><span class="line">    -u <span class="string">&quot;<span class="variable">$&#123;RANCHER_TOKEN&#125;</span>&quot;</span> \</span><br><span class="line">    -X POST \</span><br><span class="line">    -H <span class="string">&#x27;Accept: application/json&#x27;</span> \</span><br><span class="line">    -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">    <span class="variable">$&#123;RANCHER_URL&#125;</span>/v3/clusters/<span class="variable">$&#123;CLUSTER_ID&#125;</span>?action=generateKubeconfig | \</span><br><span class="line">    jq -r .config &gt; kube_config_<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li><li><p>根据前面步骤中复制的 Bearer Token 设置 RANCHER_TOKEN。RANCHER_URL 设置为 rancher server url，注意不要尾部的 <code>/</code>。</p></li><li><p>在能访问 rancher server 的任意主机上运行本脚本</p></li></ol><h2 id="具有集群管理员权限的配置文件"><a href="#具有集群管理员权限的配置文件" class="headerlink" title="具有集群管理员权限的配置文件"></a>具有集群管理员权限的配置文件</h2><p>如果你是管理员，应该备份所有集群具有管理员权限的 kubeconfig 配置文件，当集群出现异常，需要通过 kubectl 命令行去操作集群。</p><h3 id="前提要求-1"><a href="#前提要求-1" class="headerlink" title="前提要求"></a>前提要求</h3><p>需要在运行脚本的主机上安装 jq 工具</p><ol><li><p>centos</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y jq</span><br></pre></td></tr></table></figure></li><li><p>ubuntu</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt-get install jq -y</span><br></pre></td></tr></table></figure></li></ol><h3 id="脚本使用-1"><a href="#脚本使用-1" class="headerlink" title="脚本使用"></a>脚本使用</h3><ol><li><p>需要用管理员登录 Rancher UI，管理员创建的 API KEY 具有所有集群的管理员权限。</p></li><li><p>生成 API KEY，用于 Rancher UI API 的访问</p><ul><li>切换到全局视图</li><li>右上角点击个人头像，接着点击 API &amp; KEY</li><li>点击 <strong>添加 Key</strong>，<ul><li>描述可以随便填写；</li><li>自动失效时间建议选择 1 天，一天后自动删除；</li><li>作用范围不用选择，默认全部范围；</li></ul></li><li>点击 创建 后复制 Bearer Token 备用</li></ul><img src="/rancher/save-all-kubecfg/image-20200807002511457.png" class="" title="image-20200807002511457"></li><li><p>保存以下内容为任意脚本；</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line"></span><br><span class="line">RANCHER_TOKEN=<span class="string">&quot;&lt;Bearer Token&gt;&quot;</span></span><br><span class="line">RANCHER_URL=<span class="string">&#x27;https://xxxx.rancher.com&#x27;</span>  <span class="comment"># 不要加后缀 /</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 具有管理员权限的 kube_config</span></span><br><span class="line"><span class="comment">## 获取集群 ID 列表</span></span><br><span class="line">CLUSTER_ID_LIST=$(</span><br><span class="line">    curl -LSs \</span><br><span class="line">    -u <span class="string">&quot;<span class="variable">$&#123;RANCHER_TOKEN&#125;</span>&quot;</span> \</span><br><span class="line">    -X GET \</span><br><span class="line">    -H <span class="string">&#x27;Accept: application/json&#x27;</span> \</span><br><span class="line">    -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">    <span class="variable">$&#123;RANCHER_URL&#125;</span>/v3/clusters/ | jq -r .data[].<span class="built_in">id</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 轮训集群 ID 列表，获取每个集群 system 项目 ID，然后获取 kube_config</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> CLUSTER_ID <span class="keyword">in</span> <span class="variable">$&#123;CLUSTER_ID_LIST&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="comment"># 获取集群 $&#123;CLUSTER_ID&#125; system 项目 ID</span></span><br><span class="line">    SYSTEM_PROJECT_ID=$(</span><br><span class="line">        curl -LSs \</span><br><span class="line">        -u <span class="string">&quot;<span class="variable">$&#123;RANCHER_TOKEN&#125;</span>&quot;</span> \</span><br><span class="line">        -X GET \</span><br><span class="line">        -H <span class="string">&#x27;Accept: application/json&#x27;</span> \</span><br><span class="line">        -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">        <span class="variable">$&#123;RANCHER_URL&#125;</span>/v3/clusters/<span class="variable">$&#123;CLUSTER_ID&#125;</span>/projects?name=System | jq -r <span class="string">&quot;.data[].id&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取集群 $&#123;CLUSTER_ID&#125; full-cluster-state 配置映射文件，老版本是通过 full-cluster-state 配置映射来保存 admin_kube_config 配置文件</span></span><br><span class="line">    CONFIGMAP_FULL_CLUSTER_STATE=$(</span><br><span class="line">        curl -LSs \</span><br><span class="line">        -u <span class="string">&quot;<span class="variable">$&#123;RANCHER_TOKEN&#125;</span>&quot;</span> \</span><br><span class="line">        -X GET \</span><br><span class="line">        -H <span class="string">&#x27;Accept: application/json&#x27;</span> \</span><br><span class="line">        -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">        <span class="variable">$&#123;RANCHER_URL&#125;</span>/v3/project/<span class="variable">$&#123;SYSTEM_PROJECT_ID&#125;</span>/configMaps/kube-system:full-cluster-state | \</span><br><span class="line">        jq -r .data.\&quot;full-cluster-state\&quot; | \</span><br><span class="line">        jq -r .currentState.certificatesBundle.\&quot;kube-admin\&quot;.config</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取集群 $&#123;CLUSTER_ID&#125; kube-admin 密文，老版本是通过 kube-admin 密文来保存 admin_kube_config 配置文件</span></span><br><span class="line">    SECRETS_KUBE_ADMIN=$(</span><br><span class="line">        curl -LSs \</span><br><span class="line">        -u <span class="string">&quot;<span class="variable">$&#123;RANCHER_TOKEN&#125;</span>&quot;</span> \</span><br><span class="line">        -X GET \</span><br><span class="line">        -H <span class="string">&#x27;Accept: application/json&#x27;</span> \</span><br><span class="line">        -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">        <span class="variable">$&#123;RANCHER_URL&#125;</span>/v3/project/<span class="variable">$&#123;SYSTEM_PROJECT_ID&#125;</span>/namespacedSecrets/kube-system:kube-admin | \</span><br><span class="line">        jq -r .data.Config</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">$&#123;CONFIGMAP_FULL_CLUSTER_STATE&#125;</span> != <span class="string">&#x27;null&#x27;</span> ]]; <span class="keyword">then</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;CONFIGMAP_FULL_CLUSTER_STATE&#125;</span>&quot;</span> &gt; kube_config_<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> [[ <span class="variable">$&#123;SECRETS_KUBE_ADMIN&#125;</span> != <span class="string">&#x27;null&#x27;</span> ]]; <span class="keyword">then</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;SECRETS_KUBE_ADMIN&#125;</span>&quot;</span> &gt; kube_config_<span class="variable">$&#123;CLUSTER_ID&#125;</span>.yaml</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;没有 full-cluster-state configMaps 和 kube-admin Secrets&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li><li><p>根据前面步骤中复制的 Bearer Token 设置 RANCHER_TOKEN。RANCHER_URL 设置为 rancher server url，注意不要尾部的 <code>/</code>。</p></li><li><p>在能访问 rancher server 的任意主机上运行本脚本</p></li></ol><h2 id="单个集群保存"><a href="#单个集群保存" class="headerlink" title="单个集群保存"></a>单个集群保存</h2><p>以上两种方法均是通过 Rancher API 去保存 kubeconfig 配置文件。Rancher API 请求是通过 Rancher server 与 Rancher agent 创建的 websocket 双向隧道去访问的 K8S 集群，如果 Rancher server 与 Rancher agent 的连接出现异常，那么这个时候是无法通过 Rancher API 去保存 kubeconfig 文件。如果早期没有备份 kubeconfig 文件，在 Rancher server 与 Rancher agent 的连接出现异常后，Rancher UI 将无法复制 kubeconfig 配置文件。这个时候到每个集群上去手动保存 kubeconfig 文件，操作方法参考：<a href="/rancher/restore-kubecfg/">恢复丢失的 kubecfg 文件</a>。</p>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> kubeconfig </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用 BLKTRACE 和 BTT 分析磁盘 IO 性能</title>
      <link href="/linux/disk/blktrace-btt-test-io/"/>
      <url>/linux/disk/blktrace-btt-test-io/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/disk/blktrace-btt-test-io/" target="_blank" title="https://www.xtplayer.cn/linux/disk/blktrace-btt-test-io/">https://www.xtplayer.cn/linux/disk/blktrace-btt-test-io/</a></p><p>平时我们在 Linux 上查看磁盘 I&#x2F;O 性能，可能我们首先就会想到 <strong>iostat</strong> 命令（包含于 sysstat 软件包中）或者 <strong>iotop</strong> 命令。<strong>iostat</strong> 或者 <strong>iotop</strong> 仅提供了汇总的简单视图，其中很重要的参数就是 await，await 表示单个 I&#x2F;O 所需的平均时间，但它同时包含了 I&#x2F;O Scheduler 所消耗的时间和硬件所消耗的时间，所以不能作为硬件性能的指标。如果需要知道更多的细节数据，<strong>iostat</strong> 将无法满足需求。我们需要更强大的工具来深入了解 I&#x2F;O 性能问题，我们需要用到 blktrace。</p><p>blktrace 提供了对通用块层（block layer）的 I&#x2F;O 跟踪机制，它能抓取详细的 I&#x2F;O 请求（request），并发送到用户态空间。</p><p>blktrace 主要由 3 部分组成：</p><ul><li>内核组件</li><li>将内核的 I&#x2F;O 跟踪信息记录到用户空间的实用程序</li><li>分析和查看跟踪信息的实用程序</li></ul><p>blktrace 在通过调试文件系统(中继)传递的缓冲区中接收来自内核的数据。每个被跟踪的设备都在挂载目录中为 debugfs 创建了一个文件，debugfs 挂载目录默认是：&#x2F;sys&#x2F;kernel&#x2F;debug</p><h2 id="blktrace-的原理"><a href="#blktrace-的原理" class="headerlink" title="blktrace 的原理"></a>blktrace 的原理</h2><p>一个 I&#x2F;O 请求，从应用层到底层块设备，路径如下图所示：</p><img src="/linux/disk/blktrace-btt-test-io/Linux-storage-stack-diagram_v4.0.png" class="" title="img"><p>从上图可以看出 I&#x2F;O 路径是很复杂，将 I&#x2F;O 路径简化一下，大致为：</p><img src="/linux/disk/blktrace-btt-test-io/io_path_simple.png" class="" title="img"><p>一个 I&#x2F;O 请求进入 block layer 之后，可能会经历下面的过程：</p><ul><li>Remap: 可能被 DM(Device Mapper) 或 MD(Multiple Device, Software RAID) remap 到其它设备；</li><li>Split: 可能会因为 I&#x2F;O 请求与扇区边界未对齐、或者 size 太大而被分拆 (split) 成多个物理 I&#x2F;O；</li><li>Merge: 可能会因为与其它 I&#x2F;O 请求的物理位置相邻而合并 (merge) 成一个 I&#x2F;O；</li><li>被 I&#x2F;O Scheduler 依照调度策略发送给 Driver；</li><li>被 Driver 提交给硬件，经过 HBA、电缆（光纤、网线等）、交换机（SAN 或网络），最后到达存储设备，设备完成 I&#x2F;O 请求之后再返回结果。</li></ul><h3 id="blktrace-总体架构"><a href="#blktrace-总体架构" class="headerlink" title="blktrace 总体架构"></a>blktrace 总体架构</h3><img src="/linux/disk/blktrace-btt-test-io/blktrace_architecture.png" class="" title="img"><h3 id="blktrace-事件输出"><a href="#blktrace-事件输出" class="headerlink" title="blktrace 事件输出"></a>blktrace 事件输出</h3><img src="/linux/disk/blktrace-btt-test-io/blktrace_out.jpg" class="" title="img"><ul><li>第一个字段：8,0 这个字段是设备号 major device ID 和 minor device ID。</li><li>第二个字段：3 表示 CPU</li><li>第三个字段：11 序列号</li><li>第四个字段：0.009507758 Time Stamp 是时间偏移</li><li>第五个字段：PID 本次 I&#x2F;O 对应的进程 ID</li><li>第六个字段：Event，这个字段非常重要，反映了 I&#x2F;O 进行到了那一步</li><li>第七个字段：R 表示 Read， W 是 Write，D 表示 block，B 表示 Barrier Operation</li><li>第八个字段：223490+56，表示的是起始 block number 和 number of blocks，即我们常说的 Offset 和 Size</li><li>第九个字段：进程名</li></ul><p>其中第六个字段非常有用：每一个字母都代表了 I&#x2F;O 请求所经历的某个阶段。</p><h3 id="最重要的几个阶段"><a href="#最重要的几个阶段" class="headerlink" title="最重要的几个阶段"></a>最重要的几个阶段</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Q – 即将生成 I/O 请求</span><br><span class="line">|</span><br><span class="line">G – I/O 请求生成</span><br><span class="line">|</span><br><span class="line">I – I/O 请求进入 I/O Scheduler 队列</span><br><span class="line">|</span><br><span class="line">D – I/O 请求进入 Driver</span><br><span class="line">|</span><br><span class="line">C – I/O 请求执行完毕</span><br></pre></td></tr></table></figure><p>根据以上步骤对应的时间戳就可以计算出 I&#x2F;O 请求在每个阶段所消耗的时间：</p><ul><li>Q2G – 生成 I&#x2F;O 请求所消耗的时间，包括 remap 和 split 的时间；</li><li>G2I – I&#x2F;O 请求进入 I&#x2F;O Scheduler 所消耗的时间，包括 merge 的时间；</li><li>I2D – I&#x2F;O 请求在 I&#x2F;O Scheduler 中等待的时间，可以作为 I&#x2F;O Scheduler 性能的指标；</li><li>D2C – I&#x2F;O 请求在 Driver 和硬件上所消耗的时间，可以作为硬件性能的指标；</li><li>Q2C – 整个 I&#x2F;O 请求所消耗的时间(Q2I + I2D + D2C &#x3D; Q2C)，相当于 iostat 的 await。</li></ul><p>整个 I&#x2F;O 路径，分成很多段，每一段开始的时候，都会有一个时间戳，根据上一段开始的时间和下一段开始的时间，就可以得到 I&#x2F;O 路径各段花费的时间。service time，也就是反应块设备处理能力的指标，就是从 D 到 C 所花费的时间，简称 D2C。而 iostat 输出中的 await，即整个 I&#x2F;O 从生成请求到 I&#x2F;O 请求执行完毕，即从 Q 到 C 所花费的时间，我们简称 Q2C。我们知道 Linux 有 I&#x2F;O scheduler，调度器的效率如何，I2D 是重要的指标。</p><p>这只是 blktrace 输出的一个部分，很明显，我们还能拿到 offset 和 size，根据 offset，我们能拿到某一段时间里，应用程序都访问了整个块设备的那些 block，从而绘制出块设备访问轨迹图。另外还有 size 和第七个字段（Read or Write），我们可以知道 I&#x2F;O size 的分布直方图。对于本文来讲，我们就是要根据 blktrace 来获取这些信息。</p><h3 id="blktrace-event-速查表"><a href="#blktrace-event-速查表" class="headerlink" title="blktrace event 速查表"></a>blktrace event 速查表</h3><img src="/linux/disk/blktrace-btt-test-io/blktrace_events.png" class="" title="img"><ul><li>A remap 对于栈式设备，进来的 I&#x2F;O 将被重新映射到 I&#x2F;O 栈中的具体设备。</li><li>X split 对于做了 Raid 或进行了 device mapper(dm) 的设备，进来的 I&#x2F;O 可能需要切割，然后发送给不同的设备。</li><li>Q queued I&#x2F;O 进入 block layer，将要被 request 代码处理（即将生成 I&#x2F;O 请求）。</li><li>G get request I&#x2F;O 请求（request）生成，为 I&#x2F;O 分配一个 request 结构体。</li><li>M back merge 之前已经存在的 I&#x2F;O request 的终止 block 号，和该 I&#x2F;O 的起始 block 号一致，就会合并，也就是向后合并。</li><li>F front merge 之前已经存在的 I&#x2F;O request 的起始 block 号，和该 I&#x2F;O 的终止 block 号一致，就会合并，也就是向前合并。</li><li>I inserted I&#x2F;O 请求被插入到 I&#x2F;O scheduler 队列。</li><li>S sleep 没有可用的 request 结构体，也就是 I&#x2F;O 满了，只能等待有 request 结构体完成释放。</li><li>P plug 当一个 I&#x2F;O 入队一个空队列时，Linux 会锁住这个队列，不处理该 I&#x2F;O，这样做是为了等待一会，看有没有新的 I&#x2F;O 进来，可以合并。</li><li>U unplug 当队列中已经有 I&#x2F;O request 时，会放开这个队列，准备向磁盘驱动发送该 I&#x2F;O。这个动作的触发条件是：超时（plug 的时候，会设置超时时间）；或者是有一些 I&#x2F;O 在队列中（多于 1 个 I&#x2F;O）。</li><li>D issued I&#x2F;O 将会被传送给磁盘驱动程序处理。</li><li>C complete I&#x2F;O 处理被磁盘处理完成。</li></ul><h3 id="blktrace-汇总输出"><a href="#blktrace-汇总输出" class="headerlink" title="blktrace 汇总输出"></a>blktrace 汇总输出</h3><img src="/linux/disk/blktrace-btt-test-io/summary-output.jpg" class="" title="summary-output"><h2 id="安装-blktrace"><a href="#安装-blktrace" class="headerlink" title="安装 blktrace"></a>安装 blktrace</h2><ol><li>centos</li></ol>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install blktrace</span><br></pre></td></tr></table></figure><ol><li>ubuntu</li></ol>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt-get install blktrace</span><br></pre></td></tr></table></figure><h2 id="blktrace-用法"><a href="#blktrace-用法" class="headerlink" title="blktrace 用法"></a>blktrace 用法</h2><p>接下来简单介绍这些工具的使用，这三个命令都包含在 blktrace 中，使用 blktrace 需要挂载 debugfs。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -t debugfs debugfs /sys/kernel/debug</span><br></pre></td></tr></table></figure><h3 id="查看实时数据"><a href="#查看实时数据" class="headerlink" title="查看实时数据"></a>查看实时数据</h3><p>(可选) 利用 blktrace 查看实时数据的方法，假设要看的硬盘为 sdb。</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">blktrace -d /dev/sdb -o – | blkparse -i –</span><br></pre></td></tr></table></figure><p>这个命令会持续输出，可按 ctrl＋C 停止。也可以先用如下命令采集信息，待所有信息采集完毕后，统一分析所有采集到的数据。搜集信息的命令如下：</p><h3 id="blktrace-收集数据"><a href="#blktrace-收集数据" class="headerlink" title="blktrace 收集数据"></a>blktrace 收集数据</h3><p>利用 blktrace 把数据记录在文件里，以供事后分析。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">blktrace -d /dev/sdb</span><br></pre></td></tr></table></figure><blockquote><p>默认的输出文件名是 <code>sdb.blktrace.&lt;cpu&gt;</code>，每个 CPU 对应一个文件，也可以用 -o 参数指定自己的输出文件名。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-rw-r--r-- 1 manu manu  1.3M Jul  6 19:58 sdb.blktrace.0</span><br><span class="line">-rw-r--r-- 1 manu manu  823K Jul  6 19:58 sdb.blktrace.1</span><br><span class="line">-rw-r--r-- 1 manu manu  2.8M Jul  6 19:58 sdb.blktrace.10</span><br><span class="line">-rw-r--r-- 1 manu manu  1.9M Jul  6 19:58 sdb.blktrace.11</span><br><span class="line">-rw-r--r-- 1 manu manu  474K Jul  6 19:58 sdb.blktrace.12</span><br><span class="line">-rw-r--r-- 1 manu manu  271K Jul  6 19:58 sdb.blktrace.13</span><br><span class="line">-rw-r--r-- 1 manu manu  578K Jul  6 19:58 sdb.blktrace.14</span><br><span class="line">-rw-r--r-- 1 manu manu  375K Jul  6 19:58 sdb.blktrace.15</span><br><span class="line">-rw-r--r-- 1 manu manu  382K Jul  6 19:58 sdb.blktrace.16</span><br><span class="line">-rw-r--r-- 1 manu manu  478K Jul  6 19:58 sdb.blktrace.17</span><br><span class="line">-rw-r--r-- 1 manu manu  839K Jul  6 19:58 sdb.blktrace.18</span><br><span class="line">-rw-r--r-- 1 manu manu  848K Jul  6 19:58 sdb.blktrace.19</span><br><span class="line">-rw-r--r-- 1 manu manu  1.6M Jul  6 19:58 sdb.blktrace.2</span><br><span class="line">-rw-r--r-- 1 manu manu  652K Jul  6 19:58 sdb.blktrace.20</span><br><span class="line">-rw-r--r-- 1 manu manu  738K Jul  6 19:58 sdb.blktrace.21</span><br><span class="line">-rw-r--r-- 1 manu manu  594K Jul  6 19:58 sdb.blktrace.22</span><br><span class="line">-rw-r--r-- 1 manu manu  527K Jul  6 19:58 sdb.blktrace.23</span><br><span class="line">-rw-r--r-- 1 manu manu 1005K Jul  6 19:58 sdb.blktrace.3</span><br><span class="line">-rw-r--r-- 1 manu manu  1.2M Jul  6 19:58 sdb.blktrace.4</span><br><span class="line">-rw-r--r-- 1 manu manu  511K Jul  6 19:58 sdb.blktrace.5</span><br><span class="line">-rw-r--r-- 1 manu manu  2.3M Jul  6 19:58 sdb.blktrace.6</span><br><span class="line">-rw-r--r-- 1 manu manu  1.3M Jul  6 19:58 sdb.blktrace.7</span><br><span class="line">-rw-r--r-- 1 manu manu  2.1M Jul  6 19:58 sdb.blktrace.8</span><br><span class="line">-rw-r--r-- 1 manu manu  1.1M Jul  6 19:58 sdb.blktrace.9</span><br></pre></td></tr></table></figure><h2 id="blkparse-用法"><a href="#blkparse-用法" class="headerlink" title="blkparse 用法"></a>blkparse 用法</h2><p>有了 blktrace 收集的数据，我们可以通过 blkparse -i sdb 来分析采集的数据：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">8,16   7     2147     0.999400390 630169  I   W 447379872 + 8 [kworker/u482:0]</span><br><span class="line">8,16   7     2148     0.999400653 630169  I   W 447380040 + 8 [kworker/u482:0]</span><br><span class="line">8,16   7     2149     0.999401057 630169  I   W 447380088 + 16 [kworker/u482:0]</span><br><span class="line">8,16   7     2150     0.999401364 630169  I   W 447380176 + 8 [kworker/u482:0]</span><br><span class="line">8,16   7     2151     0.999401521 630169  I   W 453543312 + 8 [kworker/u482:0]</span><br><span class="line">8,16   7     2152     0.999401843 630169  I   W 453543328 + 8 [kworker/u482:0]</span><br><span class="line">8,16   7     2153     0.999402195 630169  U   N [kworker/u482:0] 14</span><br><span class="line">8,16   6     5648     0.999403047 16921  C   W 347875880 + 8 [0]</span><br><span class="line">8,16   6     5649     0.999406293 16921  D   W 301856632 + 8 [ceph-osd]</span><br><span class="line">8,16   6     5650     0.999421040 16921  C   W 354834456 + 8 [0]</span><br><span class="line">8,16   6     5651     0.999423900 16921  D   W 301857280 + 8 [ceph-osd]</span><br><span class="line">8,16   7     2154     0.999442195 630169  A   W 425409840 + 8 &lt;- (8,22) 131806512</span><br><span class="line">8,16   7     2155     0.999442601 630169  Q   W 425409840 + 8 [kworker/u482:0]</span><br><span class="line">8,16   7     2156     0.999444277 630169  G   W 425409840 + 8 [kworker/u482:0]</span><br><span class="line">8,16   7     2157     0.999445177 630169  P   N [kworker/u482:0]</span><br><span class="line">8,16   7     2158     0.999446341 630169  I   W 425409840 + 8 [kworker/u482:0]</span><br><span class="line">8,16   7     2159     0.999446773 630169 UT   N [kworker/u482:0] 1</span><br><span class="line">8,16   6     5652     0.999452685 16921  C   W 354834520 + 8 [0]</span><br><span class="line">8,16   6     5653     0.999455613 16921  D   W 301857336 + 8 [ceph-osd]</span><br><span class="line">8,16   6     5654     0.999470425 16921  C   W 393228176 + 8 [0]</span><br><span class="line">8,16   6     5655     0.999474127 16921  D   W 411554968 + 8 [ceph-osd]</span><br><span class="line">8,16   6     5656     0.999488551 16921  C   W 393228560 + 8 [0]</span><br><span class="line">8,16   6     5657     0.999491549 16921  D   W 411556112 + 8 [ceph-osd]</span><br><span class="line">8,16   6     5658     0.999594849 16923  C   W 393230152 + 16 [0]</span><br><span class="line">8,16   6     5659     0.999604038 16923  D   W 432877368 + 8 [ceph-osd]</span><br><span class="line">8,16   6     5660     0.999610322 16923  C   W 487390128 + 8 [0]</span><br><span class="line">8,16   6     5661     0.999614654 16923  D   W 432879632 + 8 [ceph-osd]</span><br><span class="line">8,16   6     5662     0.999628284 16923  C   W 487391344 + 8 [0]</span><br><span class="line">8,16   6     5663     0.999632014 16923  D   W 432879680 + 8 [ceph-osd]</span><br><span class="line">8,16   6     5664     0.999646122 16923  C   W 293759504 + 8 [0]</span><br></pre></td></tr></table></figure><p>blkparse 只是将 blktrace 保存的数据转成可以人工阅读的格式，由于数据量通常很大，人工分析并不轻松。因此接下来需要使用 btt 对 blktrace 保存的数据进行自动分析。</p><h2 id="btt-用法"><a href="#btt-用法" class="headerlink" title="btt 用法"></a>btt 用法</h2><ol><li><p>首先，需要使用 blkparse 工具将可以将 blktrace 收集的对应不同 cpu 的多个文件聚合成一个文件：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">blkparse -i sdb -d sdb.blktrace.bin</span><br></pre></td></tr></table></figure></li><li><p>然后使用 btt 分析 sdb.blktrace.bin，可以获得类似以下数据信息：</p></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">==================== All Devices ====================</span><br><span class="line"></span><br><span class="line">            ALL           MIN           AVG           MAX           N</span><br><span class="line">--------------- ------------- ------------- ------------- -----------</span><br><span class="line"></span><br><span class="line">Q2Q               0.000000001   0.000159747   0.025292639       62150</span><br><span class="line">Q2G               0.000000233   0.000001380   0.000056343       52423</span><br><span class="line">G2I               0.000000146   0.000027084   0.005031317       48516</span><br><span class="line">Q2M               0.000000142   0.000000751   0.000021613        9728</span><br><span class="line">I2D               0.000000096   0.001534463   0.022469688       52423</span><br><span class="line">M2D               0.000000647   0.002617691   0.022445412        5821</span><br><span class="line">D2C               0.000046189   0.000779355   0.007860766       62151</span><br><span class="line">Q2C               0.000051089   0.002522832   0.026096657       62151</span><br><span class="line"></span><br><span class="line">==================== Device Overhead ====================</span><br><span class="line"></span><br><span class="line">       DEV |       Q2G       G2I       Q2M       I2D       D2C</span><br><span class="line">---------- | --------- --------- --------- --------- ---------</span><br><span class="line"> (  8, 16) |   0.0461%   0.8380%   0.0047%  51.3029%  30.8921%</span><br><span class="line">---------- | --------- --------- --------- --------- ---------</span><br><span class="line">   Overall |   0.0461%   0.8380%   0.0047%  51.3029%  30.8921%</span><br><span class="line"></span><br><span class="line">==================== Device Merge Information ====================</span><br><span class="line"></span><br><span class="line">       DEV |       <span class="comment">#Q       #D   Ratio |   BLKmin   BLKavg   BLKmax    Total</span></span><br><span class="line">---------- | -------- -------- ------- | -------- -------- -------- --------</span><br><span class="line"> (  8, 16) |    62151    52246     1.2 |        1       20      664  1051700</span><br><span class="line"></span><br><span class="line">==================== Device Q2Q Seek Information ====================</span><br><span class="line"></span><br><span class="line">       DEV |          NSEEKS            MEAN          MEDIAN | MODE</span><br><span class="line">---------- | --------------- --------------- --------------- | ---------------</span><br><span class="line"> (  8, 16) |           62151      42079658.0               0 | 0(17159)</span><br><span class="line">---------- | --------------- --------------- --------------- | ---------------</span><br><span class="line">   Overall |          NSEEKS            MEAN          MEDIAN | MODE</span><br><span class="line">   Average |           62151      42079658.0               0 | 0(17159)</span><br><span class="line"></span><br><span class="line">==================== Device D2D Seek Information ====================</span><br><span class="line"></span><br><span class="line">       DEV |          NSEEKS            MEAN          MEDIAN | MODE</span><br><span class="line">---------- | --------------- --------------- --------------- | ---------------</span><br><span class="line"> (  8, 16) |           52246      39892356.2               0 | 0(9249)</span><br><span class="line">---------- | --------------- --------------- --------------- | ---------------</span><br><span class="line">   Overall |          NSEEKS            MEAN          MEDIAN | MODE</span><br><span class="line">   Average |           52246      39892356.2               0 | 0(9249)</span><br></pre></td></tr></table></figure><img src="/linux/disk/blktrace-btt-test-io/btt-output.jpg" class="" title="btt-output"><img src="/linux/disk/blktrace-btt-test-io/btt-IO-scheduler.jpg" class="" title="btt-IO-scheduler"><p>注意： D2C 和 Q2C，一个是表征块设备性能的关键指标，另一个是客户发起请求到收到响应的时间。可以看出 D2C 平均在 0.000779355 秒，即 0.7 毫秒 Q2C 平均在 0.002522832 秒，即 2.5 毫秒，</p><p>无论是 service time 还是客户感知到的 await time，都是非常短。但是 D2C 花费的时间只占整个 Q2C 的 30%，51% 以上的时间花费在 I2D。下面我们看下 D2C 和 Q2C 随着时间的分布情况：</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="http://duch.mimuw.edu.pl/~lichota/09-10/Optymalizacja-open-source/Materialy/10%20-%20Dysk/gelato_ICE06apr_blktrace_brunelle_hp.pdf">http://duch.mimuw.edu.pl/~lichota&#x2F;09-10&#x2F;Optymalizacja-open-source&#x2F;Materialy&#x2F;10%20-%20Dysk&#x2F;gelato_ICE06apr_blktrace_brunelle_hp.pdf</a><br><a href="http://wiki.dreamrunner.org/public_html/Low_Latency_Programming/blktrace.html">http://wiki.dreamrunner.org/public_html&#x2F;Low_Latency_Programming&#x2F;blktrace.html</a><br><a href="http://fibrevillage.com/storage/531-how-to-use-blktrace-and-btt-to-debug-and-tune-disk-io-on-linux">http://fibrevillage.com/storage/531-how-to-use-blktrace-and-btt-to-debug-and-tune-disk-io-on-linux</a><br><a href="http://wiki.dreamrunner.org/public_html/Low_Latency_Programming/blktrace.html">http://wiki.dreamrunner.org/public_html&#x2F;Low_Latency_Programming&#x2F;blktrace.html</a><br><a href="https://bean-li.github.io/blktrace-to-report/">https://bean-li.github.io/blktrace-to-report/</a></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> disk </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BLKTRACE </tag>
            
            <tag> BTT </tag>
            
            <tag> I/O </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>恢复 rkestate 状态文件</title>
      <link href="/rancher/backup-restore/restore-rkestate/"/>
      <url>/rancher/backup-restore/restore-rkestate/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/backup-restore/restore-rkestate/" target="_blank" title="https://www.xtplayer.cn/rancher/backup-restore/restore-rkestate/">https://www.xtplayer.cn/rancher/backup-restore/restore-rkestate/</a></p><p>Kubernetes 集群状态由 Kubernetes 集群中的集群配置文件 <code>cluster.yml</code> 和<code>组件证书</code>组成。由 RKE 生成，但根据 RKE 版本不同，集群状态的保存方式不同。</p><ul><li>在 v0.2.0 之前，RKE 将 Kubernetes 集群状态保存为 <code>secret</code>。更新状态时，RKE 会提取 <code>secret</code>，<code>更新/更改</code> 状态并保存新 <code>secret</code>。</li><li>从 v0.2.0 开始，RKE 在集群配置文件 <code>cluster.yml</code> 的同一目录中创建 <code>cluster.rkestate</code> 文件。该 <code>.rkestate</code> 文件包含集群的当前状态，包括 <code>RKE 配置和证书</code>。需要保留此文件以更新集群或通过 RKE 对集群执行任何操作。</li></ul><h2 id="状态文件转换"><a href="#状态文件转换" class="headerlink" title="状态文件转换"></a>状态文件转换</h2><p>如果是通过 <code>rke v0.2.0</code> 之前版本创建的 Kubernetes 集群，那么建议升级 rke 版本到最新版本。</p><p><code>rke v0.2.0</code> 以前的版本，是通过 <code>pki.bundle.tar.gz</code> 来保存组件证书。而 <code>rke v0.2.0</code> 及以后的版本通过<code>.rkestate</code> 来保存组件证书。</p><p>在 rke 升级到最新版本后，需要有一个过渡操作。通过原始是 rke 配置文件，重新运行 <code>rke up</code> 将会自动生成<code>.rkestate</code> 文件。</p><h2 id="找回-rkestate"><a href="#找回-rkestate" class="headerlink" title="找回 .rkestate"></a>找回 .rkestate</h2><p>假如 <code>.rkestate</code> 无意间丢失或者损坏，可以通过集群中的配置映射文件恢复 .rkestate。</p><h3 id="通过-kubectl-配置文件找回"><a href="#通过-kubectl-配置文件找回" class="headerlink" title="通过 kubectl 配置文件找回"></a>通过 kubectl 配置文件找回</h3><p>如果 <code>.rkestate</code> 丢失，但 kubecfg 未丢失， kubectl 还可以正常连接集群，可以运行以下命令找回 <code>.rkestate</code>:</p><blockquote><p>注意: rke 在创建集群时，会自动创建 <code>.rkestate</code> 和 kubecfg 文件。<code>.rkestate</code> 和 kubecfg 文件的命名规则与 rke 配置文件名有一定关系。比如 rke 配置文件名为 cluster.yml，那么生成的 <code>.rkestate</code> 为 <code>cluster.rkestate</code>，生成的 kubecfg 为 <code>kube_config_cluster.yml</code>。<br>可以总结为：<br>1、kubecfg 命名规则：<code>kube_config_&lt;rke-cfg-name&gt;.yml</code><br>2、.rkestate 命名规则：<code>&lt;rke-cfg-name&gt;.rkestate</code></p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义 rke 配置文件名称，如果还存在则按存在的文件名填写；如果不存在，则随便填写，比如名为 rancher-cluster，文件后缀 yaml/yml 不用填写。</span></span><br><span class="line">rke_config_name=rancher-cluster</span><br><span class="line"></span><br><span class="line">kubecfg=kube_config_cluster.yml</span><br><span class="line"></span><br><span class="line">kubectl --kubeconfig=<span class="variable">$&#123;kubecfg&#125;</span> -n kube-system \</span><br><span class="line">  get configmap full-cluster-state -o json | \</span><br><span class="line">  jq -r .data.\&quot;full-cluster-state\&quot; | \</span><br><span class="line">  jq -r . &gt; <span class="variable">$&#123;rke_config_name&#125;</span>.rkestate</span><br></pre></td></tr></table></figure><h3 id="通过-master-节点找回"><a href="#通过-master-节点找回" class="headerlink" title="通过 master 节点找回"></a>通过 master 节点找回</h3><p>如果本地的 <code>.rkestate 和 kubecfg 文件一并丢失</code>，则需要登录到 master 节点进行恢复，在 master 节点运行以下脚本：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [[ -f /etc/kubernetes/ssl/kubecfg-kube-node.yaml ]]; <span class="keyword">then</span></span><br><span class="line">  kubecfg=/etc/kubernetes/ssl/kubecfg-kube-node.yaml</span><br><span class="line"><span class="keyword">elif</span> [[ -f /etc/kubernetes/ssl/kubecfg-kube-node.yml ]]; <span class="keyword">then</span></span><br><span class="line">  kubecfg=/etc/kubernetes/ssl/kubecfg-kube-node.yml</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&#x27;kubecfg 配置文件不存在&#x27;</span></span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 Rancher Agent 镜像</span></span><br><span class="line">RANCHER_IMAGE=$( docker images --filter=label=io.cattle.agent=<span class="literal">true</span> | grep <span class="string">&#x27;v2.&#x27;</span> | \</span><br><span class="line">  grep -v -E <span class="string">&#x27;rc|alpha|&lt;none&gt;&#x27;</span> | <span class="built_in">head</span> -n 1 | awk <span class="string">&#x27;&#123;print $3&#125;&#x27;</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 rke 配置文件名称，如果还存在则按存在的文件名填写；如果不存在，则随便填写，比如名为 rancher-cluster，文件后缀 yaml/yml 不用填写。</span></span><br><span class="line">rke_config_name=rancher-cluster</span><br><span class="line"></span><br><span class="line">docker run --<span class="built_in">rm</span> --net=host \</span><br><span class="line">  -v $(docker inspect kubelet --format \</span><br><span class="line">  <span class="string">&#x27;&#123;&#123; range .Mounts &#125;&#125;&#123;&#123; if eq .Destination &quot;/etc/kubernetes&quot; &#125;&#125;&#123;&#123; .Source &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#x27;</span>)/ssl:/etc/kubernetes/ssl:ro \</span><br><span class="line">  --entrypoint bash <span class="variable">$&#123;RANCHER_IMAGE&#125;</span> \</span><br><span class="line">  -c <span class="string">&#x27;kubectl --kubeconfig=$&#123;kubecfg&#125; \</span></span><br><span class="line"><span class="string">  -n kube-system get configmap full-cluster-state -o json | \</span></span><br><span class="line"><span class="string">  jq -r .data.\&quot;full-cluster-state\&quot; | \</span></span><br><span class="line"><span class="string">  jq -r .&#x27;</span> &gt; <span class="variable">$&#123;rke_config_name&#125;</span>.rkestate</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
          <category> backup-restore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rke </tag>
            
            <tag> backup-restore </tag>
            
            <tag> rkestate </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher K8S 集群恢复丢失的 kubeconfig 配置文件</title>
      <link href="/rancher/restore-kubecfg/"/>
      <url>/rancher/restore-kubecfg/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/rancher/restore-kubecfg/" target="_blank" title="https://www.xtplayer.cn/rancher/restore-kubecfg/">https://www.xtplayer.cn/rancher/restore-kubecfg/</a></p><p>分析 Rancher UI 生成的 kubeconfig 文件可以发现，第一个 <code>server</code> 对应的是 Rancher Server 的  <code>url 或者 IP</code>。当 kubectl 访问 <code>K8S API SERVER</code> 的时候，请求是先发送到 Rancher，然后再通过 <code>cluster agent</code> 转发给 <code>K8S API SERVER</code>。</p><img src="/rancher/restore-kubecfg/image-20190514185322798.9732ff02.png" class="" title="image-20190514185322798"><p>在 Rancher v2.2.2 以前的版本，Rancher UI 生成的 kubecfg 文件中只设置了一个 <code>server</code>。从 Rancher v2.2.2 开始，从 Rancher UI 创建的集群默认开启<code>授权集群访问地址</code>。创建好集群后 Rancher UI 生成的 kubecfg 文件中将显示多个 master 节点 IP 对应的 <code>server</code>。</p><img src="/rancher/restore-kubecfg/image-20190514185026706.0c9b50a4.png" class="" title="image-20190514185026706"><img src="/rancher/restore-kubecfg/image-20190514184126478.53c7a398.png" class="" title="image-20190514184126478"><p>因此，<code>Rancher v2.2.2</code> 以及之后版本通过 Rancher UI 创建的集群，如果 Rancher Server 无法访问，那么可以通过 <code>kubectl --kubeconfig=xxx --context=xxx</code> 来切换 <code>server</code>，但是前提是需要提前从 Rancher UI 保存 kubeconfig 文件。</p><p>如果 Rancher Server 无法访问，对于 <code>Rancher v2.2.2</code> 之前的版本或者未提前保存 kubeconfig 的，可通过以下脚本找回 <code>kube-admin</code> 配置文件。</p><blockquote><p><strong>注意:</strong> 以下脚本需要在业务集群上执行，任意一个节点即可。保存以下文本为 <code>restore-kube-config.sh</code></p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">help</span></span> ()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; ================================================================ &#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; --master-ip: 指定 Master 节点 IP，任意一个 K8S Master 节点 IP 即可。&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; 使用示例：bash restore-kube-config.sh --master-ip=1.1.1.1 &#x27;</span></span><br><span class="line">    <span class="built_in">echo</span>  <span class="string">&#x27; ================================================================&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&quot;<span class="variable">$1</span>&quot;</span> <span class="keyword">in</span></span><br><span class="line">    -h|--<span class="built_in">help</span>) <span class="built_in">help</span>; <span class="built_in">exit</span>;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$1</span> == <span class="string">&#x27;&#x27;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">help</span>;</span><br><span class="line">    <span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">CMDOPTS=<span class="string">&quot;$*&quot;</span></span><br><span class="line"><span class="keyword">for</span> OPTS <span class="keyword">in</span> <span class="variable">$CMDOPTS</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    key=$(<span class="built_in">echo</span> <span class="variable">$&#123;OPTS&#125;</span> | awk -F<span class="string">&quot;=&quot;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> )</span><br><span class="line">    value=$(<span class="built_in">echo</span> <span class="variable">$&#123;OPTS&#125;</span> | awk -F<span class="string">&quot;=&quot;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span> )</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;<span class="variable">$key</span>&quot;</span> <span class="keyword">in</span></span><br><span class="line">        --master-ip) K8S_MASTER_NODE_IP=<span class="variable">$value</span> ;;</span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 Rancher Agent 镜像</span></span><br><span class="line">RANCHER_IMAGE=$( docker images --filter=label=io.cattle.agent=<span class="literal">true</span> |grep <span class="string">&#x27;v2.&#x27;</span> | \</span><br><span class="line">grep -v -E <span class="string">&#x27;rc|alpha|&lt;none&gt;&#x27;</span> | <span class="built_in">head</span> -n 1 | awk <span class="string">&#x27;&#123;print $3&#125;&#x27;</span> )</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -d /etc/kubernetes/ssl ]]; <span class="keyword">then</span></span><br><span class="line">  K8S_SSLDIR=/etc/kubernetes/ssl</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&#x27;/etc/kubernetes/ssl 目录不存在&#x27;</span></span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">CHECK_CLUSTER_STATE_CONFIGMAP=$( docker run --<span class="built_in">rm</span> --entrypoint bash --net=host \</span><br><span class="line">-v <span class="variable">$K8S_SSLDIR</span>:/etc/kubernetes/ssl:ro <span class="variable">$RANCHER_IMAGE</span> -c <span class="string">&#x27;\</span></span><br><span class="line"><span class="string">if kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml \</span></span><br><span class="line"><span class="string">-n kube-system get configmap full-cluster-state | grep full-cluster-state &gt; /dev/null; then \</span></span><br><span class="line"><span class="string">echo &#x27;</span><span class="built_in">yes</span><span class="string">&#x27;; else echo &#x27;</span>no<span class="string">&#x27;; fi&#x27;</span> )</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$CHECK_CLUSTER_STATE_CONFIGMAP</span> != <span class="string">&#x27;yes&#x27;</span> ]]; <span class="keyword">then</span></span><br><span class="line"></span><br><span class="line">  docker run --<span class="built_in">rm</span> --net=host \</span><br><span class="line">  --entrypoint bash \</span><br><span class="line">  -e K8S_MASTER_NODE_IP=<span class="variable">$K8S_MASTER_NODE_IP</span> \</span><br><span class="line">  -v <span class="variable">$K8S_SSLDIR</span>:/etc/kubernetes/ssl:ro \</span><br><span class="line">  <span class="variable">$RANCHER_IMAGE</span> \</span><br><span class="line">  -c <span class="string">&#x27;\</span></span><br><span class="line"><span class="string">  kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml \</span></span><br><span class="line"><span class="string">  -n kube-system \</span></span><br><span class="line"><span class="string">  get secret kube-admin -o jsonpath=&#123;.data.Config&#125; | base64 --decode | \</span></span><br><span class="line"><span class="string">  sed -e &quot;/^[[:space:]]*server:/ s_:.*_: \&quot;https://$&#123;K8S_MASTER_NODE_IP&#125;:6443\&quot;_&quot;&#x27;</span> &gt; kubeconfig_admin.yaml</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> [[ -s kubeconfig_admin.yaml ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;恢复成功，执行以下命令测试：&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;kubectl --kubeconfig kubeconfig_admin.yaml get nodes&quot;</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;kubeconfig 恢复失败。&quot;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"></span><br><span class="line">  docker run --<span class="built_in">rm</span> --entrypoint bash --net=host \</span><br><span class="line">  -e K8S_MASTER_NODE_IP=<span class="variable">$K8S_MASTER_NODE_IP</span> \</span><br><span class="line">  -v <span class="variable">$K8S_SSLDIR</span>:/etc/kubernetes/ssl:ro \</span><br><span class="line">  <span class="variable">$RANCHER_IMAGE</span> \</span><br><span class="line">  -c <span class="string">&#x27;\</span></span><br><span class="line"><span class="string">  kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml \</span></span><br><span class="line"><span class="string">  -n kube-system \</span></span><br><span class="line"><span class="string">  get configmap full-cluster-state -o json | \</span></span><br><span class="line"><span class="string">  jq -r .data.\&quot;full-cluster-state\&quot; | \</span></span><br><span class="line"><span class="string">  jq -r .currentState.certificatesBundle.\&quot;kube-admin\&quot;.config | \</span></span><br><span class="line"><span class="string">  sed -e &quot;/^[[:space:]]*server:/ s_:.*_: \&quot;https://$&#123;K8S_MASTER_NODE_IP&#125;:6443\&quot;_&quot;&#x27;</span> &gt; kubeconfig_admin.yaml</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> [[ -s kubeconfig_admin.yaml ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;恢复成功，执行以下命令测试：&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;kubectl --kubeconfig kubeconfig_admin.yaml get nodes&quot;</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;kubeconfig 恢复失败。&quot;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rancher </tag>
            
            <tag> 恢复 kubecfg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sysstat 记录主机资源历史使用</title>
      <link href="/linux/sysstat/"/>
      <url>/linux/sysstat/</url>
      
        <content type="html"><![CDATA[<p><span>本文永久链接: </span><i class="fa fa-creative-commons"></i><a rel="noopener external nofollow noreferrer" href="https://www.xtplayer.cn/linux/sysstat/" target="_blank" title="https://www.xtplayer.cn/linux/sysstat/">https://www.xtplayer.cn/linux/sysstat/</a></p><p>很多系统负载过高的时候我们是无法立即获知或者立即解决的，当检测到或者知道历史的高负载状况时，可能需要回放历史监控数据，这时 sar 命令就派上用场了。sar 命令来自 <code>sysstat</code> 工具包，可以记录系统的 CPU 负载、I&#x2F;O 状况和内存使用记录，便于历史数据的回放。</p><ul><li><p>Ubuntu 系统上，sysstat 的配置文件在 <code>/etc/default/sysstat</code>，sysstat 默认关闭，通过将该文件中的 ENABLED 改为”true”启用；历史日志的存放位置为 <code>/var/log/sysstat</code></p></li><li><p>Red Hat 系统上，sysstat 的配置文件在 <code>/etc/sysconfig/sysstat</code> 文件，历史日志的存放位置为 <code>/var/log/sa</code></p></li></ul><p>两种系统上，统计信息都是每 10 分钟记录一次，每天的 23:59 会分割统计文件，这些操作的频率都在 <code>/etc/cron.d/sysstat</code> 文件配置。</p><ol><li>sar 命令查看 CPU、内存和磁盘记录</li></ol><p>　　默认情况下，sar 命令显示当天的统计信息，不带参数显示 CPU 统计信息，参数-r 显示收集的内存记录，-b 显示磁盘 I&#x2F;O</p><p>例：使用 sar 命令查看当天 CPU 使用</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sar</span></span><br><span class="line">Linux 3.13.0-55-generic (ISeR-Server1)     08/12/2015     _x86_64_    (4 CPU)</span><br><span class="line"></span><br><span class="line">12:00:01 AM     CPU     %user     %<span class="built_in">nice</span>   %system   %iowait    %steal     %idle</span><br><span class="line">12:05:01 AM     all      3.83      0.02      4.24      0.61      0.00     91.30</span><br><span class="line">12:15:01 AM     all      3.57      0.02      4.28      0.58      0.00     91.54</span><br><span class="line">12:25:01 AM     all      3.83      0.02      5.16      0.60      0.00     90.39</span><br><span class="line">12:35:01 AM     all      3.98      0.02      5.66      0.58      0.00     89.76</span><br><span class="line">12:45:01 AM     all      3.86      0.02      5.26      0.59      0.00     90.28</span><br><span class="line">12:55:01 AM     all      3.77      0.02      5.19      0.60      0.00     90.42</span><br></pre></td></tr></table></figure><p>例：使用 sar 命令查看当天内存使用</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sar -r</span></span><br><span class="line">Linux 3.13.0-55-generic (ISeR-Server1)     08/12/2015     _x86_64_    (4 CPU)</span><br><span class="line"></span><br><span class="line">12:00:01 AM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty</span><br><span class="line">12:05:01 AM   6420736   5839392     47.63    242640   1366912   6811944     55.56   4324000   1202152        24</span><br><span class="line">12:15:01 AM   6423128   5837000     47.61    242640   1367348   6830944     55.72   4320608   1202400        48</span><br><span class="line">12:25:01 AM   6430984   5829144     47.55    242640   1367548   6814980     55.59   4314376   1202468        48</span><br><span class="line">12:35:01 AM   6422924   5837204     47.61    242640   1367848   6817224     55.60   4321604   1202576        48</span><br><span class="line">12:45:01 AM   6427300   5832828     47.58    242640   1368056   6822240     55.65   4318412   1202572        28</span><br></pre></td></tr></table></figure><p>例：使用 sar 命令查看当天 IO 统计记录</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sar -b</span></span><br><span class="line">Linux 3.13.0-55-generic (ISeR-Server1)     08/12/2015     _x86_64_    (4 CPU)</span><br><span class="line"></span><br><span class="line">12:00:01 AM       tps      rtps      wtps   bread/s   bwrtn/s</span><br><span class="line">12:05:01 AM      7.44      0.00      7.44      0.00    279.22</span><br><span class="line">12:15:01 AM      6.45      0.00      6.45      0.00    255.84</span><br><span class="line">12:25:01 AM      6.59      0.00      6.59      0.00    260.20</span><br><span class="line">12:35:01 AM      6.51      0.00      6.51      0.00    261.42</span><br><span class="line">12:45:01 AM      6.42      0.00      6.42      0.00    255.79</span><br></pre></td></tr></table></figure><ol start="2"><li>使用 sar 查看指定时间、指定日期的历史记录</li></ol><p>例：使用参数-s 和-e 限定查看的时间</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sar -s 20:00:00</span></span><br><span class="line">Linux 3.13.0-55-generic (ISeR-Server1)     08/12/2015     _x86_64_    (4 CPU)</span><br><span class="line"></span><br><span class="line">08:05:01 PM     CPU     %user     %<span class="built_in">nice</span>   %system   %iowait    %steal     %idle</span><br><span class="line">08:15:01 PM     all      3.98      0.02      6.07      0.58      0.00     89.34</span><br><span class="line">08:25:01 PM     all      4.32      0.02      5.74      0.58      0.00     89.34</span><br><span class="line">Average:        all      4.15      0.02      5.91      0.58      0.00     89.34</span><br></pre></td></tr></table></figure><p>例：使用参数-f 查看本月内之前某一天的历史统计信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sar -f /var/log/sysstat/sa08</span></span><br><span class="line">Linux 3.13.0-55-generic (ISeR-Server1)     08/08/2015     _x86_64_    (4 CPU)</span><br><span class="line"></span><br><span class="line">12:00:01 AM     CPU     %user     %<span class="built_in">nice</span>   %system   %iowait    %steal     %idle</span><br><span class="line">12:05:01 AM     all      3.65      0.02      2.79      0.60      0.00     92.94</span><br><span class="line">12:15:01 AM     all      3.45      0.02      3.03      0.56      0.00     92.94</span><br><span class="line">12:25:01 AM     all      3.43      0.02      3.25      0.56      0.00     92.74</span><br><span class="line">12:35:01 AM     all      3.44      0.01      3.09      0.56      0.00     92.89</span><br><span class="line">12:45:01 AM     all      3.25      0.02      1.35      0.55      0.00     94.83</span><br><span class="line">12:55:01 AM     all      3.36      0.02      1.77      0.56      0.00     94.29</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sysstat </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>404</title>
      <link href="//404.html"/>
      <url>//404.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="关于本站"><a href="#关于本站" class="headerlink" title="关于本站"></a>关于本站</h2><ul><li>IT老男孩 - 原名系统玩家，分享 IT 相关技术文章，分享工作中的最佳实践。</li></ul><h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><ul><li>非计算机专业毕业，“半路出家” 的 IT 人，非程序猿。</li><li>熟悉 Linux，熟悉网络，熟悉 kubernetes，熟悉 Docker，精通 Rancher。</li><li>目前就职于 SUSE，高级支持工程师，主要负责 Rancher 产品售后技术支持工作。</li></ul><h2 id="关于-Rancher"><a href="#关于-Rancher" class="headerlink" title="关于 Rancher"></a>关于 Rancher</h2><ul><li><a href="https://www.rancher.cn/">www.rancher.cn</a></li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>广告测试</title>
      <link href="/ad/index.html"/>
      <url>/ad/index.html</url>
      
        <content type="html"><![CDATA[<div class="_zdin68aixj"></div><script type="text/javascript">    (window.slotbydup = window.slotbydup || []).push({        id: "u6415424",        container: "_zdin68aixj",        async: true    });</script><div class="_ohuqnjrgi"></div><script type="text/javascript">    (window.slotbydup = window.slotbydup || []).push({        id: "u6417461",        container: "_ohuqnjrgi",        async: true    });</script><div class="_6biyg7tejt4"></div><script type="text/javascript">    (window.slotbydup = window.slotbydup || []).push({        id: "u6452663",        container: "_6biyg7tejt4",        async: true    });</script><!-- 多条广告如下脚本只需引入一次 -->]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>文章分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>gallery</title>
      <link href="/gallery/index.html"/>
      <url>/gallery/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友情链接</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>文章标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
