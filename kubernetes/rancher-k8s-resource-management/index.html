<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Rancher k8s 资源管理 | IT老男孩</title><meta name="keywords" content="IT老男孩,rancher,github rancher,rke,rke2,k3s,docker,kubernetes,devops,多集群管理,容器,Linux运维,网络运维,最佳实践,cgroup,cgroup"><meta name="author" content="IT老男孩"><meta name="copyright" content="IT老男孩"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="cgroup 简介控制群组 (control group)(简称cgroup) 是 Linux kernel 的一项功能。从使用的角度看，cgroup 是一个目录树结构，目录中可以创建多层子目录，这些目录称为**cgroup 目录**。在一些场景中为了体现层级关系，还会称为**cgroup 子目录**。 通过 cgroup 可对 CPU 时间片、系统内存、磁盘 IO、网络带宽等资源进行精细化控制，"><meta property="og:type" content="article"><meta property="og:title" content="Rancher k8s 资源管理"><meta property="og:url" content="https://www.xtplayer.cn/kubernetes/rancher-k8s-resource-management/index.html"><meta property="og:site_name" content="IT老男孩"><meta property="og:description" content="cgroup 简介控制群组 (control group)(简称cgroup) 是 Linux kernel 的一项功能。从使用的角度看，cgroup 是一个目录树结构，目录中可以创建多层子目录，这些目录称为**cgroup 目录**。在一些场景中为了体现层级关系，还会称为**cgroup 子目录**。 通过 cgroup 可对 CPU 时间片、系统内存、磁盘 IO、网络带宽等资源进行精细化控制，"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.xtplayer.cn/img/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2020-09-17T05:12:40.000Z"><meta property="article:modified_time" content="2021-01-30T09:29:34.000Z"><meta property="article:author" content="IT老男孩"><meta property="article:tag" content="cgroup"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://www.xtplayer.cn/img/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" type="image/png" href="/img/favicon.png"><link rel="canonical" href="https://www.xtplayer.cn/kubernetes/rancher-k8s-resource-management/"><link rel="preconnect" href="https//cdn.jsdelivr.net"><link rel="preconnect" href="https//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="https//hm.baidu.com"><link rel="preconnect" href="https//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet preload" as="font" href="/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="/css/snackbar.min.css" media="print" onload='this.media="all"'><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-4244806813321801",enable_page_level_ads:"true"})</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?08d495d3996be233cf4355e47874fd02";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-WDVQSZ43MX"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-WDVQSZ43MX")</script><script>!function(){var c=document.createElement("script");c.src="https://sf1-scmcdn-tos.pstatp.com/goofy/ttzz/push.js?abca38a75f1ee646121b4cdefd4e13ce89e669412cb977bb98b66029e7fc3cfc8d38a1f1352cfca035e8cdfca27dc395b6c71280cecfa54b697791769b400b8d",c.id="ttzz";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(c,e)}(window)</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!1,highlightHeightLimit:!1},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#121212",position:"bottom-center"},source:{jQuery:"/js/jquery.min.js",justifiedGallery:{js:"/js/jquery.justifiedGallery.min.js",css:"/css/justifiedGallery.min.css"},fancybox:{js:"/js/jquery.fancybox.min.js",css:"/css/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!0}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2021-01-30 17:29:34"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)});const t=saveToLocal.get("aside-status");void 0!==t&&("hide"===t?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"))})(window)</script><style>#toggle-sidebar{left:100px}</style><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="IT老男孩" type="application/atom+xml"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpeg" onerror='onerror=null,src="/img/friend_404.gif"' width="110px" height="110px" alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">132</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">96</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">40</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/ad/"><i class="fa-fw fas fa-heart"></i> <span>广告</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">IT老男孩</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page" href="/ad/"><i class="fa-fw fas fa-heart"></i> <span>广告</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div class="ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4244806813321801" data-ad-slot="4594418186" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div id="post-info"><h1 class="post-title">Rancher k8s 资源管理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表:</span> <time class="post-meta-date-created" datetime="2020-09-17T05:12:40.000Z" title="发表 2020-09-17 13:12:40">2020-09-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新:</span> <time class="post-meta-date-updated" datetime="2021-01-30T09:29:34.000Z" title="更新 2021-01-30 17:29:34">2021-01-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><span class="post-meta-label">分类:</span> <a class="post-meta-categories" href="/categories/kubernetes/">kubernetes</a></span><span class="post-meta tags"><span class="post-meta-separator">|</span><i class="fas fa-tag post-meta-icon"></i><span class="post-meta-label">标签:</span> <a class="post-meta-tags" href="/tags/cgroup/">cgroup</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span> <span class="word-count">9.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span> <span>41分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Rancher k8s 资源管理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="cgroup-简介"><a href="#cgroup-简介" class="headerlink" title="cgroup 简介"></a>cgroup 简介</h2><p>控制群组 (<em>control group</em>)(简称<em>cgroup</em>) 是 Linux kernel 的一项功能。从使用的角度看，cgroup 是一个目录树结构，目录中可以创建多层子目录，这些目录称为<code>**cgroup 目录**</code>。在一些场景中为了体现层级关系，还会称为<code>**cgroup 子目录**</code>。</p><p>通过 cgroup 可对 CPU 时间片、系统内存、磁盘 IO、网络带宽等资源进行精细化控制，以便硬件资源可以在应用程序和用户间智能分配，从而增加整体效率。</p><p>通过将 cgroup 层级与 <strong>systemd</strong> 单位树绑定，可以把资源管理设置从进程级别转换至应用程序级别。因此，可以使用<strong>systemctl</strong>指令或通过修改 <strong>systemd</strong> 服务配置文件来管理系统资源。更多关于 systemd 相关配置请查阅附件文档。</p><h3 id="Linux-Kernel-的-cgroup-资源管控器"><a href="#Linux-Kernel-的-cgroup-资源管控器" class="headerlink" title="Linux Kernel 的 cgroup 资源管控器"></a>Linux Kernel 的 cgroup 资源管控器</h3><p>cgroup 资源管控器也称为 cgroup 子系统，代表一种单一资源：如 CPU 时间片或者内存。</p><p>Linux kernel 提供一系列资源管控器，由 <strong>systemd</strong> 自动挂载。如需了解目前已挂载的资源管控器列表，可通过查看文件: <strong>&#x2F;proc&#x2F;cgroups</strong>，或使用 <strong>lssubsys</strong> 工具查看。</p><p>在 <code>centos7+、Redhat7+、Ubuntu16+</code> 等 以 systemd 作为进程初始化工具的系统中，默认 cgroup 由 <strong>systemd</strong> 自动挂载到 <code>/sys/fs/cgroup</code> 目录下。在 <code>/sys/fs/cgroup</code> 目录下将自动创建以下 cgroup 子系统：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@alihost01:/sys/fs/cgroup<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 15 root root 380 Jun 21 14:45 ./</span><br><span class="line">drwxr-xr-x 11 root root   0 Jul  1 18:07 ../</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 blkio/</span><br><span class="line">lrwxrwxrwx  1 root root  11 Jun 21 14:45 cpu -&gt; cpu,cpuacct/</span><br><span class="line">lrwxrwxrwx  1 root root  11 Jun 21 14:45 cpuacct -&gt; cpu,cpuacct/</span><br><span class="line">drwxr-xr-x  2 root root  40 Jun 21 14:45 cpuacct,cpu/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 cpu,cpuacct/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 cpuset/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 devices/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 freezer/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 hugetlb/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jun 21 18:14 memory/</span><br><span class="line">lrwxrwxrwx  1 root root  16 Jun 21 14:45 net_cls -&gt; net_cls,net_prio/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 net_cls,net_prio/</span><br><span class="line">lrwxrwxrwx  1 root root  16 Jun 21 14:45 net_prio -&gt; net_cls,net_prio/</span><br><span class="line">drwxr-xr-x  2 root root  40 Jun 21 14:45 net_prio,net_cls/</span><br><span class="line">dr-xr-xr-x  4 root root   0 Jul  1 18:07 perf_event/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 pids/</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  1 18:07 systemd/</span><br></pre></td></tr></table></figure><p>目前 Linux 支持下面 12 种常用的 cgroup 子系统：</p><ul><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt">cpu</a> (since Linux 2.6.24; CONFIG_cgroup_SCHED)<br>用来限制 cgroup 的 CPU 使用率。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpuacct.txt">cpuacct</a> (since Linux 2.6.24; CONFIG_cgroup_CPUACCT)<br>统计 cgroup 的 CPU 的使用率。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt">cpuset</a> (since Linux 2.6.24; CONFIG_CPUSETS)<br>绑定 cgroup 到指定 CPUs 和 NUMA 节点。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt">memory</a> (since Linux 2.6.25; CONFIG_MEMCG)<br>统计和限制 cgroup 的内存的使用率，包括 process memory, kernel memory, 和 swap。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt">devices</a> (since Linux 2.6.26; CONFIG_cgroup_DEVICE)<br>限制 cgroup 创建(mknod)和访问设备的权限。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/freezer-subsystem.txt">freezer</a> (since Linux 2.6.28; CONFIG_cgroup_FREEZER)<br>suspend 和 restore 一个 cgroup 中的所有进程。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/net_cls.txt">net_cls</a> (since Linux 2.6.29; CONFIG_cgroup_NET_CLASSID)<br>将一个 cgroup 中进程创建的所有网络包加上一个 classid 标记，用于<a target="_blank" rel="noopener external nofollow noreferrer" href="http://man7.org/linux/man-pages/man8/tc.8.html">tc</a>和 iptables。 只对发出去的网络包生效，对收到的网络包不起作用。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/blkio-controller.txt">blkio</a> (since Linux 2.6.33; CONFIG_BLK_cgroup)<br>限制 cgroup 访问块设备的 IO 速度。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/perf-record.txt">perf_event</a> (since Linux 2.6.39; CONFIG_cgroup_PERF)<br>对 cgroup 进行性能监控</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/net_prio.txt">net_prio</a> (since Linux 3.3; CONFIG_cgroup_NET_PRIO)<br>针对每个网络接口设置 cgroup 的访问优先级。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/hugetlb.txt">hugetlb</a> (since Linux 3.5; CONFIG_cgroup_HUGETLB)<br>限制 cgroup 的 huge pages 的使用量。</li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kernel.org/doc/Documentation/cgroup-v1/pids.txt">pids</a> (since Linux 4.3; CONFIG_cgroup_PIDS)<br>限制一个 cgroup 及其子 cgroup 中的总进程数。</li></ul><blockquote><p><strong>注意：</strong> <code>/sys/fs/cgroup/systemd</code> 目录非 cgroup 子系统，是 systemd 维护的自己使用的的层级结构。</p></blockquote><h3 id="cgroup-层级结构"><a href="#cgroup-层级结构" class="headerlink" title="cgroup 层级结构"></a>cgroup 层级结构</h3><p>以 memory 子系统为例，其他子系统类似。</p><p>进入 <code>/sys/fs/cgroup/memory</code> 目录，可以看到以下内容：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">dr-xr-xr-x  5 root root   0 Jul  2 09:01 ./</span><br><span class="line">drwxr-xr-x 13 root root 340 Jul  2 08:59 ../</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.clone_children</span><br><span class="line">--w--w--w-  1 root root   0 Jul  2 09:01 cgroup.event_control</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.procs</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 cgroup.sane_behavior</span><br><span class="line">drwxr-xr-x  2 root root   0 Jul  2 08:59 init.scope/</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.failcnt</span><br><span class="line">--w-------  1 root root   0 Jul  2 09:01 memory.force_empty</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.failcnt</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.kmem.slabinfo</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.tcp.failcnt</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.tcp.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.tcp.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.kmem.tcp.usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.kmem.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.max_usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.memsw.failcnt</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.memsw.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.memsw.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.memsw.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.move_charge_at_immigrate</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.numa_stat</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.oom_control</span><br><span class="line">----------  1 root root   0 Jul  2 09:01 memory.pressure_level</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.soft_limit_in_bytes</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.stat</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.swappiness</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 memory.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 08:59 memory.use_hierarchy</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 notify_on_release</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 release_agent</span><br><span class="line">drwxr-xr-x 64 root root   0 Jul  2 08:59 system.slice/</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 tasks</span><br><span class="line">drwxr-xr-x  3 root root   0 Jul  2 08:59 user.slice/</span><br></pre></td></tr></table></figure><ul><li><p>根 cgroup</p><p>虽然 <code>/sys/fs/cgroup/memory</code> 属于 <strong>cgroup 子系统</strong>，但它也是当前子系统的 <strong>根 cgroup</strong>，所以在 <strong>&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory</strong> 中可以看到相应的配置文件，并且根 cgroup 不支持资源限制。</p></li><li><p>子 cgroup</p><p>在 <strong>&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory</strong> 目录（<strong>根 cgroup</strong>）中看到的文件夹，比如 <strong>system.slice</strong>，叫做 <strong>子 cgroup</strong>。</p><p>进入 system.slice 目录可以发现，子 cgroup 与根 cgroup 拥有相同的配置文件。如果要限制内存最大使用量，可通过配置子 cgroup 的 <code>memory.limit_in_bytes</code> 进行限制，默认为 <code>-1</code> 不做限制。子 cgroup 中还可以创建子 cgroup，达到资源的更细化控制。</p></li><li><p>创建子 cgroup</p><p>可以在 <code>/sys/fs/cgroup/memory</code> 目录中，通过 <strong>mkdir</strong> 来创建文件夹，从而创建子 cgroup，子 cgroup 中配置文件将会自动生成。<code>通过 mkdir 创建的子 cgroup 是临时的，重启主机后子 cgroup 将会丢失</code>。</p></li><li><p>实践</p><p>不建议对顶级 cgroup 做资源限制，这样会导致其他子 cgroup 资源限制受影响。建议根据应用类型创建不同的子 cgroup，把应用绑定在不同的子 cgroup 中。</p></li><li><p>配置文件说明</p><ul><li><p>cgroup.clone_children<br>这个文件只对 cpuset 子系统有影响，当该文件的内容为 1 时，新创建的 cgroup 将会继承父 cgroup 的配置，即从父 cgroup 里面拷贝配置文件来初始化新 cgroup，可以参考<a target="_blank" rel="noopener external nofollow noreferrer" href="https://lkml.org/lkml/2010/7/29/368">这里</a></p></li><li><p>cgroup.procs<br>当前 cgroup 中的所有进程 PID，可以手动把进程 PID 添加到当前 cgroup.procs 中，以实现进程与 cgroup 绑定。 系统不保证进程 PID 是顺序排列的，且进程 PID 有可能重复</p></li><li><p>cgroup.sane_behavior<br>具体功能不详，可以参考<a target="_blank" rel="noopener external nofollow noreferrer" href="https://lkml.org/lkml/2014/7/2/684">这里</a>。</p></li><li><p>notify_on_release<br>该文件的内容为 1 时，当 cgroup 退出时（不再包含任何进程和子 cgroup），将调用 release_agent 里面配置的命令。新 cgroup 被创建时将默认继承父 cgroup 的这项配置。</p></li><li><p>release_agent<br>里面包含了 cgroup 退出时将会执行的命令，系统调用该命令时会将相应 cgroup 的相对路径当作参数传进去。 注意：这个文件只会存在于 root cgroup 下面，其他 cgroup 里面不会有这个文件。</p></li><li><p>tasks<br>当前 cgroup 中的所有线程 ID，当 PID 被添加到当前的 cgroup.procs 时，会自动把对应的线程添加到当前 tasks 中。系统不保证线程 ID 是顺序排列的。</p><p><strong>更多文件说明可以查看附件文档的附录 A。</strong></p></li></ul></li></ul><h2 id="原生-Docker-容器资源限制"><a href="#原生-Docker-容器资源限制" class="headerlink" title="原生 Docker 容器资源限制"></a>原生 Docker 容器资源限制</h2><blockquote><p><strong>注意：</strong> 以下内容均以 memory 子系统为例</p></blockquote><ol><li><p>通过执行 <code>docker run -tid --memory 1G alpine</code> 命令运行一个容器；</p></li><li><p>docker 在创建容器时，会在每个 cgroup 子系统的根 cgroup 目录下自动创建 <code>docker</code> cgroup 目录。比如：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">dr-xr-xr-x  6 root root   0 Jul  2 09:08 ./</span><br><span class="line">drwxr-xr-x 13 root root 340 Jul  2 08:59 ../</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.clone_children</span><br><span class="line">--w--w--w-  1 root root   0 Jul  2 09:01 cgroup.event_control</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.procs</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 cgroup.sane_behavior</span><br><span class="line">drwxr-xr-x  3 root root   0 Jul  2 09:08 docker/</span><br><span class="line">drwxr-xr-x  2 root root   0 Jul  2 09:01 init.scope/</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.failcnt</span><br><span class="line">--w-------  1 root root   0 Jul  2 09:01 memory.force_empty</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.failcnt</span><br></pre></td></tr></table></figure></li><li><p>容器的组成</p><p>从宿主机角度看，一个运行的完整容器是以进程形式存在。运行一个容器后通过 <code>ps -ef</code> 可以查看进程相互依赖关系：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root       1311   1230  0 09:01 pts/0    00:00:00 -bash</span><br><span class="line">root       1406      1  0 09:08 ?        00:00:06 /usr/bin/dockerd -H fd://</span><br><span class="line">root       1414   1406  0 09:08 ?        00:00:12 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root       2727   1414  0 09:30 ?        00:00:00 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/    47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9 -addr</span><br><span class="line">root       2762   2727  0 09:30 pts/0    00:00:00 /bin/sh</span><br><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>2762 为容器中的进程，其父进程 2727 为 docker-containerd-shim 进程，docker-containerd-shim 由 docker-containerd 管理，其父进程为 1414（docker-containerd）。</p></li><li><p>容器与 cgroup 的绑定关系</p><p>通过执行 <code>cd /sys/fs/cgroup/memory; systemd-cgls</code> 可以查看到主机上所有应用进程与 cgroup 的关系。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">├─memory</span><br><span class="line">│ ├─docker</span><br><span class="line">│ │ └─47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9</span><br><span class="line">│ │   └─2762 /bin/sh</span><br><span class="line">│ ├─system.slice</span><br><span class="line">│ │ ├─mdadm.service</span><br><span class="line">│ │ ├─rsyslog.service</span><br><span class="line">│ │ │ └─920 /usr/sbin/rsyslogd -n</span><br><span class="line">│ │ ├─docker.service</span><br><span class="line">│ │ │ ├─1406 /usr/bin/dockerd -H fd://</span><br><span class="line">│ │ │ ├─1414 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">│ │ │ └─2727 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9 -address /var/run/docker/containerd/docker</span><br><span class="line">│ │ ├─lxcfs.service</span><br><span class="line">│ │ │ └─892 /usr/bin/lxcfs /var/lib/lxcfs/</span><br><span class="line">│ │ └─acpid.service</span><br><span class="line">│ │   └─905 /usr/sbin/acpid</span><br><span class="line">│ └─user.slice</span><br></pre></td></tr></table></figure><p>通过 <code>systemd-cgls</code> 可以发现，docker-containerd-shim 被绑定在 <strong>system.slice cgroup</strong> 中，可以理解为它是属于 dcoker 系统级的进程，而容器中应用进程则绑定到 <strong>docker cgroup</strong> 中。</p></li><li><p>进入 <code>docker</code> cgroup 目录，可以看到以容器 ID 为名创建的 cgroup 目录和其他配置文件；</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 3 root root 0 Jul  2 09:30 ./</span><br><span class="line">dr-xr-xr-x 6 root root 0 Jul  2 09:08 ../</span><br><span class="line">drwxr-xr-x 2 root root 0 Jul  2 09:40 47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9/</span><br><span class="line">-rw-r--r-- 1 root root 0 Jul  2 09:11 cgroup.clone_children</span><br><span class="line">--w--w--w- 1 root root 0 Jul  2 09:11 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root root 0 Jul  2 09:11 cgroup.procs</span><br><span class="line">-rw-r--r-- 1 root root 0 Jul  2 09:11 memory.failcnt</span><br></pre></td></tr></table></figure></li><li><p>在 <code>docker</code> cgroup 目录中，执行 <code>cat memory.limit_in_bytes</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker<span class="comment"># cat memory.limit_in_bytes</span></span><br><span class="line">9223372036854771712</span><br><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>可以看到结果是一个很大的值，表示不受限制。</p></li><li><p>查看内存限制值</p><p>进入 <strong>容器 ID</strong> 命名的 cgroup 目录，执行 <code>cat memory.limit_in_bytes</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9<span class="comment"># cat memory.limit_in_bytes</span></span><br><span class="line">1073741824</span><br><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>因为在启动容器的时候有添加内存限制参数 <code>--memory 1G</code>，所以这里的值正好是 1G。</p></li><li><p>验证当前 cgroup 绑定的进程 PID</p><ul><li><p>验证应用进程 PID</p><p>在当前子 cgroup 目录下，执行 <code>cat cgroup.procs</code></p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9<span class="comment"># cat cgroup.procs</span></span><br><span class="line">2762</span><br><span class="line">root@ubuntu1:/sys/fs/cgroup/memory/docker/47cc04ac55619ec7358123c71b89a5a5ba77dd9e584e8c1af58e4cc12e0f47a9<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>得到的进程 PID 正好是容器中应用进程的 PID，从而验证了第 4 步 <strong>容器与 cgroup 的绑定关系</strong> 中返回的结果。</p><ul><li><p>验证 docker-containerd-shim PID</p><p>多运行几个容器，然后通过 <code>ps -ef</code> 确定 docker-containerd-shim 的 PID 号。</p><p>接着执行 <code>cat /sys/fs/cgroup/memory/system.slice/docker.service/cgroup.procs</code> 查看 PID 号。</p><p>可以发现 docker-containerd-shim 进程 PID 全部被绑定到 docker.service 子 cgroup 中。</p></li></ul></li><li><p><strong>总结</strong></p><ul><li><p>在使用 docker 创建容器时，会自动在每个 <strong>cgroup 子系统</strong> 中创建 <code>docker</code> cgroup 目录，<code>docker</code> cgroup 目录默认不做资源限制。然后会以容器 ID 为名称，在 <code>docker</code> cgroup 目录下创建 <strong>子 cgroup 目录</strong>，假设 <code>docker run</code> 的时候添加了内存限制参数（–memory ），那么会 <strong>自动修改以容器 ID 命名的 cgroup 目录</strong> 下的 <code>memory.limit_in_bytes</code> 文件，这样就实现了对单个容器最大内存使用的限制。</p></li><li><p>因为是以容器 ID 为名称创建的子 cgroup 目录，所以所有的子 cgroup 不会冲突，并且对一个子 cgroup 做资源限制，不会影响其他子 cgroup。<strong>每个容器中的所有进程将会绑定在以当前容器 ID 命名的子 cgroup 组中，容器 docker-containerd-shim 进程 PID 将会统一绑定在 <code>/sys/fs/cgroup/memory/system.slice/docker.service</code></strong> cgroup 组中。</p></li><li><p>根据上面的逻辑，如果想控制所有容器进程内存使用量不超过预期值，那么只需要配置 <strong>docker</strong> cgroup 资源使用量即可。</p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /sys/fs/cgroup/memory/docker</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;10G&#x27;</span> &gt; memory.limit_in_bytes</span><br></pre></td></tr></table></figure></li></ol><h2 id="Kubernets-Pod-资源限制"><a href="#Kubernets-Pod-资源限制" class="headerlink" title="Kubernets Pod 资源限制"></a>Kubernets Pod 资源限制</h2><ol><li><p>kubelet 在创建 Pod 时，如果没有通过参数 <code>--cgroup-root</code>（参数使用后续讲解）指定顶级 cgroup 组，那么会自动在 cgroup 子系统根 cgroup 中创建 <code>kubepods</code> cgroup 目录。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory<span class="comment"># ll</span></span><br><span class="line">total 0</span><br><span class="line">dr-xr-xr-x  7 root root   0 Jul  2 16:29 ./</span><br><span class="line">drwxr-xr-x 15 root root 380 Jul  2 16:29 ../</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.clone_children</span><br><span class="line">--w--w--w-  1 root root   0 Jul  2 09:01 cgroup.event_control</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 cgroup.procs</span><br><span class="line">-r--r--r--  1 root root   0 Jul  2 09:01 cgroup.sane_behavior</span><br><span class="line">drwxr-xr-x  8 root root   0 Jul  2 16:29 docker/</span><br><span class="line">drwxr-xr-x  2 root root   0 Jul  2 09:01 init.scope/</span><br><span class="line">drwxr-xr-x  4 root root   0 Jul  2 16:30 kubepods/</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.failcnt</span><br><span class="line">--w-------  1 root root   0 Jul  2 09:01 memory.force_empty</span><br><span class="line">-rw-r--r--  1 root root   0 Jul  2 09:01 memory.kmem.failcnt</span><br></pre></td></tr></table></figure></li><li><p>与原生 Docker 容器相似，通过执行 <code>cd /sys/fs/cgroup/memory; systemd-cgls</code> 查询 Pod 与 cgroup 绑定关系.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu1:/sys/fs/cgroup/memory<span class="comment"># cd /sys/fs/cgroup/memory; systemd-cgls</span></span><br><span class="line">Working directory /sys/fs/cgroup/memory:</span><br><span class="line">├─docker</span><br><span class="line">│ ├─8f2e3c82eb801f692c889b9d6b84b1a7c245c3d782798ef60e1e23f58e5ed130</span><br><span class="line">│ │ └─5188 /usr/local/bin/etcd --peer-client-cert-auth --client-cert-auth --advertise-client-urls=https://1.1.1.128:2379,https://1.1.1.128:4001 --listen-client-urls=https://0.0.0.0:2379     --trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem --</span><br><span class="line">│ ├─38241e69344a759d5e1c54dc2f74c5d2323d123f896f03fdb80fb971d060ce17</span><br><span class="line">│ │ └─6291 kubelet --serialize-image-pulls=<span class="literal">false</span> --registry-qps=0 --allow-privileged=<span class="literal">true</span> --authentication-token-webhook=<span class="literal">true</span> --read-only-port=0 --cluster-domain=cluster.local     --kube-reserved=cpu=0.25,memory=2000Mi --cni-conf-dir=/etc/cni</span><br><span class="line">│ ├─0ada8f8d47dc91a9e07e4e533bab36f83eabd3d427c89b65be91e7302fbc30a9</span><br><span class="line">│ │ └─kube-proxy</span><br><span class="line">│ │   └─6818 kube-proxy --hostname-override=1.1.1.128 --kubeconfig=/etc/kubernetes/ssl/kubecfg-kube-proxy.yaml --v=2 --healthz-bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16</span><br><span class="line">├─system.slice</span><br><span class="line">│ ├─mdadm.service</span><br><span class="line">│ │ └─975 /sbin/mdadm --monitor --pid-file /run/mdadm/monitor.pid --daemonise --scan --syslog</span><br><span class="line">│ ├─rsyslog.service</span><br><span class="line">│ │ └─920 /usr/sbin/rsyslogd -n</span><br><span class="line">│ ├─docker.service</span><br><span class="line">│ │ ├─3465 /usr/bin/dockerd -H fd://</span><br><span class="line">│ │ ├─3474 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">│ │ ├─5170 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/8f2e3c82eb801f692c889b9d6b84b1a7c245c3d782798ef60e1e23f58e5ed130     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─5414 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/bb8280628aca338887460df574cb947f6045bc31dd8bbf107722aa7534f2c07d     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─5699 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/3d71cb60793053da0456db1882b87290877f477a6b2f4dd58244ed9c53b4a30a     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─5989 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/d58f02925fac52add81d765e2c34ea54ed59ddfc2cacc545e029a790e5344d76     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─6274 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/38241e69344a759d5e1c54dc2f74c5d2323d123f896f03fdb80fb971d060ce17     -address /var/run/docker/containerd/docker-c</span><br><span class="line">│ │ ├─6800 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/0ada8f8d47dc91a9e07e4e533bab36f83eabd3d427c89b65be91e7302fbc30a9     -address /var/run/docker/containerd/docker-c</span><br><span class="line">└─kubepods</span><br><span class="line">  ├─burstable</span><br><span class="line">  │ ├─pod87a169c0-9ca3-11e9-a530-000c29fe6663</span><br><span class="line">  │ │ ├─57dcc6eb699c8a3ffbbe79ff5500a165dd200b7be6faec8f4298c4bea11a00db</span><br><span class="line">  │ │ │ └─8133 /pause</span><br><span class="line">  │ │ ├─082df6b4c1e5bc1778755061a0d9d3fac044c51720bdadd6b1218acb749ce7d0</span><br><span class="line">  │ │ │ └─8459 /kube-dns --domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2</span><br><span class="line">  │ │ ├─2d398ff9b51acf2abca20490c77650236b1b7960d7f8bb3f565d22b53aa45d40</span><br><span class="line">  │ │ │ ├─8555 /dnsmasq-nanny -v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=<span class="literal">true</span> -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1<span class="comment">#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/i</span></span><br><span class="line">  │ │ │ └─8643 /usr/sbin/dnsmasq -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1<span class="comment">#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053</span></span><br><span class="line">  │ │ └─c97d22a0f7d7ae03139f9409ae4e9f81d2a9731c3c2622fdebe1474109d6e7ed</span><br><span class="line">  │ │   └─8620 /sidecar --v=2 --logtostderr --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A</span><br><span class="line">  └─besteffort</span><br><span class="line">    └─pod8b3e50ed-9ca3-11e9-a530-000c29fe6663</span><br><span class="line">      ├─6ee56058fb4fd0a4a4aaa184af867eb51b053f26587e8e5614512f23c518fd3c</span><br><span class="line">      │ └─8942 /metrics-server --kubelet-insecure-tls --kubelet-preferred-address-types=InternalIP --logtostderr</span><br><span class="line">      └─5586cf42f50a251536af28e58e540b4192780063e1f6eacb7c9efbe0e0ca9856</span><br><span class="line">        └─8740 /pause</span><br></pre></td></tr></table></figure></li><li><p>可以确定，Pod 相关的进程会全部绑定到 <strong>kubepods cgroup</strong> 组中。</p></li><li><p>kubepods cgroup 又分为两个子 cgroup：<strong>burstable 和 besteffort。</strong></p></li></ol><p>根据 Pod 配置的资源限制参数的不同，将自动将 Pod 中的进程绑定到不同的子 cgroup 中（在 QoS 服务质量管理部分将说明 Pod 绑定子 cgroup 的逻辑）。</p><p>下文中说到的 K8S 集群资源预留，就是通过限制 <strong>docker cgroup</strong> 和 <strong>kubepods cgroup</strong> 的资源来达到整体的资源平衡。</p><h2 id="K8S-集群资源预留"><a href="#K8S-集群资源预留" class="headerlink" title="K8S 集群资源预留"></a>K8S 集群资源预留</h2><p>为了保证节点可以正常稳定运行，需要对节点资源进行合理的功能性划分与限制。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">      node-capacity (节点总资源)</span><br><span class="line">--------------------------------------------</span><br><span class="line">|     kube-reserved (kube 组件预留资源)       |</span><br><span class="line">|-----------------------------------------|</span><br><span class="line">|     system-reserved (系统服务预留资源）     |</span><br><span class="line">|-----------------------------------------|</span><br><span class="line">|     eviction-threshold (驱逐阈值)         |</span><br><span class="line">|-----------------------------------------|</span><br><span class="line">|      allocatable (Pod 可分配)              |</span><br><span class="line">--------------------------------------------</span><br></pre></td></tr></table></figure><p>根据以上表格，可以大致把节点总的资源划分为四小块。</p><ul><li><p><strong>Node-Capacity</strong></p><p>节点总的资源。</p></li><li><p><strong>Kube-Reserved</strong></p><p>给 k8s 系统组件预留的资源（包括 kubelet、kube-apiserver、kube-scheduler 等）。</p></li><li><p><strong>System-Reserved</strong></p><p>给 Linux 系统进程（kernel、sshd、Dockerd 等）预留的资源。</p></li><li><p><strong>Eviction-Threshold</strong></p><p>硬驱逐阈值，当节点可用内存值低于此值时，kubelet 会进行 Pod 的驱逐。</p></li><li><p><strong>Allocatable</strong></p><p>真正可供节点上 Pod 使用的资源总容量，<code>kube-scheduler</code> 调度 Pod 时参考此值（kubectl describe node 可查看），节点上所有 Pods 的资源请求值（request）不超过 Allocatable。</p><p>可通过一个公式计算可供 Pod 使用资源总量：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Allocatable] = [Node-Capacity] - [Kube-Reserved] - [System-Reserved] - [Eviction-Threshold]</span><br></pre></td></tr></table></figure><p>从公式可以看出，如果不设置 <code>kube-reserved、system-reserved、Hard-Eviction-Threshold</code>，节点上可以让 Pod 使用的资源总量等于节点的总资源量。如果不做资源划分与限制，Pod 与宿主机系统进程以及 k8s 系统组件争抢资源，导致主机资源耗尽出现异常，例如常见的 Docker 运行卡顿、ssh 无法连接、K8S 节点未就绪 (NotReady)。</p></li></ul><h3 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h3><p>kubelet 的启动参数中涉及资源预留的主要有以下几个：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--cgroups-per-qos</span><br><span class="line">--cgroup-driver</span><br><span class="line">--cgroup-root</span><br><span class="line">--enforce-node-allocatable</span><br><span class="line">--kube-reserved</span><br><span class="line">--kube-reserved-cgroup</span><br><span class="line">--system-reserved</span><br><span class="line">--system-reserved-cgroup</span><br><span class="line">--eviction-hard</span><br><span class="line">--eviction-soft</span><br><span class="line">--eviction-soft-grace-period</span><br><span class="line">--eviction-max-pod-grace-period</span><br><span class="line">--eviction-pressure-transition-period</span><br></pre></td></tr></table></figure><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li><p><code>--cgroups-per-qos</code></p><p>默认开启（true）</p><p>开启这个参数后，所有 Pod 的 cgroup 都将挂载到 kubelet 管理的 cgroup 目录下。要想启用节点资源限制，必须开启此参数。</p></li><li><p><code>--cgroup-driver</code></p><p>指定 kubelet 使用的 cgroup driver，默认 <code>cgroupfs</code>，可以选择 <code>systemd</code>，这个值需要与 Docker Runtime 所使用的 cgroup Driver 保持一致。rke1 创建的集群，因为是以容器运行的 kubelet 无法调用 <code>systemd</code> ，所以这个值一定要为 <code>cgroupfs</code>。</p></li><li><p><code>--cgroup-root</code></p><p>指定 Pod 使用的顶级 cgroup，默认为空，即把 Pod cgroup 挂载到根 cgroup 下，建议默认为空。这个 cgroup 组就是前面说到的 <strong>kubepods</strong> 组，默认 kubelet 会自动创建。如果不想使用默认的 cgroup，则需要先手动创建 cgroup，不然 kubelet 无法启动。</p></li><li><p><code>--kube-reserved</code></p><p>为 kube 系统组件预留的资源值，这个值只是用于调度计算，并不是实际限制。</p><p>示例配置：<code>--kube-reserved=cpu=1,memory=1Gi,ephemeral-storage=10Gi</code>。</p></li><li><p><code>--kube-reserved-cgroup</code></p><p>用于 kube 系统组件资源限制的 cgroup 组，如果要对 kube 系统组件做资源限制则需要配置这个 cgroup 组。rke 集群环境中，K8S 系统核心组件均以原生 docker 容器运行，那么其绑定的 cgroup 组为 <code>docker</code>。所以，如果是 rancher 创建的集群或者 RKE 创建的集群，这个参数需要配置为 <code>/docker</code></p><blockquote><p>注意，这里指定的 cgroup 及其子系统需要预先创建好，kubelet 不会自动创建。如果配置为 <code>/docker</code>，docker 已经自动创建 <code>docker cgroup</code>，则不需要再手动创建。</p></blockquote></li><li><p><code>--system-reserved</code></p><p>为宿主机系统组件预留的资源值，这个值只是用于调度计算，并不是实际限制。</p><p>示例配置：<code>--system-reserved=cpu=1,memory=1Gi,ephemeral-storage=10Gi</code>。</p></li><li><p><code>--system-reserved-cgroup</code></p><p>用于宿主机系统组件资源限制的 cgroup 组，如果要对宿主机系统组件做资源限制则需要配置这个 cgroup 组。建议不配置这个参数，它会使用默认的 cgroup 组。</p><blockquote><p>注意，这里指定的 cgroup 及其子系统需要预先创建好，kubelet 不会自动创建。</p></blockquote></li><li><p><code>--enforce-node-allocatable</code></p><p>这个参数可以理解为资源限制的开关。</p><p>前面说到的 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 仅用于调度计算，当配置了这两个参数后，也就告诉 <strong>调度器</strong> kube 系统组件和宿主机系统服务已经预留了一部分资源，<strong>调度器</strong> 会根据 <code>Allocatable</code> 计算公式计算出可供 Pod 调度的实际资源值。</p><p>但在未做 资源限制 的情况下，Pod 实际使用的资源是可以超过 Pod 可调度的资源值。如果要保证 Pod 实际使用不会超过 <code>Allocatable</code> 计算的实际可调度的资源，则需要通过 <code>--enforce-node-allocatable</code> 开启资源限制功能。</p><p><code>--enforce-node-allocatable</code> 支持三种类型进程的资源限制：<code>pods</code>，<code>kube-reserved</code>，<code>system-reserve</code>。</p><p>这三种类型可以同时选择或者只选择其中一种或者多种。资源限制 功能通过宿主机的 cgroup 来实现，不管选择哪一种类型，都需要指定对应的 cgroup 组，并且 cgroup 组需要预先创建好，kubelet 不会自动创建，如果配置的 cgroup 组不存在，则 kubelet 启动会报错。</p><p>假设设置 <code>--enforce-node-allocatable=pod,kube-reserved,system-reserved</code>，参数配置好之后 kubelet 将会把 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 配置的预留值写入 <code>--kube-reserved-cgroup</code> 和 <code>--system-reserved-cgroup</code> 对应 cgroup 组的 <code>memory.limit_in_bytes</code> 文件中。Pod 进程对应的 cgroup 组默认为 <code>kubepods</code>，执行 <code>cat /sys/fs/cgroup/memory/kubepods/memory.limit_in_bytes</code> 可以发现限制的值正好为 <code>node-capacity - kube-reserved - system-reserved</code>。</p><p>根据以上配置就把整个节点资源精确划分为三部分来使用。但是实际应用中发现，如果设置 <code>kube-reserved</code> 和 <code>system-reserve</code> 的值较小，当集群负载上去之后因为资源被限制导致 K8S 基础组件运行出现异常。如果 <code>kube-reserved</code> 和 <code>system-reserve</code> 设置的值较大，相应的 Pod 能使用的资源又会较少，而 K8S 系统组件也不会一直使用那么多资源，从而造成不必要的资源浪费。</p><p>在实际应用中，建议只限制 Pod 的资源，即配置 <code>--enforce-node-allocatable=pod</code>。这样就不会把 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 配置的预留值写入 <code>--kube-reserved-cgroup</code> 和 <code>--system-reserved-cgroup</code> 对应 cgroup 组的 <code>memory.limit_in_bytes</code> 文件中。</p></li><li><p><code>--eviction-soft</code></p><p>软驱逐阈值。</p><p>为了避免资源压力导致系统不稳定，当 <code>节点总资源 - kube 系统组件预留 - 宿主机系统服务预留 - Pod 实际使用资源</code> 小于软驱逐阈值的时候，kubelet 触发驱逐 Pod 的信号。</p><ul><li><p><code>--eviction-soft-grace-period</code></p><p>超过软驱逐阈值时并不会立即执行驱逐，它会等待 <code>--eviction-soft-grace-period</code> 配置的时间。在这段时间内，kubelet 每 <code>10s</code> 会重新获取监控数据，如果最后一次获取的数据仍然触发了驱逐阈值，最后才会执行 Pod 驱逐。</p></li><li><p><code>--eviction-max-pod-grace-period</code></p><p>强制驱逐 Pod 宽限期。</p><p>驱逐 Pod 时会先发送 <code>SIGTERM</code> 信号给 Pod，然后 Pod 再发送 SIGTERM 信号给容器并等待容器停止运行，默认等待 <code>30s</code>。如果在这段时间内容器没有退出，则 kubelet 会发送 <code>SIGKILL</code> 信号强制删除 Pod，通过 <code>--eviction-max-pod-grace-period</code> 可以指定 Pod 终止的宽限时间。我们也可以通过 <code>pod.Spec.TerminationGracePeriodSeconds</code> 配置 Pod 终止的宽限时间，Rancher 部署的应用默认为 <code>30S</code>。如果配置了 <code>pod.Spec.TerminationGracePeriodSeconds</code> 和 <code>--eviction-max-pod-grace-period</code>，将会取两者最小值作为 Pod 最终终止时间。</p></li></ul></li><li><p><code>--eviction-hard</code></p><p>硬驱逐阈值。</p><p>硬驱逐阈值与软驱逐阈值类似，硬驱逐阈值没有缓冲时间，当 <code>节点总资源 - kube 系统组件预留 - 宿主机系统服务预留 - Pod 实际使用资源</code> 小于硬驱逐阈值的时候将会立即执行驱逐，没有等待时间，强制执行 KILL Pod。</p><p>(Pods 驱逐顺序下文会继续说明)</p></li></ul><h2 id="Pod-QoS-服务质量管理"><a href="#Pod-QoS-服务质量管理" class="headerlink" title="Pod QoS 服务质量管理"></a>Pod QoS 服务质量管理</h2><p>QoS 的英文全称为 <code>Quality of Service</code> ,中文名为”服务质量”。QOS 实现资源有效调度和分配，从而提高资源利用率。<code>kubernetes</code> 针对不同服务的预期资源要求，通过 QoS（Quality of Service）来对 Pod 进行服务质量管理。</p><p>对于 Pod 来说，服务质量体现在两个指标上：一个指标是 CPU，另一个指标是内存。</p><p>如果未对资源进行限制，一些以 Pod 运行的关键服务进程，可能因为内存资源紧张触发 OOM 而被系统 kill 掉，或者被限制 CPU 使用导致进程被暂停。在 kubernetes 中，每个 Pod 都有个 QoS 标记，通过这个 Qos 标记来对 Pod 进行服务质量管理。在实际运行过程中，当节点资源紧张的时候，kubernetes 根据 Pod 具有的不同 QoS 标记，采取不同的处理策略。</p><blockquote><p><strong>已知问题</strong>: QOS 目前不支持 swap，所有 QoS 策略基于 swap 禁止的基础上。</p></blockquote><h3 id="QOS-级别"><a href="#QOS-级别" class="headerlink" title="QOS 级别"></a>QOS 级别</h3><table><thead><tr><th align="left">QoS 级别</th><th>QoS 介绍</th></tr></thead><tbody><tr><td align="left">BestEffort</td><td>Pod 中的所有容器都没有指定 CPU 和内存的 requests 和 limits，那么这个 Pod 的 QoS 就是 BestEffort 级别</td></tr><tr><td align="left">Burstable</td><td>Pod 中只要有一个容器，这个容器 requests 和 limits 的设置同其他容器设置的不一致，那么这个 Pod 的 QoS 就是 Burstable 级别</td></tr><tr><td align="left">Guaranteed</td><td>Pod 中所有容器都必须统一设置了 limits，并且设置参数都一致，如果有一个容器要设置 requests，那么所有容器都要设置，并设置参数同 limits 一致，那么这个 Pod 的 QoS 就是 Guaranteed 级别</td></tr></tbody></table><h3 id="资源回收策略"><a href="#资源回收策略" class="headerlink" title="资源回收策略"></a>资源回收策略</h3><p>当 kubernetes 集群中某个节点上可用资源比较小时，kubernetes 提供了资源回收策略保证被调度到该节点 pod 服务正常运行。当节点上的内存或者 CPU 资源耗尽时，可能会造成该节点上正在运行的 pod 服务不稳定。Kubernetes 通过 kubelet 来进行回收策略控制，保证节点上 pod 在节点资源比较小时可以稳定运行。</p><ol><li><p>可压缩资源：CPU</p><p>当 Pod 使用的 CPU 超过设置的 <code>limits</code> 值，Pod 中进程使用 CPU 会被限制，但不会被 kill。</p></li><li><p>不可压缩资源：memory、storage</p><p>Kubernetes 通过 cgroup 设置 Pod QoS 级别，当资源不足时先 kill 优先级低的 Pod，在实际使用过程中，通过 OOM 分数值来实现，OOM 分数值从 0-1000。</p><p><strong>OOM 分数值根据 <code>OOM_ADJ</code> 参数计算得出：</strong></p><table><thead><tr><th>Name</th><th>OOM_ADJ</th></tr></thead><tbody><tr><td>sshd 等系统进程（sshd／dmevented &#x2F; systemd-udevd）</td><td>-1000</td></tr><tr><td>K8S 管理进程（kubelet&#x2F;docker&#x2F; journalctl）</td><td>-999</td></tr><tr><td>Guaranteed Pod</td><td>-998</td></tr><tr><td>其它进程（内核 init 进程等）</td><td>0</td></tr><tr><td>Burstable Pod</td><td>min(max(2, 1000 –<br>(1000 * memoryRequestBytes) &#x2F; machineMemoryCapacityBytes), 999)</td></tr><tr><td>BestEffort Pod</td><td>1000</td></tr></tbody></table><p>OOM_ADJ 参数值越大，计算出来 OOM 分数越高，表明该 Pod 优先级就越低，当出现资源竞争时会越早被 kill 掉。对于 OOM_ADJ 参数是 <code>-1000</code> 的，表示永远不会因为 OOM 而被 kill 掉。</p></li><li><p>QoS Pods 驱逐顺序</p><p>如果节点资源不足要驱逐 Pod 或 OOM Kill 进程，将按以下顺序进行驱逐：</p><ul><li>Best-Effort 类型：该类型 Pods 会最先被驱逐或者被 Kill；</li><li>Burstable 类型：在没有 Best-Effort Pod 可以被驱逐时，该类型 Pods 会被驱逐或者 kill 掉(其中较大预留但资源使用较少的 Pod 会最后被驱逐或者 Kill)。</li><li>Guaranteed 类型：系统用完了全部内存、且没有 Burstable 与 Best-Effort container 可以被 kill，该类型的 Pods 会被 kill 掉。</li></ul></li></ol><blockquote><p>注：如果 Pod 进程因使用超过 limites 值而非 Node 资源紧张导致的 Kill，系统倾向于在原节点上重启该 Container，或在原节点或者其他节点重新创建一个 Pod。</p></blockquote><h2 id="Pod-优先级"><a href="#Pod-优先级" class="headerlink" title="Pod 优先级"></a>Pod 优先级</h2><p>RKE-Kubernets 集群核心组件以原生 docker 容器运行，因为主机资源固定，那么可以通过<strong>kubepods cgroup</strong>限制应用 Pod 进程使用的资源最大量，从而保证 RKE-Kubernets 集群核心组件和宿主机系统服务不受资源不足的影响。</p><p>但是有一些 Kubernets 系统组件，比如 DNS，它们也是以 Pod 方式运行并绑定在<strong>kubepods cgroup</strong>中。如果其他应用 Pod 进程使用了<strong>kubepods cgroup</strong>限制的最大内存资源，将会触发系统 <code>OOM</code> 或者因为资源紧张导致服务运行不正常。</p><p><strong>根据 <code>Pod QoS 服务质量</code> 的特性，在节点资源不足时，会先驱逐优先级最低的 Pod。因此，为了保证 Kubernets 系统组件的正常运行防止被驱逐，需要提升 Kubernets 系统组件的优先级。</strong></p><p>Kubernets 具有提高 Pod 优先级的功能，从 1.8 到 1.10 版本，默认没有开启 Pod 优先级和抢占。为了启用该功能，需要在 API server 和 scheduler 的启动参数中设置：</p><p><code>--feature-gates=PodPriority=true</code></p><p>在 API server 中还需要设置如下启动参数：</p><p><code>--runtime-config=scheduling.k8s.io/v1alpha1=true</code></p><p>Pod 优先级指明 pod 的相对重要程度。在 1.9 之前的版本中，如果 pod 因为资源问题无法调度，则 kubernetes 尝试抢占低优先级 pod 资源，将它们排挤掉，为高优先级 pod 提供运行条件。</p><p>在 1.9 及之后的版本中，pod 优先级会影响 pod 的调度顺序及当节点资源不足时的驱逐顺序。即调度时优先部署高优先级 pod，当节点资源不足时先行驱逐低优先级 pod。</p><p>在 1.11 之前的版本中，pod 优先级是 alpha 特性，在 1.11 版本中变成 beta 特性，并保证在后续版本继续支持。alpha 版本中默认禁止，需要明确打开，beta 版本默认打开，关系如下表：</p><table><thead><tr><th>Kubernetes Version</th><th>Priority and Preemption State</th><th>默认启用</th></tr></thead><tbody><tr><td>1.8</td><td>alpha</td><td>no</td></tr><tr><td>1.9</td><td>alpha</td><td>no</td></tr><tr><td>1.10</td><td>alpha</td><td>no</td></tr><tr><td>1.11</td><td>beta</td><td>yes</td></tr><tr><td>1.14</td><td>stable</td><td>yes</td></tr></tbody></table><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="docker-service-配置"><a href="#docker-service-配置" class="headerlink" title="docker.service 配置"></a>docker.service 配置</h3><p>对于 CentOS 系统，docker.service 默认位于 <code>/usr/lib/systemd/system/docker.service</code>；</p><p>对于 Ubuntu 系统，docker.service 默认位于 <code>/lib/systemd/system/docker.service</code>。</p><p>编辑 <code>docker.service</code>，添加以下参数。</p><ul><li><p>防止 docker 服务被 OOM KILL</p><p>docker 服务属于整个容器平台的核心基础服务。在宿主机系统内存不足时会触发 OOM KILL，docker 服务不是系统服务，因此 docker 服务进程很可能会被系统 KILL。为了防止 docker 进程被 KILL，可以在 <code>docker.service</code> 中配置 <code>OOMScoreAdjust=-1000</code> 以禁止被 OOM KILL。</p></li><li><p>防止 docker 服务内存溢出</p><p>docker 服务有时候出现异常，会出现占用很多内存资源的情况。为了防止 docker 服务占用整个节点资源，需要对服务做内存限制。在 docker.service 中添加 <code>MemoryLimit=xxG</code> 以限制 docker 服务使用最大内存。</p></li><li><p>开启 iptables 转发链</p><p>因为目前是通过 iptables 进行转发通信，而 iptables FORWARD 链默认是丢弃模式(Chain FORWARD (policy DROP)。为了保证通信正常，在启动 docker 前自动把 <code>iptables FORWARD</code> 链打开。</p><p><code>ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT</code> (centos)</p><p><code>ExecStartPost=/sbin/iptables -P FORWARD ACCEPT</code> (ubuntu)</p></li><li><p>docker 推荐配置</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">mkdir -p /etc/docker/</span><br><span class="line">touch /etc/docker/daemon.json</span><br><span class="line"></span><br><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;log-driver&quot;</span><span class="punctuation">:</span> <span class="string">&quot;json-file&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;log-opts&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;max-size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;100m&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;max-file&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;oom-score-adjust&quot;</span><span class="punctuation">:</span> <span class="number">-1000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;max-concurrent-downloads&quot;</span><span class="punctuation">:</span> <span class="number">10</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;max-concurrent-uploads&quot;</span><span class="punctuation">:</span> <span class="number">10</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;registry-mirrors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;https://7bezldxe.mirror.aliyuncs.com&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;storage-driver&quot;</span><span class="punctuation">:</span> <span class="string">&quot;overlay2&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;storage-opts&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;overlay2.override_kernel_check=true&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h3 id="RKE-配置-Kubernetes-集群资源预留"><a href="#RKE-配置-Kubernetes-集群资源预留" class="headerlink" title="RKE 配置 Kubernetes 集群资源预留"></a>RKE 配置 Kubernetes 集群资源预留</h3><p>rke 参考配置文件，rke 版本大于等于 <code>v0.2.4</code></p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">address:</span> <span class="string">&lt;节点</span> <span class="string">IP&gt;</span></span><br><span class="line">      <span class="attr">user:</span> <span class="string">&lt;user&gt;</span></span><br><span class="line">      <span class="attr">role:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">controlplane</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">etcd</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">worker</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ignore_docker_version:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">ssh_key_path:</span> <span class="string">&lt;修改为实际路径&gt;</span></span><br><span class="line"><span class="attr">ssh_agent_auth:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#private_registries:</span></span><br><span class="line"><span class="comment">#    - url: registry.cn-shanghai.aliyuncs.com</span></span><br><span class="line"><span class="comment">##      user:</span></span><br><span class="line"><span class="comment">##      password:</span></span><br><span class="line"><span class="comment">#      is_default: true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">cluster_name:</span> <span class="string">demo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">    <span class="attr">etcd:</span></span><br><span class="line">      <span class="comment">## rke 版本大于等于 0.2.x 或 rancher 版本大于等于 2.2.0 时使用</span></span><br><span class="line">      <span class="attr">backup_config:</span></span><br><span class="line">        <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">interval_hours:</span> <span class="number">12</span></span><br><span class="line">        <span class="attr">retention:</span> <span class="number">6</span></span><br><span class="line">       <span class="attr">quota-backend-bytes:</span> <span class="string">&#x27;4294967296&#x27;</span></span><br><span class="line">       <span class="attr">auto-compaction-retention:</span> <span class="number">240</span> <span class="comment">#(单位小时)</span></span><br><span class="line">    <span class="attr">kube-api:</span></span><br><span class="line">      <span class="attr">service_cluster_ip_range:</span> <span class="number">10.43</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">      <span class="attr">service_node_port_range:</span> <span class="number">30000</span><span class="number">-32767</span></span><br><span class="line">      <span class="attr">pod_security_policy:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">extra_args:</span></span><br><span class="line">        <span class="attr">audit-log-path:</span> <span class="string">&quot;-&quot;</span></span><br><span class="line">        <span class="attr">delete-collection-workers:</span> <span class="number">3</span></span><br><span class="line">        <span class="attr">v:</span> <span class="number">4</span></span><br><span class="line">    <span class="attr">kube-controller:</span></span><br><span class="line">      <span class="attr">cluster_cidr:</span> <span class="number">10.42</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">      <span class="attr">service_cluster_ip_range:</span> <span class="number">10.43</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">      <span class="attr">extra_args:</span></span><br><span class="line">        <span class="comment">## 控制器定时与节点通信以检查通信是否正常，周期默认 5s</span></span><br><span class="line">        <span class="attr">node-monitor-period:</span> <span class="string">&#x27;5s&#x27;</span></span><br><span class="line">        <span class="comment">## 当节点通信失败后，再等一段时间 kubernetes 判定节点为 notready 状态。</span></span><br><span class="line">        <span class="comment">## 这个时间段必须是 kubelet 的 nodeStatusUpdateFrequency(默认 10s)的 N 倍，</span></span><br><span class="line">        <span class="comment">## 其中 N 表示允许 kubelet 同步节点状态的重试次数，默认 40s。</span></span><br><span class="line">        <span class="attr">node-monitor-grace-period:</span> <span class="string">&#x27;20s&#x27;</span></span><br><span class="line">        <span class="comment">## 再持续通信失败一段时间后，kubernetes 判定节点为 unhealthy 状态，默认 1m0s。</span></span><br><span class="line">        <span class="attr">node-startup-grace-period:</span> <span class="string">&#x27;30s&#x27;</span></span><br><span class="line">        <span class="comment">## 再持续失联一段时间，kubernetes 开始迁移失联节点的 Pod，默认 5m0s。</span></span><br><span class="line">        <span class="attr">pod-eviction-timeout:</span> <span class="string">&#x27;1m&#x27;</span></span><br><span class="line">    <span class="attr">kubelet:</span></span><br><span class="line">      <span class="attr">cluster_domain:</span> <span class="string">cluster.local</span></span><br><span class="line">      <span class="attr">cluster_dns_server:</span> <span class="number">10.43</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">      <span class="attr">fail_swap_on:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">extra_args:</span></span><br><span class="line">        <span class="comment">## 修改节点最大 Pod 数量</span></span><br><span class="line">        <span class="attr">max-pods:</span> <span class="string">&quot;250&quot;</span></span><br><span class="line">        <span class="comment">## 密文和配置映射同步时间，默认 1 分钟</span></span><br><span class="line">        <span class="attr">sync-frequency:</span> <span class="string">&#x27;3s&#x27;</span></span><br><span class="line">        <span class="comment">## Kubelet 进程可以打开的文件数（默认 1000000）,根据节点配置情况调整</span></span><br><span class="line">        <span class="attr">max-open-files:</span> <span class="string">&#x27;2000000&#x27;</span></span><br><span class="line">        <span class="comment">## 与 apiserver 会话时的并发数，默认是 10</span></span><br><span class="line">        <span class="attr">kube-api-burst:</span> <span class="string">&#x27;30&#x27;</span></span><br><span class="line">        <span class="comment">## 与 apiserver 会话时的 QPS,默认是 5</span></span><br><span class="line">        <span class="attr">kube-api-qps:</span> <span class="string">&#x27;15&#x27;</span></span><br><span class="line">        <span class="comment">## kubelet 默认一次拉取一个镜像，设置为 false 可以同时拉取多个镜像，</span></span><br><span class="line">        <span class="comment">## 前提是存储驱动要为 overlay2，对应的 Dokcer 也需要增加下载并发数</span></span><br><span class="line">        <span class="attr">serialize-image-pulls:</span> <span class="string">&#x27;false&#x27;</span></span><br><span class="line">        <span class="comment">## 拉取镜像的最大并发数，registry-burst 不能超过 registry-qps ，</span></span><br><span class="line">        <span class="comment">## 仅当 registry-qps 大于 0(零)时生效，(默认 10)。如果 registry-qps 为 0 则不限制(默认 5)。</span></span><br><span class="line">        <span class="attr">registry-burst:</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">        <span class="attr">registry-qps:</span> <span class="string">&#x27;0&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">cgroups-per-qos:</span> <span class="string">&#x27;true&#x27;</span></span><br><span class="line">        <span class="comment"># 这里一定要为 cgroupfs</span></span><br><span class="line">        <span class="attr">cgroup-driver:</span> <span class="string">&#x27;cgroupfs&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">enforce-node-allocatable:</span> <span class="string">&#x27;pods,kube-reserved&#x27;</span> <span class="comment"># &#x27;pods,kube-reserved,system-reserved&#x27;</span></span><br><span class="line">        <span class="attr">system-reserved:</span> <span class="string">&#x27;cpu=1,memory=500Mi&#x27;</span></span><br><span class="line">        <span class="comment"># 根据实际资源调整</span></span><br><span class="line">        <span class="attr">kube-reserved:</span> <span class="string">&#x27;cpu=1,memory=1Gi&#x27;</span></span><br><span class="line">        <span class="comment"># RKE 集群需要设置为 `/docker`</span></span><br><span class="line">        <span class="attr">kube-reserved-cgroup:</span> <span class="string">&#x27;/docker&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置进行 pod 驱逐的阈值，这个参数只支持内存和磁盘。通过--eviction-hard 标志预留一些内存后，当 Allocatable 可用内存降至保留值以下时，kubelet 将会对 pod 进行驱逐。</span></span><br><span class="line">        <span class="comment">## 硬阈值，当值小于 100M 就成触发驱逐</span></span><br><span class="line">        <span class="attr">eviction-hard:</span> <span class="string">&#x27;memory.available&lt;100Mi&#x27;</span></span><br><span class="line">        <span class="comment"># 软阈值，当可用值小于 500Mi，</span></span><br><span class="line">        <span class="attr">eviction-soft:</span> <span class="string">&#x27;memory.available&lt;500Mi&#x27;</span></span><br><span class="line">        <span class="attr">eviction-soft-grace-period:</span> <span class="string">&#x27;memory.available=1m30s&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">eviction-max-pod-grace-period:</span> <span class="string">&#x27;30&#x27;</span></span><br><span class="line">        <span class="attr">eviction-pressure-transition-period:</span> <span class="string">&#x27;30s&#x27;</span></span><br><span class="line"></span><br><span class="line">      <span class="attr">extra_binds:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Currently, only authentication strategy supported is x509.</span></span><br><span class="line"><span class="comment"># You can optionally create additional SANs (hostnames or IPs) to add to</span></span><br><span class="line"><span class="comment">#  the API server PKI certificate.</span></span><br><span class="line"><span class="comment"># This is useful if you want to use a load balancer for the control plane servers.</span></span><br><span class="line"><span class="attr">authentication:</span></span><br><span class="line">    <span class="attr">strategy:</span> <span class="string">&quot;x509|webhook&quot;</span></span><br><span class="line">    <span class="attr">sans:</span></span><br><span class="line">      <span class="comment"># 此处配置备用域名或 IP，当主域名或者 IP 无法访问时，可通过备用域名或 IP 访问</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;192.168.1.100&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;www.test.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Kubernetes 认证模式</span></span><br><span class="line"><span class="comment">## Use `mode: rbac` 启用 RBAC</span></span><br><span class="line"><span class="comment">## Use `mode: none` 禁用 认证</span></span><br><span class="line"><span class="attr">authorization:</span></span><br><span class="line">    <span class="attr">mode:</span> <span class="string">rbac</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add-ons are deployed using kubernetes jobs. RKE will give up on trying to get the job status after this timeout in seconds..</span></span><br><span class="line"><span class="attr">addon_job_timeout:</span> <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有几个网络插件可以选择：`flannel、canal、calico`，Rancher2 默认 canal</span></span><br><span class="line"><span class="attr">network:</span></span><br><span class="line">    <span class="attr">plugin:</span> <span class="string">canal</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">      <span class="attr">canal_iface:</span> <span class="string">eth0</span></span><br><span class="line">      <span class="attr">flannel_backend_type:</span> <span class="string">&quot;vxlan&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 目前只支持 nginx ingress controller</span></span><br><span class="line"><span class="comment">## 可以设置 `provider: none` 来禁用 ingress controller</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ingress:</span></span><br><span class="line">    <span class="attr">provider:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">node_selector:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">ingress</span></span><br><span class="line"><span class="comment"># 配置 dns 上游 dns 服务器</span></span><br><span class="line"><span class="comment">## 可用 rke 版本 v0.2.0</span></span><br><span class="line"><span class="attr">dns:</span></span><br><span class="line">    <span class="attr">provider:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">upstreamnameservers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">114.114</span><span class="number">.114</span><span class="number">.114</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">1.2</span><span class="number">.4</span><span class="number">.8</span></span><br></pre></td></tr></table></figure><h3 id="配置-Pod-优先级"><a href="#配置-Pod-优先级" class="headerlink" title="配置 Pod 优先级"></a>配置 Pod 优先级</h3><p>PriorityClass 是一个不受命名空间约束的对象，它定义了优先级类名与优先级整数值的映射。它的名称通过 PriorityClass 对象 <strong>metadata</strong> 中的 name 字段指定。value 值越大，优先级越高。</p><p>PriorityClass 对象的值可以是小于或者等于 10 亿的 32 位整数值。更大的数值被保留给那些通常不应该取代或者驱逐的关键的系统级 Pod 使用。集群管理员应该为它们想要的每个此类映射创建一个 PriorityClass 对象。</p><p><strong>注意:</strong> 优先级配置需要在集群基础组件配置完成后再执行。</p><ol><li><p><strong>配置 PriorityClass</strong></p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PriorityClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">high-priority-system-pod</span></span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">preemptionPolicy:</span> <span class="string">PreemptLowerPriority</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&quot;This priority class should be used for XYZ service pods only.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">metadata.name：名称</span></span><br><span class="line"><span class="string">value：小于或者等于</span> <span class="number">10</span> <span class="string">亿的</span> <span class="number">32</span> <span class="string">位任意整数值，数字越大优先级越高，超过一亿的数字被系统保留，用于指派给系统组件。</span></span><br><span class="line"><span class="string">globalDefault：是否应用于全局</span> <span class="string">pod</span> <span class="string">策略</span></span><br><span class="line"><span class="string">description：描述信息</span></span><br><span class="line"><span class="attr">preemptionPolicy:</span> <span class="string">抢占功能。设置为</span> <span class="string">Never</span> <span class="string">表示不抢占，默认为</span> <span class="string">PreemptLowerPriority，1.15</span> <span class="string">之后默认禁用抢占功能。</span></span><br></pre></td></tr></table></figure><ul><li>如果升级现有集群启用此功能，那些已经存在系统里面的 Pod 的优先级将会设置为 0。</li><li>此外，将一个<strong>PriorityClass</strong>的 globalDefault 设置为 true，不会改变系统中已经存在的 Pod 的优先级。也就是说，PriorityClass 的值只能用于在 PriorityClass 添加之后创建的那些 Pod 。</li><li>如果您删除一个 PriorityClass，那些使用了该 PriorityClass 的 Pod 将会保持不变，但是，该 PriorityClass 的名称不能在新创建 Pod 时使用。</li></ul><p></p></li><li><p><strong>设置 Pod priority</strong></p><p>在配置 Pod 时，设置其<strong>priorityClass Name</strong>字段为可用 PriorityClass 名称，则在创建 Pod 时由允入控制器将名称转换成对应数字。如果没有找到相应的 PriorityClass，Pod 将会被拒绝创建。示例如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">APP_NS=kube-system</span><br><span class="line"></span><br><span class="line">APP=canal</span><br><span class="line">WORKLOAD_TYPE=daemonsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line">APP=kube-dns</span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line">APP=kube-dns-autoscaler</span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line">APP=metrics-server</span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f -</span><br><span class="line"></span><br><span class="line"><span class="comment">##################</span></span><br><span class="line">APP_NS=cattle-system</span><br><span class="line"></span><br><span class="line">APP=rancher <span class="comment"># 仅 local 集群执行</span></span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f -</span><br><span class="line"></span><br><span class="line">APP=cattle-cluster-agent</span><br><span class="line">WORKLOAD_TYPE=deployments</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f -</span><br><span class="line"></span><br><span class="line">APP=cattle-node-agent</span><br><span class="line">WORKLOAD_TYPE=daemonsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line"><span class="comment">##################</span></span><br><span class="line">APP_NS=ingress-nginx</span><br><span class="line"></span><br><span class="line">APP=nginx-ingress-controller</span><br><span class="line">WORKLOAD_TYPE=daemonsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line"><span class="comment">##################</span></span><br><span class="line"><span class="comment"># 启用集群监控和告警的集群，执行力以下命令</span></span><br><span class="line">APP_NS=cattle-prometheus</span><br><span class="line"></span><br><span class="line">APP=prometheus-cluster-monitoring</span><br><span class="line">WORKLOAD_TYPE=statefulsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br><span class="line"></span><br><span class="line">APP=alertmanager-cluster-alerting</span><br><span class="line">WORKLOAD_TYPE=statefulsets</span><br><span class="line">kubectl -n <span class="variable">$APP_NS</span> get <span class="variable">$WORKLOAD_TYPE</span> <span class="variable">$APP</span> -o json |jq <span class="string">&#x27;.spec.template.spec += &#123;&quot;priorityClassName&quot;: &quot;high-priority-system-pod&quot;&#125;&#x27;</span> | kubectl -n <span class="variable">$APP_NS</span> apply -f - </span><br></pre></td></tr></table></figure></li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined" rel="external nofollow noreferrer">IT老男孩</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://www.xtplayer.cn/kubernetes/rancher-k8s-resource-management/">https://www.xtplayer.cn/kubernetes/rancher-k8s-resource-management/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a rel="noopener noreferrer" href="https://www.xtplayer.cn" target="_blank">IT老男孩</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/cgroup/">cgroup</a></div><div class="post_share"></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/etcd/monitor-etcd-quickly-using-prometheus/"><img class="prev-cover" src="/img/gkihqEjXxJ5UZ1C.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">快速使用 Prometheus 监控 Etcd</div></div></a></div><div class="next-post pull-right"><a href="/kubernetes/fast-migration-pod-when-node-unavailable/"><img class="next-cover" src="/img/gkihqEjXxJ5UZ1C.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">K8S 节点不可用时快速迁移 Pods</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i> <span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/kubernetes/about-kubernetes-1.20/" title="了解 Kubernetes 1.20"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-15</div><div class="title">了解 Kubernetes 1.20</div></div></a></div><div><a href="/kubernetes/api-server-and-etcd-health-state-check/" title="Api Server 和 ETCD 健康状态检查"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-22</div><div class="title">Api Server 和 ETCD 健康状态检查</div></div></a></div><div><a href="/kubernetes/automatic-k8s-deployment-tool/" title="K8S 自动部署工具"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-06</div><div class="title">K8S 自动部署工具</div></div></a></div><div><a href="/kubernetes/autoscaling/" title="Kubernetes 规范中标签的重要性"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-19</div><div class="title">Kubernetes 规范中标签的重要性</div></div></a></div><div><a href="/kubernetes/concepts-and-matters/" title="Kubernetes 概念及其重要性"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-15</div><div class="title">Kubernetes 概念及其重要性</div></div></a></div><div><a href="/kubernetes/controller-manager-and-scheduler-unavailable/" title="记一次 controller manager and scheduler unavailable 问题分析"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-24</div><div class="title">记一次 controller manager and scheduler unavailable 问题分析</div></div></a></div><div><a href="/kubernetes/custom-serviceaccount/" title="自定义服务账户（Service Account）"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-09</div><div class="title">自定义服务账户（Service Account）</div></div></a></div><div><a href="/kubernetes/docker-credentials-secret/" title="记一次镜像仓库凭证相关问题分析"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-23</div><div class="title">记一次镜像仓库凭证相关问题分析</div></div></a></div><div><a href="/kubernetes/fast-migration-pod-when-node-unavailable/" title="K8S 节点不可用时快速迁移 Pods"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-16</div><div class="title">K8S 节点不可用时快速迁移 Pods</div></div></a></div><div><a href="/kubernetes/forces-delete-terminated-namespace/" title="强制删除 Terminating 状态的 namespace"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-01</div><div class="title">强制删除 Terminating 状态的 namespace</div></div></a></div><div><a href="/kubernetes/ingress-configuration-demo/" title="Ingress 常用配置（持续更新）"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-11</div><div class="title">Ingress 常用配置（持续更新）</div></div></a></div><div><a href="/kubernetes/k8s-automatic-elastic-expansion/" title="Kubernetes 自动弹性伸缩"><img class="cover" src="/img/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-02</div><div class="title">Kubernetes 自动弹性伸缩</div></div></a></div></div></div><div class="ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4244806813321801" data-ad-slot="9845679202" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/avatar.jpeg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' width="110px" height="110px" alt="avatar"><div class="author-info__name">IT老男孩</div><div class="author-info__description">IT老男孩 - 原名系统玩家，分享 IT 相关技术文章，分享工作中的最佳实践。</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">132</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">96</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">40</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/rancher"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xiaoluhong" target="_blank" rel="external nofollow noreferrer noopener" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1044281310@qq.com" target="_blank" rel="external nofollow noreferrer noopener" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">我的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录导航</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#cgroup-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">cgroup 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linux-Kernel-%E7%9A%84-cgroup-%E8%B5%84%E6%BA%90%E7%AE%A1%E6%8E%A7%E5%99%A8"><span class="toc-number">1.1.</span> <span class="toc-text">Linux Kernel 的 cgroup 资源管控器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cgroup-%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.</span> <span class="toc-text">cgroup 层级结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%94%9F-Docker-%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6"><span class="toc-number">2.</span> <span class="toc-text">原生 Docker 容器资源限制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kubernets-Pod-%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6"><span class="toc-number">3.</span> <span class="toc-text">Kubernets Pod 资源限制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K8S-%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E9%A2%84%E7%95%99"><span class="toc-number">4.</span> <span class="toc-text">K8S 集群资源预留</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">配置参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-number">4.2.</span> <span class="toc-text">参数说明</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pod-QoS-%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F%E7%AE%A1%E7%90%86"><span class="toc-number">5.</span> <span class="toc-text">Pod QoS 服务质量管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#QOS-%E7%BA%A7%E5%88%AB"><span class="toc-number">5.1.</span> <span class="toc-text">QOS 级别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B5%84%E6%BA%90%E5%9B%9E%E6%94%B6%E7%AD%96%E7%95%A5"><span class="toc-number">5.2.</span> <span class="toc-text">资源回收策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pod-%E4%BC%98%E5%85%88%E7%BA%A7"><span class="toc-number">6.</span> <span class="toc-text">Pod 优先级</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5"><span class="toc-number">7.</span> <span class="toc-text">实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#docker-service-%E9%85%8D%E7%BD%AE"><span class="toc-number">7.1.</span> <span class="toc-text">docker.service 配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RKE-%E9%85%8D%E7%BD%AE-Kubernetes-%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E9%A2%84%E7%95%99"><span class="toc-number">7.2.</span> <span class="toc-text">RKE 配置 Kubernetes 集群资源预留</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-Pod-%E4%BC%98%E5%85%88%E7%BA%A7"><span class="toc-number">7.3.</span> <span class="toc-text">配置 Pod 优先级</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/rancher/waiting-for-node-to-register-either-cluster-is-not-ready-for-registering-or-etcd-and-controlplane-node-have-to-be-registered-first/" title="Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first">Waiting for node to register. Either cluster is not ready for registering or etcd and controlplane node have to be registered first</a><time datetime="2022-09-17T06:33:50.000Z" title="发表 2022-09-17 14:33:50">2022-09-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/mysql/how-to-get-the-sizes-of-the-tables-of-a-mysql-database/" title="如何查看 MySQL 数据库容量大小，表容量大小，索引容量大小？">如何查看 MySQL 数据库容量大小，表容量大小，索引容量大小？</a><time datetime="2022-05-17T08:48:50.000Z" title="发表 2022-05-17 16:48:50">2022-05-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/rancher/either-cluster-is-not-ready-for-registering/" title="node-agent 报错 Either cluster is not ready for registering, cluster is currently provisioning, or etcd, controlplane and worker node have to be registered">node-agent 报错 Either cluster is not ready for registering, cluster is currently provisioning, or etcd, controlplane and worker node have to be registered</a><time datetime="2022-05-14T05:13:09.000Z" title="发表 2022-05-14 13:13:09">2022-05-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/prometheus/prometheus-adapter/" title="Prometheus Adapter 安装">Prometheus Adapter 安装</a><time datetime="2022-04-04T08:26:12.000Z" title="发表 2022-04-04 16:26:12">2022-04-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/prometheus/custom-parameter/" title="自定义集群监控参数">自定义集群监控参数</a><time datetime="2022-03-28T11:10:20.000Z" title="发表 2022-03-28 19:10:20">2022-03-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/rancher/rancher-new-forums/" title="全新论坛启用，Rancher 中文社区迈入新阶段">全新论坛启用，Rancher 中文社区迈入新阶段</a><time datetime="2022-03-25T13:16:03.000Z" title="发表 2022-03-25 21:16:03">2022-03-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/coredns/accelerate-external-domain-resolution/" title="coreDNS 加速外部域名解析">coreDNS 加速外部域名解析</a><time datetime="2021-07-17T05:12:27.000Z" title="发表 2021-07-17 13:12:27">2021-07-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/longhorn/ai-at-the-edge-with-k3s-nvidia-jetson-nano-object-detection-real-time-video-analytics-src/" title="AI at the Edge with K3s and NVIDIA Jetson Nano: Object Detection and Real-Time Video Analytics">AI at the Edge with K3s and NVIDIA Jetson Nano: Object Detection and Real-Time Video Analytics</a><time datetime="2021-04-20T16:22:57.000Z" title="发表 2021-04-21 00:22:57">2021-04-21</time></div></div></div></div><div class="card-widget ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4244806813321801" data-ad-slot="5141213090" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By IT老男孩</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" rel="noopener external nofollow noreferrer" href="https://beian.miit.gov.cn"><img class="icp-icon" width="25px" height="17px" src="/img/icp.png" alt="icp"><span>蜀ICP备20023095号-1</span></a> <a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=51010702002006"><span>川公网安备51010702002006号</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text" aria-label="Search"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/instantpage.min.js" type="module"></script><script src="/js/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><div class="aplayer no-destroy" data-id="5169987169" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listfolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><link rel="stylesheet" href="/css/APlayer.min.css" media="print" onload='this.media="all"'><script src="/js/APlayer.min.js"></script><script src="/js/Meting.min.js"></script><script src="/js/pjax.min.js"></script><script>let pjaxSelectors=["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"];var pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!0,scrollRestoration:!1});document.addEventListener("pjax:complete",(function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach(e=>{const t=document.createElement("script"),o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)}),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof chatBtnFn&&chatBtnFn(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","G-WDVQSZ43MX",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll(),"object"==typeof preloader&&preloader.endLoading()})),document.addEventListener("pjax:send",(function(){if("object"==typeof preloader&&preloader.initLoading(),window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();const e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode")})),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})</script><script async data-pjax src="/js/busuanzi.pure.mini.js"></script><script>!function(){var e=document.createElement("script");e.src="https://sf1-scmcdn-tos.pstatp.com/goofy/ttzz/push.js?abca38a75f1ee646121b4cdefd4e13cef04782c449861f0dbdffcd879911999e0ea61db5c17bca91cb6b39891e5c5ae8de9c557b8186e0f895b49e96a4810ca7",e.id="ttzz";var c=document.getElementsByTagName("script")[0];c.parentNode.insertBefore(e,c)}(window)</script></div></body></html>